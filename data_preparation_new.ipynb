{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0ca15d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üß¨ DRUGREFLECTOR DATA PREPROCESSING - Enhanced Version v2\n",
      "   With intelligent dose filtering methods\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "üî¨ LINCS 2020 Data Loader - Enhanced Version v2\n",
      "================================================================================\n",
      "Data directory: D:\\ÁßëÁ†î\\Models\\drugreflector\\datasets\\LINCS2020\n",
      "Dataset: Expanded CMap LINCS Resource 2020\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "STEP 1: Loading metadata\n",
      "================================================================================\n",
      "üìñ Loading gene information...\n",
      "   File: geneinfo_beta.txt\n",
      "   ‚úì Loaded 12,328 genes\n",
      "   ‚úì Columns: ['gene_id', 'gene_symbol', 'ensembl_id', 'gene_title', 'gene_type', 'src', 'feature_space']\n",
      "   ‚úì Using 'feature_space' column to identify landmarks\n",
      "   ‚úì Landmark genes: 978\n",
      "   ‚úì Expected: 978\n",
      "\n",
      "   Landmark column indices (first 10): [2154 2155 2156 2157 2158 2159 2160 2161 2162 2163]\n",
      "   Sample IDs: ['27336', '79716', '128', '3909', '80746']\n",
      "   Sample symbols: ['KIAA0355', 'AKR7A2', 'SMNDC1', 'ZFP36', 'PGM1']\n",
      "\n",
      "üìñ Loading cell information...\n",
      "   File: cellinfo_beta.txt\n",
      "   ‚úì Loaded 240 cell lines\n",
      "   ‚úì Columns: ['cell_iname', 'cellosaurus_id', 'donor_age', 'donor_age_death', 'donor_disease_age_onset', 'doubling_time', 'growth_medium', 'provider_catalog_id', 'feature_id', 'cell_type']...\n",
      "   ‚úì Unique cell lines (cell_iname): 240\n",
      "\n",
      "   Sample cell lines:\n",
      "     - 1HAE\n",
      "     - AALE\n",
      "     - AG06263_2\n",
      "     - AG06840_A\n",
      "     - AG078N1_1\n",
      "\n",
      "üìñ Loading compound information...\n",
      "   File: compoundinfo_beta.txt\n",
      "   ‚úì Loaded 39,321 compounds\n",
      "   ‚úì Columns: ['pert_id', 'cmap_name', 'target', 'moa', 'canonical_smiles', 'inchi_key', 'compound_aliases']\n",
      "   ‚úì Unique perturbagens: 34419\n",
      "\n",
      "   Sample compounds:\n",
      "     - BRD-A08715367: L-theanine\n",
      "     - BRD-A12237696: L-citrulline\n",
      "     - BRD-A18795974: BRD-A18795974\n",
      "\n",
      "================================================================================\n",
      "STEP 2: Loading Level 4 signatures\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "üìñ Loading Level 4 Signatures\n",
      "================================================================================\n",
      "File: level4_beta_trt_cp_n1805898x12328.gctx\n",
      "\n",
      "üìñ Reading GCTX file: level4_beta_trt_cp_n1805898x12328.gctx\n",
      "   File size: 82.94 GB\n",
      "   ‚ö†Ô∏è  Large file detected. Using memory-optimized loading...\n",
      "üìä Inspecting HDF5 structure...\n",
      "   Available keys: ['0']\n",
      "   ‚úì Matrix shape: (1805898, 12328) (samples √ó genes)\n",
      "\n",
      "üìã Loading metadata...\n",
      "   Reading sample metadata from: /0/META/ROW\n",
      "   Available row fields: ['id']\n",
      "   ‚úì Loaded 1 sample metadata fields\n",
      "   Reading gene metadata from: /0/META/COL\n",
      "   Available col fields: ['id']\n",
      "   ‚úì Loaded 1 gene metadata fields\n",
      "\n",
      "   ‚ö†Ô∏è Detected ROW/COL swap. Correcting...\n",
      "\n",
      "   Sample metadata columns: ['id']...\n",
      "   Gene metadata columns: ['id']\n",
      "\n",
      "üî¨ Filtering to landmark genes...\n",
      "   ‚úì Using 978 landmark features out of 12328 total\n",
      "\n",
      "üéØ Loading data (memory-optimized)...\n",
      "   Reading 978 columns out of 12328...\n",
      "   Loading rows 1,220,000 to 1,805,898... (100.0%)\n",
      "   Finalizing matrix...\n",
      "   ‚úì Final matrix shape: (1805898, 978)\n",
      "   ‚úì Memory usage: 6.58 GB\n",
      "   ‚úì Data type: float32\n",
      "\n",
      "üìñ Loading instance information...\n",
      "   File: instinfo_beta.txt\n",
      "   ‚úì Loaded 3,026,460 instances\n",
      "   ‚úì Columns: ['bead_batch', 'nearest_dose', 'pert_dose', 'pert_dose_unit', 'pert_idose', 'pert_time', 'pert_itime', 'pert_time_unit', 'cell_mfc_name', 'pert_mfc_id']...\n",
      "   ‚úì Using 'sample_id' as join key\n",
      "   üîó Merging GCTX metadata with instinfo on 'sample_id'...\n",
      "\n",
      "================================================================================\n",
      "STEP 3: Preparing training data\n",
      "================================================================================\n",
      "\n",
      "[DEBUG] row_meta shape: (1805898, 30)\n",
      "[DEBUG] columns (first 15): ['sample_id', 'bead_batch', 'nearest_dose', 'pert_dose', 'pert_dose_unit', 'pert_idose', 'pert_time', 'pert_itime', 'pert_time_unit', 'cell_mfc_name', 'pert_mfc_id', 'det_plate', 'det_well', 'rna_plate', 'rna_well']\n",
      "[DEBUG] Example row:\n",
      "sample_id     ABY001_A375_XH_X1_B15:A13\n",
      "pert_id                   BRD-K66175015\n",
      "pert_type                        trt_cp\n",
      "cell_iname                         A375\n",
      "pert_time                          24.0\n",
      "pert_dose                          10.0\n",
      "Name: 0, dtype: object\n",
      "\n",
      "================================================================================\n",
      "üîç DRUGREFLECTOR QUALITY CONTROL PIPELINE - Enhanced v2\n",
      "================================================================================\n",
      "Initial samples: 1,805,898\n",
      "Initial memory: 6.58 GB\n",
      "\n",
      "üìã Available metadata columns:\n",
      "   - sample_id: ABY001_A375_XH_X1_B15:A13\n",
      "   - bead_batch: b15\n",
      "   - nearest_dose: 10.0\n",
      "   - pert_dose: 10.0\n",
      "   - pert_dose_unit: uM\n",
      "   - pert_idose: 10 uM\n",
      "   - pert_time: 24.0\n",
      "   - pert_itime: 24 h\n",
      "   - pert_time_unit: h\n",
      "   - cell_mfc_name: A375\n",
      "   - pert_mfc_id: BRD-K66175015\n",
      "   - det_plate: ABY001_A375_XH_X1_B15\n",
      "   - det_well: A13\n",
      "   - rna_plate: ABY001_A375_XH_X1\n",
      "   - rna_well: A13\n",
      "   ... and 15 more\n",
      "\n",
      "‚úì Using 'cell_iname' as cell line identifier\n",
      "\n",
      "Initial compounds: 34,419\n",
      "\n",
      "================================================================================\n",
      "FILTER 1: Remove DOS compounds (keep trt_cp only)\n",
      "================================================================================\n",
      "   pert_type value counts:\n",
      "     - trt_cp: 1,805,898 samples\n",
      "\n",
      "  ‚úì Keeping only 'trt_cp' perturbations\n",
      "  Removed 0 non-trt_cp observations\n",
      "  Remaining samples: 1,805,898\n",
      "  Remaining compounds: 34,419\n",
      "\n",
      "================================================================================\n",
      "FILTER 2: Remove compounds with <5 observations\n",
      "================================================================================\n",
      "  Compounds with ‚â•5 observations: 22,731/874\n",
      "  Remaining samples: 1,777,129\n",
      "  Remaining compounds: 22,731\n",
      "\n",
      "================================================================================\n",
      "FILTER 3: Remove observations with cosine similarity <0.12\n",
      "================================================================================\n",
      "\n",
      "üìä Calculating similarities (Numba-accelerated)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Computing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22731/22731 [17:07<00:00, 22.13it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úì Mean similarity: 0.3843\n",
      "  Removed 37,549 low-similarity observations\n",
      "  Remaining samples: 1,739,580\n",
      "\n",
      "================================================================================\n",
      "FILTER 4: Intelligent Dose Filtering\n",
      "================================================================================\n",
      "  Step 1/3: Parsing dose values...\n",
      "  ‚úì Successfully parsed: 1,805,392 entries\n",
      "  ‚ö†Ô∏è  Missing/invalid doses: 506 entries\n",
      "\n",
      "  Step 2/3: Converting to ¬µM...\n",
      "  Dose unit distribution:\n",
      "    - nan: 506 samples\n",
      "    - e: 30 samples\n",
      "\n",
      "  Step 3/3: Applying dose filter (Method: method2)...\n",
      "\n",
      "   üìä Method 2: Central value fold-range filtering (0.2x-50x, using median)\n",
      "      Using vectorized operations for 22,528 compounds...\n",
      "      ‚úì Removed 349,858 samples\n",
      "      ‚úì Remaining: 1,389,722 samples\n",
      "\n",
      "  ‚úÖ Filter 4 Results:\n",
      "  ‚úì Removed 349,858 samples\n",
      "  ‚úì Remaining samples: 1,389,722\n",
      "  ‚úì Remaining compounds: 22,528\n",
      "\n",
      "  üìä Generating dose distribution visualizations...\n",
      "  ‚úì Saved figure: D:\\ÁßëÁ†î\\Models\\drugreflector\\visualizations\\dose_distribution_comparison.png\n",
      "  ‚úì Saved figure: D:\\ÁßëÁ†î\\Models\\drugreflector\\visualizations\\top_doses_comparison.png\n",
      "\n",
      "  ‚úÖ Visualization complete!\n",
      "     Figures saved to: D:\\ÁßëÁ†î\\Models\\drugreflector\\visualizations\n",
      "\n",
      "================================================================================\n",
      "FILTER 5: Keep only measurements at [6, 24] hours\n",
      "================================================================================\n",
      "   Available timepoints (numeric):\n",
      "     - 24.0 hours: 1,338,831 samples\n",
      "     - 6.0 hours: 398,943 samples\n",
      "     - 3.0 hours: 31,925 samples\n",
      "     - 48.0 hours: 18,614 samples\n",
      "     - 4.0 hours: 11,372 samples\n",
      "     - 72.0 hours: 2,308 samples\n",
      "     - 2.0 hours: 1,135 samples\n",
      "     - 12.0 hours: 1,083 samples\n",
      "     - -666.0 hours: 581 samples\n",
      "     - 120.0 hours: 367 samples\n",
      "  ‚úì Kept samples at [6, 24] hours\n",
      "  Removed 47,124 observations (invalid timepoint)\n",
      "  Remaining samples: 1,342,598\n",
      "\n",
      "================================================================================\n",
      "FILTER 6: Remove cell lines with <300 compounds\n",
      "================================================================================\n",
      "  Total cell lines before filtering: 221\n",
      "  Compounds per cell line (mean): 831.3\n",
      "  Compounds per cell line (median): 254\n",
      "  Compounds per cell line (min): 1\n",
      "  Compounds per cell line (max): 15254\n",
      "\n",
      "  Cell lines with ‚â•300 compounds: 94/221\n",
      "\n",
      "  Sample removed cell lines (showing up to 10):\n",
      "    - 5637: 3 compounds\n",
      "    - AN3CA: 3 compounds\n",
      "    - BICR6: 119 compounds\n",
      "    - BJAB: 134 compounds\n",
      "    - BT20: 198 compounds\n",
      "    - BT474: 119 compounds\n",
      "    - C42: 5 compounds\n",
      "    - CD34: 210 compounds\n",
      "    - COV434: 118 compounds\n",
      "    - CW2: 3 compounds\n",
      "\n",
      "  Removed 100,872 observations from 127 cell lines\n",
      "  Remaining samples: 1,241,726\n",
      "  Remaining compounds: 22,412\n",
      "  Remaining cell lines: 94\n",
      "\n",
      "================================================================================\n",
      "FILTER 7: Remove compounds in <5 or >200 cell lines\n",
      "================================================================================\n",
      "  Compounds in 5-200 cell lines: 11,171/22,412\n",
      "  Removed 115,983 observations\n",
      "  Remaining samples: 1,125,743\n",
      "  Remaining compounds: 11,171\n",
      "\n",
      "================================================================================\n",
      "‚úÖ FINAL DATASET\n",
      "================================================================================\n",
      "  Extracted 1,125,743 samples\n",
      "  Memory usage: 4.10 GB\n",
      "\n",
      "  Total samples: 1,125,743\n",
      "  Total compounds: 11,171\n",
      "  Cell lines: 94\n",
      "  Gene features: 978\n",
      "  Samples per compound (mean): 100.8\n",
      "  Samples per compound (median): 57\n",
      "  Compounds with >100 observations: 2,927\n",
      "\n",
      "üìä Comparison with paper (SI page 2):\n",
      "  Paper: 425,242 obs, 9,597 compounds, 52 cell lines\n",
      "  Ours:  1,125,743 obs, 11,171 compounds, 94 cell lines\n",
      "  Compound retention rate: 32.5%\n",
      "  ‚úì Using geneinfo_beta.txt for gene names\n",
      "\n",
      "================================================================================\n",
      "STEP 4: Creating 3-fold splits\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "üé≤ Creating 3-fold cross-validation splits\n",
      "================================================================================\n",
      "\n",
      "üìä Fold statistics:\n",
      "   Fold 0: 378,867 samples, 11,171 compounds\n",
      "   Fold 1: 375,317 samples, 11,171 compounds\n",
      "   Fold 2: 371,559 samples, 11,171 compounds\n",
      "\n",
      "================================================================================\n",
      "STEP 5: Saving processed data\n",
      "================================================================================\n",
      "üíæ Saving to: D:\\ÁßëÁ†î\\Models\\drugreflector\\processed_data\\training_data_lincs2020_method1.pkl\n",
      "‚úì Saved successfully!\n",
      "   File size: 4491.6 MB\n",
      "\n",
      "================================================================================\n",
      "‚úÖ DATA PREPARATION COMPLETE!\n",
      "================================================================================\n",
      "üìÅ Output: D:\\ÁßëÁ†î\\Models\\drugreflector\\processed_data\\training_data_lincs2020_method1.pkl\n",
      "\n",
      "üìä Final dataset:\n",
      "   ‚Ä¢ Samples: 1,125,743\n",
      "   ‚Ä¢ Compounds: 11,171\n",
      "   ‚Ä¢ Genes: 978\n",
      "   ‚Ä¢ Memory: 4.10 GB\n",
      "\n",
      "üìà Comparison with paper:\n",
      "   Samples: 1,125,743 / 425,242 (264.7%)\n",
      "   Compounds: 11,171 / 9,597 (116.4%)\n",
      "\n",
      "üéØ Ready for training!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "LINCS 2020Êï∞ÊçÆÂä†ËΩΩÂíåÈ¢ÑÂ§ÑÁêÜËÑöÊú¨ - ‰ºòÂåñÁâàÊú¨ v2\n",
    "Êñ∞Â¢ûÊô∫ËÉΩÂâÇÈáèËøáÊª§ÊñπÊ≥ïÔºö\n",
    "- ÊñπÊ≥ï1ÔºöÁôæÂàÜ‰ΩçÊï∞ËøáÊª§Ôºà‰øùÁïô‰∏≠Èó¥80%Ôºâ\n",
    "- ÊñπÊ≥ï2Ôºö‰∏≠‰ΩçÊï∞ÂÄçÊï∞ËøáÊª§Ôºà0.1x-10xÔºâ\n",
    "- ÊñπÊ≥ï3ÔºöËûçÂêàÊñπÊ≥ï\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import gzip\n",
    "import shutil\n",
    "import glob\n",
    "from scipy.spatial.distance import cosine, pdist, squareform\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from numba import jit, prange\n",
    "from sklearn.preprocessing import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, Tuple, Optional\n",
    "import warnings\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class LINCS2020DataLoader:\n",
    "    \"\"\"\n",
    "    Âä†ËΩΩÂíåÈ¢ÑÂ§ÑÁêÜLINCS 2020Êï∞ÊçÆ - ‰ºòÂåñÁâà\n",
    "    ‰øÆÊ≠£DOSËøáÊª§ÂíåÊó∂Èó¥ÁÇπÂåπÈÖçÔºåÊñ∞Â¢ûÊô∫ËÉΩÂâÇÈáèËøáÊª§\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.gene_info = None\n",
    "        self.cell_info = None\n",
    "        self.compound_info = None\n",
    "        self.inst_info = None  \n",
    "        self.signatures = None\n",
    "        self.decompressed_files = []\n",
    "        \n",
    "        self.cache_dir = self.data_dir / \"_cache\"\n",
    "        self.cache_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"üî¨ LINCS 2020 Data Loader - Enhanced Version v2\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Data directory: {self.data_dir}\")\n",
    "        print(f\"Dataset: Expanded CMap LINCS Resource 2020\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    def load_gene_info(self):\n",
    "        \"\"\"Âä†ËΩΩÂü∫Âõ†‰ø°ÊÅØÔºåËé∑Âèñ978‰∏™landmark genes\"\"\"\n",
    "        gene_file = self.data_dir / \"geneinfo_beta.txt\"\n",
    "        \n",
    "        if not gene_file.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"‚ùå Gene info file not found: {gene_file}\\n\"\n",
    "                f\"   Please download 'geneinfo_beta.txt' from:\\n\"\n",
    "                f\"   https://clue.io/data/CMap2020#LINCS2020\"\n",
    "            )\n",
    "        \n",
    "        print(f\"üìñ Loading gene information...\")\n",
    "        print(f\"   File: {gene_file.name}\")\n",
    "        \n",
    "        gene_info = pd.read_csv(gene_file, sep='\\t')\n",
    "        \n",
    "        print(f\"   ‚úì Loaded {len(gene_info):,} genes\")\n",
    "        print(f\"   ‚úì Columns: {list(gene_info.columns)}\")\n",
    "        \n",
    "        if 'feature_space' in gene_info.columns:\n",
    "            landmark_mask = gene_info['feature_space'] == 'landmark'\n",
    "            landmark_genes = gene_info[landmark_mask].copy()\n",
    "            print(f\"   ‚úì Using 'feature_space' column to identify landmarks\")\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Cannot identify landmark genes. 'feature_space' column not found.\\n\"\n",
    "                f\"Available columns: {list(gene_info.columns)}\"\n",
    "            )\n",
    "\n",
    "        print(f\"   ‚úì Landmark genes: {len(landmark_genes):,}\")\n",
    "        print(f\"   ‚úì Expected: 978\")\n",
    "\n",
    "        self.landmark_col_indices = np.where(landmark_mask.values)[0]\n",
    "        print(f\"\\n   Landmark column indices (first 10): {self.landmark_col_indices[:10]}\")\n",
    "\n",
    "        self.landmark_gene_ids = set(landmark_genes['gene_id'].astype(str).values)\n",
    "        self.landmark_gene_symbols = set(landmark_genes['gene_symbol'].astype(str).values)\n",
    "\n",
    "        print(f\"   Sample IDs: {list(self.landmark_gene_ids)[:5]}\")\n",
    "        print(f\"   Sample symbols: {list(self.landmark_gene_symbols)[:5]}\")\n",
    "\n",
    "        self.gene_info = gene_info\n",
    "        return gene_info\n",
    "    \n",
    "    def load_cell_info(self):\n",
    "        \"\"\"Âä†ËΩΩÁªÜËÉûÁ≥ª‰ø°ÊÅØ\"\"\"\n",
    "        cell_file = self.data_dir / \"cellinfo_beta.txt\"\n",
    "        \n",
    "        if not cell_file.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"‚ùå Cell info file not found: {cell_file}\\n\"\n",
    "                f\"   Please download 'cellinfo_beta.txt' from:\\n\"\n",
    "                f\"   https://clue.io/data/CMap2020#LINCS2020\"\n",
    "            )\n",
    "        \n",
    "        print(f\"\\nüìñ Loading cell information...\")\n",
    "        print(f\"   File: {cell_file.name}\")\n",
    "        \n",
    "        cell_info = pd.read_csv(cell_file, sep='\\t')\n",
    "        \n",
    "        print(f\"   ‚úì Loaded {len(cell_info):,} cell lines\")\n",
    "        print(f\"   ‚úì Columns: {list(cell_info.columns[:10])}...\")\n",
    "        \n",
    "        if 'cell_iname' in cell_info.columns:\n",
    "            print(f\"   ‚úì Unique cell lines (cell_iname): {cell_info['cell_iname'].nunique()}\")\n",
    "        \n",
    "        print(f\"\\n   Sample cell lines:\")\n",
    "        for cell in cell_info['cell_iname'].head(5).values:\n",
    "            print(f\"     - {cell}\")\n",
    "        \n",
    "        self.cell_info = cell_info\n",
    "        return cell_info\n",
    "    \n",
    "    def load_compound_info(self):\n",
    "        \"\"\"Âä†ËΩΩÂåñÂêàÁâ©‰ø°ÊÅØ\"\"\"\n",
    "        compound_file = self.data_dir / \"compoundinfo_beta.txt\"\n",
    "        \n",
    "        if not compound_file.exists():\n",
    "            print(f\"‚ö†Ô∏è  Compound info file not found: {compound_file}\")\n",
    "            print(f\"   This file is optional but recommended.\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"\\nüìñ Loading compound information...\")\n",
    "        print(f\"   File: {compound_file.name}\")\n",
    "        \n",
    "        compound_info = pd.read_csv(compound_file, sep='\\t')\n",
    "        \n",
    "        print(f\"   ‚úì Loaded {len(compound_info):,} compounds\")\n",
    "        print(f\"   ‚úì Columns: {list(compound_info.columns)}\")\n",
    "        \n",
    "        if 'pert_id' in compound_info.columns:\n",
    "            print(f\"   ‚úì Unique perturbagens: {compound_info['pert_id'].nunique()}\")\n",
    "        \n",
    "        print(f\"\\n   Sample compounds:\")\n",
    "        for _, row in compound_info.head(3).iterrows():\n",
    "            name = row.get('cmap_name', 'Unknown')\n",
    "            print(f\"     - {row['pert_id']}: {name}\")\n",
    "        \n",
    "        self.compound_info = compound_info\n",
    "        return compound_info\n",
    "    \n",
    "    def load_instance_info(self):\n",
    "        \"\"\"Âä†ËΩΩÂÆû‰æã‰ø°ÊÅØ\"\"\"\n",
    "        inst_file = self.data_dir / \"instinfo_beta.txt\"\n",
    "\n",
    "        if not inst_file.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"‚ùå Instance info file not found: {inst_file}\\n\"\n",
    "                f\"   Please download 'instinfo_beta.txt' from:\\n\"\n",
    "                f\"   https://clue.io/data/CMap2020#LINCS2020\"\n",
    "            )\n",
    "\n",
    "        print(f\"\\nüìñ Loading instance information...\")\n",
    "        print(f\"   File: {inst_file.name}\")\n",
    "\n",
    "        inst_info = pd.read_csv(inst_file, sep='\\t')\n",
    "\n",
    "        print(f\"   ‚úì Loaded {len(inst_info):,} instances\")\n",
    "        print(f\"   ‚úì Columns: {list(inst_info.columns[:10])}...\")\n",
    "\n",
    "        if 'inst_id' in inst_info.columns:\n",
    "            self.instance_join_col = 'inst_id'\n",
    "        elif 'sample_id' in inst_info.columns:\n",
    "            self.instance_join_col = 'sample_id'\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Neither 'inst_id' nor 'sample_id' found in instinfo_beta.txt. \"\n",
    "                f\"Available columns: {list(inst_info.columns)}\"\n",
    "            )\n",
    "\n",
    "        print(f\"   ‚úì Using '{self.instance_join_col}' as join key\")\n",
    "\n",
    "        self.inst_info = inst_info\n",
    "        return inst_info\n",
    "\n",
    "    def decompress_gctx_file(self, gctx_file):\n",
    "        \"\"\"Â¶ÇÊûúGCTXÊñá‰ª∂Ë¢´ÂéãÁº©ÔºåÂàôËß£Âéã\"\"\"\n",
    "        gctx_file = Path(gctx_file)\n",
    "        \n",
    "        if not gctx_file.exists():\n",
    "            raise FileNotFoundError(f\"GCTX file not found: {gctx_file}\")\n",
    "        \n",
    "        if str(gctx_file).endswith('.gz'):\n",
    "            print(f\"‚ö†Ô∏è  Detected compressed GCTX file: {gctx_file.name}\")\n",
    "            \n",
    "            decompressed_dir = self.data_dir / \"_decompressed\"\n",
    "            decompressed_dir.mkdir(exist_ok=True)\n",
    "            \n",
    "            output_path = decompressed_dir / gctx_file.stem\n",
    "            \n",
    "            if output_path.exists():\n",
    "                print(f\"‚úì Found existing decompressed file: {output_path.name}\")\n",
    "                return str(output_path)\n",
    "            \n",
    "            print(f\"üì¶ Decompressing...\")\n",
    "            with gzip.open(gctx_file, 'rb') as f_in:\n",
    "                with open(output_path, 'wb') as f_out:\n",
    "                    shutil.copyfileobj(f_in, f_out, length=8*1024*1024)\n",
    "            \n",
    "            print(f\"‚úì Decompressed to: {output_path}\")\n",
    "            return str(output_path)\n",
    "        \n",
    "        return str(gctx_file)\n",
    "    \n",
    "    def read_gctx(self, gctx_file, use_landmark_only=True):\n",
    "        \"\"\"ËØªÂèñGCTXÊñá‰ª∂ - ÂÜÖÂ≠ò‰ºòÂåñÁâà\"\"\"\n",
    "        gctx_file = str(gctx_file)\n",
    "        print(f\"\\nüìñ Reading GCTX file: {Path(gctx_file).name}\")\n",
    "        \n",
    "        file_size_gb = Path(gctx_file).stat().st_size / (1024**3)\n",
    "        print(f\"   File size: {file_size_gb:.2f} GB\")\n",
    "        \n",
    "        if file_size_gb > 50:\n",
    "            print(f\"   ‚ö†Ô∏è  Large file detected. Using memory-optimized loading...\")\n",
    "        \n",
    "        gctx_file = self.decompress_gctx_file(gctx_file)\n",
    "        \n",
    "        with h5py.File(gctx_file, 'r') as f:\n",
    "            print(f\"üìä Inspecting HDF5 structure...\")\n",
    "            print(f\"   Available keys: {list(f.keys())}\")\n",
    "            \n",
    "            if '/0/DATA/0/matrix' in f:\n",
    "                matrix_dataset = f['/0/DATA/0/matrix']\n",
    "                row_path = '/0/META/ROW'\n",
    "                col_path = '/0/META/COL'\n",
    "            elif '/matrix' in f:\n",
    "                matrix_dataset = f['/matrix']\n",
    "                row_path = '/row'\n",
    "                col_path = '/col'\n",
    "            else:\n",
    "                raise ValueError(f\"Cannot find matrix in GCTX file. Available keys: {list(f.keys())}\")\n",
    "            \n",
    "            matrix_shape = matrix_dataset.shape\n",
    "            print(f\"   ‚úì Matrix shape: {matrix_shape} (samples √ó genes)\")\n",
    "            \n",
    "            print(f\"\\nüìã Loading metadata...\")\n",
    "            \n",
    "            # ËØªÂèñÊ†∑Êú¨ÂÖÉÊï∞ÊçÆ\n",
    "            sample_meta = {}\n",
    "            if row_path in f:\n",
    "                print(f\"   Reading sample metadata from: {row_path}\")\n",
    "                print(f\"   Available row fields: {list(f[row_path].keys())}\")\n",
    "                \n",
    "                for key in f[row_path].keys():\n",
    "                    data = f[f'{row_path}/{key}'][:]\n",
    "                    if data.dtype.kind in ['S', 'O', 'U']:\n",
    "                        try:\n",
    "                            sample_meta[key] = np.char.decode(data.astype('S'), 'utf-8')\n",
    "                        except:\n",
    "                            sample_meta[key] = data.astype(str)\n",
    "                    else:\n",
    "                        sample_meta[key] = data\n",
    "                \n",
    "                print(f\"   ‚úì Loaded {len(sample_meta)} sample metadata fields\")\n",
    "            else:\n",
    "                raise ValueError(f\"Cannot find row metadata at: {row_path}\")\n",
    "            \n",
    "            # ËØªÂèñÂü∫Âõ†ÂÖÉÊï∞ÊçÆ\n",
    "            gene_meta = {}\n",
    "            if col_path in f:\n",
    "                print(f\"   Reading gene metadata from: {col_path}\")\n",
    "                print(f\"   Available col fields: {list(f[col_path].keys())}\")\n",
    "                \n",
    "                for key in f[col_path].keys():\n",
    "                    data = f[f'{col_path}/{key}'][:]\n",
    "                    if data.dtype.kind in ['S', 'O', 'U']:\n",
    "                        try:\n",
    "                            gene_meta[key] = np.char.decode(data.astype('S'), 'utf-8')\n",
    "                        except:\n",
    "                            gene_meta[key] = data.astype(str)\n",
    "                    else:\n",
    "                        gene_meta[key] = data\n",
    "                \n",
    "                print(f\"   ‚úì Loaded {len(gene_meta)} gene metadata fields\")\n",
    "            else:\n",
    "                raise ValueError(f\"Cannot find col metadata at: {col_path}\")\n",
    "            \n",
    "            sample_df = pd.DataFrame(sample_meta)\n",
    "            gene_df = pd.DataFrame(gene_meta)\n",
    "\n",
    "            # Ëá™Âä®Ê£ÄÊµãÂπ∂‰∫§Êç¢ROW/COL\n",
    "            if self.gene_info is not None:\n",
    "                n_features_expected = len(self.gene_info)\n",
    "                if len(sample_df) == n_features_expected and len(gene_df) != n_features_expected:\n",
    "                    print(f\"\\n   ‚ö†Ô∏è Detected ROW/COL swap. Correcting...\")\n",
    "                    sample_df, gene_df = gene_df, sample_df\n",
    "\n",
    "            print(f\"\\n   Sample metadata columns: {list(sample_df.columns[:10])}...\")\n",
    "            print(f\"   Gene metadata columns: {list(gene_df.columns)}\")\n",
    "            \n",
    "            # Á°ÆÂÆölandmarkÂü∫Âõ†ÁöÑÂàóÁ¥¢Âºï\n",
    "            landmark_col_indices = None\n",
    "            if use_landmark_only:\n",
    "                print(f\"\\nüî¨ Filtering to landmark genes...\")\n",
    "\n",
    "                if (self.gene_info is None) or (not hasattr(self, \"landmark_col_indices\")):\n",
    "                    print(f\"   Loading gene info to get landmark indices...\")\n",
    "                    self.load_gene_info()\n",
    "\n",
    "                if len(self.gene_info) != matrix_shape[1]:\n",
    "                    print(f\"   ‚ö†Ô∏è  Warning: geneinfo rows ({len(self.gene_info)}) \"\n",
    "                        f\"!= GCTX feature count ({matrix_shape[1]})\")\n",
    "                \n",
    "                landmark_col_indices = np.array(self.landmark_col_indices, dtype=int)\n",
    "                print(f\"   ‚úì Using {len(landmark_col_indices)} landmark features \"\n",
    "                    f\"out of {matrix_shape[1]} total\")\n",
    "            \n",
    "            # ÂÜÖÂ≠ò‰ºòÂåñÔºöÂàÜÂùóËØªÂèñ\n",
    "            print(f\"\\nüéØ Loading data (memory-optimized)...\")\n",
    "            if landmark_col_indices is not None:\n",
    "                print(f\"   Reading {len(landmark_col_indices)} columns out of {matrix_shape[1]}...\")\n",
    "                chunk_size = 610000\n",
    "                chunks = []\n",
    "                \n",
    "                for start_idx in range(0, matrix_shape[0], chunk_size):\n",
    "                    end_idx = min(start_idx + chunk_size, matrix_shape[0])\n",
    "                    print(f\"   Loading rows {start_idx:,} to {end_idx:,}... ({end_idx/matrix_shape[0]*100:.1f}%)\", end='\\r')\n",
    "                    \n",
    "                    chunk = matrix_dataset[start_idx:end_idx, landmark_col_indices].astype(np.float32)\n",
    "                    chunks.append(chunk)\n",
    "                    \n",
    "                    if len(chunks) >= 10:\n",
    "                        print(f\"\\n   Consolidating chunks...\")\n",
    "                        merged = np.vstack(chunks)\n",
    "                        chunks = [merged]\n",
    "                        gc.collect()\n",
    "                \n",
    "                print(f\"\\n   Finalizing matrix...\")\n",
    "                matrix = np.vstack(chunks) if len(chunks) > 1 else chunks[0]\n",
    "                del chunks\n",
    "                gc.collect()\n",
    "            else:\n",
    "                print(f\"   Reading full matrix...\")\n",
    "                matrix = matrix_dataset[:].astype(np.float32)\n",
    "            \n",
    "            print(f\"   ‚úì Final matrix shape: {matrix.shape}\")\n",
    "            print(f\"   ‚úì Memory usage: {matrix.nbytes / (1024**3):.2f} GB\")\n",
    "            print(f\"   ‚úì Data type: {matrix.dtype}\")\n",
    "        \n",
    "        return matrix, sample_df, gene_df\n",
    "    \n",
    "    def load_level4_signatures(self):\n",
    "        \"\"\"Âä†ËΩΩLevel 4Êï∞ÊçÆ\"\"\"\n",
    "        level4_file = self.data_dir / \"level4_beta_trt_cp_n1805898x12328.gctx\"\n",
    "        \n",
    "        if not level4_file.exists():\n",
    "            pattern = self.data_dir / \"level4_beta_trt_cp*.gctx\"\n",
    "            files = glob.glob(str(pattern))\n",
    "            \n",
    "            if not files:\n",
    "                raise FileNotFoundError(\n",
    "                    f\"‚ùå Level 4 file not found: {level4_file}\\n\"\n",
    "                    f\"   Please download from: https://clue.io/data/CMap2020#LINCS2020\"\n",
    "                )\n",
    "            \n",
    "            level4_file = files[0]\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"üìñ Loading Level 4 Signatures\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"File: {level4_file.name}\")\n",
    "        \n",
    "        matrix, sample_meta, gene_meta = self.read_gctx(\n",
    "            level4_file, \n",
    "            use_landmark_only=True\n",
    "        )\n",
    "\n",
    "        # Áî®instinfoË°•ÈΩêmetadata\n",
    "        if 'id' in sample_meta.columns:\n",
    "            sample_meta = sample_meta.rename(columns={'id': 'sample_id'})\n",
    "        elif 'sample_id' not in sample_meta.columns:\n",
    "            raise ValueError(\"Cannot find 'id' or 'sample_id' in GCTX ROW metadata\")\n",
    "\n",
    "        if self.inst_info is None:\n",
    "            self.load_instance_info()\n",
    "\n",
    "        inst_info = self.inst_info\n",
    "        join_col = getattr(self, \"instance_join_col\", None)\n",
    "        \n",
    "        if join_col is None:\n",
    "            join_col = 'sample_id' if 'sample_id' in inst_info.columns else 'inst_id'\n",
    "\n",
    "        if join_col not in sample_meta.columns:\n",
    "            if join_col == 'inst_id' and 'sample_id' in sample_meta.columns:\n",
    "                print(\"   ‚ö†Ô∏è Using 'sample_id' instead of 'inst_id'\")\n",
    "                join_col = 'sample_id'\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Join column '{join_col}' not found in GCTX metadata. \"\n",
    "                    f\"Available: {list(sample_meta.columns)}\"\n",
    "                )\n",
    "\n",
    "        print(f\"   üîó Merging GCTX metadata with instinfo on '{join_col}'...\")\n",
    "        merged_meta = sample_meta.merge(inst_info, on=join_col, how='left')\n",
    "\n",
    "        if 'pert_id' not in merged_meta.columns:\n",
    "            raise ValueError(\"After merging, 'pert_id' is still missing\")\n",
    "\n",
    "        n_missing = merged_meta['pert_id'].isna().sum()\n",
    "        if n_missing > 0:\n",
    "            print(f\"   ‚ö†Ô∏è {n_missing:,} rows have missing pert_id\")\n",
    "\n",
    "        self.signatures = {\n",
    "            'matrix': matrix,\n",
    "            'row_meta': merged_meta,\n",
    "            'col_meta': gene_meta\n",
    "        }\n",
    "\n",
    "        return matrix, merged_meta, gene_meta\n",
    "    \n",
    "    def calculate_cosine_similarity_to_nearest_replicate(\n",
    "        self, \n",
    "        matrix: np.ndarray, \n",
    "        pert_ids: pd.Series\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        ‰ΩøÁî®Numba JITÁºñËØëÁöÑË∂ÖÈ´òÊïàÁâàÊú¨\n",
    "        \n",
    "        ËÆ°ÁÆóÊØè‰∏™Ê†∑Êú¨‰∏éÂÖ∂ÊúÄËøëÂêåÂåñÂêàÁâ©Â§çÂà∂ÂìÅÁöÑ‰ΩôÂº¶Áõ∏‰ººÂ∫¶\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"\\nüìä Calculating similarities (Numba-accelerated)...\")\n",
    "        \n",
    "        # È¢ÑÂΩí‰∏ÄÂåñ\n",
    "        matrix_norm = normalize(matrix, norm='l2', axis=1).astype(np.float32)\n",
    "        \n",
    "        @jit(nopython=True, parallel=True, fastmath=True)\n",
    "        def compute_max_similarities(data, indices, n_total):\n",
    "            \"\"\"NumbaÂä†ÈÄüÁöÑÊ†∏ÂøÉËÆ°ÁÆó\"\"\"\n",
    "            n = len(indices)\n",
    "            result = np.full(n_total, -np.inf, dtype=np.float32)\n",
    "            \n",
    "            for i in prange(n):\n",
    "                idx_i = indices[i]\n",
    "                vec_i = data[i]\n",
    "                max_sim = -np.inf\n",
    "                \n",
    "                for j in range(n):\n",
    "                    if i != j:\n",
    "                        # ÁÇπÁßØÔºàÂ∑≤ÂΩí‰∏ÄÂåñ = ‰ΩôÂº¶Áõ∏‰ººÂ∫¶Ôºâ\n",
    "                        sim = np.dot(vec_i, data[j])\n",
    "                        if sim > max_sim:\n",
    "                            max_sim = sim\n",
    "                \n",
    "                result[idx_i] = max_sim\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        nearest_similarities = np.zeros(len(pert_ids), dtype=np.float32)\n",
    "        \n",
    "        # ÊåâÂåñÂêàÁâ©ÂàÜÁªÑ\n",
    "        pert_ids_array = pert_ids.values\n",
    "        unique_perts = np.unique(pert_ids_array)\n",
    "        \n",
    "        from tqdm import tqdm\n",
    "        for pert_id in tqdm(unique_perts, desc=\"   Computing\"):\n",
    "            mask = pert_ids_array == pert_id\n",
    "            indices = np.where(mask)[0]\n",
    "            \n",
    "            if len(indices) < 2:\n",
    "                nearest_similarities[indices] = 0.0\n",
    "                continue\n",
    "            \n",
    "            pert_data = matrix_norm[mask]\n",
    "            sims = compute_max_similarities(pert_data, indices, len(pert_ids))\n",
    "            nearest_similarities[indices] = sims[indices]\n",
    "        \n",
    "        print(f\"   ‚úì Mean similarity: {nearest_similarities.mean():.4f}\")\n",
    "        return nearest_similarities\n",
    "    \n",
    "    def dose_filter_method1(self, row_meta, valid_mask, percentile_range=(10, 90)):\n",
    "        \"\"\"\n",
    "        ÊñπÊ≥ï1ÔºöÁôæÂàÜ‰ΩçÊï∞ËøáÊª§ - ÂêëÈáèÂåñ‰ºòÂåñÁâàÊú¨\n",
    "        \n",
    "        ÊÄßËÉΩÊèêÂçáÔºö‰ΩøÁî®groupby + rankËøõË°åÊâπÈáèÁôæÂàÜ‰ΩçËÆ°ÁÆóÔºåÈÅøÂÖçÂæ™ÁéØ\n",
    "        \"\"\"\n",
    "        print(f\"\\n   üìä Method 1: Percentile-based filtering ({percentile_range[0]}th-{percentile_range[1]}th percentile)\")\n",
    "        print(f\"      Using vectorized operations for {row_meta[valid_mask]['pert_id'].nunique():,} compounds...\")\n",
    "        \n",
    "        # ÂàõÂª∫Â∑•‰ΩúDataFrameÔºàÂè™ÂåÖÂê´ÊúâÊïàÊ†∑Êú¨Ôºâ\n",
    "        work_df = row_meta[valid_mask][['pert_id', 'dose_uM']].copy()\n",
    "        work_df = work_df[work_df['dose_uM'].notna()].copy()\n",
    "        \n",
    "        if len(work_df) == 0:\n",
    "            print(f\"      ‚ö†Ô∏è  No valid dose data\")\n",
    "            return valid_mask\n",
    "        \n",
    "        # ÊâπÈáèËÆ°ÁÆóÊØè‰∏™Ê†∑Êú¨Âú®ÂÖ∂ÂåñÂêàÁâ©ÂÜÖÁöÑÁôæÂàÜ‰ΩçÊéíÂêç\n",
    "        work_df['dose_rank_pct'] = work_df.groupby('pert_id')['dose_uM'].rank(pct=True) * 100\n",
    "        \n",
    "        # ‰∏ÄÊ¨°ÊÄßÊ†áËÆ∞ÊâÄÊúâÂú®ËåÉÂõ¥ÂÜÖÁöÑÊ†∑Êú¨\n",
    "        in_percentile_range = (work_df['dose_rank_pct'] >= percentile_range[0]) & \\\n",
    "                            (work_df['dose_rank_pct'] <= percentile_range[1])\n",
    "        \n",
    "        # ÂêëÈáèÂåñËÆ°ÁÆóÊØè‰∏™ÂâÇÈáèÂÄºÁöÑ\"Â§ßÈÉ®ÂàÜÂú®ÂÜÖ/Âú®Â§ñ\"Âà§Êñ≠\n",
    "        # ÂØπ‰∫éÊØè‰∏™ (compound, dose) ÁªÑÂêàÔºåÁªüËÆ°ÊúâÂ§öÂ∞ëÂú®ËåÉÂõ¥ÂÜÖ\n",
    "        dose_groups = work_df.groupby(['pert_id', 'dose_uM']).agg({\n",
    "            'dose_rank_pct': ['count']  # ÊÄªÊï∞\n",
    "        }).reset_index()\n",
    "        dose_groups.columns = ['pert_id', 'dose_uM', 'total_count']\n",
    "        \n",
    "        # ÁªüËÆ°Âú®ËåÉÂõ¥ÂÜÖÁöÑÊï∞Èáè\n",
    "        in_range_counts = work_df[in_percentile_range].groupby(['pert_id', 'dose_uM']).size().reset_index(name='in_range_count')\n",
    "        dose_groups = dose_groups.merge(in_range_counts, on=['pert_id', 'dose_uM'], how='left')\n",
    "        dose_groups['in_range_count'] = dose_groups['in_range_count'].fillna(0)\n",
    "        dose_groups['out_range_count'] = dose_groups['total_count'] - dose_groups['in_range_count']\n",
    "        \n",
    "        # Â§ßÈÉ®ÂàÜÂú®ËåÉÂõ¥ÂÜÖÁöÑÂâÇÈáèÂÄº‰øùÁïô\n",
    "        dose_groups['keep'] = dose_groups['in_range_count'] > dose_groups['out_range_count']\n",
    "        \n",
    "        # ÂàõÂª∫ (compound, dose) -> keep ÁöÑÊò†Â∞Ñ\n",
    "        keep_doses = dose_groups[dose_groups['keep']][['pert_id', 'dose_uM']]\n",
    "        keep_doses['_keep_flag'] = True\n",
    "        \n",
    "        # ÊâπÈáèÂêàÂπ∂Âà§Êñ≠ÁªìÊûú\n",
    "        work_df = work_df.merge(keep_doses, on=['pert_id', 'dose_uM'], how='left')\n",
    "        work_df['_keep_flag'] = work_df['_keep_flag'].fillna(False)\n",
    "        \n",
    "        # ÊûÑÂª∫ÊúÄÁªàmask\n",
    "        dose_mask = np.zeros(len(row_meta), dtype=bool)\n",
    "        dose_mask[valid_mask] = True\n",
    "        \n",
    "        # Â∞Ü‰∏ç‰øùÁïôÁöÑÊ†∑Êú¨Ê†áËÆ∞‰∏∫False\n",
    "        valid_indices = np.where(valid_mask)[0]\n",
    "        valid_dose_indices = work_df.index\n",
    "        remove_indices = valid_dose_indices[~work_df['_keep_flag'].values]\n",
    "        dose_mask[remove_indices] = False\n",
    "        \n",
    "        n_removed = valid_mask.sum() - dose_mask.sum()\n",
    "        print(f\"      ‚úì Removed {n_removed:,} samples\")\n",
    "        print(f\"      ‚úì Remaining: {dose_mask.sum():,} samples\")\n",
    "        \n",
    "        return dose_mask\n",
    "    \n",
    "    def dose_filter_method2(self, row_meta, valid_mask, fold_range=(0.1, 10), central_measure='median'):\n",
    "        \"\"\"\n",
    "        ÊñπÊ≥ï2Ôºö‰∏≠‰ΩçÊï∞/‰ºóÊï∞ÂÄçÊï∞ËøáÊª§ - ÂêëÈáèÂåñ‰ºòÂåñÁâàÊú¨\n",
    "        \n",
    "        ÊÄßËÉΩÊèêÂçáÔºö‰ΩøÁî®groupby.transform()ÊâπÈáèËÆ°ÁÆóÔºå‰∏ÄÊ¨°ÊÄßÂÆåÊàêÊâÄÊúâÂà§Êñ≠\n",
    "        \"\"\"\n",
    "        print(f\"\\n   üìä Method 2: Central value fold-range filtering ({fold_range[0]}x-{fold_range[1]}x, using {central_measure})\")\n",
    "        print(f\"      Using vectorized operations for {row_meta[valid_mask]['pert_id'].nunique():,} compounds...\")\n",
    "        \n",
    "        # ÂàõÂª∫Â∑•‰ΩúDataFrame\n",
    "        work_df = row_meta[valid_mask][['pert_id', 'dose_uM']].copy()\n",
    "        work_df = work_df[work_df['dose_uM'].notna()].copy()\n",
    "        \n",
    "        if len(work_df) == 0:\n",
    "            print(f\"      ‚ö†Ô∏è  No valid dose data\")\n",
    "            return valid_mask\n",
    "        \n",
    "        # ÊâπÈáèËÆ°ÁÆóÊØè‰∏™ÂåñÂêàÁâ©ÁöÑ‰∏≠ÂøÉÂÄºÔºà‰∏ÄÊ¨°ÊÄßÂÆåÊàêÔºâ\n",
    "        if central_measure == 'median':\n",
    "            # ‰ΩøÁî®transformÁõ¥Êé•Â∞Ü‰∏≠‰ΩçÊï∞Êò†Â∞ÑÂà∞ÊØè‰∏ÄË°å\n",
    "            work_df['central_value'] = work_df.groupby('pert_id')['dose_uM'].transform('median')\n",
    "        elif central_measure == 'mode':\n",
    "            # ‰ºóÊï∞ÈúÄË¶ÅÁâπÊÆäÂ§ÑÁêÜÔºà‰ºóÊï∞ÂèØËÉΩ‰∏çÂîØ‰∏ÄÔºåÂèñÁ¨¨‰∏Ä‰∏™Ôºâ\n",
    "            def first_mode(x):\n",
    "                mode_vals = x.mode()\n",
    "                return mode_vals[0] if len(mode_vals) > 0 else x.median()\n",
    "            \n",
    "            work_df['central_value'] = work_df.groupby('pert_id')['dose_uM'].transform(first_mode)\n",
    "        else:\n",
    "            work_df['central_value'] = work_df.groupby('pert_id')['dose_uM'].transform('median')\n",
    "        \n",
    "        # ÂêëÈáèÂåñËÆ°ÁÆó‰∏ä‰∏ãÁïå\n",
    "        work_df['lower_bound'] = work_df['central_value'] * fold_range[0]\n",
    "        work_df['upper_bound'] = work_df['central_value'] * fold_range[1]\n",
    "        \n",
    "        # ÂêëÈáèÂåñÂà§Êñ≠ÊâÄÊúâÊ†∑Êú¨ÊòØÂê¶Âú®ËåÉÂõ¥ÂÜÖ\n",
    "        work_df['keep'] = (work_df['dose_uM'] >= work_df['lower_bound']) & \\\n",
    "                        (work_df['dose_uM'] <= work_df['upper_bound'])\n",
    "        \n",
    "        # ÊûÑÂª∫ÊúÄÁªàmask\n",
    "        dose_mask = np.zeros(len(row_meta), dtype=bool)\n",
    "        dose_mask[valid_mask] = True\n",
    "        \n",
    "        # Ê†áËÆ∞‰∏ç‰øùÁïôÁöÑÊ†∑Êú¨\n",
    "        remove_indices = work_df.index[~work_df['keep'].values]\n",
    "        dose_mask[remove_indices] = False\n",
    "        \n",
    "        n_removed = valid_mask.sum() - dose_mask.sum()\n",
    "        print(f\"      ‚úì Removed {n_removed:,} samples\")\n",
    "        print(f\"      ‚úì Remaining: {dose_mask.sum():,} samples\")\n",
    "        \n",
    "        return dose_mask\n",
    "    \n",
    "    def dose_filter_method3(self, row_meta, valid_mask, percentile_range=(10, 90), \n",
    "                           fold_range=(0.1, 10), central_measure='median'):\n",
    "        \"\"\"\n",
    "        ÊñπÊ≥ï3ÔºöËûçÂêàÊñπÊ≥ï1ÂíåÊñπÊ≥ï2\n",
    "        \n",
    "        ÂêåÊó∂Êª°Ë∂≥ÁôæÂàÜ‰ΩçÊï∞Êù°‰ª∂ÂíåÂÄçÊï∞Êù°‰ª∂ÁöÑÊ†∑Êú¨Êâç‰øùÁïôÔºàÂèñ‰∫§ÈõÜÔºâ\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        row_meta : pd.DataFrame\n",
    "            ÂåÖÂê´ 'pert_id' Âíå 'dose_uM' ÂàóÁöÑÂÖÉÊï∞ÊçÆ\n",
    "        valid_mask : np.ndarray\n",
    "            ÂΩìÂâçÊúâÊïàÊ†∑Êú¨ÁöÑmask\n",
    "        percentile_range : tuple\n",
    "            ÊñπÊ≥ï1ÁöÑÁôæÂàÜ‰ΩçÊï∞ËåÉÂõ¥\n",
    "        fold_range : tuple\n",
    "            ÊñπÊ≥ï2ÁöÑÂÄçÊï∞ËåÉÂõ¥\n",
    "        central_measure : str\n",
    "            ÊñπÊ≥ï2‰ΩøÁî®ÁöÑ‰∏≠ÂøÉÂ∫¶Èáè\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        np.ndarray : Êõ¥Êñ∞ÂêéÁöÑvalid_mask\n",
    "        \"\"\"\n",
    "        print(f\"\\n   üìä Method 3: Hybrid filtering (Method 1 AND Method 2)\")\n",
    "        \n",
    "        # Â∫îÁî®ÊñπÊ≥ï1\n",
    "        mask1 = self.dose_filter_method1(row_meta, valid_mask, percentile_range)\n",
    "        \n",
    "        # Â∫îÁî®ÊñπÊ≥ï2\n",
    "        mask2 = self.dose_filter_method2(row_meta, valid_mask, fold_range, central_measure)\n",
    "        \n",
    "        # Âèñ‰∫§ÈõÜ\n",
    "        hybrid_mask = mask1 & mask2\n",
    "        \n",
    "        n_removed_method1 = valid_mask.sum() - mask1.sum()\n",
    "        n_removed_method2 = valid_mask.sum() - mask2.sum()\n",
    "        n_removed_hybrid = valid_mask.sum() - hybrid_mask.sum()\n",
    "        \n",
    "        print(f\"\\n      Summary:\")\n",
    "        print(f\"      - Method 1 alone would remove: {n_removed_method1:,} samples\")\n",
    "        print(f\"      - Method 2 alone would remove: {n_removed_method2:,} samples\")\n",
    "        print(f\"      - Hybrid method removes: {n_removed_hybrid:,} samples\")\n",
    "        \n",
    "        return hybrid_mask\n",
    "    \n",
    "    def prepare_training_data(\n",
    "        self, \n",
    "        min_observations_per_compound=5,\n",
    "        min_replicate_similarity=0.12,\n",
    "        dose_filter_method='method1',  # 'method1', 'method2', 'method3', or None\n",
    "        dose_percentile_range=(10, 90),\n",
    "        dose_fold_range=(0.1, 10),\n",
    "        dose_central_measure='median',\n",
    "        valid_timepoints=[6, 24],\n",
    "        min_cell_lines=5,\n",
    "        max_cell_lines=200,\n",
    "        min_compounds_per_cell=200,\n",
    "        remove_dos=True,\n",
    "        enable_visualization=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        ÂáÜÂ§áËÆ≠ÁªÉÊï∞ÊçÆ\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        dose_filter_method : str or None\n",
    "            ÂâÇÈáèËøáÊª§ÊñπÊ≥ï: 'method1', 'method2', 'method3', Êàñ None (‰∏çËøáÊª§ÂâÇÈáè)\n",
    "        dose_percentile_range : tuple\n",
    "            ÊñπÊ≥ï1ÁöÑÁôæÂàÜ‰ΩçÊï∞ËåÉÂõ¥Ôºå‰æãÂ¶Ç (10, 90) ‰øùÁïô‰∏≠Èó¥80%\n",
    "        dose_fold_range : tuple\n",
    "            ÊñπÊ≥ï2ÁöÑÂÄçÊï∞ËåÉÂõ¥Ôºå‰æãÂ¶Ç (0.1, 10)\n",
    "        dose_central_measure : str\n",
    "            ÊñπÊ≥ï2‰ΩøÁî®ÁöÑ‰∏≠ÂøÉÂ∫¶Èáè: 'median' Êàñ 'mode'\n",
    "        enable_visualization : bool\n",
    "            ÊòØÂê¶ÁîüÊàêÂèØËßÜÂåñÂõæË°®\n",
    "        \"\"\"\n",
    "        if self.signatures is None:\n",
    "            raise ValueError(\"Please load signatures first\")\n",
    "        \n",
    "        matrix = self.signatures['matrix']\n",
    "        row_meta = self.signatures['row_meta'].copy()\n",
    "        col_meta = self.signatures['col_meta']\n",
    "        \n",
    "        print(f\"\\n[DEBUG] row_meta shape: {row_meta.shape}\")\n",
    "        print(f\"[DEBUG] columns (first 15): {list(row_meta.columns[:15])}\")\n",
    "        print(f\"[DEBUG] Example row:\")\n",
    "        print(row_meta.iloc[0][['sample_id', 'pert_id', 'pert_type', 'cell_iname', 'pert_time', 'pert_dose']])\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"üîç DRUGREFLECTOR QUALITY CONTROL PIPELINE - Enhanced v2\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Initial samples: {len(row_meta):,}\")\n",
    "        print(f\"Initial memory: {matrix.nbytes / (1024**3):.2f} GB\")\n",
    "        \n",
    "        print(f\"\\nüìã Available metadata columns:\")\n",
    "        for col in row_meta.columns[:15]:\n",
    "            sample_val = row_meta[col].iloc[0] if len(row_meta) > 0 else 'N/A'\n",
    "            print(f\"   - {col}: {sample_val}\")\n",
    "        if len(row_meta.columns) > 15:\n",
    "            print(f\"   ... and {len(row_meta.columns) - 15} more\")\n",
    "        \n",
    "        # Ê£ÄÊü•ÂøÖÈúÄÂ≠óÊÆµ\n",
    "        required_fields = ['pert_id']\n",
    "        missing_fields = [f for f in required_fields if f not in row_meta.columns]\n",
    "        if missing_fields:\n",
    "            raise ValueError(f\"Missing required fields: {missing_fields}\")\n",
    "        \n",
    "        # Á°ÆÂÆöÁªÜËÉûÁ≥ªIDÂ≠óÊÆµ\n",
    "        cell_id_col = None\n",
    "        for possible_col in ['cell_id', 'cell_iname', 'cell_mfc_name']:\n",
    "            if possible_col in row_meta.columns:\n",
    "                cell_id_col = possible_col\n",
    "                print(f\"\\n‚úì Using '{cell_id_col}' as cell line identifier\")\n",
    "                break\n",
    "        \n",
    "        if cell_id_col is None:\n",
    "            print(f\"\\n‚ö†Ô∏è  Warning: Cannot find cell line identifier\")\n",
    "        \n",
    "        valid_mask = np.ones(len(row_meta), dtype=bool)\n",
    "        \n",
    "        initial_compounds = row_meta['pert_id'].nunique()\n",
    "        print(f\"\\nInitial compounds: {initial_compounds:,}\")\n",
    "        \n",
    "        # ========== Filter 1: Remove DOS compounds ==========\n",
    "        if remove_dos:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"FILTER 1: Remove DOS compounds (keep trt_cp only)\")\n",
    "            print(f\"{'='*80}\")\n",
    "            \n",
    "            if 'pert_type' in row_meta.columns:\n",
    "                print(f\"   pert_type value counts:\")\n",
    "                pert_type_counts = row_meta['pert_type'].value_counts()\n",
    "                for ptype, count in pert_type_counts.items():\n",
    "                    print(f\"     - {ptype}: {count:,} samples\")\n",
    "                \n",
    "                dos_mask = row_meta['pert_type'] == 'trt_cp'\n",
    "                n_removed = (~dos_mask).sum()\n",
    "                \n",
    "                print(f\"\\n  ‚úì Keeping only 'trt_cp' perturbations\")\n",
    "                print(f\"  Removed {n_removed:,} non-trt_cp observations\")\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è 'pert_type' not found, using fallback method\")\n",
    "                dos_mask = ~row_meta['pert_id'].str.contains('DOS', case=False, na=False)\n",
    "                n_removed = (~dos_mask).sum()\n",
    "                print(f\"  Removed {n_removed:,} DOS observations\")\n",
    "            \n",
    "            valid_mask &= dos_mask\n",
    "            print(f\"  Remaining samples: {valid_mask.sum():,}\")\n",
    "            print(f\"  Remaining compounds: {row_meta.loc[valid_mask, 'pert_id'].nunique():,}\")\n",
    "        \n",
    "        # ========== Filter 2: Minimum observations ==========\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FILTER 2: Remove compounds with <{min_observations_per_compound} observations\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        valid_meta = row_meta[valid_mask]\n",
    "        obs_counts = valid_meta.groupby('pert_id').size()\n",
    "        valid_perts = obs_counts[obs_counts >= min_observations_per_compound].index\n",
    "        \n",
    "        print(f\"  Compounds with ‚â•{min_observations_per_compound} observations: \"\n",
    "              f\"{len(valid_perts):,}/{obs_counts.nunique():,}\")\n",
    "        \n",
    "        obs_mask = row_meta['pert_id'].isin(valid_perts)\n",
    "        valid_mask &= obs_mask\n",
    "        \n",
    "        print(f\"  Remaining samples: {valid_mask.sum():,}\")\n",
    "        print(f\"  Remaining compounds: {row_meta.loc[valid_mask, 'pert_id'].nunique():,}\")\n",
    "        \n",
    "        del valid_meta, obs_counts, obs_mask\n",
    "        gc.collect()\n",
    "            \n",
    "        # ========== Filter 3: Cosine similarity ==========\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FILTER 3: Remove observations with cosine similarity <{min_replicate_similarity}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        valid_indices = np.where(valid_mask)[0]\n",
    "        valid_matrix = matrix[valid_mask]\n",
    "        valid_pert_ids = row_meta.loc[valid_mask, 'pert_id'].reset_index(drop=True)\n",
    "        \n",
    "        nearest_similarities = self.calculate_cosine_similarity_to_nearest_replicate(\n",
    "            valid_matrix, \n",
    "            valid_pert_ids\n",
    "        )\n",
    "        \n",
    "        full_similarities = np.zeros(len(row_meta), dtype=np.float32)\n",
    "        full_similarities[valid_indices] = nearest_similarities\n",
    "        \n",
    "        sim_mask = (full_similarities >= min_replicate_similarity) | (~valid_mask)\n",
    "        n_removed_sim = (~sim_mask & valid_mask).sum()\n",
    "        valid_mask &= sim_mask\n",
    "        \n",
    "        print(f\"  Removed {n_removed_sim:,} low-similarity observations\")\n",
    "        print(f\"  Remaining samples: {valid_mask.sum():,}\")\n",
    "        \n",
    "        del valid_matrix, valid_pert_ids, nearest_similarities, full_similarities, sim_mask\n",
    "        gc.collect()\n",
    "        \n",
    "        # ========== Filter 4: Dose selection (Êñ∞ÊñπÊ≥ï) ==========\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FILTER 4: Intelligent Dose Filtering\")\n",
    "        print(f\"{'='*80}\")\n",
    "\n",
    "        if 'pert_dose' in row_meta.columns and dose_filter_method is not None:\n",
    "            \n",
    "            # Step 1: Ëß£ÊûêÂâÇÈáèÂÄº\n",
    "            print(f\"  Step 1/3: Parsing dose values...\")\n",
    "            dose_str = row_meta['pert_dose'].astype(str)\n",
    "            row_meta['dose_value'] = pd.to_numeric(\n",
    "                dose_str.str.replace(r'[^\\d.]', '', regex=True), \n",
    "                errors='coerce'\n",
    "            )\n",
    "            row_meta['dose_unit'] = dose_str.str.extract(r'([a-zA-Z]+)', expand=False).str.lower()\n",
    "            \n",
    "            valid_dose_count = row_meta['dose_value'].notna().sum()\n",
    "            invalid_dose_count = row_meta['dose_value'].isna().sum()\n",
    "            print(f\"  ‚úì Successfully parsed: {valid_dose_count:,} entries\")\n",
    "            print(f\"  ‚ö†Ô∏è  Missing/invalid doses: {invalid_dose_count:,} entries\")\n",
    "            \n",
    "            # Step 2: ËΩ¨Êç¢Âçï‰ΩçÂà∞ ¬µM\n",
    "            print(f\"\\n  Step 2/3: Converting to ¬µM...\")\n",
    "            dose_value = row_meta['dose_value'].values\n",
    "            dose_unit = row_meta['dose_unit'].values\n",
    "            \n",
    "            conditions = [\n",
    "                dose_unit == 'nm',\n",
    "                dose_unit == 'mm'\n",
    "            ]\n",
    "            choices = [\n",
    "                dose_value / 1000,\n",
    "                dose_value * 1000\n",
    "            ]\n",
    "            row_meta['dose_uM'] = np.select(conditions, choices, default=dose_value)\n",
    "            \n",
    "            print(f\"  Dose unit distribution:\")\n",
    "            unit_counts = row_meta['dose_unit'].value_counts()\n",
    "            for unit, count in unit_counts.head(5).items():\n",
    "                print(f\"    - {unit}: {count:,} samples\")\n",
    "            \n",
    "            # Step 3: Â∫îÁî®ÈÄâÂÆöÁöÑÂâÇÈáèËøáÊª§ÊñπÊ≥ï\n",
    "            print(f\"\\n  Step 3/3: Applying dose filter (Method: {dose_filter_method})...\")\n",
    "            \n",
    "            n_before_dose = valid_mask.sum()\n",
    "            \n",
    "            if dose_filter_method == 'method1':\n",
    "                valid_mask = self.dose_filter_method1(\n",
    "                    row_meta, valid_mask, dose_percentile_range\n",
    "                )\n",
    "            elif dose_filter_method == 'method2':\n",
    "                valid_mask = self.dose_filter_method2(\n",
    "                    row_meta, valid_mask, dose_fold_range, dose_central_measure\n",
    "                )\n",
    "            elif dose_filter_method == 'method3':\n",
    "                valid_mask = self.dose_filter_method3(\n",
    "                    row_meta, valid_mask, dose_percentile_range, \n",
    "                    dose_fold_range, dose_central_measure\n",
    "                )\n",
    "            else:\n",
    "                print(f\"  ‚ö†Ô∏è  Unknown method '{dose_filter_method}', skipping dose filter\")\n",
    "            \n",
    "            n_removed_dose = n_before_dose - valid_mask.sum()\n",
    "            \n",
    "            print(f\"\\n  ‚úÖ Filter 4 Results:\")\n",
    "            print(f\"  ‚úì Removed {n_removed_dose:,} samples\")\n",
    "            print(f\"  ‚úì Remaining samples: {valid_mask.sum():,}\")\n",
    "            print(f\"  ‚úì Remaining compounds: {row_meta.loc[valid_mask, 'pert_id'].nunique():,}\")\n",
    "            \n",
    "            # ÂèØËßÜÂåñÔºàÂ¶ÇÊûúÂêØÁî®Ôºâ\n",
    "            if enable_visualization:\n",
    "                self._visualize_dose_filtering(row_meta, valid_mask, valid_indices)\n",
    "                \n",
    "        elif dose_filter_method is None:\n",
    "            print(f\"  ‚ÑπÔ∏è  Dose filtering disabled (dose_filter_method=None)\")\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è  'pert_dose' not found, skipping dose filter\")\n",
    "        \n",
    "        # ========== Filter 5: Timepoint selection ==========\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FILTER 5: Keep only measurements at {valid_timepoints} hours\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        if 'pert_time' in row_meta.columns:\n",
    "            row_meta['time_numeric'] = pd.to_numeric(row_meta['pert_time'], errors='coerce')\n",
    "            \n",
    "            print(f\"   Available timepoints (numeric):\")\n",
    "            time_counts = row_meta['time_numeric'].value_counts().head(10)\n",
    "            for time_val, count in time_counts.items():\n",
    "                print(f\"     - {time_val} hours: {count:,} samples\")\n",
    "            \n",
    "            time_mask = row_meta['time_numeric'].isin(valid_timepoints)\n",
    "            \n",
    "            if time_mask.sum() == 0:\n",
    "                print(f\"  ‚ö†Ô∏è  No samples match timepoints {valid_timepoints}\")\n",
    "                print(f\"  Skipping timepoint filter...\")\n",
    "            else:\n",
    "                n_before = valid_mask.sum()\n",
    "                valid_mask &= time_mask\n",
    "                n_removed = n_before - valid_mask.sum()\n",
    "                \n",
    "                print(f\"  ‚úì Kept samples at {valid_timepoints} hours\")\n",
    "                print(f\"  Removed {n_removed:,} observations (invalid timepoint)\")\n",
    "                print(f\"  Remaining samples: {valid_mask.sum():,}\")\n",
    "            \n",
    "            del time_mask\n",
    "            gc.collect()\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è  'pert_time' not found, skipping timepoint filter\")\n",
    "\n",
    "        # ========== Filter 6: Remove cell lines with <N compounds ==========\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FILTER 6: Remove cell lines with <{min_compounds_per_cell} compounds\")\n",
    "        print(f\"{'='*80}\")\n",
    "\n",
    "        if cell_id_col is not None:\n",
    "            valid_meta = row_meta[valid_mask]\n",
    "            \n",
    "            compounds_per_cell = valid_meta.groupby(cell_id_col)['pert_id'].nunique()\n",
    "            \n",
    "            print(f\"  Total cell lines before filtering: {len(compounds_per_cell):,}\")\n",
    "            print(f\"  Compounds per cell line (mean): {compounds_per_cell.mean():.1f}\")\n",
    "            print(f\"  Compounds per cell line (median): {compounds_per_cell.median():.0f}\")\n",
    "            print(f\"  Compounds per cell line (min): {compounds_per_cell.min():.0f}\")\n",
    "            print(f\"  Compounds per cell line (max): {compounds_per_cell.max():.0f}\")\n",
    "            \n",
    "            valid_cells = compounds_per_cell[compounds_per_cell >= min_compounds_per_cell].index\n",
    "            \n",
    "            print(f\"\\n  Cell lines with ‚â•{min_compounds_per_cell} compounds: \"\n",
    "                f\"{len(valid_cells):,}/{len(compounds_per_cell):,}\")\n",
    "            \n",
    "            removed_cells = compounds_per_cell[compounds_per_cell < min_compounds_per_cell]\n",
    "            if len(removed_cells) > 0:\n",
    "                print(f\"\\n  Sample removed cell lines (showing up to 10):\")\n",
    "                for cell, n_compounds in removed_cells.head(10).items():\n",
    "                    print(f\"    - {cell}: {n_compounds} compounds\")\n",
    "            \n",
    "            cell_filter_mask = row_meta[cell_id_col].isin(valid_cells)\n",
    "            n_before = valid_mask.sum()\n",
    "            valid_mask &= cell_filter_mask\n",
    "            n_removed = n_before - valid_mask.sum()\n",
    "            \n",
    "            print(f\"\\n  Removed {n_removed:,} observations from {len(removed_cells):,} cell lines\")\n",
    "            print(f\"  Remaining samples: {valid_mask.sum():,}\")\n",
    "            print(f\"  Remaining compounds: {row_meta.loc[valid_mask, 'pert_id'].nunique():,}\")\n",
    "            print(f\"  Remaining cell lines: {len(valid_cells):,}\")\n",
    "            \n",
    "            del valid_meta, compounds_per_cell, cell_filter_mask, removed_cells\n",
    "            gc.collect()\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è  Cell line column not found, skipping filter\")\n",
    "        \n",
    "        # ========== Filter 7: Cell line count ==========\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FILTER 7: Remove compounds in <{min_cell_lines} or >{max_cell_lines} cell lines\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        if cell_id_col is not None:\n",
    "            valid_meta = row_meta[valid_mask]\n",
    "            cell_line_counts = valid_meta.groupby('pert_id')[cell_id_col].nunique()\n",
    "            valid_perts_cell = cell_line_counts[\n",
    "                (cell_line_counts >= min_cell_lines) & \n",
    "                (cell_line_counts <= max_cell_lines)\n",
    "            ].index\n",
    "            \n",
    "            print(f\"  Compounds in {min_cell_lines}-{max_cell_lines} cell lines: \"\n",
    "                  f\"{len(valid_perts_cell):,}/{len(cell_line_counts):,}\")\n",
    "            \n",
    "            cell_mask = row_meta['pert_id'].isin(valid_perts_cell)\n",
    "            n_before = valid_mask.sum()\n",
    "            valid_mask &= cell_mask\n",
    "            n_removed = n_before - valid_mask.sum()\n",
    "            \n",
    "            print(f\"  Removed {n_removed:,} observations\")\n",
    "            print(f\"  Remaining samples: {valid_mask.sum():,}\")\n",
    "            print(f\"  Remaining compounds: {row_meta.loc[valid_mask, 'pert_id'].nunique():,}\")\n",
    "            \n",
    "            del valid_meta, cell_line_counts, cell_mask\n",
    "            gc.collect()\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è  Cell line column not found, skipping\")\n",
    "        \n",
    "        # ========== ÊúÄÁªàÊèêÂèñÊï∞ÊçÆ ==========\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"‚úÖ FINAL DATASET\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        final_matrix = matrix[valid_mask].copy()\n",
    "        final_meta = row_meta[valid_mask].reset_index(drop=True)\n",
    "        \n",
    "        del matrix\n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"  Extracted {len(final_matrix):,} samples\")\n",
    "        print(f\"  Memory usage: {final_matrix.nbytes / (1024**3):.2f} GB\")\n",
    "        \n",
    "        # ÂàõÂª∫Ê†áÁ≠æ\n",
    "        unique_perts = sorted(final_meta['pert_id'].unique())\n",
    "        pert_to_idx = {pert: idx for idx, pert in enumerate(unique_perts)}\n",
    "        labels = np.array([pert_to_idx[p] for p in final_meta['pert_id']], dtype=np.int32)\n",
    "        \n",
    "        final_compounds = len(unique_perts)\n",
    "        final_samples = len(final_matrix)\n",
    "        \n",
    "        if cell_id_col:\n",
    "            final_cells = final_meta[cell_id_col].nunique()\n",
    "        else:\n",
    "            final_cells = 'Unknown'\n",
    "        \n",
    "        print(f\"\\n  Total samples: {final_samples:,}\")\n",
    "        print(f\"  Total compounds: {final_compounds:,}\")\n",
    "        print(f\"  Cell lines: {final_cells}\")\n",
    "        print(f\"  Gene features: {final_matrix.shape[1]}\")\n",
    "        \n",
    "        compound_obs = final_meta.groupby('pert_id').size()\n",
    "        print(f\"  Samples per compound (mean): {final_samples / final_compounds:.1f}\")\n",
    "        print(f\"  Samples per compound (median): {compound_obs.median():.0f}\")\n",
    "        \n",
    "        n_compounds_100plus = (compound_obs > 100).sum()\n",
    "        print(f\"  Compounds with >100 observations: {n_compounds_100plus:,}\")\n",
    "        \n",
    "        print(f\"\\nüìä Comparison with paper (SI page 2):\")\n",
    "        print(f\"  Paper: 425,242 obs, 9,597 compounds, 52 cell lines\")\n",
    "        print(f\"  Ours:  {final_samples:,} obs, {final_compounds:,} compounds, {final_cells} cell lines\")\n",
    "        \n",
    "        if initial_compounds > 0:\n",
    "            retention_rate = (final_compounds / initial_compounds) * 100\n",
    "            print(f\"  Compound retention rate: {retention_rate:.1f}%\")\n",
    "        \n",
    "        # Âü∫Âõ†Âêç\n",
    "        if (self.gene_info is not None) and hasattr(self, \"landmark_col_indices\"):\n",
    "            gi = self.gene_info\n",
    "            landmark_mask = gi['feature_space'] == 'landmark'\n",
    "            landmark_geneinfo = gi[landmark_mask]\n",
    "            gene_names = list(landmark_geneinfo['gene_symbol'].values)\n",
    "            print(f\"  ‚úì Using geneinfo_beta.txt for gene names\")\n",
    "        else:\n",
    "            gene_name_col = None\n",
    "            for possible_col in ['gene_symbol', 'pr_gene_symbol', 'symbol', 'id']:\n",
    "                if possible_col in col_meta.columns:\n",
    "                    gene_name_col = possible_col\n",
    "                    break\n",
    "            if gene_name_col is None:\n",
    "                gene_name_col = col_meta.columns[0]\n",
    "            gene_names = list(col_meta[gene_name_col].values)\n",
    "            print(f\"  ‚ö†Ô∏è Using '{gene_name_col}' from col_meta for gene names\")\n",
    "\n",
    "        training_data = {\n",
    "            'X': final_matrix,\n",
    "            'y': labels,\n",
    "            'folds': np.zeros(len(final_matrix), dtype=np.int32),\n",
    "            'sample_meta': final_meta,\n",
    "            'metadata': final_meta,\n",
    "            'gene_names': gene_names,\n",
    "            'compound_names': list(unique_perts),\n",
    "            'pert_to_idx': pert_to_idx\n",
    "        }\n",
    "\n",
    "        gc.collect()\n",
    "        \n",
    "        return training_data\n",
    "    \n",
    "    def _visualize_dose_filtering(self, row_meta, valid_mask, valid_indices):\n",
    "        \"\"\"ÁîüÊàêÂâÇÈáèËøáÊª§ÁöÑÂèØËßÜÂåñÂõæË°®\"\"\"\n",
    "        print(f\"\\n  üìä Generating dose distribution visualizations...\")\n",
    "        \n",
    "        viz_dir = Path(\"D:/ÁßëÁ†î/Models/drugreflector/visualizations\")\n",
    "        viz_dir.mkdir(exist_ok=True, parents=True)\n",
    "        \n",
    "        # Á°Æ‰øùvalid_indicesÂ≠òÂú®‰∏îÊúâÊïà\n",
    "        if not hasattr(self, 'valid_indices') or valid_indices is None:\n",
    "            valid_indices = np.where(valid_mask)[0]\n",
    "        \n",
    "        # Ëé∑ÂèñËøáÊª§ÂâçÂêéÁöÑÂâÇÈáèÊï∞ÊçÆ\n",
    "        doses_before = row_meta.loc[valid_indices, 'dose_uM'].dropna()\n",
    "        doses_after = row_meta.loc[valid_mask, 'dose_uM'].dropna()\n",
    "        \n",
    "        if len(doses_before) == 0 or len(doses_after) == 0:\n",
    "            print(f\"  ‚ö†Ô∏è  Insufficient dose data for visualization\")\n",
    "            return\n",
    "        \n",
    "        # ===== Âõæ1: ÂâÇÈáèÂàÜÂ∏ÉÂØπÊØî =====\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        \n",
    "        # Â≠êÂõæ1: ËøáÊª§Ââç\n",
    "        ax1 = axes[0]\n",
    "        log_bins = np.logspace(np.log10(max(0.001, doses_before.min())), \n",
    "                               np.log10(doses_before.max()), 50)\n",
    "        ax1.hist(doses_before, bins=log_bins, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "        ax1.set_xscale('log')\n",
    "        ax1.set_xlabel('Dose (¬µM, log scale)', fontsize=11)\n",
    "        ax1.set_ylabel('Number of samples', fontsize=11)\n",
    "        ax1.set_title('Dose Distribution Before Filtering', fontsize=12, fontweight='bold')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Â≠êÂõæ2: ËøáÊª§Âêé\n",
    "        ax2 = axes[1]\n",
    "        log_bins = np.logspace(np.log10(max(0.001, doses_after.min())), \n",
    "                               np.log10(doses_after.max()), 50)\n",
    "        ax2.hist(doses_after, bins=log_bins, edgecolor='black', alpha=0.7, color='forestgreen')\n",
    "        ax2.set_xscale('log')\n",
    "        ax2.set_xlabel('Dose (¬µM, log scale)', fontsize=11)\n",
    "        ax2.set_ylabel('Number of samples', fontsize=11)\n",
    "        ax2.set_title('Dose Distribution After Filtering', fontsize=12, fontweight='bold')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        output_path = viz_dir / \"dose_distribution_comparison.png\"\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"  ‚úì Saved figure: {output_path}\")\n",
    "        plt.close()\n",
    "        \n",
    "        # ===== Âõæ2: TopÂâÇÈáèÂÄºÁªüËÆ° =====\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        \n",
    "        # Â≠êÂõæ1: ËøáÊª§ÂâçTop 15\n",
    "        ax1 = axes[0]\n",
    "        top_doses_before = doses_before.value_counts().head(15)\n",
    "        bars1 = ax1.barh(range(len(top_doses_before)), top_doses_before.values, \n",
    "                         color='steelblue', edgecolor='black', alpha=0.8)\n",
    "        ax1.set_yticks(range(len(top_doses_before)))\n",
    "        ax1.set_yticklabels([f'{d:.2f} ¬µM' for d in top_doses_before.index])\n",
    "        ax1.set_xlabel('Sample Count', fontsize=11)\n",
    "        ax1.set_ylabel('Dose', fontsize=11)\n",
    "        ax1.set_title('Top 15 Doses Before Filtering', fontsize=12, fontweight='bold')\n",
    "        ax1.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        for i, val in enumerate(top_doses_before.values):\n",
    "            ax1.text(val, i, f' {val:,}', va='center', fontsize=8)\n",
    "        \n",
    "        # Â≠êÂõæ2: ËøáÊª§ÂêéTop 15\n",
    "        ax2 = axes[1]\n",
    "        top_doses_after = doses_after.value_counts().head(15)\n",
    "        bars2 = ax2.barh(range(len(top_doses_after)), top_doses_after.values, \n",
    "                         color='forestgreen', edgecolor='black', alpha=0.8)\n",
    "        ax2.set_yticks(range(len(top_doses_after)))\n",
    "        ax2.set_yticklabels([f'{d:.2f} ¬µM' for d in top_doses_after.index])\n",
    "        ax2.set_xlabel('Sample Count', fontsize=11)\n",
    "        ax2.set_ylabel('Dose', fontsize=11)\n",
    "        ax2.set_title('Top 15 Doses After Filtering', fontsize=12, fontweight='bold')\n",
    "        ax2.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        for i, val in enumerate(top_doses_after.values):\n",
    "            ax2.text(val, i, f' {val:,}', va='center', fontsize=8)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        output_path2 = viz_dir / \"top_doses_comparison.png\"\n",
    "        plt.savefig(output_path2, dpi=300, bbox_inches='tight')\n",
    "        print(f\"  ‚úì Saved figure: {output_path2}\")\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"\\n  ‚úÖ Visualization complete!\")\n",
    "        print(f\"     Figures saved to: {viz_dir}\")\n",
    "    \n",
    "    def create_3fold_splits(self, training_data, random_state=42):\n",
    "        \"\"\"ÂàõÂª∫3Êäò‰∫§ÂèâÈ™åËØÅÂàíÂàÜ\"\"\"\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        sample_meta = training_data['sample_meta']\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"üé≤ Creating 3-fold cross-validation splits\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        folds = np.zeros(len(sample_meta), dtype=np.int32)\n",
    "        \n",
    "        for pert_id in sample_meta['pert_id'].unique():\n",
    "            pert_mask = sample_meta['pert_id'] == pert_id\n",
    "            pert_indices = np.where(pert_mask)[0]\n",
    "            \n",
    "            np.random.shuffle(pert_indices)\n",
    "            n_samples = len(pert_indices)\n",
    "            \n",
    "            fold_sizes = [n_samples // 3] * 3\n",
    "            for i in range(n_samples % 3):\n",
    "                fold_sizes[i] += 1\n",
    "            \n",
    "            start_idx = 0\n",
    "            for fold_id, size in enumerate(fold_sizes):\n",
    "                end_idx = start_idx + size\n",
    "                folds[pert_indices[start_idx:end_idx]] = fold_id\n",
    "                start_idx = end_idx\n",
    "        \n",
    "        training_data['folds'] = folds\n",
    "        \n",
    "        print(f\"\\nüìä Fold statistics:\")\n",
    "        for fold_id in range(3):\n",
    "            fold_mask = folds == fold_id\n",
    "            n_samples = fold_mask.sum()\n",
    "            n_compounds = sample_meta[fold_mask]['pert_id'].nunique()\n",
    "            print(f\"   Fold {fold_id}: {n_samples:,} samples, {n_compounds:,} compounds\")\n",
    "        \n",
    "        return training_data\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"‰∏ªÁ®ãÂ∫è - ‰ºòÂåñÁâà v2\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"üß¨ DRUGREFLECTOR DATA PREPROCESSING - Enhanced Version v2\")\n",
    "    print(\"   With intelligent dose filtering methods\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    data_dir = \"D:/ÁßëÁ†î/Models/drugreflector/datasets/LINCS2020\"\n",
    "    loader = LINCS2020DataLoader(data_dir)\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Âä†ËΩΩÂÖÉÊï∞ÊçÆ\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"STEP 1: Loading metadata\")\n",
    "        print(\"=\" * 80)\n",
    "        gene_info = loader.load_gene_info()\n",
    "        cell_info = loader.load_cell_info()\n",
    "        compound_info = loader.load_compound_info()\n",
    "        \n",
    "        # Step 2: Âä†ËΩΩLevel 4 signatures\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"STEP 2: Loading Level 4 signatures\")\n",
    "        print(\"=\" * 80)\n",
    "        matrix, row_meta, col_meta = loader.load_level4_signatures()\n",
    "        \n",
    "        # Step 3: ÂáÜÂ§áËÆ≠ÁªÉÊï∞ÊçÆ\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"STEP 3: Preparing training data\")\n",
    "        print(\"=\" * 80)\n",
    "        training_data = loader.prepare_training_data(\n",
    "            min_observations_per_compound=5,\n",
    "            min_replicate_similarity=0.12,\n",
    "            dose_filter_method='method2',  # 'method1', 'method2', 'method3', or None\n",
    "            dose_percentile_range=(10, 90),  # ‰øùÁïô‰∏≠Èó¥80%\n",
    "            dose_fold_range=(0.2, 50),       # 0.1xÂà∞10x\n",
    "            dose_central_measure='median',   # ‰ΩøÁî®‰∏≠‰ΩçÊï∞\n",
    "            valid_timepoints=[6, 24],\n",
    "            min_cell_lines=5,\n",
    "            max_cell_lines=200,\n",
    "            min_compounds_per_cell=300,\n",
    "            remove_dos=True,\n",
    "            enable_visualization=True  # ÂêØÁî®ÂèØËßÜÂåñ\n",
    "        )\n",
    "        \n",
    "        # Step 4: ÂàõÂª∫3ÊäòÂàíÂàÜ\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"STEP 4: Creating 3-fold splits\")\n",
    "        print(\"=\" * 80)\n",
    "        training_data = loader.create_3fold_splits(training_data)\n",
    "        \n",
    "        # Step 5: ‰øùÂ≠ò\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"STEP 5: Saving processed data\")\n",
    "        print(\"=\" * 80)\n",
    "        output_dir = Path(\"D:/ÁßëÁ†î/Models/drugreflector/processed_data\")\n",
    "        output_dir.mkdir(exist_ok=True, parents=True)\n",
    "        \n",
    "        # Ê†πÊçÆdose_filter_methodÂëΩÂêçÊñá‰ª∂\n",
    "        dose_method = 'method1'  # ‰∏é‰∏äÈù¢ÁöÑËÆæÁΩÆ‰øùÊåÅ‰∏ÄËá¥\n",
    "        output_file = output_dir / f\"training_data_lincs2020_{dose_method}.pkl\"\n",
    "        print(f\"üíæ Saving to: {output_file}\")\n",
    "        \n",
    "        with open(output_file, 'wb') as f:\n",
    "            pickle.dump(training_data, f, protocol=4)\n",
    "        \n",
    "        print(f\"‚úì Saved successfully!\")\n",
    "        print(f\"   File size: {output_file.stat().st_size / (1024**2):.1f} MB\")\n",
    "        \n",
    "        # ÊúÄÁªàÊëòË¶Å\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"‚úÖ DATA PREPARATION COMPLETE!\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"üìÅ Output: {output_file}\")\n",
    "        print(f\"\\nüìä Final dataset:\")\n",
    "        print(f\"   ‚Ä¢ Samples: {len(training_data['X']):,}\")\n",
    "        print(f\"   ‚Ä¢ Compounds: {len(training_data['compound_names']):,}\")\n",
    "        print(f\"   ‚Ä¢ Genes: {training_data['X'].shape[1]}\")\n",
    "        print(f\"   ‚Ä¢ Memory: {training_data['X'].nbytes / (1024**3):.2f} GB\")\n",
    "        \n",
    "        # ‰∏éËÆ∫ÊñáÂØπÊØî\n",
    "        paper_samples = 425242\n",
    "        paper_compounds = 9597\n",
    "        our_samples = len(training_data['X'])\n",
    "        our_compounds = len(training_data['compound_names'])\n",
    "        \n",
    "        print(f\"\\nüìà Comparison with paper:\")\n",
    "        print(f\"   Samples: {our_samples:,} / {paper_samples:,} ({our_samples/paper_samples*100:.1f}%)\")\n",
    "        print(f\"   Compounds: {our_compounds:,} / {paper_compounds:,} ({our_compounds/paper_compounds*100:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nüéØ Ready for training!\")\n",
    "        \n",
    "        return training_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n\" + \"=\" * 80)\n",
    "        print(\"‚ùå ERROR\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"   Type: {type(e).__name__}\")\n",
    "        print(f\"   Message: {e}\")\n",
    "        \n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    training_data = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95c05067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üî¨ APPLYING CHEMICAL FILTERS TO TRAINING DATA\n",
      "================================================================================\n",
      "\n",
      "üìñ Loading training data...\n",
      "   ‚úì Loaded: 590,101 samples, 7,217 compounds\n",
      "   ‚úì Training data contains: 7,217 unique compounds\n",
      "üìñ Loading NIBR filters from: SubstructureFilter_HitTriaging_wPubChemExamples.csv\n",
      "   ‚úì Loaded 444 NIBR filter patterns\n",
      "   ‚úì Successfully added 444 NIBR filters to catalog\n",
      "================================================================================\n",
      "üß™ Chemical Filter Initialized\n",
      "================================================================================\n",
      "Based on DrugReflector SI (page 2)\n",
      "================================================================================\n",
      "\n",
      "üìñ Loading compound information...\n",
      "   ‚úì Loaded 39,321 compounds\n",
      "   ‚úì Columns: ['pert_id', 'cmap_name', 'target', 'moa', 'canonical_smiles', 'inchi_key', 'compound_aliases']\n",
      "   ‚úì Using 'canonical_smiles' for molecular structures\n",
      "   ‚úì Compounds with valid SMILES: 33,531\n",
      "\n",
      "================================================================================\n",
      "üß™ APPLYING ALL CHEMICAL FILTERS\n",
      "================================================================================\n",
      "\n",
      "‚ö†Ô∏è  Filtering only 7,217 compounds present in training data\n",
      "   Compounds to check: 9,558\n",
      "Initial compounds: 9,558\n",
      "\n",
      "================================================================================\n",
      "CHEMICAL FILTER 1: Molecular Weight (60-1000 Da)\n",
      "================================================================================\n",
      "  ‚úì Passed: 7,217 compounds\n",
      "  ‚úó Failed (out of MW range): 0 compounds\n",
      "  ‚ö†Ô∏è  No SMILES: 0 compounds\n",
      "  ‚ö†Ô∏è  Invalid SMILES: 0 compounds\n",
      "\n",
      "================================================================================\n",
      "CHEMICAL FILTER 2: Covalent Motifs (‚â§1)\n",
      "================================================================================\n",
      "Checking 20 covalent SMARTS patterns...\n",
      "  ‚úì Successfully compiled 20 SMARTS patterns\n",
      "  ‚úì Passed: 7,217 compounds\n",
      "  ‚úó Failed (>1 motifs): 0 compounds\n",
      "\n",
      "================================================================================\n",
      "CHEMICAL FILTER 3: NIBR Structure Flags (‚â§9)\n",
      "================================================================================\n",
      "Using official NIBR filter catalog (Schuffenhauer et al. 2020)\n",
      "  ‚úì Catalog contains 444 filter patterns\n",
      "  ‚úì Passed: 7,217 compounds\n",
      "  ‚úó Failed (>9 flags): 0 compounds\n",
      "\n",
      "  Flag distribution (all compounds):\n",
      "    ‚Ä¢ Mean flags: 0.07\n",
      "    ‚Ä¢ Median flags: 0\n",
      "    ‚Ä¢ Max flags: 2\n",
      "\n",
      "  Flag count histogram:\n",
      "    ‚Ä¢ 0 flags: 3,417 compounds\n",
      "    ‚Ä¢ 1 flags: 237 compounds\n",
      "    ‚Ä¢ 2 flags: 8 compounds\n",
      "\n",
      "================================================================================\n",
      "CHEMICAL FILTER 4: BRENK Criteria\n",
      "================================================================================\n",
      "  ‚úì Passed: 7,217 compounds\n",
      "  ‚úó Failed (BRENK violations): 0 compounds\n",
      "\n",
      "================================================================================\n",
      "üìä CHEMICAL FILTER SUMMARY\n",
      "================================================================================\n",
      "  Initial compounds: 9,558\n",
      "  Passed Filter 1 (MW): 7,217\n",
      "  Passed Filter 2 (Covalent): 7,217\n",
      "  Passed Filter 3 (NIBR): 7,217\n",
      "  Passed Filter 4 (BRENK): 7,217\n",
      "  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  ‚úÖ Passed ALL filters: 7,217\n",
      "  ‚ùå Removed: 2,341\n",
      "  Retention rate: 75.5%\n",
      "\n",
      "================================================================================\n",
      "üìä FILTERING TRAINING DATA\n",
      "================================================================================\n",
      "  Compounds in training data: 7,217\n",
      "  Compounds passed filters: 7,217\n",
      "  Compounds to remove: 0\n",
      "  Removing 0 samples from 0 compounds...\n",
      "\n",
      "================================================================================\n",
      "‚úÖ FILTERING COMPLETE\n",
      "================================================================================\n",
      "  Before chemical filters:\n",
      "    ‚Ä¢ Samples: 590,101\n",
      "    ‚Ä¢ Compounds: 7,217\n",
      "  After chemical filters:\n",
      "    ‚Ä¢ Samples: 590,101 (100.0%)\n",
      "    ‚Ä¢ Compounds: 7,217 (100.0%)\n",
      "    ‚Ä¢ Cell lines: 94\n",
      "  Removed:\n",
      "    ‚Ä¢ Samples: 0\n",
      "    ‚Ä¢ Compounds: 0\n",
      "\n",
      "üìä Comparison with paper (SI page 2):\n",
      "  Paper: 425,242 obs, 9,597 compounds, 52 cells\n",
      "  Ours:  590,101 obs, 7,217 compounds, 94 cells\n",
      "  Match rate:\n",
      "    ‚Ä¢ Samples: 138.8%\n",
      "    ‚Ä¢ Compounds: 75.2%\n",
      "    ‚Ä¢ Cell lines: 180.8%\n",
      "\n",
      "üíæ Saving filtered data to: D:\\ÁßëÁ†î\\Models\\drugreflector\\processed_data\\training_data_lincs2020_method1_.pkl\n",
      "   ‚úì Saved successfully! (2481.5 MB)\n",
      "\n",
      "================================================================================\n",
      "‚úÖ CHEMICAL FILTERING COMPLETE!\n",
      "================================================================================\n",
      "üìÅ Final output: D:\\ÁßëÁ†î\\Models\\drugreflector\\processed_data\\training_data_lincs2020_method1_.pkl\n",
      "üéØ Ready for training!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "LINCS 2020 Chemical Filters\n",
    "Ê†πÊçÆDrugReflectorËÆ∫ÊñáSIÁ¨¨2È°µÂÆûÁé∞ÂåñÂ≠¶ËøáÊª§\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# RDKit imports\n",
    "try:\n",
    "    from rdkit import Chem\n",
    "    from rdkit.Chem import Descriptors, AllChem, FilterCatalog\n",
    "    from rdkit.Chem.FilterCatalog import FilterCatalogParams\n",
    "    RDKIT_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  RDKit not installed. Install with: conda install -c conda-forge rdkit\")\n",
    "    RDKIT_AVAILABLE = False\n",
    "\n",
    "\n",
    "class ChemicalFilter:\n",
    "    \"\"\"\n",
    "    ÂåñÂ≠¶ËøáÊª§Âô® - Ê†πÊçÆDrugReflector SIÂÆûÁé∞\n",
    "    \n",
    "    Filters applied (SI page 2):\n",
    "    1. Molecular weight: 60-1000 Da (inclusive)\n",
    "    2. No more than 1 covalent motif (SMARTS-defined)\n",
    "    3. No more than 9 NIBR structure flags\n",
    "    4. Pass BRENK criteria\n",
    "    5. Must not match 30 SMARTS patterns (not disclosed)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, compound_info_path: str, nibr_filter_csv: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        ÂàùÂßãÂåñÂåñÂ≠¶ËøáÊª§Âô®\n",
    "        \n",
    "        ÂèÇÊï∞Ôºö\n",
    "            compound_info_path: compoundinfo_beta.txtË∑ØÂæÑ\n",
    "            nibr_filter_csv: NIBRËøáÊª§Âô®CSVÊñá‰ª∂Ë∑ØÂæÑÔºàÂèØÈÄâÔºâ\n",
    "        \"\"\"\n",
    "        if not RDKIT_AVAILABLE:\n",
    "            raise ImportError(\"RDKit is required for chemical filtering\")\n",
    "        \n",
    "        self.compound_info_path = Path(compound_info_path)\n",
    "        self.compound_info = None\n",
    "        self.filtered_compounds = set()\n",
    "        \n",
    "        # ÂàùÂßãÂåñBRENKËøáÊª§Âô®\n",
    "        self.brenk_catalog = self._init_brenk_filter()\n",
    "        \n",
    "        # ÂàùÂßãÂåñÂÖ±‰ª∑Âü∫Âõ¢SMARTS\n",
    "        self.covalent_smarts = self._init_covalent_smarts()\n",
    "        \n",
    "        # üî• Êñ∞Â¢ûÔºöÂàùÂßãÂåñNIBRËøáÊª§Âô®ÁõÆÂΩï\n",
    "        self.nibr_catalog = self._init_nibr_filter(nibr_filter_csv)\n",
    "        \n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"üß™ Chemical Filter Initialized\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Based on DrugReflector SI (page 2)\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    def _init_brenk_filter(self):\n",
    "        \"\"\"ÂàùÂßãÂåñBRENKËøáÊª§Âô®\"\"\"\n",
    "        params = FilterCatalogParams()\n",
    "        params.AddCatalog(FilterCatalogParams.FilterCatalogs.BRENK)\n",
    "        return FilterCatalog.FilterCatalog(params)\n",
    "    \n",
    "    def _init_covalent_smarts(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        ÂÖ±‰ª∑Âü∫Âõ¢SMARTSÊ®°Âºè\n",
    "        \n",
    "        ÂèÇËÄÉÔºö\n",
    "        - Backman et al. 2019 (ChEMBL structural alerts)\n",
    "        - Common covalent warheads in drug discovery\n",
    "        \"\"\"\n",
    "        return [\n",
    "            # Michael acceptors\n",
    "            'C=CC(=O)',  # Œ±,Œ≤-unsaturated carbonyl\n",
    "            'C=CC(=O)N',  # acrylamide\n",
    "            'C=CC#N',  # acrylonitrile\n",
    "            \n",
    "            # Electrophilic carbonyls\n",
    "            '[C;!R](=O)Cl',  # acyl chloride\n",
    "            '[C;!R](=O)O[C;!R](=O)',  # anhydride\n",
    "            'C(=O)N=[N+]=[N-]',  # acyl azide\n",
    "            \n",
    "            # Epoxides and aziridines\n",
    "            'C1OC1',  # epoxide\n",
    "            'C1NC1',  # aziridine\n",
    "            \n",
    "            # Haloacetamides\n",
    "            'ClCC(=O)N',  # chloroacetamide\n",
    "            'BrCC(=O)N',  # bromoacetamide\n",
    "            \n",
    "            # Aldehydes (reactive)\n",
    "            '[CH;!R]=O',  # aldehyde\n",
    "            \n",
    "            # Isocyanates/isothiocyanates\n",
    "            'N=C=O',  # isocyanate\n",
    "            'N=C=S',  # isothiocyanate\n",
    "            \n",
    "            # Sulfonyl halides\n",
    "            'S(=O)(=O)Cl',  # sulfonyl chloride\n",
    "            'S(=O)(=O)F',  # sulfonyl fluoride\n",
    "            \n",
    "            # Nitriles (activated)\n",
    "            '[C;!R]#N',  # nitrile (non-aromatic)\n",
    "            \n",
    "            # Peroxides\n",
    "            'OO',  # peroxide\n",
    "            \n",
    "            # Beta-lactams (strained)\n",
    "            'C1(=O)NCC1',  # beta-lactam\n",
    "            \n",
    "            # Activated esters\n",
    "            'C(=O)OC(=O)',  # activated ester\n",
    "            \n",
    "            # Quinones\n",
    "            'C1=CC(=O)C=CC1=O',  # quinone\n",
    "        ]\n",
    "    \n",
    "    def _init_nibr_filter(self, csv_path: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        ÂàùÂßãÂåñNIBRËøáÊª§Âô®ÁõÆÂΩïÔºàÂü∫‰∫éÂÆòÊñπ444‰∏™SMARTSÔºâ\n",
    "        \n",
    "        ÂèÇÊï∞Ôºö\n",
    "            csv_path: NIBRËøáÊª§Âô®CSVÊñá‰ª∂Ë∑ØÂæÑÔºàÂèØÈÄâÔºâ\n",
    "            \n",
    "        ËøîÂõûÔºö\n",
    "            FilterCatalogÂØπË±°\n",
    "        \"\"\"\n",
    "        # üî• Á°ÆÂÆöCSVÊñá‰ª∂Ë∑ØÂæÑ\n",
    "        if csv_path is None:\n",
    "            # ÈªòËÆ§Ë∑ØÂæÑÔºöÂΩìÂâçËÑöÊú¨ÁõÆÂΩï‰∏ãÁöÑchem_filterÊñá‰ª∂Â§π\n",
    "            try:\n",
    "                # Â∞ùËØï‰ΩøÁî® __file__ÔºàÂú®ËÑöÊú¨Ê®°Âºè‰∏ãÔºâ\n",
    "                script_dir = Path(__file__).parent\n",
    "            except NameError:\n",
    "                # Âú®‰∫§‰∫íÂºèÁéØÂ¢É‰∏≠‰ΩøÁî®ÂΩìÂâçÂ∑•‰ΩúÁõÆÂΩï\n",
    "                script_dir = Path(os.getcwd())\n",
    "            csv_path = script_dir / \"chem_filter\" / \"SubstructureFilter_HitTriaging_wPubChemExamples.csv\"\n",
    "        else:\n",
    "            csv_path = Path(csv_path)\n",
    "        \n",
    "        if not csv_path.exists():\n",
    "            print(f\"‚ö†Ô∏è  NIBR filter CSV not found: {csv_path}\")\n",
    "            print(f\"   Using fallback: empty NIBR catalog\")\n",
    "            return FilterCatalog.FilterCatalog()\n",
    "        \n",
    "        print(f\"üìñ Loading NIBR filters from: {csv_path.name}\")\n",
    "        \n",
    "        # ËØªÂèñCSVÂπ∂ÊûÑÂª∫FilterCatalog\n",
    "        try:\n",
    "            nibr_df = pd.read_csv(csv_path)\n",
    "            print(f\"   ‚úì Loaded {len(nibr_df)} NIBR filter patterns\")\n",
    "            \n",
    "            nibr_catalog = FilterCatalog.FilterCatalog()\n",
    "            \n",
    "            n_added = 0\n",
    "            for idx, row in nibr_df.iterrows():\n",
    "                try:\n",
    "                    # ÊèêÂèñÂèÇÊï∞\n",
    "                    pattern_name = row['PATTERN_NAME']\n",
    "                    smarts = row['SMARTS']\n",
    "                    min_count = 1 if row['MIN_COUNT'] == 0 else int(row['MIN_COUNT'])\n",
    "                    severity = int(row['SEVERITY_SCORE'])\n",
    "                    covalent = int(row['COVALENT'])\n",
    "                    special_mol = int(row['SPECIAL_MOL'])\n",
    "                    set_name = row['SET_NAME']\n",
    "                    \n",
    "                    # ÊûÑÂª∫ËøáÊª§Âô®ÂêçÁß∞Ôºà‰∏éÂÆòÊñπ‰ª£Á†Å‰∏ÄËá¥Ôºâ\n",
    "                    filter_name = f\"{pattern_name}_min({min_count})__{severity}__{covalent}__{special_mol}\"\n",
    "                    \n",
    "                    # ÂàõÂª∫SMARTSÂåπÈÖçÂô®\n",
    "                    matcher = FilterCatalog.SmartsMatcher(filter_name, smarts, min_count)\n",
    "                    \n",
    "                    # Ê∑ªÂä†Âà∞ÁõÆÂΩï\n",
    "                    entry = FilterCatalog.FilterCatalogEntry(filter_name, matcher)\n",
    "                    entry.SetProp('Scope', set_name)\n",
    "                    entry.SetProp('Severity', str(severity))\n",
    "                    nibr_catalog.AddEntry(entry)\n",
    "                    \n",
    "                    n_added += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Failed to add filter {idx}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            print(f\"   ‚úì Successfully added {n_added} NIBR filters to catalog\")\n",
    "            return nibr_catalog\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error loading NIBR filters: {e}\")\n",
    "            print(f\"   Using fallback: empty NIBR catalog\")\n",
    "            return FilterCatalog.FilterCatalog()\n",
    "    \n",
    "    def load_compound_info(self):\n",
    "        \"\"\"Âä†ËΩΩÂåñÂêàÁâ©‰ø°ÊÅØ\"\"\"\n",
    "        print(f\"üìñ Loading compound information...\")\n",
    "        \n",
    "        if not self.compound_info_path.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"Compound info file not found: {self.compound_info_path}\"\n",
    "            )\n",
    "        \n",
    "        self.compound_info = pd.read_csv(self.compound_info_path, sep='\\t')\n",
    "        \n",
    "        print(f\"   ‚úì Loaded {len(self.compound_info):,} compounds\")\n",
    "        print(f\"   ‚úì Columns: {list(self.compound_info.columns)}\")\n",
    "        \n",
    "        # Ê£ÄÊü•SMILESÂàó\n",
    "        smiles_col = None\n",
    "        for col in ['canonical_smiles', 'smiles', 'SMILES']:\n",
    "            if col in self.compound_info.columns:\n",
    "                smiles_col = col\n",
    "                break\n",
    "        \n",
    "        if smiles_col is None:\n",
    "            raise ValueError(\n",
    "                f\"SMILES column not found. Available columns: {list(self.compound_info.columns)}\"\n",
    "            )\n",
    "        \n",
    "        self.smiles_col = smiles_col\n",
    "        print(f\"   ‚úì Using '{smiles_col}' for molecular structures\")\n",
    "        \n",
    "        # Ê£ÄÊü•ÊúâÊïàSMILESÊï∞Èáè\n",
    "        valid_smiles = self.compound_info[smiles_col].notna().sum()\n",
    "        print(f\"   ‚úì Compounds with valid SMILES: {valid_smiles:,}\")\n",
    "        \n",
    "        return self.compound_info\n",
    "    \n",
    "    def filter_1_molecular_weight(self, min_mw=60, max_mw=1000) -> set:\n",
    "        \"\"\"\n",
    "        Filter 1: ÂàÜÂ≠êÈáèËøáÊª§ (60-1000 Da)\n",
    "        \n",
    "        ÂèÇÊï∞Ôºö\n",
    "            min_mw: ÊúÄÂ∞èÂàÜÂ≠êÈáèÔºàÈªòËÆ§60Ôºâ\n",
    "            max_mw: ÊúÄÂ§ßÂàÜÂ≠êÈáèÔºàÈªòËÆ§1000Ôºâ\n",
    "        \n",
    "        ËøîÂõûÔºö\n",
    "            ÈÄöËøáËøáÊª§ÁöÑÂåñÂêàÁâ©IDÈõÜÂêà\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"CHEMICAL FILTER 1: Molecular Weight ({min_mw}-{max_mw} Da)\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        passed = set()\n",
    "        failed_mw = []\n",
    "        no_smiles = 0\n",
    "        invalid_smiles = 0\n",
    "        \n",
    "        for idx, row in self.compound_info.iterrows():\n",
    "            pert_id = row['pert_id']\n",
    "            smiles = row[self.smiles_col]\n",
    "            \n",
    "            # Ê£ÄÊü•SMILESÊòØÂê¶Â≠òÂú®\n",
    "            if pd.isna(smiles) or smiles == '':\n",
    "                no_smiles += 1\n",
    "                continue\n",
    "            \n",
    "            # Ëß£ÊûêÂàÜÂ≠ê\n",
    "            mol = Chem.MolFromSmiles(str(smiles))\n",
    "            if mol is None:\n",
    "                invalid_smiles += 1\n",
    "                continue\n",
    "            \n",
    "            # ËÆ°ÁÆóÂàÜÂ≠êÈáè\n",
    "            mw = Descriptors.MolWt(mol)\n",
    "            \n",
    "            if min_mw <= mw <= max_mw:\n",
    "                passed.add(pert_id)\n",
    "            else:\n",
    "                failed_mw.append((pert_id, mw))\n",
    "        \n",
    "        print(f\"  ‚úì Passed: {len(passed):,} compounds\")\n",
    "        print(f\"  ‚úó Failed (out of MW range): {len(failed_mw):,} compounds\")\n",
    "        print(f\"  ‚ö†Ô∏è  No SMILES: {no_smiles:,} compounds\")\n",
    "        print(f\"  ‚ö†Ô∏è  Invalid SMILES: {invalid_smiles:,} compounds\")\n",
    "        \n",
    "        if failed_mw:\n",
    "            failed_df = pd.DataFrame(failed_mw, columns=['pert_id', 'MW'])\n",
    "            print(f\"\\n  MW distribution of failed compounds:\")\n",
    "            print(f\"    ‚Ä¢ Mean: {failed_df['MW'].mean():.1f} Da\")\n",
    "            print(f\"    ‚Ä¢ Min: {failed_df['MW'].min():.1f} Da\")\n",
    "            print(f\"    ‚Ä¢ Max: {failed_df['MW'].max():.1f} Da\")\n",
    "        \n",
    "        return passed\n",
    "    \n",
    "    def filter_2_covalent_motifs(self, max_motifs=1) -> set:\n",
    "        \"\"\"\n",
    "        Filter 2: ÂÖ±‰ª∑Âü∫Âõ¢ËøáÊª§ (‚â§1‰∏™)\n",
    "        \n",
    "        ÂèÇÊï∞Ôºö\n",
    "            max_motifs: ÂÖÅËÆ∏ÁöÑÊúÄÂ§ßÂÖ±‰ª∑Âü∫Âõ¢Êï∞ÔºàÈªòËÆ§1Ôºâ\n",
    "        \n",
    "        ËøîÂõûÔºö\n",
    "            ÈÄöËøáËøáÊª§ÁöÑÂåñÂêàÁâ©IDÈõÜÂêà\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"CHEMICAL FILTER 2: Covalent Motifs (‚â§{max_motifs})\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Checking {len(self.covalent_smarts)} covalent SMARTS patterns...\")\n",
    "        \n",
    "        passed = set()\n",
    "        failed_counts = []\n",
    "        \n",
    "        # È¢ÑÁºñËØëSMARTS\n",
    "        smarts_mols = []\n",
    "        for smarts in self.covalent_smarts:\n",
    "            try:\n",
    "                smarts_mol = Chem.MolFromSmarts(smarts)\n",
    "                if smarts_mol:\n",
    "                    smarts_mols.append(smarts_mol)\n",
    "            except:\n",
    "                print(f\"  ‚ö†Ô∏è  Invalid SMARTS: {smarts}\")\n",
    "        \n",
    "        print(f\"  ‚úì Successfully compiled {len(smarts_mols)} SMARTS patterns\")\n",
    "        \n",
    "        for idx, row in self.compound_info.iterrows():\n",
    "            pert_id = row['pert_id']\n",
    "            smiles = row[self.smiles_col]\n",
    "            \n",
    "            if pd.isna(smiles):\n",
    "                continue\n",
    "            \n",
    "            mol = Chem.MolFromSmiles(str(smiles))\n",
    "            if mol is None:\n",
    "                continue\n",
    "            \n",
    "            # ËÆ°Êï∞ÂåπÈÖçÁöÑÂÖ±‰ª∑Âü∫Âõ¢\n",
    "            motif_count = 0\n",
    "            for smarts_mol in smarts_mols:\n",
    "                if mol.HasSubstructMatch(smarts_mol):\n",
    "                    motif_count += 1\n",
    "            \n",
    "            if motif_count <= max_motifs:\n",
    "                passed.add(pert_id)\n",
    "            else:\n",
    "                failed_counts.append(motif_count)\n",
    "        \n",
    "        print(f\"  ‚úì Passed: {len(passed):,} compounds\")\n",
    "        print(f\"  ‚úó Failed (>{max_motifs} motifs): {len(failed_counts):,} compounds\")\n",
    "        \n",
    "        if failed_counts:\n",
    "            print(f\"\\n  Distribution of failed compounds:\")\n",
    "            print(f\"    ‚Ä¢ Mean motifs: {np.mean(failed_counts):.1f}\")\n",
    "            print(f\"    ‚Ä¢ Max motifs: {max(failed_counts)}\")\n",
    "        \n",
    "        return passed\n",
    "    \n",
    "    def filter_3_nibr_flags(self, max_flags=9) -> set:\n",
    "        \"\"\"\n",
    "        Filter 3: NIBRÁªìÊûÑÊ†áËÆ∞ (‚â§9‰∏™)\n",
    "        \n",
    "        ‰ΩøÁî®ÂÆòÊñπNIBR 444‰∏™SMARTSËøáÊª§Âô®\n",
    "        ÂèÇËÄÉÔºöSchuffenhauer et al. 2020, J. Med. Chem.\n",
    "        \n",
    "        ÂèÇÊï∞Ôºö\n",
    "            max_flags: ÂÖÅËÆ∏ÁöÑÊúÄÂ§ßÊ†áËÆ∞Êï∞ÔºàÈªòËÆ§9ÔºåÁ¨¶ÂêàSIË¶ÅÊ±ÇÔºâ\n",
    "        \n",
    "        ËøîÂõûÔºö\n",
    "            ÈÄöËøáËøáÊª§ÁöÑÂåñÂêàÁâ©IDÈõÜÂêà\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"CHEMICAL FILTER 3: NIBR Structure Flags (‚â§{max_flags})\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Using official NIBR filter catalog (Schuffenhauer et al. 2020)\")\n",
    "        \n",
    "        if self.nibr_catalog is None or self.nibr_catalog.GetNumEntries() == 0:\n",
    "            print(f\"  ‚ö†Ô∏è  NIBR catalog is empty, skipping this filter\")\n",
    "            # ËøîÂõûÊâÄÊúâÂåñÂêàÁâ©ÔºàÁõ∏ÂΩì‰∫é‰∏çËøáÊª§Ôºâ\n",
    "            return set(self.compound_info['pert_id'].values)\n",
    "        \n",
    "        print(f\"  ‚úì Catalog contains {self.nibr_catalog.GetNumEntries()} filter patterns\")\n",
    "        \n",
    "        passed = set()\n",
    "        failed_counts = []\n",
    "        flag_distribution = []\n",
    "        \n",
    "        for idx, row in self.compound_info.iterrows():\n",
    "            pert_id = row['pert_id']\n",
    "            smiles = row[self.smiles_col]\n",
    "            \n",
    "            if pd.isna(smiles):\n",
    "                continue\n",
    "            \n",
    "            mol = Chem.MolFromSmiles(str(smiles))\n",
    "            if mol is None:\n",
    "                continue\n",
    "            \n",
    "            # üî• ‰ΩøÁî®FilterCatalogËé∑ÂèñÊâÄÊúâÂåπÈÖç\n",
    "            matches = self.nibr_catalog.GetMatches(mol)\n",
    "            \n",
    "            if len(matches) == 0:\n",
    "                # Ê≤°ÊúâÂåπÈÖç‰ªª‰ΩïflagÔºåÈÄöËøá\n",
    "                flag_count = 0\n",
    "                passed.add(pert_id)\n",
    "            else:\n",
    "                # üî• ËÆ°ÁÆóflagÊï∞ÈáèÔºàÂèÇËÄÉÂÆòÊñπ‰ª£Á†ÅÈÄªËæëÔºâ\n",
    "                # Âè™ÁªüËÆ°severity=1ÁöÑFLAGÔºåseverity=2ÁöÑEXCLUDEÂçïÁã¨Â§ÑÁêÜ\n",
    "                severity_scores = []\n",
    "                for entry in matches:\n",
    "                    # ÊèêÂèñseverity‰ø°ÊÅØ\n",
    "                    desc = entry.GetDescription()\n",
    "                    # Ê†ºÂºèÔºöpattern_name_min(X)__severity__covalent__special_mol\n",
    "                    parts = desc.split('__')\n",
    "                    if len(parts) >= 2:\n",
    "                        severity = int(parts[1])\n",
    "                        severity_scores.append(severity)\n",
    "                \n",
    "                # üî• ÂÖ≥ÈîÆÈÄªËæëÔºà‰∏éÂÆòÊñπ‰ª£Á†Å‰∏ÄËá¥ÔºâÔºö\n",
    "                # Â¶ÇÊûúÊúâseverity=2ÔºàEXCLUDEÔºâÔºåÁõ¥Êé•Ê†áËÆ∞‰∏∫10ÔºàÂøÖÈ°ªÊéíÈô§Ôºâ\n",
    "                if 2 in severity_scores:\n",
    "                    flag_count = 10  # Ë∂ÖËøáÈòàÂÄºÔºåÂøÖÈ°ªÊéíÈô§\n",
    "                else:\n",
    "                    # Âê¶ÂàôÔºåflag_count = severity=1ÁöÑÊï∞Èáè\n",
    "                    flag_count = sum(1 for s in severity_scores if s == 1)\n",
    "                \n",
    "                flag_distribution.append(flag_count)\n",
    "                \n",
    "                if flag_count <= max_flags:\n",
    "                    passed.add(pert_id)\n",
    "                else:\n",
    "                    failed_counts.append(flag_count)\n",
    "        \n",
    "        print(f\"  ‚úì Passed: {len(passed):,} compounds\")\n",
    "        print(f\"  ‚úó Failed (>{max_flags} flags): {len(failed_counts):,} compounds\")\n",
    "        \n",
    "        if flag_distribution:\n",
    "            print(f\"\\n  Flag distribution (all compounds):\")\n",
    "            print(f\"    ‚Ä¢ Mean flags: {np.mean(flag_distribution):.2f}\")\n",
    "            print(f\"    ‚Ä¢ Median flags: {np.median(flag_distribution):.0f}\")\n",
    "            print(f\"    ‚Ä¢ Max flags: {max(flag_distribution)}\")\n",
    "            \n",
    "            # ÁªüËÆ°ÂêÑflagÊï∞ÈáèÁöÑÂàÜÂ∏É\n",
    "            from collections import Counter\n",
    "            flag_counts = Counter(flag_distribution)\n",
    "            print(f\"\\n  Flag count histogram:\")\n",
    "            for count in sorted(flag_counts.keys())[:15]:  # Âè™ÊòæÁ§∫Ââç15‰∏™\n",
    "                n_compounds = flag_counts[count]\n",
    "                print(f\"    ‚Ä¢ {count} flags: {n_compounds:,} compounds\")\n",
    "        \n",
    "        if failed_counts:\n",
    "            print(f\"\\n  Failed compounds statistics:\")\n",
    "            print(f\"    ‚Ä¢ Mean flags: {np.mean(failed_counts):.2f}\")\n",
    "            print(f\"    ‚Ä¢ Max flags: {max(failed_counts)}\")\n",
    "        \n",
    "        return passed\n",
    "    \n",
    "    def filter_4_brenk(self) -> set:\n",
    "        \"\"\"\n",
    "        Filter 4: BRENKËøáÊª§Âô®\n",
    "        \n",
    "        BRENKËøáÊª§Âô®ËØÜÂà´‰∏çËâØÁöÑÂåñÂ≠¶Âü∫Âõ¢ÂíåÂèçÂ∫îÊÄßÂÆòËÉΩÂõ¢\n",
    "        \n",
    "        ËøîÂõûÔºö\n",
    "            ÈÄöËøáËøáÊª§ÁöÑÂåñÂêàÁâ©IDÈõÜÂêà\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"CHEMICAL FILTER 4: BRENK Criteria\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        passed = set()\n",
    "        failed = []\n",
    "        \n",
    "        for idx, row in self.compound_info.iterrows():\n",
    "            pert_id = row['pert_id']\n",
    "            smiles = row[self.smiles_col]\n",
    "            \n",
    "            if pd.isna(smiles):\n",
    "                continue\n",
    "            \n",
    "            mol = Chem.MolFromSmiles(str(smiles))\n",
    "            if mol is None:\n",
    "                continue\n",
    "            \n",
    "            # Ê£ÄÊü•BRENKËøáÊª§Âô®\n",
    "            matches = self.brenk_catalog.GetMatches(mol)\n",
    "            \n",
    "            if len(matches) == 0:\n",
    "                passed.add(pert_id)\n",
    "            else:\n",
    "                failed.append((pert_id, len(matches)))\n",
    "        \n",
    "        print(f\"  ‚úì Passed: {len(passed):,} compounds\")\n",
    "        print(f\"  ‚úó Failed (BRENK violations): {len(failed):,} compounds\")\n",
    "        \n",
    "        return passed\n",
    "    \n",
    "    def apply_all_filters(self, target_compounds: Optional[set] = None) -> set:\n",
    "        \"\"\"\n",
    "        Â∫îÁî®ÊâÄÊúâÂåñÂ≠¶ËøáÊª§Âô®\n",
    "        \n",
    "        ËøîÂõûÔºö\n",
    "            target_compounds: ÈúÄË¶ÅËøáÊª§ÁöÑÂåñÂêàÁâ©ÈõÜÂêàÔºàÂ¶ÇÊûúÊèê‰æõÔºåÂàôÂè™ËøáÊª§Ëøô‰∫õÂåñÂêàÁâ©Ôºâ\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"üß™ APPLYING ALL CHEMICAL FILTERS\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        if self.compound_info is None:\n",
    "            self.load_compound_info()\n",
    "        \n",
    "        #  Â¶ÇÊûúÊåáÂÆö‰∫ÜÁõÆÊ†áÂåñÂêàÁâ©ÔºåÂè™ËøáÊª§Ëøô‰∫õ\n",
    "        if target_compounds is not None:\n",
    "            print(f\"\\n‚ö†Ô∏è  Filtering only {len(target_compounds):,} compounds present in training data\")\n",
    "            # ‰∏¥Êó∂‰øùÂ≠òÂéüÂßãÊï∞ÊçÆ\n",
    "            original_compound_info = self.compound_info.copy()\n",
    "            # Âè™‰øùÁïôÁõÆÊ†áÂåñÂêàÁâ©\n",
    "            self.compound_info = self.compound_info[\n",
    "                self.compound_info['pert_id'].isin(target_compounds)\n",
    "            ]\n",
    "            print(f\"   Compounds to check: {len(self.compound_info):,}\")\n",
    "        \n",
    "        initial_compounds = len(self.compound_info)\n",
    "        print(f\"Initial compounds: {initial_compounds:,}\")\n",
    "        \n",
    "        # Â∫îÁî®ÂêÑ‰∏™ËøáÊª§Âô®\n",
    "        passed_mw = self.filter_1_molecular_weight()\n",
    "        passed_covalent = self.filter_2_covalent_motifs()\n",
    "        passed_nibr = self.filter_3_nibr_flags()\n",
    "        passed_brenk = self.filter_4_brenk()\n",
    "        \n",
    "        # Âèñ‰∫§ÈõÜ\n",
    "        passed_all = passed_mw & passed_covalent & passed_nibr & passed_brenk\n",
    "        \n",
    "        # Â¶ÇÊûúÊòØÈíàÂØπÁõÆÊ†áÂåñÂêàÁâ©ÁöÑËøáÊª§ÔºåÊÅ¢Â§çÂéüÂßãÊï∞ÊçÆ\n",
    "        if target_compounds is not None:\n",
    "            self.compound_info = original_compound_info\n",
    "        \n",
    "        # ÁªüËÆ°\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"üìä CHEMICAL FILTER SUMMARY\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"  Initial compounds: {initial_compounds:,}\")\n",
    "        print(f\"  Passed Filter 1 (MW): {len(passed_mw):,}\")\n",
    "        print(f\"  Passed Filter 2 (Covalent): {len(passed_covalent):,}\")\n",
    "        print(f\"  Passed Filter 3 (NIBR): {len(passed_nibr):,}\")\n",
    "        print(f\"  Passed Filter 4 (BRENK): {len(passed_brenk):,}\")\n",
    "        print(f\"  {'‚îÄ'*80}\")\n",
    "        print(f\"  ‚úÖ Passed ALL filters: {len(passed_all):,}\")\n",
    "        print(f\"  ‚ùå Removed: {initial_compounds - len(passed_all):,}\")\n",
    "        print(f\"  Retention rate: {len(passed_all)/initial_compounds*100:.1f}%\")\n",
    "        \n",
    "        self.filtered_compounds = passed_all\n",
    "        return passed_all\n",
    "\n",
    "\n",
    "def apply_chemical_filters_to_training_data(\n",
    "    training_data_path: str,\n",
    "    compound_info_path: str,\n",
    "    output_path: Optional[str] = None,\n",
    "    nibr_filter_csv: Optional[str] = None\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    ÂØπËÆ≠ÁªÉÊï∞ÊçÆÂ∫îÁî®ÂåñÂ≠¶ËøáÊª§Âô®\n",
    "    \n",
    "    ÂèÇÊï∞Ôºö\n",
    "        training_data_path: ËÆ≠ÁªÉÊï∞ÊçÆpklÊñá‰ª∂Ë∑ØÂæÑ\n",
    "        compound_info_path: compoundinfo_beta.txtË∑ØÂæÑ\n",
    "        output_path: ËæìÂá∫Ë∑ØÂæÑÔºàÂèØÈÄâÔºâ\n",
    "        nibr_filter_csv: NIBRËøáÊª§Âô®CSVË∑ØÂæÑÔºàÂèØÈÄâÔºâ\n",
    "    \n",
    "    ËøîÂõûÔºö\n",
    "        ËøáÊª§ÂêéÁöÑËÆ≠ÁªÉÊï∞ÊçÆÂ≠óÂÖ∏\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üî¨ APPLYING CHEMICAL FILTERS TO TRAINING DATA\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # 1. Âä†ËΩΩËÆ≠ÁªÉÊï∞ÊçÆ\n",
    "    print(f\"üìñ Loading training data...\")\n",
    "    with open(training_data_path, 'rb') as f:\n",
    "        training_data = pickle.load(f)\n",
    "    \n",
    "    initial_samples = len(training_data['X'])\n",
    "    initial_compounds = len(training_data['compound_names'])\n",
    "    \n",
    "    print(f\"   ‚úì Loaded: {initial_samples:,} samples, {initial_compounds:,} compounds\")\n",
    "    \n",
    "    # Ëé∑ÂèñËÆ≠ÁªÉÊï∞ÊçÆ‰∏≠ÁöÑÂåñÂêàÁâ©\n",
    "    metadata = training_data['sample_meta']\n",
    "    training_compounds = set(metadata['pert_id'].unique())\n",
    "    print(f\"   ‚úì Training data contains: {len(training_compounds):,} unique compounds\")\n",
    "    \n",
    "    # 2. Â∫îÁî®ÂåñÂ≠¶ËøáÊª§Âô®ÔºàÂè™ÈíàÂØπËÆ≠ÁªÉÊï∞ÊçÆ‰∏≠ÁöÑÂåñÂêàÁâ©Ôºâ\n",
    "    filter_obj = ChemicalFilter(compound_info_path, nibr_filter_csv=nibr_filter_csv)  # üî• ‰º†ÈÄíÂèÇÊï∞\n",
    "    filter_obj.load_compound_info()\n",
    "    valid_compounds = filter_obj.apply_all_filters(target_compounds=training_compounds)\n",
    "    \n",
    "    # 3. ËøáÊª§ËÆ≠ÁªÉÊï∞ÊçÆ\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üìä FILTERING TRAINING DATA\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # valid_compoundsÁé∞Âú®ÊòØtraining_compoundsÁöÑÂ≠êÈõÜ\n",
    "    print(f\"  Compounds in training data: {len(training_compounds):,}\")\n",
    "    print(f\"  Compounds passed filters: {len(valid_compounds):,}\")\n",
    "    print(f\"  Compounds to remove: {len(training_compounds - valid_compounds):,}\")\n",
    "    \n",
    "    valid_mask = metadata['pert_id'].isin(valid_compounds)\n",
    "    \n",
    "    n_removed_samples = (~valid_mask).sum()\n",
    "    n_removed_compounds = len(training_compounds - valid_compounds)  # ‰ªéËÆ≠ÁªÉÊï∞ÊçÆ‰∏≠ÁßªÈô§ÁöÑ\n",
    "\n",
    "    print(f\"  Removing {n_removed_samples:,} samples from {n_removed_compounds:,} compounds...\")\n",
    "    \n",
    "    # ÂàõÂª∫Êñ∞ÁöÑËÆ≠ÁªÉÊï∞ÊçÆ\n",
    "    filtered_data = {\n",
    "        'X': training_data['X'][valid_mask],\n",
    "        'y': None,  # ÈúÄË¶ÅÈáçÊñ∞ÁºñÁ†Å\n",
    "        'folds': training_data['folds'][valid_mask],\n",
    "        'sample_meta': metadata[valid_mask].reset_index(drop=True),\n",
    "        'metadata': metadata[valid_mask].reset_index(drop=True),\n",
    "        'gene_names': training_data['gene_names'],\n",
    "        'compound_names': None,  # ÈúÄË¶ÅÊõ¥Êñ∞\n",
    "        'pert_to_idx': None  # ÈúÄË¶ÅÈáçÊñ∞ÊûÑÂª∫\n",
    "    }\n",
    "    \n",
    "    # ÈáçÊñ∞ÊûÑÂª∫ÂåñÂêàÁâ©Êò†Â∞ÑÂíåÊ†áÁ≠æ\n",
    "    new_compound_names = sorted(list(valid_compounds))\n",
    "    new_pert_to_idx = {pert: idx for idx, pert in enumerate(new_compound_names)}\n",
    "    new_labels = np.array([\n",
    "        new_pert_to_idx[pert] \n",
    "        for pert in filtered_data['sample_meta']['pert_id']\n",
    "    ], dtype=np.int32)\n",
    "    \n",
    "    filtered_data['compound_names'] = new_compound_names\n",
    "    filtered_data['pert_to_idx'] = new_pert_to_idx\n",
    "    filtered_data['y'] = new_labels\n",
    "    \n",
    "    # ÁªüËÆ°\n",
    "    final_samples = len(filtered_data['X'])\n",
    "    final_compounds = len(new_compound_names)\n",
    "    \n",
    "    if 'cell_iname' in filtered_data['sample_meta'].columns:\n",
    "        final_cells = filtered_data['sample_meta']['cell_iname'].nunique()\n",
    "    else:\n",
    "        final_cells = 'Unknown'\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"‚úÖ FILTERING COMPLETE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"  Before chemical filters:\")\n",
    "    print(f\"    ‚Ä¢ Samples: {initial_samples:,}\")\n",
    "    print(f\"    ‚Ä¢ Compounds: {initial_compounds:,}\")\n",
    "    print(f\"  After chemical filters:\")\n",
    "    print(f\"    ‚Ä¢ Samples: {final_samples:,} ({final_samples/initial_samples*100:.1f}%)\")\n",
    "    print(f\"    ‚Ä¢ Compounds: {final_compounds:,} ({final_compounds/initial_compounds*100:.1f}%)\")\n",
    "    print(f\"    ‚Ä¢ Cell lines: {final_cells}\")\n",
    "    print(f\"  Removed:\")\n",
    "    print(f\"    ‚Ä¢ Samples: {n_removed_samples:,}\")\n",
    "    print(f\"    ‚Ä¢ Compounds: {n_removed_compounds:,}\")\n",
    "    \n",
    "    # ‰∏éËÆ∫ÊñáÂØπÊØî\n",
    "    paper_samples = 425242\n",
    "    paper_compounds = 9597\n",
    "    paper_cells = 52\n",
    "    \n",
    "    print(f\"\\nüìä Comparison with paper (SI page 2):\")\n",
    "    print(f\"  Paper: {paper_samples:,} obs, {paper_compounds:,} compounds, {paper_cells} cells\")\n",
    "    print(f\"  Ours:  {final_samples:,} obs, {final_compounds:,} compounds, {final_cells} cells\")\n",
    "    print(f\"  Match rate:\")\n",
    "    print(f\"    ‚Ä¢ Samples: {final_samples/paper_samples*100:.1f}%\")\n",
    "    print(f\"    ‚Ä¢ Compounds: {final_compounds/paper_compounds*100:.1f}%\")\n",
    "    if isinstance(final_cells, int):\n",
    "        print(f\"    ‚Ä¢ Cell lines: {final_cells/paper_cells*100:.1f}%\")\n",
    "    \n",
    "    # 4. ‰øùÂ≠ò\n",
    "    if output_path is None:\n",
    "        input_path = Path(training_data_path)\n",
    "        output_path = input_path.parent / f\"{input_path.stem}_chemfiltered.pkl\"\n",
    "    else:\n",
    "        output_path = Path(output_path)\n",
    "    \n",
    "    print(f\"\\nüíæ Saving filtered data to: {output_path}\")\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(output_path, 'wb') as f:\n",
    "        pickle.dump(filtered_data, f, protocol=4)\n",
    "    \n",
    "    file_size_mb = output_path.stat().st_size / (1024**2)\n",
    "    print(f\"   ‚úì Saved successfully! ({file_size_mb:.1f} MB)\")\n",
    "    \n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"‰∏ªÁ®ãÂ∫è - Â∫îÁî®ÂåñÂ≠¶ËøáÊª§Âô®\"\"\"\n",
    "    \n",
    "    # ÈÖçÁΩÆË∑ØÂæÑ\n",
    "    data_dir = \"D:/ÁßëÁ†î/Models/drugreflector/datasets/LINCS2020\"\n",
    "    processed_dir = \"D:/ÁßëÁ†î/Models/drugreflector/processed_data\"\n",
    "    \n",
    "    training_data_path = Path(processed_dir) / \"training_data_lincs2020_method1.pkl\"\n",
    "    compound_info_path = Path(data_dir) / \"compoundinfo_beta.txt\"\n",
    "    output_path = Path(processed_dir) / \"training_data_lincs2020_method1_.pkl\"\n",
    "    \n",
    "    # NIBRËøáÊª§Âô®CSVË∑ØÂæÑ\n",
    "    # CSVÊñá‰ª∂Âú®ÂΩìÂâçËÑöÊú¨ÂêåÁ∫ßÁöÑchem_filterÊñá‰ª∂Â§π‰∏ã\n",
    "    try:\n",
    "        # Â∞ùËØï‰ΩøÁî® __file__ÔºàÂú®ËÑöÊú¨Ê®°Âºè‰∏ãÔºâ\n",
    "        script_dir = Path(__file__).parent\n",
    "    except NameError:\n",
    "        # Âú®‰∫§‰∫íÂºèÁéØÂ¢É‰∏≠‰ΩøÁî®ÂΩìÂâçÂ∑•‰ΩúÁõÆÂΩï\n",
    "        script_dir = Path(os.getcwd())\n",
    "    nibr_csv_path = script_dir / \"chem_filter\" / \"SubstructureFilter_HitTriaging_wPubChemExamples.csv\"\n",
    "    \n",
    "    # Ê£ÄÊü•Êñá‰ª∂\n",
    "    if not training_data_path.exists():\n",
    "        print(f\"‚ùå Training data not found: {training_data_path}\")\n",
    "        print(f\"   Please run data preprocessing first!\")\n",
    "        return\n",
    "    \n",
    "    if not compound_info_path.exists():\n",
    "        print(f\"‚ùå Compound info not found: {compound_info_path}\")\n",
    "        return\n",
    "    \n",
    "    # Ê£ÄÊü•NIBR CSVÊñá‰ª∂\n",
    "    if not nibr_csv_path.exists():\n",
    "        print(f\"‚ö†Ô∏è  NIBR filter CSV not found: {nibr_csv_path}\")\n",
    "        print(f\"   Will use fallback NIBR filters (less accurate)\")\n",
    "        nibr_csv_path = None\n",
    "        \n",
    "    # Â∫îÁî®ËøáÊª§Âô®\n",
    "    try:\n",
    "        filtered_data = apply_chemical_filters_to_training_data(\n",
    "            training_data_path=str(training_data_path),\n",
    "            compound_info_path=str(compound_info_path),\n",
    "            output_path=str(output_path)\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"‚úÖ CHEMICAL FILTERING COMPLETE!\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"üìÅ Final output: {output_path}\")\n",
    "        print(f\"üéØ Ready for training!\")\n",
    "        \n",
    "        return filtered_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"‚ùå ERROR\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"   Type: {type(e).__name__}\")\n",
    "        print(f\"   Message: {e}\")\n",
    "        \n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    filtered_data = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
