{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a656975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ§¬ DRUGREFLECTOR DATA PREPROCESSING (Memory-Optimized v2)\n",
      "   Updated for actual LINCS 2020 data format\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "ğŸ”¬ LINCS 2020 Data Loader Initialized (Memory-Optimized v2)\n",
      "================================================================================\n",
      "Data directory: E:\\ç§‘ç ”\\Models\\drugreflector\\datasets\\LINCS2020\n",
      "Dataset: Expanded CMap LINCS Resource 2020\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "STEP 1: Loading metadata\n",
      "================================================================================\n",
      "ğŸ“– Loading gene information...\n",
      "   File: geneinfo_beta.txt\n",
      "   âœ“ Loaded 12,328 genes\n",
      "   âœ“ Columns: ['gene_id', 'gene_symbol', 'ensembl_id', 'gene_title', 'gene_type', 'src', 'feature_space']\n",
      "   âœ“ Using 'feature_space' column to identify landmarks\n",
      "   âœ“ Landmark genes: 978\n",
      "   âœ“ Expected: 978\n",
      "\n",
      "   Landmark column indices (first 10): [2154 2155 2156 2157 2158 2159 2160 2161 2162 2163]\n",
      "   Sample IDs for matching: ['54807', '51203', '9709', '332', '56889']\n",
      "   Sample symbols for matching: ['POLR1C', 'ATF6', 'PPIE', 'KLHL9', 'MAST2']\n",
      "\n",
      "ğŸ“– Loading cell information...\n",
      "   File: cellinfo_beta.txt\n",
      "   âœ“ Loaded 240 cell lines\n",
      "   âœ“ Columns: ['cell_iname', 'cellosaurus_id', 'donor_age', 'donor_age_death', 'donor_disease_age_onset', 'doubling_time', 'growth_medium', 'provider_catalog_id', 'feature_id', 'cell_type']...\n",
      "   âœ“ Unique cell lines (cell_iname): 240\n",
      "\n",
      "   Sample cell lines:\n",
      "     - 1HAE\n",
      "     - AALE\n",
      "     - AG06263_2\n",
      "     - AG06840_A\n",
      "     - AG078N1_1\n",
      "\n",
      "ğŸ“– Loading compound information...\n",
      "   File: compoundinfo_beta.txt\n",
      "   âœ“ Loaded 39,321 compounds\n",
      "   âœ“ Columns: ['pert_id', 'cmap_name', 'target', 'moa', 'canonical_smiles', 'inchi_key', 'compound_aliases']\n",
      "   âœ“ Unique perturbagens: 34419\n",
      "\n",
      "   Sample compounds:\n",
      "     - BRD-A08715367: L-theanine\n",
      "     - BRD-A12237696: L-citrulline\n",
      "     - BRD-A18795974: BRD-A18795974\n",
      "\n",
      "================================================================================\n",
      "STEP 2: Loading Level 4 signatures\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "ğŸ“– Loading Level 4 Signatures\n",
      "================================================================================\n",
      "File: level4_beta_trt_cp_n1805898x12328.gctx\n",
      "\n",
      "ğŸ“– Reading GCTX file: level4_beta_trt_cp_n1805898x12328.gctx\n",
      "   File size: 82.94 GB\n",
      "   âš ï¸  Large file detected. Using memory-optimized loading...\n",
      "ğŸ“Š Inspecting HDF5 structure...\n",
      "   Available keys: ['0']\n",
      "   âœ“ Matrix shape: (1805898, 12328) (samples Ã— genes)\n",
      "\n",
      "ğŸ“‹ Loading metadata...\n",
      "   Reading sample metadata from: /0/META/ROW\n",
      "   Available row fields: ['id']\n",
      "   âœ“ Loaded 1 sample metadata fields\n",
      "   Reading gene metadata from: /0/META/COL\n",
      "   Available col fields: ['id']\n",
      "   âœ“ Loaded 1 gene metadata fields\n",
      "\n",
      "   âš ï¸ Detected that ROW metadata has 12328 entries, matching geneinfo (12328). Swapping ROW/COL metadata so that samples correspond to matrix rows.\n",
      "\n",
      "   Sample metadata columns: ['id']...\n",
      "   Gene metadata columns: ['id']\n",
      "\n",
      "ğŸ”¬ Filtering to landmark genes...\n",
      "   âœ“ Using 978 landmark features out of 12328 total\n",
      "\n",
      "ğŸ¯ Loading data (memory-optimized)...\n",
      "   Reading 978 columns out of 12328...\n",
      "   Loading rows 1,220,000 to 1,805,898... (100.0%)\n",
      "   Finalizing matrix...\n",
      "   âœ“ Final matrix shape: (1805898, 978)\n",
      "   âœ“ Memory usage: 6.58 GB\n",
      "   âœ“ Data type: float32\n",
      "\n",
      "ğŸ“– Loading instance information...\n",
      "   File: instinfo_beta.txt\n",
      "   âœ“ Loaded 3,026,460 instances\n",
      "   âœ“ Columns: ['bead_batch', 'nearest_dose', 'pert_dose', 'pert_dose_unit', 'pert_idose', 'pert_time', 'pert_itime', 'pert_time_unit', 'cell_mfc_name', 'pert_mfc_id']...\n",
      "\n",
      "================================================================================\n",
      "âŒ ERROR DURING DATA PREPARATION\n",
      "================================================================================\n",
      "   Type: ValueError\n",
      "   Message: 'inst_id' column not found in instinfo_beta.txt. Available columns: ['bead_batch', 'nearest_dose', 'pert_dose', 'pert_dose_unit', 'pert_idose', 'pert_time', 'pert_itime', 'pert_time_unit', 'cell_mfc_name', 'pert_mfc_id', 'det_plate', 'det_well', 'rna_plate', 'rna_well', 'count_mean', 'count_cv', 'qc_f_logp', 'qc_iqr', 'qc_slope', 'pert_id', 'sample_id', 'pert_type', 'cell_iname', 'qc_pass', 'dyn_range', 'inv_level_10', 'build_name', 'failure_mode', 'project_code', 'cmap_name']\n",
      "\n",
      "ğŸ’¡ Troubleshooting:\n",
      "   1. Verify all files are in: E:\\ç§‘ç ”\\Models\\drugreflector\\datasets\\LINCS2020\n",
      "   2. Check file formats match the expected TSV structure\n",
      "   3. Ensure sufficient RAM (32GB+ recommended)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\11234\\AppData\\Local\\Temp\\ipykernel_8724\\3830637168.py\", line 944, in main\n",
      "    matrix, row_meta, col_meta = loader.load_level4_signatures()\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\11234\\AppData\\Local\\Temp\\ipykernel_8724\\3830637168.py\", line 441, in load_level4_signatures\n",
      "    self.load_instance_info()\n",
      "  File \"C:\\Users\\11234\\AppData\\Local\\Temp\\ipykernel_8724\\3830637168.py\", line 210, in load_instance_info\n",
      "    raise ValueError(\n",
      "ValueError: 'inst_id' column not found in instinfo_beta.txt. Available columns: ['bead_batch', 'nearest_dose', 'pert_dose', 'pert_dose_unit', 'pert_idose', 'pert_time', 'pert_itime', 'pert_time_unit', 'cell_mfc_name', 'pert_mfc_id', 'det_plate', 'det_well', 'rna_plate', 'rna_well', 'count_mean', 'count_cv', 'qc_f_logp', 'qc_iqr', 'qc_slope', 'pert_id', 'sample_id', 'pert_type', 'cell_iname', 'qc_pass', 'dyn_range', 'inv_level_10', 'build_name', 'failure_mode', 'project_code', 'cmap_name']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "LINCS 2020æ•°æ®åŠ è½½å’Œé¢„å¤„ç†è„šæœ¬ - å†…å­˜ä¼˜åŒ–ç‰ˆï¼ˆä¿®æ­£ç‰ˆï¼‰\n",
    "æ ¹æ®å®é™…æ•°æ®æ ¼å¼ä¿®æ­£å­—æ®µå\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import gzip\n",
    "import shutil\n",
    "import glob\n",
    "from scipy.spatial.distance import cosine, pdist, squareform\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, Tuple, Optional\n",
    "import warnings\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class LINCS2020DataLoader:\n",
    "    \"\"\"\n",
    "    åŠ è½½å’Œé¢„å¤„ç†LINCS 2020æ•°æ® - å†…å­˜ä¼˜åŒ–ç‰ˆï¼ˆä¿®æ­£ç‰ˆï¼‰\n",
    "    ä¸¥æ ¼æŒ‰ç…§å®é™…æ•°æ®æ ¼å¼å¤„ç†\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.gene_info = None\n",
    "        self.cell_info = None\n",
    "        self.compound_info = None\n",
    "        self.inst_info = None  \n",
    "        self.signatures = None\n",
    "        self.decompressed_files = []\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ğŸ”¬ LINCS 2020 Data Loader Initialized (Memory-Optimized v2)\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Data directory: {self.data_dir}\")\n",
    "        print(f\"Dataset: Expanded CMap LINCS Resource 2020\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    def load_gene_info(self):\n",
    "        \"\"\"\n",
    "        åŠ è½½åŸºå› ä¿¡æ¯ï¼Œè·å–978ä¸ªlandmark genes\n",
    "        \n",
    "        å®é™…å­—æ®µï¼š\n",
    "        - gene_id: åŸºå› ID (å¦‚: 16, 23, 25)\n",
    "        - gene_symbol: åŸºå› ç¬¦å· (å¦‚: AARS, ABCF1, ABL1)\n",
    "        - feature_space: 'landmark' æˆ– 'inferred'\n",
    "        \"\"\"\n",
    "        gene_file = self.data_dir / \"geneinfo_beta.txt\"\n",
    "        \n",
    "        if not gene_file.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"âŒ Gene info file not found: {gene_file}\\n\"\n",
    "                f\"   Please download 'geneinfo_beta.txt' from:\\n\"\n",
    "                f\"   https://clue.io/data/CMap2020#LINCS2020\"\n",
    "            )\n",
    "        \n",
    "        print(f\"ğŸ“– Loading gene information...\")\n",
    "        print(f\"   File: {gene_file.name}\")\n",
    "        \n",
    "        # è¯»å–åŸºå› ä¿¡æ¯\n",
    "        gene_info = pd.read_csv(gene_file, sep='\\t')\n",
    "        \n",
    "        print(f\"   âœ“ Loaded {len(gene_info):,} genes\")\n",
    "        print(f\"   âœ“ Columns: {list(gene_info.columns)}\")\n",
    "        \n",
    "        # ç­›é€‰landmark genes (feature_space == 'landmark')\n",
    "        if 'feature_space' in gene_info.columns:\n",
    "            landmark_mask = gene_info['feature_space'] == 'landmark'\n",
    "            landmark_genes = gene_info[landmark_mask].copy()\n",
    "            print(f\"   âœ“ Using 'feature_space' column to identify landmarks\")\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Cannot identify landmark genes. 'feature_space' column not found.\\n\"\n",
    "                f\"Available columns: {list(gene_info.columns)}\"\n",
    "            )\n",
    "\n",
    "        print(f\"   âœ“ Landmark genes: {len(landmark_genes):,}\")\n",
    "        print(f\"   âœ“ Expected: 978\")\n",
    "\n",
    "        # === å…³é”®æ”¹åŠ¨ï¼šä¿å­˜â€œè¡Œå·â€ä½œä¸ºåˆ—ç´¢å¼•ï¼Œè€Œä¸æ˜¯å»åŒ¹é… GCTX çš„ meta ===\n",
    "        # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ geneinfo_beta.txt çš„è¡Œé¡ºåºå’Œ GCTX çš„åˆ—é¡ºåºä¸€è‡´\n",
    "        self.landmark_col_indices = np.where(landmark_mask.values)[0]\n",
    "        print(f\"\\n   Landmark column indices (first 10): {self.landmark_col_indices[:10]}\")\n",
    "\n",
    "        # è¿™äº›é›†åˆä»¥åå¦‚æœè¿˜æƒ³åš debug å¯ä»¥ä¿ç•™\n",
    "        self.landmark_gene_ids = set(landmark_genes['gene_id'].astype(str).values)\n",
    "        self.landmark_gene_symbols = set(landmark_genes['gene_symbol'].astype(str).values)\n",
    "\n",
    "        print(f\"   Sample IDs for matching: {list(self.landmark_gene_ids)[:5]}\")\n",
    "        print(f\"   Sample symbols for matching: {list(self.landmark_gene_symbols)[:5]}\")\n",
    "\n",
    "        # === å…³é”®æ”¹åŠ¨ï¼šgene_info ä¿å­˜â€œå…¨è¡¨â€ï¼Œè€Œä¸æ˜¯åªä¿å­˜ landmark å­é›† ===\n",
    "        self.gene_info = gene_info\n",
    "        return gene_info\n",
    "\n",
    "    \n",
    "    def load_cell_info(self):\n",
    "        \"\"\"\n",
    "        åŠ è½½ç»†èƒç³»ä¿¡æ¯\n",
    "        \n",
    "        å®é™…å­—æ®µï¼š\n",
    "        - cell_iname: ç»†èƒç³»åç§°\n",
    "        - cell_lineage: ç»†èƒè°±ç³»\n",
    "        - primary_disease: ä¸»è¦ç–¾ç—…\n",
    "        \"\"\"\n",
    "        cell_file = self.data_dir / \"cellinfo_beta.txt\"\n",
    "        \n",
    "        if not cell_file.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"âŒ Cell info file not found: {cell_file}\\n\"\n",
    "                f\"   Please download 'cellinfo_beta.txt' from:\\n\"\n",
    "                f\"   https://clue.io/data/CMap2020#LINCS2020\"\n",
    "            )\n",
    "        \n",
    "        print(f\"\\nğŸ“– Loading cell information...\")\n",
    "        print(f\"   File: {cell_file.name}\")\n",
    "        \n",
    "        cell_info = pd.read_csv(cell_file, sep='\\t')\n",
    "        \n",
    "        print(f\"   âœ“ Loaded {len(cell_info):,} cell lines\")\n",
    "        print(f\"   âœ“ Columns: {list(cell_info.columns[:10])}...\")\n",
    "        \n",
    "        # ç¡®å®šç»†èƒç³»IDåˆ—\n",
    "        if 'cell_iname' in cell_info.columns:\n",
    "            print(f\"   âœ“ Unique cell lines (cell_iname): {cell_info['cell_iname'].nunique()}\")\n",
    "        \n",
    "        # æ˜¾ç¤ºç¤ºä¾‹\n",
    "        print(f\"\\n   Sample cell lines:\")\n",
    "        for cell in cell_info['cell_iname'].head(5).values:\n",
    "            print(f\"     - {cell}\")\n",
    "        \n",
    "        self.cell_info = cell_info\n",
    "        return cell_info\n",
    "    \n",
    "    def load_compound_info(self):\n",
    "        \"\"\"\n",
    "        åŠ è½½åŒ–åˆç‰©ä¿¡æ¯\n",
    "        \n",
    "        å®é™…å­—æ®µï¼š\n",
    "        - pert_id: æ‰°åŠ¨ID (BRD-XXXXXXXXX)\n",
    "        - cmap_name: åŒ–åˆç‰©åç§°\n",
    "        - canonical_smiles: SMILESç»“æ„\n",
    "        - target: é¶ç‚¹\n",
    "        - moa: ä½œç”¨æœºåˆ¶\n",
    "        \"\"\"\n",
    "        compound_file = self.data_dir / \"compoundinfo_beta.txt\"\n",
    "        \n",
    "        if not compound_file.exists():\n",
    "            print(f\"âš ï¸  Compound info file not found: {compound_file}\")\n",
    "            print(f\"   This file is optional but recommended.\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"\\nğŸ“– Loading compound information...\")\n",
    "        print(f\"   File: {compound_file.name}\")\n",
    "        \n",
    "        compound_info = pd.read_csv(compound_file, sep='\\t')\n",
    "        \n",
    "        print(f\"   âœ“ Loaded {len(compound_info):,} compounds\")\n",
    "        print(f\"   âœ“ Columns: {list(compound_info.columns)}\")\n",
    "        \n",
    "        if 'pert_id' in compound_info.columns:\n",
    "            print(f\"   âœ“ Unique perturbagens: {compound_info['pert_id'].nunique()}\")\n",
    "        \n",
    "        # æ˜¾ç¤ºç¤ºä¾‹\n",
    "        print(f\"\\n   Sample compounds:\")\n",
    "        for _, row in compound_info.head(3).iterrows():\n",
    "            name = row.get('cmap_name', 'Unknown')\n",
    "            print(f\"     - {row['pert_id']}: {name}\")\n",
    "        \n",
    "        self.compound_info = compound_info\n",
    "        return compound_info\n",
    "    \n",
    "    def load_instance_info(self):\n",
    "        \"\"\"\n",
    "        åŠ è½½å®ä¾‹ï¼ˆwell-levelï¼‰ä¿¡æ¯ï¼šinstinfo_beta.txt\n",
    "\n",
    "        å…³é”®å­—æ®µï¼ˆLINCS2020ï¼‰ï¼š\n",
    "        - sample_id: ä¸ Level4 GCTX ä¸­ ROW/COL meta é‡Œçš„ id å¯¹åº”\n",
    "        - pert_id:   åŒ–åˆç‰© IDï¼ˆBRD-...ï¼‰\n",
    "        - cell_id: ç»†èƒç³» ID\n",
    "        - pert_time: å¤„ç†æ—¶é—´\n",
    "        - pert_dose: å‰‚é‡\n",
    "        \"\"\"\n",
    "        inst_file = self.data_dir / \"instinfo_beta.txt\"\n",
    "\n",
    "        if not inst_file.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"âŒ Instance info file not found: {inst_file}\\n\"\n",
    "                f\"   Please download 'instinfo_beta.txt' from:\\n\"\n",
    "                f\"   https://clue.io/data/CMap2020#LINCS2020\\n\"\n",
    "                f\"   It is required to obtain pert_id / cell_iname / dose / time for Level4.\"\n",
    "            )\n",
    "\n",
    "        print(f\"\\nğŸ“– Loading instance information...\")\n",
    "        print(f\"   File: {inst_file.name}\")\n",
    "\n",
    "        inst_info = pd.read_csv(inst_file, sep='\\t')\n",
    "\n",
    "        print(f\"   âœ“ Loaded {len(inst_info):,} instances\")\n",
    "        print(f\"   âœ“ Columns: {list(inst_info.columns[:10])}...\")\n",
    "\n",
    "        # âœ… å…³é”®ä¿®æ­£ï¼šLINCS2020 ç”¨çš„æ˜¯ sample_idï¼Œè€Œä¸æ˜¯ inst_id\n",
    "        if 'inst_id' in inst_info.columns:\n",
    "            self.instance_join_col = 'inst_id'\n",
    "        elif 'sample_id' in inst_info.columns:\n",
    "            self.instance_join_col = 'sample_id'\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Neither 'inst_id' nor 'sample_id' found in instinfo_beta.txt. \"\n",
    "                f\"Available columns: {list(inst_info.columns)}\"\n",
    "            )\n",
    "\n",
    "        print(f\"   âœ“ Using '{self.instance_join_col}' as join key to GCTX metadata\")\n",
    "\n",
    "        self.inst_info = inst_info\n",
    "        return inst_info\n",
    "\n",
    "    def decompress_gctx_file(self, gctx_file):\n",
    "        \"\"\"å¦‚æœGCTXæ–‡ä»¶è¢«å‹ç¼©ï¼Œåˆ™è§£å‹\"\"\"\n",
    "        gctx_file = Path(gctx_file)\n",
    "        \n",
    "        if not gctx_file.exists():\n",
    "            raise FileNotFoundError(f\"GCTX file not found: {gctx_file}\")\n",
    "        \n",
    "        if str(gctx_file).endswith('.gz'):\n",
    "            print(f\"âš ï¸  Detected compressed GCTX file: {gctx_file.name}\")\n",
    "            \n",
    "            decompressed_dir = self.data_dir / \"_decompressed\"\n",
    "            decompressed_dir.mkdir(exist_ok=True)\n",
    "            \n",
    "            output_path = decompressed_dir / gctx_file.stem\n",
    "            \n",
    "            if output_path.exists():\n",
    "                print(f\"âœ“ Found existing decompressed file: {output_path.name}\")\n",
    "                return str(output_path)\n",
    "            \n",
    "            print(f\"ğŸ“¦ Decompressing...\")\n",
    "            with gzip.open(gctx_file, 'rb') as f_in:\n",
    "                with open(output_path, 'wb') as f_out:\n",
    "                    shutil.copyfileobj(f_in, f_out, length=8*1024*1024)\n",
    "            \n",
    "            print(f\"âœ“ Decompressed to: {output_path}\")\n",
    "            return str(output_path)\n",
    "        \n",
    "        return str(gctx_file)\n",
    "    \n",
    "    def read_gctx(self, gctx_file, use_landmark_only=True):\n",
    "        \"\"\"\n",
    "        è¯»å–GCTXæ–‡ä»¶ - å†…å­˜ä¼˜åŒ–ç‰ˆ\n",
    "        \n",
    "        å…³é”®ä¼˜åŒ–ï¼š\n",
    "        1. åªè¯»å–landmarkåŸºå› åˆ—\n",
    "        2. ä½¿ç”¨float32\n",
    "        3. åˆ†å—è¯»å–\n",
    "        \"\"\"\n",
    "        gctx_file = str(gctx_file)\n",
    "        print(f\"\\nğŸ“– Reading GCTX file: {Path(gctx_file).name}\")\n",
    "        \n",
    "        file_size_gb = Path(gctx_file).stat().st_size / (1024**3)\n",
    "        print(f\"   File size: {file_size_gb:.2f} GB\")\n",
    "        \n",
    "        if file_size_gb > 50:\n",
    "            print(f\"   âš ï¸  Large file detected. Using memory-optimized loading...\")\n",
    "        \n",
    "        gctx_file = self.decompress_gctx_file(gctx_file)\n",
    "        \n",
    "        with h5py.File(gctx_file, 'r') as f:\n",
    "            print(f\"ğŸ“Š Inspecting HDF5 structure...\")\n",
    "            print(f\"   Available keys: {list(f.keys())}\")\n",
    "            \n",
    "            # å®šä½æ•°æ®çŸ©é˜µ\n",
    "            if '/0/DATA/0/matrix' in f:\n",
    "                matrix_dataset = f['/0/DATA/0/matrix']\n",
    "                row_path = '/0/META/ROW'\n",
    "                col_path = '/0/META/COL'\n",
    "            elif '/matrix' in f:\n",
    "                matrix_dataset = f['/matrix']\n",
    "                row_path = '/row'\n",
    "                col_path = '/col'\n",
    "            else:\n",
    "                raise ValueError(f\"Cannot find matrix in GCTX file. Available keys: {list(f.keys())}\")\n",
    "            \n",
    "            matrix_shape = matrix_dataset.shape\n",
    "            print(f\"   âœ“ Matrix shape: {matrix_shape} (samples Ã— genes)\")\n",
    "            \n",
    "            print(f\"\\nğŸ“‹ Loading metadata...\")\n",
    "            \n",
    "            # è¯»å–æ ·æœ¬å…ƒæ•°æ®ï¼ˆROWï¼‰\n",
    "            sample_meta = {}\n",
    "            if row_path in f:\n",
    "                print(f\"   Reading sample metadata from: {row_path}\")\n",
    "                print(f\"   Available row fields: {list(f[row_path].keys())}\")\n",
    "                \n",
    "                for key in f[row_path].keys():\n",
    "                    data = f[f'{row_path}/{key}'][:]\n",
    "                    if data.dtype.kind in ['S', 'O', 'U']:\n",
    "                        try:\n",
    "                            sample_meta[key] = np.char.decode(data.astype('S'), 'utf-8')\n",
    "                        except:\n",
    "                            sample_meta[key] = data.astype(str)\n",
    "                    else:\n",
    "                        sample_meta[key] = data\n",
    "                \n",
    "                print(f\"   âœ“ Loaded {len(sample_meta)} sample metadata fields\")\n",
    "            else:\n",
    "                raise ValueError(f\"Cannot find row metadata at: {row_path}\")\n",
    "            \n",
    "            # è¯»å–åŸºå› å…ƒæ•°æ®ï¼ˆCOLï¼‰\n",
    "            gene_meta = {}\n",
    "            if col_path in f:\n",
    "                print(f\"   Reading gene metadata from: {col_path}\")\n",
    "                print(f\"   Available col fields: {list(f[col_path].keys())}\")\n",
    "                \n",
    "                for key in f[col_path].keys():\n",
    "                    data = f[f'{col_path}/{key}'][:]\n",
    "                    if data.dtype.kind in ['S', 'O', 'U']:\n",
    "                        try:\n",
    "                            gene_meta[key] = np.char.decode(data.astype('S'), 'utf-8')\n",
    "                        except:\n",
    "                            gene_meta[key] = data.astype(str)\n",
    "                    else:\n",
    "                        gene_meta[key] = data\n",
    "                \n",
    "                print(f\"   âœ“ Loaded {len(gene_meta)} gene metadata fields\")\n",
    "            else:\n",
    "                raise ValueError(f\"Cannot find col metadata at: {col_path}\")\n",
    "            \n",
    "            sample_df = pd.DataFrame(sample_meta)\n",
    "            gene_df = pd.DataFrame(gene_meta)\n",
    "\n",
    "            # === æ–°å¢ï¼šè‡ªåŠ¨æ£€æµ‹ ROW/COL è°æ˜¯â€œæ ·æœ¬â€ã€è°æ˜¯â€œåŸºå› â€ ===\n",
    "            if self.gene_info is not None:\n",
    "                n_features_expected = len(self.gene_info)  # geneinfo_beta.txt æœ‰ 12,328 è¡Œ\n",
    "                # å¦‚æœ ROW çš„è¡Œæ•°åˆšå¥½ç­‰äº geneinfo çš„è¡Œæ•°ï¼Œè¯´æ˜ ROW æ˜¯â€œåŸºå› â€è€Œä¸æ˜¯â€œæ ·æœ¬â€ï¼Œéœ€è¦è°ƒæ¢\n",
    "                if len(sample_df) == n_features_expected and len(gene_df) != n_features_expected:\n",
    "                    print(f\"\\n   âš ï¸ Detected that ROW metadata has {len(sample_df)} entries, \"\n",
    "                        f\"matching geneinfo ({n_features_expected}). Swapping ROW/COL metadata \"\n",
    "                        f\"so that samples correspond to matrix rows.\")\n",
    "                    sample_df, gene_df = gene_df, sample_df\n",
    "\n",
    "            print(f\"\\n   Sample metadata columns: {list(sample_df.columns[:10])}...\")\n",
    "            print(f\"   Gene metadata columns: {list(gene_df.columns)}\")\n",
    "\n",
    "            \n",
    "            # ç¡®å®šlandmarkåŸºå› çš„åˆ—ç´¢å¼•\n",
    "            landmark_col_indices = None\n",
    "            if use_landmark_only:\n",
    "                print(f\"\\nğŸ”¬ Filtering to landmark genes...\")\n",
    "\n",
    "                # ç¡®ä¿å·²ç»åŠ è½½ geneinfoï¼Œå¹¶ä¸”å·²ç»è®¡ç®—å¥½ landmark_col_indices\n",
    "                if (self.gene_info is None) or (not hasattr(self, \"landmark_col_indices\")):\n",
    "                    print(f\"   Loading gene info to get landmark indices...\")\n",
    "                    self.load_gene_info()\n",
    "\n",
    "                # å®‰å…¨æ€§æ£€æŸ¥ï¼šgeneinfo è¡Œæ•°è¦å’Œ GCTX çš„åˆ—æ•°ä¸€è‡´\n",
    "                if len(self.gene_info) != matrix_shape[1]:\n",
    "                    print(f\"   âš ï¸  Warning: geneinfo rows ({len(self.gene_info)}) \"\n",
    "                        f\"!= GCTX feature count ({matrix_shape[1]}). \"\n",
    "                        f\"Please double-check that geneinfo_beta.txt matches this GCTX file.\")\n",
    "                \n",
    "                landmark_col_indices = np.array(self.landmark_col_indices, dtype=int)\n",
    "                print(f\"   âœ“ Using {len(landmark_col_indices)} landmark features \"\n",
    "                    f\"out of {matrix_shape[1]} total\")\n",
    "                \n",
    "            # ğŸ”¥ å†…å­˜ä¼˜åŒ–ï¼šåˆ†å—è¯»å–æ•°æ®\n",
    "            print(f\"\\nğŸ¯ Loading data (memory-optimized)...\")\n",
    "            if landmark_col_indices is not None:\n",
    "                print(f\"   Reading {len(landmark_col_indices)} columns out of {matrix_shape[1]}...\")\n",
    "                chunk_size = 610000  # æ¯æ¬¡è¯»å–61ä¸‡è¡Œ\n",
    "                chunks = []\n",
    "                \n",
    "                for start_idx in range(0, matrix_shape[0], chunk_size):\n",
    "                    end_idx = min(start_idx + chunk_size, matrix_shape[0])\n",
    "                    print(f\"   Loading rows {start_idx:,} to {end_idx:,}... ({end_idx/matrix_shape[0]*100:.1f}%)\", end='\\r')\n",
    "                    \n",
    "                    # è¯»å–æŒ‡å®šè¡Œå’Œåˆ—ï¼Œè½¬ä¸ºfloat32\n",
    "                    chunk = matrix_dataset[start_idx:end_idx, landmark_col_indices].astype(np.float32)\n",
    "                    chunks.append(chunk)\n",
    "                    \n",
    "                    # æ¯10ä¸ªchunkåˆå¹¶ä¸€æ¬¡\n",
    "                    if len(chunks) >= 10:\n",
    "                        print(f\"\\n   Consolidating chunks...\")\n",
    "                        merged = np.vstack(chunks)\n",
    "                        chunks = [merged]\n",
    "                        gc.collect()\n",
    "                \n",
    "                print(f\"\\n   Finalizing matrix...\")\n",
    "                matrix = np.vstack(chunks) if len(chunks) > 1 else chunks[0]\n",
    "                del chunks\n",
    "                gc.collect()\n",
    "            else:\n",
    "                # å…¨é‡è¯»å–\n",
    "                print(f\"   Reading full matrix...\")\n",
    "                matrix = matrix_dataset[:].astype(np.float32)\n",
    "            \n",
    "            print(f\"   âœ“ Final matrix shape: {matrix.shape}\")\n",
    "            print(f\"   âœ“ Memory usage: {matrix.nbytes / (1024**3):.2f} GB\")\n",
    "            print(f\"   âœ“ Data type: {matrix.dtype}\")\n",
    "        \n",
    "        return matrix, sample_df, gene_df\n",
    "    \n",
    "    def load_level4_signatures(self):\n",
    "        \"\"\"åŠ è½½Level 4æ•°æ®\"\"\"\n",
    "        level4_file = self.data_dir / \"level4_beta_trt_cp_n1805898x12328.gctx\"\n",
    "        \n",
    "        if not level4_file.exists():\n",
    "            pattern = self.data_dir / \"level4_beta_trt_cp*.gctx\"\n",
    "            files = glob.glob(str(pattern))\n",
    "            \n",
    "            if not files:\n",
    "                raise FileNotFoundError(\n",
    "                    f\"âŒ Level 4 file not found: {level4_file}\\n\"\n",
    "                    f\"   Please download from: https://clue.io/data/CMap2020#LINCS2020\\n\"\n",
    "                    f\"   Expected file: level4_beta_trt_cp_n1805898x12328.gctx (82.94 GB)\"\n",
    "                )\n",
    "            \n",
    "            level4_file = files[0]\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ğŸ“– Loading Level 4 Signatures\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"File: {level4_file.name}\")\n",
    "        \n",
    "        matrix, sample_meta, gene_meta = self.read_gctx(\n",
    "            level4_file, \n",
    "            use_landmark_only=True\n",
    "        )\n",
    "\n",
    "        # === ç”¨ instinfo_beta è¡¥é½ pert_id / cell_iname / dose / time ===\n",
    "\n",
    "        # 1) æŠŠ GCTX çš„ id é‡å‘½åä¸º sample_idï¼ˆå’Œ instinfo2020 å¯¹é½ï¼‰\n",
    "        if 'id' in sample_meta.columns:\n",
    "            # GCTX ROWï¼ˆæˆ–è€…ç»è¿‡ swap åçš„â€œæ ·æœ¬ metaâ€ï¼‰é‡Œçš„ id å½¢å¦‚\n",
    "            #   HOG001_A549_6H_X4_F1B10:P11\n",
    "            # ä¸ instinfo_beta.txt ä¸­çš„ sample_id ä¸€è‡´\n",
    "            sample_meta = sample_meta.rename(columns={'id': 'sample_id'})\n",
    "        elif 'sample_id' not in sample_meta.columns:\n",
    "            raise ValueError(\n",
    "                \"Cannot find 'id' or 'sample_id' column in GCTX ROW metadata; \"\n",
    "                \"cannot join with instinfo_beta.txt\"\n",
    "            )\n",
    "\n",
    "        # 2) è¯»å– instinfo_beta.txtï¼ˆä¼šåœ¨å†…éƒ¨è®¾ç½® self.instance_join_colï¼‰\n",
    "        if self.inst_info is None:\n",
    "            self.load_instance_info()\n",
    "\n",
    "        inst_info = self.inst_info\n",
    "\n",
    "        # 3) æ ¹æ® instinfo ä¸­çš„ join key å†³å®š merge åˆ—\n",
    "        join_col = getattr(self, \"instance_join_col\", None)\n",
    "        if join_col is None:\n",
    "            # ç†è®ºä¸Šä¸ä¼šèµ°åˆ°è¿™é‡Œï¼Œåªæ˜¯åšä¸ªä¿é™©\n",
    "            if 'sample_id' in inst_info.columns:\n",
    "                join_col = 'sample_id'\n",
    "            else:\n",
    "                join_col = 'inst_id'\n",
    "\n",
    "        # è¿™é‡Œ LINCS2020 çš„æƒ…å†µæ˜¯ï¼šjoin_col == 'sample_id'\n",
    "        if join_col not in sample_meta.columns:\n",
    "            # å¦‚æœ join_col æ˜¯ 'inst_id' è€Œ sample_meta æ²¡æœ‰ï¼Œå°±å†è¡¥ä¸€æ¬¡ä¿é™©\n",
    "            if join_col == 'inst_id' and 'sample_id' in sample_meta.columns:\n",
    "                print(\"   âš ï¸ instance_join_col is 'inst_id' but GCTX meta only has 'sample_id'; \"\n",
    "                    \"using 'sample_id' as join key instead.\")\n",
    "                join_col = 'sample_id'\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Join column '{join_col}' not found in GCTX metadata. \"\n",
    "                    f\"Available columns: {list(sample_meta.columns)}\"\n",
    "                )\n",
    "\n",
    "        print(f\"   ğŸ”— Merging GCTX metadata with instinfo_beta on '{join_col}' ...\")\n",
    "        merged_meta = sample_meta.merge(inst_info, on=join_col, how='left')\n",
    "\n",
    "        if 'pert_id' not in merged_meta.columns:\n",
    "            raise ValueError(\n",
    "                \"After merging with instinfo_beta, 'pert_id' is still missing.\\n\"\n",
    "                \"Please check that instinfo_beta.txt matches this Level4 dataset.\"\n",
    "            )\n",
    "\n",
    "        n_missing = merged_meta['pert_id'].isna().sum()\n",
    "        if n_missing > 0:\n",
    "            print(f\"   âš ï¸ instinfo merge: {n_missing:,} rows have missing pert_id\")\n",
    "\n",
    "        self.signatures = {\n",
    "            'matrix': matrix,\n",
    "            'row_meta': merged_meta,\n",
    "            'col_meta': gene_meta\n",
    "        }\n",
    "\n",
    "        return matrix, merged_meta, gene_meta\n",
    "\n",
    "    \n",
    "    def calculate_cosine_similarity_to_nearest_replicate(\n",
    "        self, \n",
    "        matrix: np.ndarray, \n",
    "        pert_ids: pd.Series,\n",
    "        batch_size: int = 500\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        è®¡ç®—æ¯ä¸ªæ ·æœ¬ä¸å…¶æœ€è¿‘replicateçš„ä½™å¼¦ç›¸ä¼¼åº¦ - å†…å­˜ä¼˜åŒ–ç‰ˆ\n",
    "        \"\"\"\n",
    "        print(f\"\\nğŸ“Š Calculating cosine similarity to nearest replicate (optimized)...\")\n",
    "        n_samples = len(pert_ids)\n",
    "        nearest_similarities = np.zeros(n_samples, dtype=np.float32)\n",
    "        \n",
    "        unique_perts = pert_ids.unique()\n",
    "        print(f\"   Processing {len(unique_perts):,} compounds...\")\n",
    "        \n",
    "        from tqdm import tqdm\n",
    "        \n",
    "        for pert_id in tqdm(unique_perts, desc=\"   Computing similarities\"):\n",
    "            pert_mask = pert_ids == pert_id\n",
    "            pert_indices = np.where(pert_mask)[0]\n",
    "            \n",
    "            if len(pert_indices) < 2:\n",
    "                nearest_similarities[pert_indices] = 0.0\n",
    "                continue\n",
    "            \n",
    "            pert_data = matrix[pert_mask].astype(np.float32)\n",
    "            n_replicates = len(pert_data)\n",
    "            \n",
    "            # åˆ†æ‰¹è®¡ç®—ï¼ˆå¤§åŒ–åˆç‰©ï¼‰\n",
    "            if n_replicates > batch_size:\n",
    "                max_sims = np.full(n_replicates, -np.inf, dtype=np.float32)\n",
    "                \n",
    "                for i in range(0, n_replicates, batch_size):\n",
    "                    end_i = min(i + batch_size, n_replicates)\n",
    "                    batch_data = pert_data[i:end_i]\n",
    "                    \n",
    "                    sim_batch = cosine_similarity(batch_data, pert_data).astype(np.float32)\n",
    "                    \n",
    "                    # å¯¹è§’çº¿è®¾ä¸º-inf\n",
    "                    for local_idx in range(len(batch_data)):\n",
    "                        global_idx = i + local_idx\n",
    "                        sim_batch[local_idx, global_idx] = -np.inf\n",
    "                    \n",
    "                    max_sims[i:end_i] = np.max(sim_batch, axis=1)\n",
    "                    \n",
    "                    del sim_batch\n",
    "                    gc.collect()\n",
    "            else:\n",
    "                # å°åŒ–åˆç‰©ç›´æ¥è®¡ç®—\n",
    "                sim_matrix = cosine_similarity(pert_data).astype(np.float32)\n",
    "                np.fill_diagonal(sim_matrix, -np.inf)\n",
    "                max_sims = np.max(sim_matrix, axis=1)\n",
    "                del sim_matrix\n",
    "            \n",
    "            nearest_similarities[pert_indices] = max_sims\n",
    "            del pert_data, max_sims\n",
    "            gc.collect()\n",
    "        \n",
    "        print(f\"   âœ“ Calculated similarities for {n_samples:,} samples\")\n",
    "        print(f\"   Mean similarity: {nearest_similarities.mean():.4f}\")\n",
    "        print(f\"   Median similarity: {np.median(nearest_similarities):.4f}\")\n",
    "        \n",
    "        return nearest_similarities\n",
    "    \n",
    "    def prepare_training_data(\n",
    "        self, \n",
    "        min_observations_per_compound=5,\n",
    "        min_replicate_similarity=0.12,\n",
    "        dose_range=(1.0, 20.0),\n",
    "        valid_timepoints=['6 h', '24 h'],\n",
    "        min_cell_lines=5,\n",
    "        max_cell_lines=40,\n",
    "        remove_dos=True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        å‡†å¤‡è®­ç»ƒæ•°æ® - å†…å­˜ä¼˜åŒ–ç‰ˆï¼ˆä¿®æ­£ç‰ˆï¼‰\n",
    "        \n",
    "        æ ¹æ®å®é™…æ•°æ®å­—æ®µä¿®æ­£å¤„ç†é€»è¾‘\n",
    "        \"\"\"\n",
    "        if self.signatures is None:\n",
    "            raise ValueError(\"Please load signatures first using load_level4_signatures()\")\n",
    "        \n",
    "        matrix = self.signatures['matrix']\n",
    "        row_meta = self.signatures['row_meta'].copy()\n",
    "        col_meta = self.signatures['col_meta']\n",
    "        \n",
    "        print(f\"\\n[DEBUG] row_meta shape after merge: {row_meta.shape}\")\n",
    "        print(f\"[DEBUG] row_meta columns (first 15): {list(row_meta.columns[:15])}\")\n",
    "        print(f\"[DEBUG] Example row:\")\n",
    "        print(row_meta.iloc[0][['sample_id', 'pert_id', 'pert_type', 'cell_iname', 'pert_time', 'pert_dose']])\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"ğŸ” DRUGREFLECTOR QUALITY CONTROL PIPELINE (Memory-Optimized v2)\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Initial samples: {len(row_meta):,}\")\n",
    "        print(f\"Initial memory: {matrix.nbytes / (1024**3):.2f} GB\")\n",
    "        \n",
    "        # æ‰“å°å¯ç”¨çš„å…ƒæ•°æ®åˆ—\n",
    "        print(f\"\\nğŸ“‹ Available metadata columns:\")\n",
    "        for col in row_meta.columns[:15]:\n",
    "            sample_val = row_meta[col].iloc[0] if len(row_meta) > 0 else 'N/A'\n",
    "            print(f\"   - {col}: {sample_val}\")\n",
    "        if len(row_meta.columns) > 15:\n",
    "            print(f\"   ... and {len(row_meta.columns) - 15} more\")\n",
    "        \n",
    "        # æ£€æŸ¥å¿…éœ€çš„å­—æ®µ\n",
    "        required_fields = ['pert_id']\n",
    "        missing_fields = [f for f in required_fields if f not in row_meta.columns]\n",
    "        if missing_fields:\n",
    "            raise ValueError(f\"Missing required fields in metadata: {missing_fields}\")\n",
    "        \n",
    "        # ç¡®å®šç»†èƒç³»IDå­—æ®µ\n",
    "        cell_id_col = None\n",
    "        for possible_col in ['cell_id', 'cell_iname', 'cell_mfc_name']:\n",
    "            if possible_col in row_meta.columns:\n",
    "                cell_id_col = possible_col\n",
    "                print(f\"\\nâœ“ Using '{cell_id_col}' as cell line identifier\")\n",
    "                break\n",
    "        \n",
    "        if cell_id_col is None:\n",
    "            print(f\"\\nâš ï¸  Warning: Cannot find cell line identifier column\")\n",
    "            print(f\"   Available columns: {list(row_meta.columns)}\")\n",
    "        \n",
    "        # ç´¯ç§¯å¸ƒå°”æ©ç \n",
    "        valid_mask = np.ones(len(row_meta), dtype=bool)\n",
    "        \n",
    "        initial_compounds = row_meta['pert_id'].nunique()\n",
    "        print(f\"\\nInitial compounds: {initial_compounds:,}\")\n",
    "        \n",
    "        # ========== Filter 1: Remove DOS compounds ==========\n",
    "        if remove_dos:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"FILTER 1: Remove DOS compounds\")\n",
    "            print(f\"{'='*80}\")\n",
    "            \n",
    "            # æ–¹æ³•1: é€šè¿‡pert_idè¯†åˆ«DOS (BRD-KXXXXXXXXX é€šå¸¸æ˜¯DOS)\n",
    "            # æ–¹æ³•2: é€šè¿‡cmap_nameè¯†åˆ«\n",
    "            dos_mask = ~row_meta['pert_id'].str.contains('DOS|BRD-K', case=False, na=False)\n",
    "            \n",
    "            # å¦‚æœæœ‰pert_typeå­—æ®µï¼ˆè™½ç„¶ç¤ºä¾‹ä¸­æ²¡æœ‰ï¼Œä½†æŸäº›ç‰ˆæœ¬å¯èƒ½æœ‰ï¼‰\n",
    "            if 'pert_type' in row_meta.columns:\n",
    "                dos_mask &= row_meta['pert_type'].isin(['trt_cp', 'CP'])\n",
    "            \n",
    "            valid_mask &= dos_mask\n",
    "            n_removed = (~dos_mask).sum()\n",
    "            \n",
    "            print(f\"  Removed {n_removed:,} DOS observations\")\n",
    "            print(f\"  Remaining samples: {valid_mask.sum():,}\")\n",
    "            print(f\"  Remaining compounds: {row_meta.loc[valid_mask, 'pert_id'].nunique():,}\")\n",
    "        \n",
    "        # ========== Filter 2: Minimum observations per compound ==========\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FILTER 2: Remove compounds with <{min_observations_per_compound} observations\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        valid_meta = row_meta[valid_mask]\n",
    "        obs_counts = valid_meta.groupby('pert_id').size()\n",
    "        valid_perts = obs_counts[obs_counts >= min_observations_per_compound].index\n",
    "        \n",
    "        print(f\"  Compounds with â‰¥{min_observations_per_compound} observations: \"\n",
    "              f\"{len(valid_perts):,}/{obs_counts.nunique():,}\")\n",
    "        \n",
    "        obs_mask = row_meta['pert_id'].isin(valid_perts)\n",
    "        valid_mask &= obs_mask\n",
    "        \n",
    "        print(f\"  Remaining samples: {valid_mask.sum():,}\")\n",
    "        print(f\"  Remaining compounds: {row_meta.loc[valid_mask, 'pert_id'].nunique():,}\")\n",
    "        \n",
    "        del valid_meta, obs_counts, obs_mask\n",
    "        gc.collect()\n",
    "        \n",
    "        # ========== Filter 3: Cosine similarity ==========\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FILTER 3: Remove observations with cosine similarity <{min_replicate_similarity}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        valid_indices = np.where(valid_mask)[0]\n",
    "        valid_matrix = matrix[valid_mask]\n",
    "        valid_pert_ids = row_meta.loc[valid_mask, 'pert_id'].reset_index(drop=True)\n",
    "        \n",
    "        nearest_similarities = self.calculate_cosine_similarity_to_nearest_replicate(\n",
    "            valid_matrix, \n",
    "            valid_pert_ids\n",
    "        )\n",
    "        \n",
    "        full_similarities = np.zeros(len(row_meta), dtype=np.float32)\n",
    "        full_similarities[valid_indices] = nearest_similarities\n",
    "        \n",
    "        sim_mask = (full_similarities >= min_replicate_similarity) | (~valid_mask)\n",
    "        n_removed_sim = (~sim_mask & valid_mask).sum()\n",
    "        valid_mask &= sim_mask\n",
    "        \n",
    "        print(f\"  Removed {n_removed_sim:,} low-similarity observations\")\n",
    "        print(f\"  Remaining samples: {valid_mask.sum():,}\")\n",
    "        \n",
    "        del valid_matrix, valid_pert_ids, nearest_similarities, full_similarities, sim_mask\n",
    "        gc.collect()\n",
    "        \n",
    "        # ========== Filter 4: Dose selection ==========\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FILTER 4: Select most frequent dose in range {dose_range[0]}-{dose_range[1]} ÂµM\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        if 'pert_dose' in row_meta.columns:\n",
    "            # è§£æå‰‚é‡\n",
    "            row_meta['dose_value'] = pd.to_numeric(\n",
    "                row_meta['pert_dose'].astype(str).str.extract(r'([\\d.]+)')[0], \n",
    "                errors='coerce'\n",
    "            )\n",
    "            \n",
    "            row_meta['dose_unit'] = row_meta['pert_dose'].astype(str).str.extract(r'([a-zA-Z]+)')[0]\n",
    "            \n",
    "            def convert_to_uM(row):\n",
    "                if pd.isna(row['dose_value']):\n",
    "                    return np.nan\n",
    "                value = row['dose_value']\n",
    "                unit = str(row['dose_unit']).lower() if pd.notna(row['dose_unit']) else 'um'\n",
    "                \n",
    "                if 'nm' in unit:\n",
    "                    return value / 1000\n",
    "                elif 'mm' in unit:\n",
    "                    return value * 1000\n",
    "                else:\n",
    "                    return value\n",
    "            \n",
    "            row_meta['dose_uM'] = row_meta.apply(convert_to_uM, axis=1)\n",
    "            \n",
    "            dose_mask = (\n",
    "                (row_meta['dose_uM'] >= dose_range[0]) & \n",
    "                (row_meta['dose_uM'] <= dose_range[1])\n",
    "            )\n",
    "            \n",
    "            print(f\"  Samples in valid dose range: {dose_mask.sum():,}\")\n",
    "            \n",
    "            # å¯¹æ¯ä¸ªåŒ–åˆç‰©é€‰æ‹©æœ€å¸¸è§å‰‚é‡\n",
    "            dose_final_mask = np.zeros(len(row_meta), dtype=bool)\n",
    "            valid_meta = row_meta[valid_mask & dose_mask]\n",
    "            \n",
    "            for pert_id in valid_meta['pert_id'].unique():\n",
    "                pert_data = valid_meta[valid_meta['pert_id'] == pert_id]\n",
    "                dose_counts = pert_data['dose_uM'].value_counts()\n",
    "                \n",
    "                if len(dose_counts) > 0:\n",
    "                    most_common_dose = dose_counts.index[0]\n",
    "                    dose_final_mask |= (\n",
    "                        (row_meta['pert_id'] == pert_id) & \n",
    "                        (row_meta['dose_uM'] == most_common_dose)\n",
    "                    )\n",
    "            \n",
    "            n_before = valid_mask.sum()\n",
    "            valid_mask &= dose_final_mask\n",
    "            n_removed = n_before - valid_mask.sum()\n",
    "            \n",
    "            print(f\"  Removed {n_removed:,} observations (invalid or non-modal dose)\")\n",
    "            print(f\"  Remaining samples: {valid_mask.sum():,}\")\n",
    "            \n",
    "            del valid_meta, dose_mask, dose_final_mask\n",
    "            gc.collect()\n",
    "        else:\n",
    "            print(f\"  âš ï¸  Warning: 'pert_dose' column not found, skipping dose filter\")\n",
    "        \n",
    "        # ========== Filter 5: Timepoint selection ==========\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FILTER 5: Keep only measurements at {valid_timepoints}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        if 'pert_time' in row_meta.columns:\n",
    "            row_meta['time_normalized'] = row_meta['pert_time'].astype(str).str.lower().str.strip()\n",
    "            \n",
    "            # è§„èŒƒåŒ–æ—¶é—´ç‚¹\n",
    "            normalized_valid_times = set()\n",
    "            for t in valid_timepoints:\n",
    "                t_lower = t.lower().strip()\n",
    "                normalized_valid_times.add(t_lower)\n",
    "                normalized_valid_times.add(t_lower.replace(' ', ''))\n",
    "                normalized_valid_times.add(t_lower.replace('h', ' h'))\n",
    "                normalized_valid_times.add(t_lower.replace('h', 'hr'))\n",
    "            \n",
    "            time_mask = row_meta['time_normalized'].isin(normalized_valid_times)\n",
    "            \n",
    "            if time_mask.sum() == 0:\n",
    "                print(f\"  âš ï¸  Warning: No samples match timepoints {valid_timepoints}\")\n",
    "                print(f\"  Available timepoints (top 10):\")\n",
    "                for tp, count in row_meta['pert_time'].value_counts().head(10).items():\n",
    "                    print(f\"    - '{tp}': {count:,} samples\")\n",
    "                print(f\"  Skipping timepoint filter...\")\n",
    "            else:\n",
    "                n_before = valid_mask.sum()\n",
    "                valid_mask &= time_mask\n",
    "                n_removed = n_before - valid_mask.sum()\n",
    "                \n",
    "                print(f\"  Removed {n_removed:,} observations (invalid timepoint)\")\n",
    "                print(f\"  Remaining samples: {valid_mask.sum():,}\")\n",
    "            \n",
    "            del time_mask\n",
    "            gc.collect()\n",
    "        else:\n",
    "            print(f\"  âš ï¸  Warning: 'pert_time' column not found, skipping timepoint filter\")\n",
    "        \n",
    "        # ========== Filter 6: Cell line count ==========\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FILTER 6: Remove compounds in <{min_cell_lines} or >{max_cell_lines} cell lines\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        if cell_id_col is not None:\n",
    "            valid_meta = row_meta[valid_mask]\n",
    "            cell_line_counts = valid_meta.groupby('pert_id')[cell_id_col].nunique()\n",
    "            valid_perts_cell = cell_line_counts[\n",
    "                (cell_line_counts >= min_cell_lines) & \n",
    "                (cell_line_counts <= max_cell_lines)\n",
    "            ].index\n",
    "            \n",
    "            print(f\"  Compounds in {min_cell_lines}-{max_cell_lines} cell lines: \"\n",
    "                  f\"{len(valid_perts_cell):,}/{len(cell_line_counts):,}\")\n",
    "            \n",
    "            cell_mask = row_meta['pert_id'].isin(valid_perts_cell)\n",
    "            n_before = valid_mask.sum()\n",
    "            valid_mask &= cell_mask\n",
    "            n_removed = n_before - valid_mask.sum()\n",
    "            \n",
    "            print(f\"  Removed {n_removed:,} observations\")\n",
    "            print(f\"  Remaining samples: {valid_mask.sum():,}\")\n",
    "            print(f\"  Remaining compounds: {row_meta.loc[valid_mask, 'pert_id'].nunique():,}\")\n",
    "            \n",
    "            del valid_meta, cell_line_counts, cell_mask\n",
    "            gc.collect()\n",
    "        else:\n",
    "            print(f\"  âš ï¸  Warning: Cell line column not found, skipping cell line filter\")\n",
    "        \n",
    "        # ========== æœ€ç»ˆæå–æ•°æ® ==========\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"âœ… FINAL DATASET - Extracting filtered data\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # åªåœ¨æœ€åæå–æ•°æ®\n",
    "        final_matrix = matrix[valid_mask].copy()\n",
    "        final_meta = row_meta[valid_mask].reset_index(drop=True)\n",
    "        \n",
    "        del matrix\n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"  Extracted {len(final_matrix):,} samples\")\n",
    "        print(f\"  Memory usage: {final_matrix.nbytes / (1024**3):.2f} GB\")\n",
    "        \n",
    "        # åˆ›å»ºæ ‡ç­¾\n",
    "        unique_perts = sorted(final_meta['pert_id'].unique())\n",
    "        pert_to_idx = {pert: idx for idx, pert in enumerate(unique_perts)}\n",
    "        labels = np.array([pert_to_idx[p] for p in final_meta['pert_id']], dtype=np.int32)\n",
    "        \n",
    "        final_compounds = len(unique_perts)\n",
    "        final_samples = len(final_matrix)\n",
    "        \n",
    "        if cell_id_col:\n",
    "            final_cells = final_meta[cell_id_col].nunique()\n",
    "        else:\n",
    "            final_cells = 'Unknown'\n",
    "        \n",
    "        print(f\"\\n  Total samples: {final_samples:,}\")\n",
    "        print(f\"  Total compounds: {final_compounds:,}\")\n",
    "        print(f\"  Cell lines: {final_cells}\")\n",
    "        print(f\"  Gene features: {final_matrix.shape[1]}\")\n",
    "        \n",
    "        compound_obs = final_meta.groupby('pert_id').size()\n",
    "        print(f\"  Samples per compound (mean): {final_samples / final_compounds:.1f}\")\n",
    "        print(f\"  Samples per compound (median): {compound_obs.median():.0f}\")\n",
    "        \n",
    "        n_compounds_100plus = (compound_obs > 100).sum()\n",
    "        print(f\"  Compounds with >100 observations: {n_compounds_100plus:,}\")\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Comparison with paper results:\")\n",
    "        print(f\"  Paper: 425,242 obs, 9,597 compounds, 52 cell lines\")\n",
    "        print(f\"  Ours:  {final_samples:,} obs, {final_compounds:,} compounds, {final_cells} cell lines\")\n",
    "        \n",
    "        if initial_compounds > 0:\n",
    "            retention_rate = (final_compounds / initial_compounds) * 100\n",
    "            print(f\"  Compound retention rate: {retention_rate:.1f}%\")\n",
    "        \n",
    "        # === æ–°å¢ï¼šåŸºå› åä¼˜å…ˆä½¿ç”¨ geneinfo_beta.txt ä¸­çš„ landmark gene_symbol ===\n",
    "        if (self.gene_info is not None) and hasattr(self, \"landmark_col_indices\"):\n",
    "            # å…ˆå–å‡ºæ ‡è®°ä¸º landmark çš„è¡Œï¼Œå†æŒ‰ç…§ landmark_col_indices çš„é¡ºåºå– gene_symbol\n",
    "            gi = self.gene_info\n",
    "            landmark_mask = gi['feature_space'] == 'landmark'\n",
    "            landmark_geneinfo = gi[landmark_mask]\n",
    "            \n",
    "            # landmark_geneinfo çš„è¡Œé¡ºåºä¸ self.landmark_col_indices ä¸€è‡´\n",
    "            gene_names = list(landmark_geneinfo['gene_symbol'].values)\n",
    "            print(f\"  Using geneinfo_beta.txt for gene names (landmark gene_symbol)\")\n",
    "        else:\n",
    "            # å›é€€æ–¹æ¡ˆï¼šè¿˜æ˜¯ä» col_meta é‡Œæ‰¾ç‚¹èƒ½ç”¨çš„\n",
    "            gene_name_col = None\n",
    "            for possible_col in ['gene_symbol', 'pr_gene_symbol', 'symbol', 'id', 'gene_id', 'pr_gene_id']:\n",
    "                if possible_col in col_meta.columns:\n",
    "                    gene_name_col = possible_col\n",
    "                    print(f\"  Using '{gene_name_col}' for gene names (from col_meta)\")\n",
    "                    break\n",
    "\n",
    "            if gene_name_col is None:\n",
    "                gene_name_col = col_meta.columns[0]\n",
    "                print(f\"  Warning: Using '{gene_name_col}' as fallback for gene names (from col_meta)\")\n",
    "\n",
    "            gene_names = list(col_meta[gene_name_col].values)\n",
    "\n",
    "        training_data = {\n",
    "            'X': final_matrix,\n",
    "            'y': labels,\n",
    "            'folds': np.zeros(len(final_matrix), dtype=np.int32),\n",
    "            'sample_meta': final_meta,\n",
    "            'metadata': final_meta,\n",
    "            'gene_names': gene_names,\n",
    "            'compound_names': list(unique_perts),\n",
    "            'pert_to_idx': pert_to_idx\n",
    "        }\n",
    "\n",
    "        gc.collect()\n",
    "        \n",
    "        return training_data\n",
    "    \n",
    "    def create_3fold_splits(self, training_data, random_state=42):\n",
    "        \"\"\"åˆ›å»º3æŠ˜äº¤å‰éªŒè¯åˆ’åˆ†\"\"\"\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        sample_meta = training_data['sample_meta']\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ğŸ² Creating 3-fold cross-validation splits\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        folds = np.zeros(len(sample_meta), dtype=np.int32)\n",
    "        \n",
    "        for pert_id in sample_meta['pert_id'].unique():\n",
    "            pert_mask = sample_meta['pert_id'] == pert_id\n",
    "            pert_indices = np.where(pert_mask)[0]\n",
    "            \n",
    "            np.random.shuffle(pert_indices)\n",
    "            n_samples = len(pert_indices)\n",
    "            \n",
    "            fold_sizes = [n_samples // 3] * 3\n",
    "            for i in range(n_samples % 3):\n",
    "                fold_sizes[i] += 1\n",
    "            \n",
    "            start_idx = 0\n",
    "            for fold_id, size in enumerate(fold_sizes):\n",
    "                end_idx = start_idx + size\n",
    "                folds[pert_indices[start_idx:end_idx]] = fold_id\n",
    "                start_idx = end_idx\n",
    "        \n",
    "        training_data['folds'] = folds\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Fold statistics:\")\n",
    "        for fold_id in range(3):\n",
    "            fold_mask = folds == fold_id\n",
    "            n_samples = fold_mask.sum()\n",
    "            n_compounds = sample_meta[fold_mask]['pert_id'].nunique()\n",
    "            print(f\"   Fold {fold_id}: {n_samples:,} samples, {n_compounds:,} compounds\")\n",
    "        \n",
    "        return training_data\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"ä¸»ç¨‹åº - å†…å­˜ä¼˜åŒ–ç‰ˆï¼ˆä¿®æ­£ç‰ˆï¼‰\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ğŸ§¬ DRUGREFLECTOR DATA PREPROCESSING (Memory-Optimized v2)\")\n",
    "    print(\"   Updated for actual LINCS 2020 data format\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    data_dir = \"E:/ç§‘ç ”/Models/drugreflector/datasets/LINCS2020\"\n",
    "    loader = LINCS2020DataLoader(data_dir)\n",
    "    \n",
    "    try:\n",
    "        # Step 1: åŠ è½½å…ƒæ•°æ®\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"STEP 1: Loading metadata\")\n",
    "        print(\"=\" * 80)\n",
    "        gene_info = loader.load_gene_info()\n",
    "        cell_info = loader.load_cell_info()\n",
    "        compound_info = loader.load_compound_info()\n",
    "        \n",
    "        # Step 2: åŠ è½½Level 4 signatures\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"STEP 2: Loading Level 4 signatures\")\n",
    "        print(\"=\" * 80)\n",
    "        matrix, row_meta, col_meta = loader.load_level4_signatures()\n",
    "        \n",
    "        # Step 3: å‡†å¤‡è®­ç»ƒæ•°æ®\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"STEP 3: Preparing training data\")\n",
    "        print(\"=\" * 80)\n",
    "        training_data = loader.prepare_training_data(\n",
    "            min_observations_per_compound=5,\n",
    "            min_replicate_similarity=0.12,\n",
    "            dose_range=(1.0, 20.0),\n",
    "            valid_timepoints=['6 h', '24 h', '6h', '24h', '6hr', '24hr', '6 hr', '24 hr'],\n",
    "            min_cell_lines=5,\n",
    "            max_cell_lines=40,\n",
    "            remove_dos=True\n",
    "        )\n",
    "        \n",
    "        # Step 4: åˆ›å»º3æŠ˜åˆ’åˆ†\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"STEP 4: Creating 3-fold splits\")\n",
    "        print(\"=\" * 80)\n",
    "        training_data = loader.create_3fold_splits(training_data)\n",
    "        \n",
    "        # Step 5: ä¿å­˜\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"STEP 5: Saving processed data\")\n",
    "        print(\"=\" * 80)\n",
    "        output_dir = Path(\"E:/ç§‘ç ”/Models/drugreflector/processed_data\")\n",
    "        output_dir.mkdir(exist_ok=True, parents=True)\n",
    "        \n",
    "        output_file = output_dir / \"training_data_lincs2020_paper_compliant.pkl\"\n",
    "        print(f\"ğŸ’¾ Saving to: {output_file}\")\n",
    "        \n",
    "        with open(output_file, 'wb') as f:\n",
    "            pickle.dump(training_data, f, protocol=4)\n",
    "        \n",
    "        print(f\"âœ“ Saved successfully!\")\n",
    "        print(f\"   File size: {output_file.stat().st_size / (1024**2):.1f} MB\")\n",
    "        \n",
    "        # æœ€ç»ˆæ‘˜è¦\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"âœ… DATA PREPARATION COMPLETE!\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"ğŸ“ Output file: {output_file}\")\n",
    "        print(f\"\\nğŸ“Š Final dataset summary:\")\n",
    "        print(f\"   â€¢ Total samples: {len(training_data['X']):,}\")\n",
    "        print(f\"   â€¢ Total compounds: {len(training_data['compound_names']):,}\")\n",
    "        print(f\"   â€¢ Gene features: {training_data['X'].shape[1]}\")\n",
    "        print(f\"   â€¢ Memory usage: {training_data['X'].nbytes / (1024**3):.2f} GB\")\n",
    "        print(f\"\\nğŸ¯ Ready for training!\")\n",
    "        \n",
    "        return training_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n\" + \"=\" * 80)\n",
    "        print(\"âŒ ERROR DURING DATA PREPARATION\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"   Type: {type(e).__name__}\")\n",
    "        print(f\"   Message: {e}\")\n",
    "        \n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        print(f\"\\nğŸ’¡ Troubleshooting:\")\n",
    "        print(f\"   1. Verify all files are in: {Path(data_dir).absolute()}\")\n",
    "        print(f\"   2. Check file formats match the expected TSV structure\")\n",
    "        print(f\"   3. Ensure sufficient RAM (32GB+ recommended)\")\n",
    "        \n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    training_data = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427e26a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ§¬ DRUGREFLECTOR DATA PREPROCESSING (Memory-Optimized)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "ğŸ”¬ LINCS 2020 Data Loader (Memory-Optimized)\n",
      "================================================================================\n",
      "Data directory: E:\\ç§‘ç ”\\Models\\drugreflector\\datasets\\LINCS2020\n",
      "Max memory: 16 GB\n",
      "Chunk size: 50,000 samples\n",
      "================================================================================\n",
      "\n",
      "ğŸ“– Loading gene information...\n",
      "   âœ“ Landmark genes: 978\n",
      "ğŸ“– Loaded 240 cell lines\n",
      "ğŸ“– Loaded 39,321 compounds\n",
      "\n",
      "ğŸ“– Reading GCTX metadata: level4_beta_trt_cp_n1805898x12328.gctx\n",
      "âœ“ Metadata loaded:\n",
      "  Samples: 12,328\n",
      "  Genes: 1805898\n",
      "  Matrix shape: (1805898, 12328)\n",
      "\n",
      "âŒ Error: LINCS2020DataLoaderOptimized.filter_metadata_first() got an unexpected keyword argument 'min_replicate_similarity'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\11234\\AppData\\Local\\Temp\\ipykernel_25884\\2025916354.py\", line 498, in main\n",
      "    training_data = loader.prepare_training_data(\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\11234\\AppData\\Local\\Temp\\ipykernel_25884\\2025916354.py\", line 390, in prepare_training_data\n",
      "    filtered_meta = self.filter_metadata_first(sample_meta, **filter_params)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: LINCS2020DataLoaderOptimized.filter_metadata_first() got an unexpected keyword argument 'min_replicate_similarity'\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemprop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
