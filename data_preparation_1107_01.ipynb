{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2b3de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 1: Loading gene and cell information\n",
      "================================================================================\n",
      "Total genes: 12328\n",
      "Landmark genes: 978\n",
      "Total cell lines: 98\n",
      "Unique cell IDs: 98\n",
      "\n",
      "================================================================================\n",
      "STEP 2: Loading Level 4 signatures\n",
      "================================================================================\n",
      "ğŸ” Searching for Level 4 file...\n",
      "   Pattern: GSE92742_Broad_LINCS_Level4_ZSPCINF_mlr12k_n*x12328.gctx.gz\n",
      "âœ“ Found: GSE92742_Broad_LINCS_Level4_ZSPCINF_mlr12k_n1319138x12328.gctx.gz\n",
      "\n",
      "ğŸ“– Reading GCTX file: GSE92742_Broad_LINCS_Level4_ZSPCINF_mlr12k_n1319138x12328.gctx.gz\n",
      "âš ï¸  Detected gzip compressed file\n",
      "âœ“ Found existing decompressed file: GSE92742_Broad_LINCS_Level4_ZSPCINF_mlr12k_n1319138x12328.gctx\n",
      "ğŸ“Š Loading matrix from HDF5...\n",
      "âœ“ Matrix shape: (1319138, 12328)\n",
      "ğŸ“‹ Loading metadata...\n",
      "\n",
      "âœ“ Data loaded successfully:\n",
      "  Matrix: (1319138, 12328) (samples Ã— genes)\n",
      "  Samples: 1319138\n",
      "  Genes: 12328\n",
      "\n",
      "ğŸ”¬ Filtering to landmark genes...\n",
      "   Landmark genes to find: 978\n",
      "   Total genes in data: 12328\n",
      "   âœ“ Matched: 978 genes\n",
      "\n",
      "ğŸ¯ Applying filter...\n",
      "   âœ“ Final matrix shape: (1319138, 978)\n",
      "\n",
      "================================================================================\n",
      "STEP 3: Preparing training data (Relaxed filters)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "ğŸ” DRUGREFLECTOR QUALITY CONTROL PIPELINE (RELAXED)\n",
      "   Based on Science 2025 SI with relaxed filtering criteria\n",
      "================================================================================\n",
      "Initial samples: 1,319,138\n",
      "\n",
      "ğŸ“‹ Checking metadata fields...\n",
      "   Available columns: ['id']\n",
      "   Using 'id' as pert_id\n",
      "   Parsing cell_id from 'id' column...\n",
      "   âœ“ Found required columns: pert_id, cell_id\n",
      "\n",
      "================================================================================\n",
      "FILTER 1: Remove DOS (Diversity-Oriented Synthesis) compounds\n",
      "================================================================================\n",
      "  Removed 0 DOS observations\n",
      "  Remaining samples: 1,319,138\n",
      "  Remaining compounds: 1,319,138\n",
      "\n",
      "================================================================================\n",
      "FILTER 2: Remove compounds with <3 observations\n",
      "         (relaxed from paper's 5)\n",
      "================================================================================\n",
      "  Compounds with â‰¥3 observations: 0/1\n",
      "  Remaining samples: 0\n",
      "  Remaining compounds: 0\n",
      "\n",
      "================================================================================\n",
      "FILTER 3: Remove observations with cosine similarity <0.1\n",
      "         (relaxed from paper's 0.12)\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Calculating cosine similarity to nearest replicate...\n",
      "   Processing 0 compounds...\n",
      "   âœ“ Calculated similarities for 0 samples\n",
      "   Mean similarity: nan\n",
      "   Median similarity: nan\n",
      "  Removed 0 low-similarity observations\n",
      "  Remaining samples: 0\n",
      "  Remaining compounds: 0\n",
      "\n",
      "================================================================================\n",
      "FILTER 4: Select most frequent dose in range 0.3-30.0 ÂµM\n",
      "         (relaxed from paper's 1-20 ÂµM)\n",
      "================================================================================\n",
      "  Warning: 'pert_dose' column not found, skipping dose filter\n",
      "\n",
      "================================================================================\n",
      "FILTER 5: Keep only measurements at ['6h', '24h', '6 h', '24 h']\n",
      "================================================================================\n",
      "  Warning: 'pert_time' column not found, skipping timepoint filter\n",
      "\n",
      "================================================================================\n",
      "FILTER 6: Remove compounds in <3 or >50 cell lines\n",
      "         (relaxed from paper's 5-40)\n",
      "================================================================================\n",
      "  Compounds in 3-50 cell lines: 0/0\n",
      "  Remaining samples: 0\n",
      "  Remaining compounds: 0\n",
      "\n",
      "================================================================================\n",
      "âœ… FINAL DATASET (After all filters)\n",
      "================================================================================\n",
      "  Total samples: 0\n",
      "  Total compounds: 0\n",
      "  Cell lines: 0\n",
      "  Gene features: 978\n",
      "\n",
      "================================================================================\n",
      "âŒ ERROR DURING DATA PREPARATION\n",
      "================================================================================\n",
      "   Type: ZeroDivisionError\n",
      "   Message: division by zero\n",
      "\n",
      "ğŸ“‹ Full traceback:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\11234\\AppData\\Local\\Temp\\ipykernel_28812\\743581544.py\", line 656, in main\n",
      "    training_data = loader.prepare_training_data(\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\11234\\AppData\\Local\\Temp\\ipykernel_28812\\743581544.py\", line 563, in prepare_training_data\n",
      "    print(f\"  Samples per compound (mean): {final_samples / final_compounds:.1f}\")\n",
      "                                            ~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~\n",
      "ZeroDivisionError: division by zero\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "LINCSæ•°æ®åŠ è½½å’Œé¢„å¤„ç†è„šæœ¬ - DrugReflectorè®ºæ–‡å¤ç°ç‰ˆï¼ˆæ”¾å®½ç­›é€‰æ¡ä»¶ï¼‰\n",
    "åŸºäºScience 2025è¡¥å……ææ–™ï¼Œä½†é€‚å½“æ”¾å®½äº†ä¸€äº›ç­›é€‰æ¡ä»¶ä»¥é¿å…è¿‡åº¦è¿‡æ»¤\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import gzip\n",
    "import shutil\n",
    "import glob\n",
    "from scipy.spatial.distance import cosine, pdist, squareform\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class LINCSDataLoader:\n",
    "    \"\"\"åŠ è½½å’Œé¢„å¤„ç†LINCS L1000æ•°æ® - ä¼˜åŒ–ç‰ˆï¼ˆæ”¾å®½ç­›é€‰æ¡ä»¶ï¼‰\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.gene_info = None\n",
    "        self.cell_info = None\n",
    "        self.signatures = None\n",
    "        self.decompressed_files = []\n",
    "        \n",
    "    def load_gene_info(self, dataset='GSE92742'):\n",
    "        \"\"\"åŠ è½½åŸºå› ä¿¡æ¯ï¼Œè·å–978ä¸ªlandmark genes\"\"\"\n",
    "        gene_file = self.data_dir / f\"{dataset}_Broad_LINCS_gene_info.txt.gz\"\n",
    "        \n",
    "        gene_info = pd.read_csv(gene_file, sep='\\t', compression='gzip')\n",
    "        \n",
    "        # ç­›é€‰landmark genes (pr_is_lm == 1)\n",
    "        landmark_genes = gene_info[gene_info['pr_is_lm'] == 1].copy()\n",
    "        \n",
    "        print(f\"Total genes: {len(gene_info)}\")\n",
    "        print(f\"Landmark genes: {len(landmark_genes)}\")\n",
    "        \n",
    "        self.gene_info = landmark_genes\n",
    "        return landmark_genes\n",
    "    \n",
    "    def load_cell_info(self, dataset='GSE92742'):\n",
    "        \"\"\"åŠ è½½ç»†èƒç³»ä¿¡æ¯\"\"\"\n",
    "        cell_file = self.data_dir / f\"{dataset}_Broad_LINCS_cell_info.txt.gz\"\n",
    "        \n",
    "        cell_info = pd.read_csv(cell_file, sep='\\t', compression='gzip')\n",
    "        \n",
    "        print(f\"Total cell lines: {len(cell_info)}\")\n",
    "        print(f\"Unique cell IDs: {cell_info['cell_id'].nunique()}\")\n",
    "        \n",
    "        self.cell_info = cell_info\n",
    "        return cell_info\n",
    "    \n",
    "    def decompress_gzip_file(self, gzip_file):\n",
    "        \"\"\"è§£å‹gzipæ–‡ä»¶åˆ°_decompressedæ–‡ä»¶å¤¹\"\"\"\n",
    "        gzip_file = str(gzip_file)\n",
    "        \n",
    "        decompressed_dir = Path(self.data_dir) / \"_decompressed\"\n",
    "        decompressed_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        original_name = Path(gzip_file).stem\n",
    "        output_path = decompressed_dir / original_name\n",
    "        \n",
    "        if output_path.exists():\n",
    "            print(f\"âœ“ Found existing decompressed file: {output_path.name}\")\n",
    "            self.decompressed_files.append(output_path)\n",
    "            return str(output_path)\n",
    "        \n",
    "        print(f\"ğŸ’¾ Decompressing to: {decompressed_dir}\")\n",
    "        \n",
    "        try:\n",
    "            print(f\"ğŸ“¦ Decompressing {Path(gzip_file).name}...\")\n",
    "            source_size = Path(gzip_file).stat().st_size / (1024**3)\n",
    "            print(f\"   Source size: ~{source_size:.1f} GB\")\n",
    "            \n",
    "            with gzip.open(gzip_file, 'rb') as f_in:\n",
    "                with open(output_path, 'wb') as f_out:\n",
    "                    shutil.copyfileobj(f_in, f_out, length=8*1024*1024)\n",
    "            \n",
    "            output_size = output_path.stat().st_size / (1024**3)\n",
    "            print(f\"âœ“ Decompressed successfully!\")\n",
    "            print(f\"âœ“ Output size: ~{output_size:.1f} GB\")\n",
    "            \n",
    "            self.decompressed_files.append(output_path)\n",
    "            return str(output_path)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"\\nâŒ Decompression failed: {e}\")\n",
    "            if output_path.exists():\n",
    "                try:\n",
    "                    output_path.unlink()\n",
    "                except:\n",
    "                    pass\n",
    "            raise RuntimeError(f\"Failed to decompress {gzip_file}: {e}\")\n",
    "    \n",
    "    def read_gctx(self, gctx_file):\n",
    "        \"\"\"è¯»å–GCTXæ–‡ä»¶ (HDF5æ ¼å¼)\"\"\"\n",
    "        gctx_file = str(gctx_file)\n",
    "        print(f\"\\nğŸ“– Reading GCTX file: {Path(gctx_file).name}\")\n",
    "        \n",
    "        if gctx_file.endswith('.gz'):\n",
    "            print(\"âš ï¸  Detected gzip compressed file\")\n",
    "            gctx_file = self.decompress_gzip_file(gctx_file)\n",
    "        \n",
    "        with h5py.File(gctx_file, 'r') as f:\n",
    "            print(f\"ğŸ“Š Loading matrix from HDF5...\")\n",
    "            matrix = f['/0/DATA/0/matrix'][:]\n",
    "            print(f\"âœ“ Matrix shape: {matrix.shape}\")\n",
    "            \n",
    "            print(f\"ğŸ“‹ Loading metadata...\")\n",
    "            \n",
    "            # è¯»å–åŸºå› å…ƒæ•°æ®ï¼ˆæ¥è‡ª ROWï¼‰\n",
    "            gene_meta = {}\n",
    "            for key in f['/0/META/ROW'].keys():\n",
    "                data = f[f'/0/META/ROW/{key}'][:]\n",
    "                if data.dtype.kind in ['S', 'O']:\n",
    "                    gene_meta[key] = np.char.decode(data.astype('S'), 'utf-8')\n",
    "                else:\n",
    "                    gene_meta[key] = data.astype(str)\n",
    "            \n",
    "            # è¯»å–æ ·æœ¬å…ƒæ•°æ®ï¼ˆæ¥è‡ª COLï¼‰\n",
    "            sample_meta = {}\n",
    "            for key in f['/0/META/COL'].keys():\n",
    "                data = f[f'/0/META/COL/{key}'][:]\n",
    "                if data.dtype.kind in ['S', 'O']:\n",
    "                    sample_meta[key] = np.char.decode(data.astype('S'), 'utf-8')\n",
    "                else:\n",
    "                    sample_meta[key] = data.astype(str)\n",
    "        \n",
    "        sample_df = pd.DataFrame(sample_meta)\n",
    "        gene_df = pd.DataFrame(gene_meta)\n",
    "        \n",
    "        print(f\"\\nâœ“ Data loaded successfully:\")\n",
    "        print(f\"  Matrix: {matrix.shape} (samples Ã— genes)\")\n",
    "        print(f\"  Samples: {len(sample_df)}\")\n",
    "        print(f\"  Genes: {len(gene_df)}\")\n",
    "        \n",
    "        assert matrix.shape[0] == len(sample_df), \\\n",
    "            f\"Matrix rows ({matrix.shape[0]}) != sample metadata ({len(sample_df)})\"\n",
    "        assert matrix.shape[1] == len(gene_df), \\\n",
    "            f\"Matrix cols ({matrix.shape[1]}) != gene metadata ({len(gene_df)})\"\n",
    "        \n",
    "        return matrix, sample_df, gene_df\n",
    "    \n",
    "    def load_level4_signatures(self, dataset='GSE92742'):\n",
    "        \"\"\"åŠ è½½Level 4æ•°æ® (Z-score normalized)\"\"\"\n",
    "        level4_pattern = self.data_dir / f\"{dataset}_Broad_LINCS_Level4_ZSPCINF_mlr12k_n*x12328.gctx.gz\"\n",
    "        \n",
    "        print(f\"ğŸ” Searching for Level 4 file...\")\n",
    "        print(f\"   Pattern: {level4_pattern.name}\")\n",
    "        files = glob.glob(str(level4_pattern))\n",
    "        \n",
    "        if not files:\n",
    "            raise FileNotFoundError(\n",
    "                f\"âŒ No Level 4 file found matching {level4_pattern}\"\n",
    "            )\n",
    "        \n",
    "        level4_file = files[0]\n",
    "        print(f\"âœ“ Found: {Path(level4_file).name}\")\n",
    "        \n",
    "        # è¯»å–GCTX\n",
    "        matrix, sample_meta, gene_meta = self.read_gctx(level4_file)\n",
    "        \n",
    "        # åªä¿ç•™landmark genes\n",
    "        print(f\"\\nğŸ”¬ Filtering to landmark genes...\")\n",
    "        if self.gene_info is None:\n",
    "            self.load_gene_info(dataset)\n",
    "        \n",
    "        landmark_ids = set(self.gene_info['pr_gene_id'].astype(str).values)\n",
    "        print(f\"   Landmark genes to find: {len(landmark_ids)}\")\n",
    "        print(f\"   Total genes in data: {len(gene_meta)}\")\n",
    "        \n",
    "        gene_mask = gene_meta['id'].isin(landmark_ids)\n",
    "        n_matched = gene_mask.sum()\n",
    "        print(f\"   âœ“ Matched: {n_matched} genes\")\n",
    "        \n",
    "        if n_matched == 0:\n",
    "            raise ValueError(\"No landmark genes matched!\")\n",
    "        \n",
    "        print(f\"\\nğŸ¯ Applying filter...\")\n",
    "        matrix = matrix[:, gene_mask]\n",
    "        gene_meta = gene_meta[gene_mask].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"   âœ“ Final matrix shape: {matrix.shape}\")\n",
    "        \n",
    "        self.signatures = {\n",
    "            'matrix': matrix,\n",
    "            'row_meta': sample_meta,\n",
    "            'col_meta': gene_meta\n",
    "        }\n",
    "        \n",
    "        return matrix, sample_meta, gene_meta\n",
    "    \n",
    "    def calculate_cosine_similarity_to_nearest_replicate(\n",
    "        self, \n",
    "        matrix: np.ndarray, \n",
    "        pert_ids: pd.Series\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        è®¡ç®—æ¯ä¸ªæ ·æœ¬ä¸å…¶æœ€è¿‘replicateçš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆå‘é‡åŒ–ä¼˜åŒ–ï¼‰\n",
    "        \"\"\"\n",
    "        print(f\"\\nğŸ“Š Calculating cosine similarity to nearest replicate...\")\n",
    "        n_samples = len(pert_ids)\n",
    "        nearest_similarities = np.zeros(n_samples)\n",
    "        \n",
    "        unique_perts = pert_ids.unique()\n",
    "        print(f\"   Processing {len(unique_perts)} compounds...\")\n",
    "        \n",
    "        for i, pert_id in enumerate(unique_perts):\n",
    "            if (i + 1) % 1000 == 0:\n",
    "                print(f\"   Progress: {i + 1}/{len(unique_perts)}\")\n",
    "            \n",
    "            pert_mask = pert_ids == pert_id\n",
    "            pert_indices = np.where(pert_mask)[0]\n",
    "            \n",
    "            if len(pert_indices) < 2:\n",
    "                # åªæœ‰ä¸€ä¸ªæ ·æœ¬ï¼Œè®¾ä¸º0ï¼ˆä¼šè¢«è¿‡æ»¤ï¼‰\n",
    "                nearest_similarities[pert_indices] = 0.0\n",
    "                continue\n",
    "            \n",
    "            pert_data = matrix[pert_mask]\n",
    "            \n",
    "            # ä½¿ç”¨sklearnå¿«é€Ÿè®¡ç®—æˆå¯¹ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "            sim_matrix = cosine_similarity(pert_data)\n",
    "            \n",
    "            # å°†å¯¹è§’çº¿è®¾ä¸º-infï¼Œé¿å…æ ·æœ¬ä¸è‡ªå·±æ¯”è¾ƒ\n",
    "            np.fill_diagonal(sim_matrix, -np.inf)\n",
    "            \n",
    "            # å¯¹æ¯ä¸ªæ ·æœ¬æ‰¾åˆ°æœ€å¤§ç›¸ä¼¼åº¦ï¼ˆæœ€è¿‘çš„replicateï¼‰\n",
    "            max_sims = np.max(sim_matrix, axis=1)\n",
    "            \n",
    "            nearest_similarities[pert_indices] = max_sims\n",
    "        \n",
    "        print(f\"   âœ“ Calculated similarities for {n_samples} samples\")\n",
    "        print(f\"   Mean similarity: {nearest_similarities.mean():.4f}\")\n",
    "        print(f\"   Median similarity: {np.median(nearest_similarities):.4f}\")\n",
    "        \n",
    "        return nearest_similarities\n",
    "    \n",
    "    def prepare_training_data(\n",
    "        self, \n",
    "        min_observations_per_compound=3,      # æ”¾å®½ï¼š5 â†’ 3\n",
    "        min_replicate_similarity=0.10,        # æ”¾å®½ï¼š0.12 â†’ 0.10\n",
    "        dose_range=(0.3, 30.0),               # æ”¾å®½ï¼š(1.0, 20.0) â†’ (0.3, 30.0)\n",
    "        valid_timepoints=['6h', '24h', '6 h', '24 h'],  # å¢åŠ æ ¼å¼\n",
    "        min_cell_lines=3,                     # æ”¾å®½ï¼š5 â†’ 3\n",
    "        max_cell_lines=50,                    # æ”¾å®½ï¼š40 â†’ 50\n",
    "        remove_dos=True,\n",
    "        skip_missing_metadata=True            # æ–°å¢ï¼šç¼ºå¤±å…ƒæ•°æ®æ—¶è·³è¿‡è€Œä¸æ˜¯å¤±è´¥\n",
    "    ):\n",
    "        \"\"\"\n",
    "        å‡†å¤‡è®­ç»ƒæ•°æ®ï¼Œé€‚å½“æ”¾å®½è®ºæ–‡çš„è¿‡æ»¤æµç¨‹\n",
    "        \n",
    "        ä¸»è¦è°ƒæ•´ï¼š\n",
    "        1. min_observations_per_compound: 5 â†’ 3ï¼ˆæ”¾å®½ï¼‰\n",
    "        2. min_replicate_similarity: 0.12 â†’ 0.10ï¼ˆæ”¾å®½ï¼‰\n",
    "        3. dose_range: 1-20ÂµM â†’ 0.3-30ÂµMï¼ˆæ”¾å®½ï¼‰\n",
    "        4. min_cell_lines: 5 â†’ 3ï¼ˆæ”¾å®½ï¼‰\n",
    "        5. max_cell_lines: 40 â†’ 50ï¼ˆæ”¾å®½ï¼‰\n",
    "        6. å¢åŠ æ—¶é—´ç‚¹æ ¼å¼çš„å®¹é”™\n",
    "        7. ç¼ºå¤±å…ƒæ•°æ®æ—¶è·³è¿‡è¿‡æ»¤è€Œä¸æ˜¯æŠ¥é”™\n",
    "        \"\"\"\n",
    "        if self.signatures is None:\n",
    "            raise ValueError(\"Please load signatures first using load_level4_signatures()\")\n",
    "        \n",
    "        matrix = self.signatures['matrix']\n",
    "        row_meta = self.signatures['row_meta'].copy()\n",
    "        col_meta = self.signatures['col_meta']\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"ğŸ” DRUGREFLECTOR QUALITY CONTROL PIPELINE (RELAXED)\")\n",
    "        print(\"   Based on Science 2025 SI with relaxed filtering criteria\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Initial samples: {len(row_meta):,}\")\n",
    "        \n",
    "        # æ£€æŸ¥å¹¶è§£æå…ƒæ•°æ®å­—æ®µ\n",
    "        print(f\"\\nğŸ“‹ Checking metadata fields...\")\n",
    "        print(f\"   Available columns: {list(row_meta.columns)}\")\n",
    "        \n",
    "        # å°è¯•ä»ä¸åŒå¯èƒ½çš„åˆ—åè·å–å¿…è¦ä¿¡æ¯\n",
    "        if 'pert_id' not in row_meta.columns:\n",
    "            # å°è¯•ä»å…¶ä»–å¯èƒ½çš„åˆ—åè·å–\n",
    "            for col in ['perturbation_id', 'pert_iname', 'id']:\n",
    "                if col in row_meta.columns:\n",
    "                    print(f\"   Using '{col}' as pert_id\")\n",
    "                    row_meta['pert_id'] = row_meta[col]\n",
    "                    break\n",
    "            \n",
    "            # å¦‚æœè¿˜æ˜¯æ‰¾ä¸åˆ°ï¼Œå°è¯•ä»idåˆ—è§£æ\n",
    "            if 'pert_id' not in row_meta.columns and 'id' in row_meta.columns:\n",
    "                print(f\"   Parsing pert_id from 'id' column...\")\n",
    "                parts = row_meta['id'].str.split('_', expand=True)\n",
    "                if parts.shape[1] >= 1:\n",
    "                    row_meta['pert_id'] = parts[0]\n",
    "        \n",
    "        if 'cell_id' not in row_meta.columns:\n",
    "            for col in ['cell_line', 'cell_iname']:\n",
    "                if col in row_meta.columns:\n",
    "                    print(f\"   Using '{col}' as cell_id\")\n",
    "                    row_meta['cell_id'] = row_meta[col]\n",
    "                    break\n",
    "            \n",
    "            if 'cell_id' not in row_meta.columns and 'id' in row_meta.columns:\n",
    "                print(f\"   Parsing cell_id from 'id' column...\")\n",
    "                parts = row_meta['id'].str.split('_', expand=True)\n",
    "                if parts.shape[1] >= 2:\n",
    "                    row_meta['cell_id'] = parts[1]\n",
    "        \n",
    "        # æ£€æŸ¥å¿…è¦çš„åˆ—\n",
    "        required_cols = ['pert_id', 'cell_id']\n",
    "        missing_cols = [col for col in required_cols if col not in row_meta.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing required columns after parsing: {missing_cols}\")\n",
    "        \n",
    "        print(f\"   âœ“ Found required columns: pert_id, cell_id\")\n",
    "        \n",
    "        working_matrix = matrix.copy()\n",
    "        working_meta = row_meta.copy()\n",
    "        initial_compounds = working_meta['pert_id'].nunique()\n",
    "        \n",
    "        # ========== Filter 1: Remove DOS compounds ==========\n",
    "        if remove_dos:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"FILTER 1: Remove DOS (Diversity-Oriented Synthesis) compounds\")\n",
    "            print(f\"{'='*80}\")\n",
    "            \n",
    "            # æ›´å®½æ¾çš„DOSè¯†åˆ«\n",
    "            if 'pert_type' in working_meta.columns:\n",
    "                dos_mask = ~working_meta['pert_type'].str.contains('DOS', case=False, na=False)\n",
    "            else:\n",
    "                # åªç§»é™¤æ˜ç¡®æ ‡è®°ä¸ºDOSçš„åŒ–åˆç‰©\n",
    "                dos_mask = ~(\n",
    "                    working_meta['pert_id'].str.contains('BRD-DOS', case=False, na=False) |\n",
    "                    working_meta['pert_id'].str.contains('BDOS', case=False, na=False)\n",
    "                )\n",
    "            \n",
    "            n_dos = (~dos_mask).sum()\n",
    "            working_matrix = working_matrix[dos_mask]\n",
    "            working_meta = working_meta[dos_mask].reset_index(drop=True)\n",
    "            \n",
    "            print(f\"  Removed {n_dos:,} DOS observations\")\n",
    "            print(f\"  Remaining samples: {len(working_meta):,}\")\n",
    "            print(f\"  Remaining compounds: {working_meta['pert_id'].nunique():,}\")\n",
    "        \n",
    "        # ========== Filter 2: Minimum observations per compound ==========\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FILTER 2: Remove compounds with <{min_observations_per_compound} observations\")\n",
    "        print(f\"         (relaxed from paper's 5)\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        obs_counts = working_meta.groupby('pert_id').size()\n",
    "        valid_perts = obs_counts[obs_counts >= min_observations_per_compound].index\n",
    "        \n",
    "        print(f\"  Compounds with â‰¥{min_observations_per_compound} observations: \"\n",
    "              f\"{len(valid_perts):,}/{obs_counts.nunique():,}\")\n",
    "        \n",
    "        obs_mask = working_meta['pert_id'].isin(valid_perts)\n",
    "        working_matrix = working_matrix[obs_mask]\n",
    "        working_meta = working_meta[obs_mask].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"  Remaining samples: {len(working_meta):,}\")\n",
    "        print(f\"  Remaining compounds: {working_meta['pert_id'].nunique():,}\")\n",
    "        \n",
    "        # ========== Filter 3: Cosine similarity to closest replicate ==========\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FILTER 3: Remove observations with cosine similarity <{min_replicate_similarity}\")\n",
    "        print(f\"         (relaxed from paper's 0.12)\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        nearest_similarities = self.calculate_cosine_similarity_to_nearest_replicate(\n",
    "            working_matrix, \n",
    "            working_meta['pert_id']\n",
    "        )\n",
    "        \n",
    "        sim_mask = nearest_similarities >= min_replicate_similarity\n",
    "        n_removed_sim = (~sim_mask).sum()\n",
    "        \n",
    "        working_matrix = working_matrix[sim_mask]\n",
    "        working_meta = working_meta[sim_mask].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"  Removed {n_removed_sim:,} low-similarity observations\")\n",
    "        print(f\"  Remaining samples: {len(working_meta):,}\")\n",
    "        print(f\"  Remaining compounds: {working_meta['pert_id'].nunique():,}\")\n",
    "        \n",
    "        # ========== Filter 4: Dose selection ==========\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FILTER 4: Select most frequent dose in range {dose_range[0]}-{dose_range[1]} ÂµM\")\n",
    "        print(f\"         (relaxed from paper's 1-20 ÂµM)\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        has_dose_info = False\n",
    "        if 'pert_dose' in working_meta.columns:\n",
    "            has_dose_info = True\n",
    "        elif 'pert_idose' in working_meta.columns:\n",
    "            working_meta['pert_dose'] = working_meta['pert_idose']\n",
    "            has_dose_info = True\n",
    "        elif 'id' in working_meta.columns:\n",
    "            # å°è¯•ä»idè§£æ\n",
    "            parts = working_meta['id'].str.split('_', expand=True)\n",
    "            if parts.shape[1] >= 4:\n",
    "                working_meta['pert_dose'] = parts[3]\n",
    "                has_dose_info = True\n",
    "        \n",
    "        if has_dose_info:\n",
    "            # è§£æå‰‚é‡å€¼\n",
    "            working_meta['dose_value'] = pd.to_numeric(\n",
    "                working_meta['pert_dose'].astype(str).str.extract(r'(\\d+\\.?\\d*)')[0], \n",
    "                errors='coerce'\n",
    "            )\n",
    "            \n",
    "            # è§£æå‰‚é‡å•ä½\n",
    "            working_meta['dose_unit'] = working_meta['pert_dose'].astype(str).str.extract(r'([a-zA-Z]+)')[0]\n",
    "            \n",
    "            # è½¬æ¢ä¸ºÂµM\n",
    "            def convert_to_uM(row):\n",
    "                if pd.isna(row['dose_value']):\n",
    "                    return np.nan\n",
    "                value = row['dose_value']\n",
    "                unit = str(row['dose_unit']).lower() if pd.notna(row['dose_unit']) else 'um'\n",
    "                \n",
    "                if 'nm' in unit:\n",
    "                    return value / 1000\n",
    "                elif 'mm' in unit:\n",
    "                    return value * 1000\n",
    "                else:\n",
    "                    return value\n",
    "            \n",
    "            working_meta['dose_uM'] = working_meta.apply(convert_to_uM, axis=1)\n",
    "            \n",
    "            # ç­›é€‰æœ‰æ•ˆå‰‚é‡èŒƒå›´\n",
    "            dose_mask = (\n",
    "                (working_meta['dose_uM'] >= dose_range[0]) & \n",
    "                (working_meta['dose_uM'] <= dose_range[1])\n",
    "            )\n",
    "            \n",
    "            # å¯¹æ¯ä¸ªåŒ–åˆç‰©ï¼Œé€‰æ‹©æœ€å¸¸è§çš„å‰‚é‡\n",
    "            valid_doses = []\n",
    "            for pert_id in working_meta['pert_id'].unique():\n",
    "                pert_mask = (working_meta['pert_id'] == pert_id) & dose_mask\n",
    "                if pert_mask.sum() == 0:\n",
    "                    # å¦‚æœæ²¡æœ‰åœ¨èŒƒå›´å†…çš„å‰‚é‡ï¼Œä¿ç•™æ‰€æœ‰å‰‚é‡\n",
    "                    if skip_missing_metadata:\n",
    "                        pert_all_mask = working_meta['pert_id'] == pert_id\n",
    "                        valid_doses.append(pert_all_mask)\n",
    "                    continue\n",
    "                \n",
    "                # æ‰¾å‡ºæœ€å¸¸è§çš„å‰‚é‡\n",
    "                dose_counts = working_meta.loc[pert_mask, 'dose_uM'].value_counts()\n",
    "                if len(dose_counts) > 0:\n",
    "                    most_common_dose = dose_counts.index[0]\n",
    "                    valid_doses.append(\n",
    "                        (working_meta['pert_id'] == pert_id) & \n",
    "                        (working_meta['dose_uM'] == most_common_dose)\n",
    "                    )\n",
    "            \n",
    "            if valid_doses:\n",
    "                dose_final_mask = pd.concat([pd.Series(mask) for mask in valid_doses], axis=1).any(axis=1)\n",
    "                n_removed_dose = (~dose_final_mask).sum()\n",
    "                \n",
    "                working_matrix = working_matrix[dose_final_mask]\n",
    "                working_meta = working_meta[dose_final_mask].reset_index(drop=True)\n",
    "                \n",
    "                print(f\"  Removed {n_removed_dose:,} observations (invalid or non-modal dose)\")\n",
    "                print(f\"  Remaining samples: {len(working_meta):,}\")\n",
    "                print(f\"  Remaining compounds: {working_meta['pert_id'].nunique():,}\")\n",
    "            else:\n",
    "                print(f\"  Warning: No valid doses found, keeping all data\")\n",
    "        else:\n",
    "            print(f\"  Warning: 'pert_dose' column not found, skipping dose filter\")\n",
    "        \n",
    "        # ========== Filter 5: Timepoint selection ==========\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FILTER 5: Keep only measurements at {valid_timepoints}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        has_time_info = False\n",
    "        if 'pert_time' in working_meta.columns:\n",
    "            has_time_info = True\n",
    "        elif 'pert_itime' in working_meta.columns:\n",
    "            working_meta['pert_time'] = working_meta['pert_itime']\n",
    "            has_time_info = True\n",
    "        elif 'id' in working_meta.columns:\n",
    "            parts = working_meta['id'].str.split('_', expand=True)\n",
    "            if parts.shape[1] >= 3:\n",
    "                working_meta['pert_time'] = parts[2]\n",
    "                has_time_info = True\n",
    "        \n",
    "        if has_time_info:\n",
    "            # æ ‡å‡†åŒ–æ—¶é—´ç‚¹æ ¼å¼ï¼ˆæ›´å®½æ¾ï¼‰\n",
    "            working_meta['time_normalized'] = working_meta['pert_time'].astype(str).str.lower().str.strip()\n",
    "            \n",
    "            # æ‰©å±•æ—¶é—´ç‚¹åŒ¹é…æ¨¡å¼\n",
    "            valid_patterns = []\n",
    "            for tp in valid_timepoints:\n",
    "                valid_patterns.append(tp.lower().strip())\n",
    "                valid_patterns.append(tp.lower().strip().replace('h', ' h'))\n",
    "                valid_patterns.append(tp.lower().strip().replace('h', 'hr'))\n",
    "                valid_patterns.append(tp.lower().strip().replace(' ', ''))\n",
    "            \n",
    "            valid_patterns = list(set(valid_patterns))\n",
    "            time_mask = working_meta['time_normalized'].isin(valid_patterns)\n",
    "            \n",
    "            n_removed_time = (~time_mask).sum()\n",
    "            \n",
    "            if skip_missing_metadata and n_removed_time == len(working_meta):\n",
    "                print(f\"  Warning: No matching timepoints found, keeping all data\")\n",
    "            else:\n",
    "                working_matrix = working_matrix[time_mask]\n",
    "                working_meta = working_meta[time_mask].reset_index(drop=True)\n",
    "                \n",
    "                print(f\"  Removed {n_removed_time:,} observations (invalid timepoint)\")\n",
    "                print(f\"  Remaining samples: {len(working_meta):,}\")\n",
    "                print(f\"  Remaining compounds: {working_meta['pert_id'].nunique():,}\")\n",
    "        else:\n",
    "            print(f\"  Warning: 'pert_time' column not found, skipping timepoint filter\")\n",
    "        \n",
    "        # ========== Filter 6: Cell line count per compound ==========\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FILTER 6: Remove compounds in <{min_cell_lines} or >{max_cell_lines} cell lines\")\n",
    "        print(f\"         (relaxed from paper's 5-40)\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        cell_line_counts = working_meta.groupby('pert_id')['cell_id'].nunique()\n",
    "        valid_perts_cell = cell_line_counts[\n",
    "            (cell_line_counts >= min_cell_lines) & \n",
    "            (cell_line_counts <= max_cell_lines)\n",
    "        ].index\n",
    "        \n",
    "        print(f\"  Compounds in {min_cell_lines}-{max_cell_lines} cell lines: \"\n",
    "              f\"{len(valid_perts_cell):,}/{len(cell_line_counts):,}\")\n",
    "        \n",
    "        cell_mask = working_meta['pert_id'].isin(valid_perts_cell)\n",
    "        working_matrix = working_matrix[cell_mask]\n",
    "        working_meta = working_meta[cell_mask].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"  Remaining samples: {len(working_meta):,}\")\n",
    "        print(f\"  Remaining compounds: {working_meta['pert_id'].nunique():,}\")\n",
    "        \n",
    "        # ========== Create final dataset ==========\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"âœ… FINAL DATASET (After all filters)\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # åˆ›å»ºåŒ–åˆç‰©æ ‡ç­¾ç¼–ç \n",
    "        unique_perts = sorted(working_meta['pert_id'].unique())\n",
    "        pert_to_idx = {pert: idx for idx, pert in enumerate(unique_perts)}\n",
    "        labels = np.array([pert_to_idx[p] for p in working_meta['pert_id']])\n",
    "        \n",
    "        final_compounds = len(unique_perts)\n",
    "        final_samples = len(working_matrix)\n",
    "        final_cells = working_meta['cell_id'].nunique()\n",
    "        \n",
    "        print(f\"  Total samples: {final_samples:,}\")\n",
    "        print(f\"  Total compounds: {final_compounds:,}\")\n",
    "        print(f\"  Cell lines: {final_cells}\")\n",
    "        print(f\"  Gene features: {working_matrix.shape[1]}\")\n",
    "        print(f\"  Samples per compound (mean): {final_samples / final_compounds:.1f}\")\n",
    "        print(f\"  Samples per compound (median): \"\n",
    "              f\"{working_meta.groupby('pert_id').size().median():.0f}\")\n",
    "        \n",
    "        # ç»Ÿè®¡åŒ–åˆç‰©åˆ†å¸ƒ\n",
    "        compound_obs = working_meta.groupby('pert_id').size()\n",
    "        n_compounds_100plus = (compound_obs > 100).sum()\n",
    "        print(f\"  Compounds with >100 observations: {n_compounds_100plus:,}\")\n",
    "        \n",
    "        # ä¸åŸæ–‡å¯¹æ¯”\n",
    "        print(f\"\\nğŸ“Š Comparison with paper results:\")\n",
    "        print(f\"  Paper: 425,242 obs, 9,597 compounds, 52 cell lines\")\n",
    "        print(f\"  Ours:  {final_samples:,} obs, {final_compounds:,} compounds, {final_cells} cell lines\")\n",
    "        \n",
    "        if initial_compounds > 0:\n",
    "            retention_rate = (final_compounds / initial_compounds) * 100\n",
    "            print(f\"  Compound retention rate: {retention_rate:.1f}%\")\n",
    "        \n",
    "        training_data = {\n",
    "            'X': working_matrix,\n",
    "            'y': labels,\n",
    "            'sample_meta': working_meta,\n",
    "            'gene_names': col_meta['id'].values,\n",
    "            'compound_names': unique_perts,\n",
    "            'pert_to_idx': pert_to_idx\n",
    "        }\n",
    "        \n",
    "        return training_data\n",
    "    \n",
    "    def create_3fold_splits(self, training_data, random_state=42):\n",
    "        \"\"\"åˆ›å»º3æŠ˜äº¤å‰éªŒè¯åˆ’åˆ†\"\"\"\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        sample_meta = training_data['sample_meta']\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ğŸ² Creating 3-fold cross-validation splits\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        folds = np.zeros(len(sample_meta), dtype=int)\n",
    "        \n",
    "        # æŒ‰åŒ–åˆç‰©åˆ†é…fold\n",
    "        for pert_id in sample_meta['pert_id'].unique():\n",
    "            pert_mask = sample_meta['pert_id'] == pert_id\n",
    "            pert_indices = np.where(pert_mask)[0]\n",
    "            \n",
    "            np.random.shuffle(pert_indices)\n",
    "            n_samples = len(pert_indices)\n",
    "            \n",
    "            fold_sizes = [n_samples // 3] * 3\n",
    "            for i in range(n_samples % 3):\n",
    "                fold_sizes[i] += 1\n",
    "            \n",
    "            start_idx = 0\n",
    "            for fold_id, size in enumerate(fold_sizes):\n",
    "                end_idx = start_idx + size\n",
    "                folds[pert_indices[start_idx:end_idx]] = fold_id\n",
    "                start_idx = end_idx\n",
    "        \n",
    "        training_data['folds'] = folds\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Fold statistics:\")\n",
    "        for fold_id in range(3):\n",
    "            fold_mask = folds == fold_id\n",
    "            n_samples = fold_mask.sum()\n",
    "            n_compounds = sample_meta[fold_mask]['pert_id'].nunique()\n",
    "            print(f\"   Fold {fold_id}: {n_samples:,} samples, {n_compounds:,} compounds\")\n",
    "        \n",
    "        return training_data\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"ä¸»ç¨‹åº - æ”¾å®½ç­›é€‰æ¡ä»¶ç‰ˆæœ¬\"\"\"\n",
    "    loader = LINCSDataLoader(\"E:/ç§‘ç ”/Models/drugreflector/datasets\")\n",
    "    \n",
    "    try:\n",
    "        # Step 1: åŠ è½½åŸºå› å’Œç»†èƒä¿¡æ¯\n",
    "        print(\"=\" * 80)\n",
    "        print(\"STEP 1: Loading gene and cell information\")\n",
    "        print(\"=\" * 80)\n",
    "        gene_info = loader.load_gene_info('GSE92742')\n",
    "        cell_info = loader.load_cell_info('GSE92742')\n",
    "        \n",
    "        # Step 2: åŠ è½½Level 4 signatures\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"STEP 2: Loading Level 4 signatures\")\n",
    "        print(\"=\" * 80)\n",
    "        matrix, row_meta, col_meta = loader.load_level4_signatures('GSE92742')\n",
    "        \n",
    "        # Step 3: å‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆæ”¾å®½æ¡ä»¶ï¼‰\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"STEP 3: Preparing training data (Relaxed filters)\")\n",
    "        print(\"=\" * 80)\n",
    "        training_data = loader.prepare_training_data(\n",
    "            min_observations_per_compound=3,    # æ”¾å®½ï¼š5 â†’ 3\n",
    "            min_replicate_similarity=0.10,      # æ”¾å®½ï¼š0.12 â†’ 0.10\n",
    "            dose_range=(0.3, 30.0),             # æ”¾å®½ï¼š1-20 â†’ 0.3-30\n",
    "            valid_timepoints=['6h', '24h', '6 h', '24 h'],\n",
    "            min_cell_lines=3,                   # æ”¾å®½ï¼š5 â†’ 3\n",
    "            max_cell_lines=50,                  # æ”¾å®½ï¼š40 â†’ 50\n",
    "            remove_dos=True,\n",
    "            skip_missing_metadata=True\n",
    "        )\n",
    "        \n",
    "        # Step 4: åˆ›å»º3æŠ˜åˆ’åˆ†\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"STEP 4: Creating 3-fold splits\")\n",
    "        print(\"=\" * 80)\n",
    "        training_data = loader.create_3fold_splits(training_data)\n",
    "        \n",
    "        # Step 5: ä¿å­˜å¤„ç†åçš„æ•°æ®\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"STEP 5: Saving processed data\")\n",
    "        print(\"=\" * 80)\n",
    "        output_dir = Path(\"E:/ç§‘ç ”/Models/drugreflector/processed_data\")\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        output_file = output_dir / \"training_data_relaxed.pkl\"\n",
    "        print(f\"ğŸ’¾ Saving to: {output_file}\")\n",
    "        \n",
    "        with open(output_file, 'wb') as f:\n",
    "            pickle.dump(training_data, f)\n",
    "        \n",
    "        print(f\"âœ“ Saved successfully!\")\n",
    "        \n",
    "        # æ‰“å°æœ€ç»ˆæ‘˜è¦\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"âœ… DATA PREPARATION COMPLETE!\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"ğŸ“ Output file: {output_file}\")\n",
    "        print(f\"\\nğŸ“Š Final dataset summary:\")\n",
    "        print(f\"   â€¢ Total samples: {len(training_data['X']):,}\")\n",
    "        print(f\"   â€¢ Total compounds: {len(training_data['compound_names']):,}\")\n",
    "        print(f\"   â€¢ Gene features: {training_data['X'].shape[1]}\")\n",
    "        print(f\"   â€¢ Data shape: {training_data['X'].shape}\")\n",
    "        print(f\"   â€¢ Average samples per compound: \"\n",
    "              f\"{len(training_data['X']) / len(training_data['compound_names']):.1f}\")\n",
    "        \n",
    "        return training_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n\" + \"=\" * 80)\n",
    "        print(\"âŒ ERROR DURING DATA PREPARATION\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"   Type: {type(e).__name__}\")\n",
    "        print(f\"   Message: {e}\")\n",
    "        print(f\"\\nğŸ“‹ Full traceback:\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    training_data = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f04385f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemprop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
