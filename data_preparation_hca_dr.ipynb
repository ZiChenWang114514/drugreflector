{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HCA-DR (Hierarchical Cell-Aware DrugReflector) æ•°æ®é¢„å¤„ç†\n",
    "\n",
    "## æ¦‚è¿°\n",
    "æœ¬notebookå®ç°HCA-DRæ¨¡å‹æ‰€éœ€çš„æ•°æ®é¢„å¤„ç†æµç¨‹ï¼š\n",
    "\n",
    "1. **åŠ è½½å·²å¤„ç†çš„Level 4è®­ç»ƒæ•°æ®** - æ‰°åŠ¨ç­¾å $\\mathbf{x}_{\\text{pert}}$\n",
    "2. **ä»Level 3 DMSOå¯¹ç…§æ•°æ®æå–ç»†èƒç³»ä¸Šä¸‹æ–‡** - $\\mathbf{x}_{\\text{ctx}}$\n",
    "3. **è®¡ç®—æ¯ä¸ªplateçš„DMSOå‡å€¼**\n",
    "4. **åº”ç”¨Rank-based Inverse Normal Transformation (INT)å½’ä¸€åŒ–**\n",
    "5. **æ„å»ºç»†èƒç³»IDæ˜ å°„**\n",
    "6. **è¾“å‡ºå¯ç›´æ¥ç”¨äºHCA-DRè®­ç»ƒçš„æ•°æ®**\n",
    "\n",
    "### æ•°å­¦å®šä¹‰\n",
    "- æ‰°åŠ¨ç­¾å: $\\mathbf{x}_{\\text{pert}} \\in \\mathbb{R}^{978}$ (v-scoreå˜æ¢å)\n",
    "- ç»†èƒç³»ä¸Šä¸‹æ–‡: $\\mathbf{x}_{\\text{ctx}} \\in \\mathbb{R}^{978}$ (INTå½’ä¸€åŒ–å)\n",
    "- ç»†èƒç³»ID: $c \\in \\{1, 2, \\ldots, C\\}$\n",
    "- è¯ç‰©æ ‡ç­¾: $y \\in \\{1, 2, \\ldots, D\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from scipy import stats\n",
    "from scipy.stats import rankdata\n",
    "from typing import Dict, Tuple, Optional, List\n",
    "import warnings\n",
    "import gc\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è®¾ç½®ç»˜å›¾é£æ ¼\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"âœ“ Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. é…ç½®è·¯å¾„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== é…ç½®è·¯å¾„ ====================\n",
    "LINCS_DATA_DIR = Path(\"E:/ç§‘ç ”/Models/drugreflector/datasets/LINCS2020\")\n",
    "PROCESSED_DATA_DIR = Path(\"E:/ç§‘ç ”/Models/drugreflector/processed_data\")\n",
    "OUTPUT_DIR = Path(\"E:/ç§‘ç ”/Models/drugreflector/processed_data\")\n",
    "VIZ_DIR = Path(\"E:/ç§‘ç ”/Models/drugreflector/visualizations/hca_dr\")\n",
    "\n",
    "# åˆ›å»ºè¾“å‡ºç›®å½•\n",
    "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "VIZ_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# è¾“å…¥æ–‡ä»¶\n",
    "TRAINING_DATA_FILE = \"training_data_lincs2020_chemfiltered_1201.pkl\"\n",
    "LEVEL3_CTL_FILE = \"level3_beta_ctl_n188708x12328.gctx\"\n",
    "GENE_INFO_FILE = \"geneinfo_beta.txt\"\n",
    "INST_INFO_FILE = \"instinfo_beta.txt\"\n",
    "\n",
    "# è¾“å‡ºæ–‡ä»¶\n",
    "OUTPUT_FILE = \"hca_dr_training_data.pkl\"\n",
    "\n",
    "print(\"ğŸ“ Path Configuration:\")\n",
    "print(f\"   LINCS data: {LINCS_DATA_DIR}\")\n",
    "print(f\"   Processed data: {PROCESSED_DATA_DIR}\")\n",
    "print(f\"   Output: {OUTPUT_DIR / OUTPUT_FILE}\")\n",
    "\n",
    "# éªŒè¯æ–‡ä»¶å­˜åœ¨\n",
    "files_to_check = [\n",
    "    (PROCESSED_DATA_DIR / TRAINING_DATA_FILE, \"Training data\"),\n",
    "    (LINCS_DATA_DIR / LEVEL3_CTL_FILE, \"Level 3 control data\"),\n",
    "    (LINCS_DATA_DIR / GENE_INFO_FILE, \"Gene info\"),\n",
    "    (LINCS_DATA_DIR / INST_INFO_FILE, \"Instance info\"),\n",
    "]\n",
    "\n",
    "print(\"\\nğŸ“‹ File Check:\")\n",
    "all_files_exist = True\n",
    "for file_path, desc in files_to_check:\n",
    "    exists = file_path.exists()\n",
    "    status = \"âœ“\" if exists else \"âœ—\"\n",
    "    print(f\"   {status} {desc}: {file_path.name}\")\n",
    "    if not exists:\n",
    "        all_files_exist = False\n",
    "\n",
    "if not all_files_exist:\n",
    "    print(\"\\nâš ï¸  Some required files are missing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. åŠ è½½åŸºå› ä¿¡æ¯å’ŒLandmarkç´¢å¼•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gene_info(gene_file: Path) -> Tuple[pd.DataFrame, np.ndarray]:\n",
    "    \"\"\"åŠ è½½åŸºå› ä¿¡æ¯å¹¶è·å–landmarkåŸºå› ç´¢å¼•\"\"\"\n",
    "    print(\"\\nğŸ“– Loading gene information...\")\n",
    "    gene_info = pd.read_csv(gene_file, sep='\\t')\n",
    "    \n",
    "    print(f\"   âœ“ Total genes: {len(gene_info):,}\")\n",
    "    print(f\"   âœ“ Columns: {list(gene_info.columns)}\")\n",
    "    \n",
    "    # è·å–landmarkåŸºå› \n",
    "    landmark_mask = gene_info['feature_space'] == 'landmark'\n",
    "    landmark_indices = np.where(landmark_mask.values)[0]\n",
    "    \n",
    "    print(f\"   âœ“ Landmark genes: {len(landmark_indices)}\")\n",
    "    \n",
    "    return gene_info, landmark_indices\n",
    "\n",
    "# åŠ è½½\n",
    "gene_info, landmark_col_indices = load_gene_info(LINCS_DATA_DIR / GENE_INFO_FILE)\n",
    "\n",
    "# æ˜¾ç¤ºéƒ¨åˆ†landmarkåŸºå› \n",
    "landmark_genes = gene_info[gene_info['feature_space'] == 'landmark']\n",
    "print(f\"\\n   Sample landmark genes:\")\n",
    "print(landmark_genes[['gene_id', 'gene_symbol', 'gene_type']].head(10).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. åŠ è½½å·²å¤„ç†çš„Level 4è®­ç»ƒæ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data(training_path: Path) -> Dict:\n",
    "    \"\"\"åŠ è½½å·²å¤„ç†çš„è®­ç»ƒæ•°æ®\"\"\"\n",
    "    print(\"\\nğŸ“– Loading processed training data...\")\n",
    "    \n",
    "    with open(training_path, 'rb') as f:\n",
    "        training_data = pickle.load(f)\n",
    "    \n",
    "    n_samples = len(training_data['X'])\n",
    "    n_compounds = len(training_data['compound_names'])\n",
    "    n_genes = training_data['X'].shape[1]\n",
    "    \n",
    "    print(f\"   âœ“ Samples: {n_samples:,}\")\n",
    "    print(f\"   âœ“ Compounds: {n_compounds:,}\")\n",
    "    print(f\"   âœ“ Genes: {n_genes}\")\n",
    "    \n",
    "    # æ£€æŸ¥metadata\n",
    "    meta = training_data['sample_meta']\n",
    "    print(f\"\\n   ğŸ“‹ Metadata columns:\")\n",
    "    print(f\"      {list(meta.columns[:15])}\")\n",
    "    \n",
    "    # æå–plateä¿¡æ¯ï¼ˆå¦‚æœéœ€è¦ï¼‰\n",
    "    if 'det_plate' not in meta.columns and 'sample_id' in meta.columns:\n",
    "        print(\"\\n   Extracting plate info from sample_id...\")\n",
    "        meta['det_plate'] = meta['sample_id'].apply(\n",
    "            lambda x: x.split(':')[0] if ':' in str(x) else str(x).rsplit('_', 1)[0]\n",
    "        )\n",
    "        training_data['sample_meta'] = meta\n",
    "    \n",
    "    print(f\"\\n   ğŸ“Š Statistics:\")\n",
    "    print(f\"      â€¢ Cell lines: {meta['cell_iname'].nunique()}\")\n",
    "    print(f\"      â€¢ Unique plates: {meta['det_plate'].nunique() if 'det_plate' in meta.columns else 'N/A'}\")\n",
    "    print(f\"      â€¢ Memory: {training_data['X'].nbytes / (1024**3):.2f} GB\")\n",
    "    \n",
    "    return training_data\n",
    "\n",
    "# åŠ è½½è®­ç»ƒæ•°æ®\n",
    "training_data = load_training_data(PROCESSED_DATA_DIR / TRAINING_DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ†æè®­ç»ƒæ•°æ®çš„ç»†èƒç³»åˆ†å¸ƒ\n",
    "meta = training_data['sample_meta']\n",
    "\n",
    "print(\"\\nğŸ“Š Cell Line Distribution in Training Data:\")\n",
    "cell_counts = meta['cell_iname'].value_counts()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# æŸ±çŠ¶å›¾ï¼šå‰20ä¸ªç»†èƒç³»\n",
    "ax1 = axes[0]\n",
    "top_cells = cell_counts.head(20)\n",
    "bars = ax1.barh(range(len(top_cells)), top_cells.values, color='steelblue', edgecolor='black')\n",
    "ax1.set_yticks(range(len(top_cells)))\n",
    "ax1.set_yticklabels(top_cells.index)\n",
    "ax1.set_xlabel('Number of Samples')\n",
    "ax1.set_title('Top 20 Cell Lines by Sample Count', fontweight='bold')\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "# æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "for bar, val in zip(bars, top_cells.values):\n",
    "    ax1.text(val + 100, bar.get_y() + bar.get_height()/2, f'{val:,}', va='center', fontsize=9)\n",
    "\n",
    "# ç›´æ–¹å›¾ï¼šæ ·æœ¬æ•°åˆ†å¸ƒ\n",
    "ax2 = axes[1]\n",
    "ax2.hist(cell_counts.values, bins=30, color='coral', edgecolor='black', alpha=0.7)\n",
    "ax2.set_xlabel('Samples per Cell Line')\n",
    "ax2.set_ylabel('Number of Cell Lines')\n",
    "ax2.set_title('Distribution of Sample Counts per Cell Line', fontweight='bold')\n",
    "ax2.axvline(cell_counts.median(), color='red', linestyle='--', label=f'Median: {cell_counts.median():.0f}')\n",
    "ax2.axvline(cell_counts.mean(), color='blue', linestyle='--', label=f'Mean: {cell_counts.mean():.0f}')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(VIZ_DIR / 'training_data_cell_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n   Total cell lines: {len(cell_counts)}\")\n",
    "print(f\"   Mean samples/cell: {cell_counts.mean():.1f}\")\n",
    "print(f\"   Median samples/cell: {cell_counts.median():.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. åŠ è½½Level 3 DMSOå¯¹ç…§æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_level3_dmso_data(level3_file: Path, \n",
    "                          inst_file: Path,\n",
    "                          landmark_indices: np.ndarray) -> Tuple[np.ndarray, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    åŠ è½½Level 3 DMSOå¯¹ç…§æ•°æ®\n",
    "    \n",
    "    Level 3æ•°æ®åŒ…å«æœªç»z-scoreæ ‡å‡†åŒ–çš„è¡¨è¾¾å€¼\n",
    "    ç”¨äºè®¡ç®—ç»†èƒç³»ä¸Šä¸‹æ–‡\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸ“– Loading Level 3 DMSO Control Data\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    file_size_gb = level3_file.stat().st_size / (1024**3)\n",
    "    print(f\"File: {level3_file.name}\")\n",
    "    print(f\"Size: {file_size_gb:.2f} GB\")\n",
    "    \n",
    "    with h5py.File(level3_file, 'r') as f:\n",
    "        # æ£€æŸ¥æ•°æ®è·¯å¾„\n",
    "        matrix_dataset = f['/0/DATA/0/matrix']\n",
    "        matrix_shape = matrix_dataset.shape\n",
    "        print(f\"\\n   Matrix shape: {matrix_shape}\")\n",
    "        \n",
    "        # è¯»å–å…ƒæ•°æ®\n",
    "        print(\"\\nğŸ“‹ Loading metadata...\")\n",
    "        sample_meta = {}\n",
    "        for key in f['/0/META/ROW'].keys():\n",
    "            data = f[f'/0/META/ROW/{key}'][:]\n",
    "            if data.dtype.kind in ['S', 'O', 'U']:\n",
    "                try:\n",
    "                    sample_meta[key] = np.char.decode(data.astype('S'), 'utf-8')\n",
    "                except:\n",
    "                    sample_meta[key] = data.astype(str)\n",
    "            else:\n",
    "                sample_meta[key] = data\n",
    "        \n",
    "        sample_df = pd.DataFrame(sample_meta)\n",
    "        \n",
    "        # æ£€æµ‹å¹¶å¤„ç†ROW/COLäº¤æ¢\n",
    "        if len(sample_df) == matrix_shape[1]:\n",
    "            print(\"   âš ï¸  Detected ROW/COL swap...\")\n",
    "            # é‡æ–°è¯»å–\n",
    "            gene_meta = sample_meta\n",
    "            sample_meta = {}\n",
    "            for key in f['/0/META/COL'].keys():\n",
    "                data = f[f'/0/META/COL/{key}'][:]\n",
    "                if data.dtype.kind in ['S', 'O', 'U']:\n",
    "                    try:\n",
    "                        sample_meta[key] = np.char.decode(data.astype('S'), 'utf-8')\n",
    "                    except:\n",
    "                        sample_meta[key] = data.astype(str)\n",
    "                else:\n",
    "                    sample_meta[key] = data\n",
    "            sample_df = pd.DataFrame(sample_meta)\n",
    "        \n",
    "        print(f\"   Sample metadata: {len(sample_df)} entries\")\n",
    "        \n",
    "        # åŠ è½½landmarkåŸºå› æ•°æ®ï¼ˆåˆ†å—ï¼‰\n",
    "        print(\"\\nğŸ”¬ Loading landmark gene data...\")\n",
    "        chunk_size = 50000\n",
    "        chunks = []\n",
    "        \n",
    "        for start_idx in range(0, matrix_shape[0], chunk_size):\n",
    "            end_idx = min(start_idx + chunk_size, matrix_shape[0])\n",
    "            pct = end_idx / matrix_shape[0] * 100\n",
    "            print(f\"   Loading rows {start_idx:,} to {end_idx:,}... ({pct:.1f}%)\", end='\\r')\n",
    "            \n",
    "            chunk = matrix_dataset[start_idx:end_idx, landmark_indices].astype(np.float32)\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        print()\n",
    "        matrix = np.vstack(chunks)\n",
    "        del chunks\n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"   âœ“ Matrix shape: {matrix.shape}\")\n",
    "        print(f\"   âœ“ Memory: {matrix.nbytes / (1024**3):.2f} GB\")\n",
    "    \n",
    "    # å¤„ç†sample_id\n",
    "    if 'id' in sample_df.columns:\n",
    "        sample_df = sample_df.rename(columns={'id': 'sample_id'})\n",
    "    \n",
    "    # ä¸instinfoåˆå¹¶\n",
    "    if inst_file.exists():\n",
    "        print(\"\\nğŸ“– Merging with instance info...\")\n",
    "        inst_info = pd.read_csv(inst_file, sep='\\t')\n",
    "        \n",
    "        if 'sample_id' in sample_df.columns and 'sample_id' in inst_info.columns:\n",
    "            sample_df = sample_df.merge(inst_info, on='sample_id', how='left')\n",
    "            print(f\"   âœ“ Merged successfully\")\n",
    "    \n",
    "    print(f\"\\n   âœ“ Level 3 data loaded: {len(matrix):,} samples\")\n",
    "    \n",
    "    return matrix, sample_df\n",
    "\n",
    "# åŠ è½½Level 3æ•°æ®\n",
    "level3_matrix, level3_meta = load_level3_dmso_data(\n",
    "    LINCS_DATA_DIR / LEVEL3_CTL_FILE,\n",
    "    LINCS_DATA_DIR / INST_INFO_FILE,\n",
    "    landmark_col_indices\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ£€æŸ¥Level 3å…ƒæ•°æ®\n",
    "print(\"\\nğŸ“‹ Level 3 Metadata Overview:\")\n",
    "print(f\"   Columns: {list(level3_meta.columns[:20])}\")\n",
    "print(f\"\\n   Sample row:\")\n",
    "print(level3_meta.iloc[0][['sample_id', 'cell_iname', 'pert_type', 'pert_iname', 'det_plate']].to_string() \n",
    "      if all(col in level3_meta.columns for col in ['sample_id', 'cell_iname', 'pert_type', 'pert_iname', 'det_plate']) \n",
    "      else level3_meta.iloc[0].head(10).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. è®¡ç®—Plate-Matched DMSOå‡å€¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_plate_dmso_means(matrix: np.ndarray, \n",
    "                              meta: pd.DataFrame) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    è®¡ç®—æ¯ä¸ªå®éªŒæ¿çš„DMSOå¯¹ç…§å‡å€¼\n",
    "    \n",
    "    è¿™æ˜¯ç»†èƒç³»ä¸Šä¸‹æ–‡ x_ctx çš„åŸå§‹å€¼ï¼ˆINTå½’ä¸€åŒ–å‰ï¼‰\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸ§ª Computing Plate-Matched DMSO Means\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # ç¡®å®šplateåˆ—\n",
    "    plate_col = None\n",
    "    for col in ['det_plate', 'rna_plate']:\n",
    "        if col in meta.columns:\n",
    "            plate_col = col\n",
    "            break\n",
    "    \n",
    "    if plate_col is None and 'sample_id' in meta.columns:\n",
    "        # ä»sample_idæå–\n",
    "        meta['det_plate'] = meta['sample_id'].apply(\n",
    "            lambda x: x.split(':')[0] if ':' in str(x) else str(x).rsplit('_', 1)[0]\n",
    "        )\n",
    "        plate_col = 'det_plate'\n",
    "    \n",
    "    print(f\"   Using '{plate_col}' as plate identifier\")\n",
    "    \n",
    "    # è¿‡æ»¤DMSOæ ·æœ¬\n",
    "    if 'pert_type' in meta.columns:\n",
    "        dmso_mask = meta['pert_type'] == 'ctl_vehicle'\n",
    "    elif 'pert_iname' in meta.columns:\n",
    "        dmso_mask = meta['pert_iname'].str.upper() == 'DMSO'\n",
    "    else:\n",
    "        dmso_mask = np.ones(len(meta), dtype=bool)\n",
    "        print(\"   âš ï¸  Using all samples as controls\")\n",
    "    \n",
    "    print(f\"   âœ“ DMSO samples: {dmso_mask.sum():,}\")\n",
    "    \n",
    "    # æŒ‰plateåˆ†ç»„è®¡ç®—å‡å€¼\n",
    "    unique_plates = meta.loc[dmso_mask, plate_col].unique()\n",
    "    print(f\"   âœ“ Unique plates: {len(unique_plates):,}\")\n",
    "    \n",
    "    dmso_plate_means = {}\n",
    "    samples_per_plate = []\n",
    "    \n",
    "    for plate in tqdm(unique_plates, desc=\"   Processing plates\"):\n",
    "        plate_mask = (meta[plate_col] == plate) & dmso_mask\n",
    "        plate_indices = np.where(plate_mask)[0]\n",
    "        \n",
    "        if len(plate_indices) > 0:\n",
    "            plate_mean = matrix[plate_indices].mean(axis=0)\n",
    "            dmso_plate_means[plate] = plate_mean\n",
    "            samples_per_plate.append(len(plate_indices))\n",
    "    \n",
    "    print(f\"\\n   âœ“ Computed means for {len(dmso_plate_means):,} plates\")\n",
    "    print(f\"   âœ“ Samples per plate: mean={np.mean(samples_per_plate):.1f}, \"\n",
    "          f\"median={np.median(samples_per_plate):.0f}\")\n",
    "    \n",
    "    return dmso_plate_means\n",
    "\n",
    "# è®¡ç®—plateå‡å€¼\n",
    "dmso_plate_means = compute_plate_dmso_means(level3_matrix, level3_meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. è®¡ç®—ç»†èƒç³»ä¸Šä¸‹æ–‡å‘é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cell_line_contexts(meta: pd.DataFrame,\n",
    "                                dmso_plate_means: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    è®¡ç®—æ¯ä¸ªç»†èƒç³»çš„ä¸Šä¸‹æ–‡å‘é‡\n",
    "    \n",
    "    å¯¹äºæ¯ä¸ªç»†èƒç³»ï¼Œè®¡ç®—å…¶æ‰€æœ‰plateçš„DMSOå‡å€¼çš„å¹³å‡\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸ”¬ Computing Cell Line Context Vectors\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # ç¡®å®šåˆ—å\n",
    "    cell_col = 'cell_iname' if 'cell_iname' in meta.columns else 'cell_mfc_name'\n",
    "    plate_col = 'det_plate' if 'det_plate' in meta.columns else 'rna_plate'\n",
    "    \n",
    "    print(f\"   Using '{cell_col}' as cell identifier\")\n",
    "    print(f\"   Using '{plate_col}' as plate identifier\")\n",
    "    \n",
    "    # æ„å»º cell -> plates æ˜ å°„\n",
    "    cell_to_plates = defaultdict(set)\n",
    "    \n",
    "    for plate in dmso_plate_means.keys():\n",
    "        plate_meta = meta[meta[plate_col] == plate]\n",
    "        if len(plate_meta) > 0:\n",
    "            cells = plate_meta[cell_col].unique()\n",
    "            for cell in cells:\n",
    "                cell_to_plates[cell].add(plate)\n",
    "    \n",
    "    print(f\"   âœ“ Cell lines with DMSO data: {len(cell_to_plates)}\")\n",
    "    \n",
    "    # è®¡ç®—æ¯ä¸ªç»†èƒç³»çš„ä¸Šä¸‹æ–‡\n",
    "    cell_contexts = {}\n",
    "    plates_per_cell = []\n",
    "    \n",
    "    for cell, plates in tqdm(cell_to_plates.items(), desc=\"   Computing contexts\"):\n",
    "        plate_means = [dmso_plate_means[p] for p in plates if p in dmso_plate_means]\n",
    "        if plate_means:\n",
    "            cell_context = np.mean(plate_means, axis=0)\n",
    "            cell_contexts[cell] = cell_context\n",
    "            plates_per_cell.append(len(plate_means))\n",
    "    \n",
    "    print(f\"\\n   âœ“ Computed contexts for {len(cell_contexts)} cell lines\")\n",
    "    print(f\"   âœ“ Plates per cell line: mean={np.mean(plates_per_cell):.1f}, \"\n",
    "          f\"median={np.median(plates_per_cell):.0f}\")\n",
    "    \n",
    "    return cell_contexts\n",
    "\n",
    "# è®¡ç®—ç»†èƒç³»ä¸Šä¸‹æ–‡\n",
    "cell_contexts_raw = compute_cell_line_contexts(level3_meta, dmso_plate_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–åŸå§‹ä¸Šä¸‹æ–‡\n",
    "print(\"\\nğŸ“Š Visualizing Raw Cell Line Contexts...\")\n",
    "\n",
    "# è½¬æ¢ä¸ºçŸ©é˜µ\n",
    "cell_ids = list(cell_contexts_raw.keys())\n",
    "context_matrix_raw = np.array([cell_contexts_raw[c] for c in cell_ids])\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# çƒ­å›¾ï¼ˆå‰50ä¸ªç»†èƒç³»ï¼Œå‰100ä¸ªåŸºå› ï¼‰\n",
    "ax1 = axes[0, 0]\n",
    "n_cells_show = min(50, len(cell_ids))\n",
    "n_genes_show = 100\n",
    "im = ax1.imshow(context_matrix_raw[:n_cells_show, :n_genes_show], aspect='auto', cmap='RdBu_r')\n",
    "ax1.set_xlabel('Gene Index')\n",
    "ax1.set_ylabel('Cell Line Index')\n",
    "ax1.set_title('Raw Context Matrix (Subset)', fontweight='bold')\n",
    "plt.colorbar(im, ax=ax1, label='Expression')\n",
    "\n",
    "# åŸºå› è¡¨è¾¾åˆ†å¸ƒ\n",
    "ax2 = axes[0, 1]\n",
    "ax2.hist(context_matrix_raw.flatten(), bins=100, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "ax2.set_xlabel('Expression Value')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Distribution of Raw Context Values', fontweight='bold')\n",
    "\n",
    "# æ¯ä¸ªåŸºå› çš„å‡å€¼åˆ†å¸ƒ\n",
    "ax3 = axes[1, 0]\n",
    "gene_means = context_matrix_raw.mean(axis=0)\n",
    "ax3.hist(gene_means, bins=50, color='coral', edgecolor='black', alpha=0.7)\n",
    "ax3.set_xlabel('Mean Expression per Gene')\n",
    "ax3.set_ylabel('Number of Genes')\n",
    "ax3.set_title('Gene-wise Mean Distribution', fontweight='bold')\n",
    "\n",
    "# æ¯ä¸ªåŸºå› çš„æ ‡å‡†å·®åˆ†å¸ƒ\n",
    "ax4 = axes[1, 1]\n",
    "gene_stds = context_matrix_raw.std(axis=0)\n",
    "ax4.hist(gene_stds, bins=50, color='forestgreen', edgecolor='black', alpha=0.7)\n",
    "ax4.set_xlabel('Std Expression per Gene')\n",
    "ax4.set_ylabel('Number of Genes')\n",
    "ax4.set_title('Gene-wise Std Distribution', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(VIZ_DIR / 'raw_context_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n   Raw context statistics:\")\n",
    "print(f\"      â€¢ Overall mean: {context_matrix_raw.mean():.4f}\")\n",
    "print(f\"      â€¢ Overall std: {context_matrix_raw.std():.4f}\")\n",
    "print(f\"      â€¢ Min: {context_matrix_raw.min():.4f}\")\n",
    "print(f\"      â€¢ Max: {context_matrix_raw.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. åº”ç”¨Rank-based Inverse Normal Transformation (INT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rank_based_int(contexts: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    åº”ç”¨Rank-based Inverse Normal Transformation (INT)\n",
    "    \n",
    "    å¯¹äºæ¯ä¸ªåŸºå› ï¼Œè®¡ç®—å…¶åœ¨æ‰€æœ‰ç»†èƒç³»ä¸Šä¸‹æ–‡ä¸­çš„æ’åï¼Œ\n",
    "    ç„¶åè½¬æ¢ä¸ºæ ‡å‡†æ­£æ€åˆ†ä½æ•°\n",
    "    \n",
    "    å…¬å¼ï¼š\n",
    "    r_{g,i} = rank({x_ctx,g^{(j)}}_{j=1}^N)\n",
    "    tilde{x}_{ctx,g}^{(i)} = Î¦^{-1}(r_{g,i} / (N+1))\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸ“Š Applying Rank-based Inverse Normal Transformation (INT)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    cell_ids = list(contexts.keys())\n",
    "    n_cells = len(cell_ids)\n",
    "    n_genes = len(list(contexts.values())[0])\n",
    "    \n",
    "    print(f\"   Cell lines: {n_cells}\")\n",
    "    print(f\"   Genes: {n_genes}\")\n",
    "    \n",
    "    # æ„å»ºçŸ©é˜µ\n",
    "    context_matrix = np.array([contexts[c] for c in cell_ids])\n",
    "    \n",
    "    # å¯¹æ¯ä¸ªåŸºå› åº”ç”¨INT\n",
    "    print(\"\\n   Applying INT per gene...\")\n",
    "    int_matrix = np.zeros_like(context_matrix)\n",
    "    \n",
    "    for g in tqdm(range(n_genes), desc=\"   Processing genes\"):\n",
    "        gene_values = context_matrix[:, g]\n",
    "        \n",
    "        # è®¡ç®—æ’åï¼ˆå¹³å‡æ’åå¤„ç†å¹¶åˆ—ï¼‰\n",
    "        ranks = rankdata(gene_values, method='average')\n",
    "        \n",
    "        # è½¬æ¢ä¸ºæ ‡å‡†æ­£æ€åˆ†ä½æ•°\n",
    "        quantiles = ranks / (n_cells + 1)\n",
    "        int_values = stats.norm.ppf(quantiles)\n",
    "        \n",
    "        int_matrix[:, g] = int_values\n",
    "    \n",
    "    # é‡å»ºå­—å…¸\n",
    "    int_contexts = {cell_ids[i]: int_matrix[i] for i in range(n_cells)}\n",
    "    \n",
    "    print(f\"\\n   ğŸ“Š INT Validation:\")\n",
    "    print(f\"      â€¢ Mean per gene (target ~0): {int_matrix.mean(axis=0).mean():.6f}\")\n",
    "    print(f\"      â€¢ Std per gene (target ~1): {int_matrix.std(axis=0).mean():.6f}\")\n",
    "    print(f\"      â€¢ Overall mean: {int_matrix.mean():.6f}\")\n",
    "    print(f\"      â€¢ Overall std: {int_matrix.std():.6f}\")\n",
    "    \n",
    "    return int_contexts, int_matrix\n",
    "\n",
    "# åº”ç”¨INT\n",
    "cell_contexts_int, int_matrix = apply_rank_based_int(cell_contexts_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–INTå½’ä¸€åŒ–æ•ˆæœ\n",
    "print(\"\\nğŸ“Š Visualizing INT Normalized Contexts...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# çƒ­å›¾\n",
    "ax1 = axes[0, 0]\n",
    "n_cells_show = min(50, int_matrix.shape[0])\n",
    "n_genes_show = 100\n",
    "im = ax1.imshow(int_matrix[:n_cells_show, :n_genes_show], aspect='auto', cmap='RdBu_r', vmin=-3, vmax=3)\n",
    "ax1.set_xlabel('Gene Index')\n",
    "ax1.set_ylabel('Cell Line Index')\n",
    "ax1.set_title('INT Normalized Context Matrix (Subset)', fontweight='bold')\n",
    "plt.colorbar(im, ax=ax1, label='Z-score')\n",
    "\n",
    "# æ•´ä½“åˆ†å¸ƒï¼ˆä¸æ ‡å‡†æ­£æ€å¯¹æ¯”ï¼‰\n",
    "ax2 = axes[0, 1]\n",
    "ax2.hist(int_matrix.flatten(), bins=100, density=True, color='steelblue', \n",
    "         edgecolor='black', alpha=0.7, label='INT Values')\n",
    "x = np.linspace(-4, 4, 100)\n",
    "ax2.plot(x, stats.norm.pdf(x), 'r-', linewidth=2, label='Standard Normal')\n",
    "ax2.set_xlabel('Value')\n",
    "ax2.set_ylabel('Density')\n",
    "ax2.set_title('INT Values vs Standard Normal', fontweight='bold')\n",
    "ax2.legend()\n",
    "\n",
    "# Q-Q Plot\n",
    "ax3 = axes[1, 0]\n",
    "sample_values = np.random.choice(int_matrix.flatten(), size=10000, replace=False)\n",
    "stats.probplot(sample_values, dist=\"norm\", plot=ax3)\n",
    "ax3.set_title('Q-Q Plot (INT vs Normal)', fontweight='bold')\n",
    "\n",
    "# å‰åå¯¹æ¯”ï¼ˆé€‰æ‹©ä¸€ä¸ªåŸºå› ï¼‰\n",
    "ax4 = axes[1, 1]\n",
    "gene_idx = 0\n",
    "ax4.scatter(context_matrix_raw[:, gene_idx], int_matrix[:, gene_idx], alpha=0.5, s=10)\n",
    "ax4.set_xlabel(f'Raw Value (Gene {gene_idx})')\n",
    "ax4.set_ylabel(f'INT Value (Gene {gene_idx})')\n",
    "ax4.set_title('Raw vs INT Transformation (Single Gene)', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(VIZ_DIR / 'int_normalization_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n   âœ“ INT normalization successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. æ„å»ºæœ€ç»ˆæ•°æ®é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cell_id_mapping(cell_contexts: Dict[str, np.ndarray],\n",
    "                          training_meta: pd.DataFrame) -> Dict[str, int]:\n",
    "    \"\"\"æ„å»ºç»†èƒç³»IDåˆ°ç´¢å¼•çš„æ˜ å°„\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸ·ï¸  Building Cell Line ID Mapping\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    training_cells = set(training_meta['cell_iname'].unique())\n",
    "    context_cells = set(cell_contexts.keys())\n",
    "    \n",
    "    common_cells = training_cells & context_cells\n",
    "    missing_cells = training_cells - context_cells\n",
    "    \n",
    "    print(f\"   Training data cell lines: {len(training_cells)}\")\n",
    "    print(f\"   Cell lines with context: {len(context_cells)}\")\n",
    "    print(f\"   Common cell lines: {len(common_cells)}\")\n",
    "    print(f\"   Missing cell lines: {len(missing_cells)}\")\n",
    "    \n",
    "    if missing_cells:\n",
    "        print(f\"\\n   âš ï¸  Missing cell lines: {list(missing_cells)[:10]}...\")\n",
    "    \n",
    "    # æ„å»ºæ˜ å°„\n",
    "    sorted_cells = sorted(list(common_cells))\n",
    "    cell_id_to_idx = {cell: idx for idx, cell in enumerate(sorted_cells)}\n",
    "    \n",
    "    print(f\"\\n   âœ“ Built mapping for {len(cell_id_to_idx)} cell lines\")\n",
    "    \n",
    "    return cell_id_to_idx, missing_cells\n",
    "\n",
    "# æ„å»ºæ˜ å°„\n",
    "cell_id_to_idx, missing_cells = build_cell_id_mapping(\n",
    "    cell_contexts_int, \n",
    "    training_data['sample_meta']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_contexts_to_samples(training_data: Dict,\n",
    "                               cell_contexts: Dict[str, np.ndarray],\n",
    "                               cell_id_to_idx: Dict[str, int]) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"å°†ç»†èƒç³»ä¸Šä¸‹æ–‡åŒ¹é…åˆ°è®­ç»ƒæ ·æœ¬\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸ”— Matching Contexts to Training Samples\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    meta = training_data['sample_meta']\n",
    "    n_samples = len(meta)\n",
    "    n_genes = 978\n",
    "    \n",
    "    context_matrix = np.zeros((n_samples, n_genes), dtype=np.float32)\n",
    "    cell_ids = np.zeros(n_samples, dtype=np.int32)\n",
    "    has_context_mask = np.zeros(n_samples, dtype=bool)\n",
    "    \n",
    "    print(f\"   Processing {n_samples:,} samples...\")\n",
    "    \n",
    "    n_matched = 0\n",
    "    n_missing = 0\n",
    "    \n",
    "    for idx in tqdm(range(len(meta)), desc=\"   Matching\"):\n",
    "        cell_iname = meta.iloc[idx]['cell_iname']\n",
    "        \n",
    "        if cell_iname in cell_contexts and cell_iname in cell_id_to_idx:\n",
    "            context_matrix[idx] = cell_contexts[cell_iname]\n",
    "            cell_ids[idx] = cell_id_to_idx[cell_iname]\n",
    "            has_context_mask[idx] = True\n",
    "            n_matched += 1\n",
    "        else:\n",
    "            context_matrix[idx] = np.zeros(n_genes, dtype=np.float32)\n",
    "            cell_ids[idx] = -1\n",
    "            has_context_mask[idx] = False\n",
    "            n_missing += 1\n",
    "    \n",
    "    print(f\"\\n   âœ“ Matched: {n_matched:,} ({n_matched/n_samples*100:.1f}%)\")\n",
    "    print(f\"   âš ï¸  Missing: {n_missing:,} ({n_missing/n_samples*100:.1f}%)\")\n",
    "    \n",
    "    return context_matrix, cell_ids, has_context_mask\n",
    "\n",
    "# åŒ¹é…ä¸Šä¸‹æ–‡\n",
    "X_ctx, cell_ids_array, has_context_mask = match_contexts_to_samples(\n",
    "    training_data, cell_contexts_int, cell_id_to_idx\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ„å»ºæœ€ç»ˆæ•°æ®é›†\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸš€ Building Final HCA-DR Dataset\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# è¿‡æ»¤æ²¡æœ‰ä¸Šä¸‹æ–‡çš„æ ·æœ¬\n",
    "FILTER_NO_CONTEXT = True\n",
    "\n",
    "if FILTER_NO_CONTEXT:\n",
    "    valid_mask = has_context_mask\n",
    "    n_before = len(has_context_mask)\n",
    "    n_after = valid_mask.sum()\n",
    "    \n",
    "    print(f\"   Filtering samples without context...\")\n",
    "    print(f\"   Before: {n_before:,}\")\n",
    "    print(f\"   After: {n_after:,}\")\n",
    "    print(f\"   Removed: {n_before - n_after:,}\")\n",
    "    \n",
    "    # è¿‡æ»¤æ•°æ®\n",
    "    X_pert = training_data['X'][valid_mask]\n",
    "    X_ctx_filtered = X_ctx[valid_mask]\n",
    "    cell_ids_filtered = cell_ids_array[valid_mask]\n",
    "    folds = training_data['folds'][valid_mask]\n",
    "    sample_meta = training_data['sample_meta'][valid_mask].reset_index(drop=True)\n",
    "    \n",
    "    # é‡æ–°æ„å»ºåŒ–åˆç‰©æ ‡ç­¾\n",
    "    unique_perts = sorted(sample_meta['pert_id'].unique())\n",
    "    new_pert_to_idx = {pert: idx for idx, pert in enumerate(unique_perts)}\n",
    "    y = np.array([new_pert_to_idx[p] for p in sample_meta['pert_id']], dtype=np.int32)\n",
    "    \n",
    "    compound_names = unique_perts\n",
    "    pert_to_idx = new_pert_to_idx\n",
    "else:\n",
    "    X_pert = training_data['X']\n",
    "    X_ctx_filtered = X_ctx\n",
    "    cell_ids_filtered = cell_ids_array\n",
    "    folds = training_data['folds']\n",
    "    sample_meta = training_data['sample_meta']\n",
    "    y = training_data['y']\n",
    "    compound_names = training_data['compound_names']\n",
    "    pert_to_idx = training_data['pert_to_idx']\n",
    "\n",
    "print(f\"\\n   âœ“ X_pert shape: {X_pert.shape}\")\n",
    "print(f\"   âœ“ X_ctx shape: {X_ctx_filtered.shape}\")\n",
    "print(f\"   âœ“ y shape: {y.shape}\")\n",
    "print(f\"   âœ“ cell_ids shape: {cell_ids_filtered.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ„å»ºæœ€ç»ˆæ•°æ®å­—å…¸\n",
    "hca_dr_data = {\n",
    "    # æ ¸å¿ƒæ•°æ®\n",
    "    'X_pert': X_pert,                    # æ‰°åŠ¨ç­¾å (n_samples, 978)\n",
    "    'X_ctx': X_ctx_filtered,             # ç»†èƒç³»ä¸Šä¸‹æ–‡ (n_samples, 978)\n",
    "    'y': y,                              # è¯ç‰©æ ‡ç­¾ (n_samples,)\n",
    "    'cell_ids': cell_ids_filtered,       # ç»†èƒç³»ID (n_samples,)\n",
    "    'folds': folds,                      # äº¤å‰éªŒè¯æŠ˜ (n_samples,)\n",
    "    \n",
    "    # å…ƒæ•°æ®\n",
    "    'sample_meta': sample_meta,\n",
    "    'gene_names': training_data['gene_names'],\n",
    "    'compound_names': compound_names,\n",
    "    'pert_to_idx': pert_to_idx,\n",
    "    'cell_id_to_idx': cell_id_to_idx,\n",
    "    'idx_to_cell_id': {v: k for k, v in cell_id_to_idx.items()},\n",
    "    \n",
    "    # åŸå§‹ä¸Šä¸‹æ–‡ï¼ˆç”¨äºåˆ†æï¼‰\n",
    "    'cell_contexts_raw': cell_contexts_raw,\n",
    "    'cell_contexts_int': cell_contexts_int,\n",
    "    \n",
    "    # ç»Ÿè®¡ä¿¡æ¯\n",
    "    'n_samples': len(X_pert),\n",
    "    'n_compounds': len(compound_names),\n",
    "    'n_genes': X_pert.shape[1],\n",
    "    'n_cell_lines': len(cell_id_to_idx),\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… HCA-DR Dataset Summary\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"   â€¢ Samples: {hca_dr_data['n_samples']:,}\")\n",
    "print(f\"   â€¢ Compounds: {hca_dr_data['n_compounds']:,}\")\n",
    "print(f\"   â€¢ Genes: {hca_dr_data['n_genes']}\")\n",
    "print(f\"   â€¢ Cell lines: {hca_dr_data['n_cell_lines']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ä¿å­˜æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜\n",
    "output_path = OUTPUT_DIR / OUTPUT_FILE\n",
    "print(f\"\\nğŸ’¾ Saving HCA-DR data to: {output_path}\")\n",
    "\n",
    "with open(output_path, 'wb') as f:\n",
    "    pickle.dump(hca_dr_data, f, protocol=4)\n",
    "\n",
    "file_size_mb = output_path.stat().st_size / (1024**2)\n",
    "print(f\"   âœ“ Saved successfully! ({file_size_mb:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. æ•°æ®éªŒè¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# éªŒè¯åŠ è½½\n",
    "print(\"\\nğŸ“– Verifying saved data...\")\n",
    "\n",
    "with open(output_path, 'rb') as f:\n",
    "    loaded_data = pickle.load(f)\n",
    "\n",
    "print(f\"\\n   âœ“ Data keys: {list(loaded_data.keys())}\")\n",
    "print(f\"   âœ“ X_pert shape: {loaded_data['X_pert'].shape}\")\n",
    "print(f\"   âœ“ X_ctx shape: {loaded_data['X_ctx'].shape}\")\n",
    "print(f\"   âœ“ y shape: {loaded_data['y'].shape}\")\n",
    "print(f\"   âœ“ cell_ids shape: {loaded_data['cell_ids'].shape}\")\n",
    "\n",
    "# éªŒè¯æ•°æ®ä¸€è‡´æ€§\n",
    "assert np.array_equal(loaded_data['X_pert'], hca_dr_data['X_pert']), \"X_pert mismatch!\"\n",
    "assert np.array_equal(loaded_data['X_ctx'], hca_dr_data['X_ctx']), \"X_ctx mismatch!\"\n",
    "assert np.array_equal(loaded_data['y'], hca_dr_data['y']), \"y mismatch!\"\n",
    "print(f\"\\n   âœ“ Data integrity verified!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æœ€ç»ˆå¯è§†åŒ–\n",
    "print(\"\\nğŸ“Š Final Data Visualization...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "# 1. X_pertåˆ†å¸ƒ\n",
    "ax1 = axes[0, 0]\n",
    "ax1.hist(loaded_data['X_pert'].flatten(), bins=100, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "ax1.set_xlabel('Value')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('X_pert (Perturbation Signatures) Distribution', fontweight='bold')\n",
    "\n",
    "# 2. X_ctxåˆ†å¸ƒ\n",
    "ax2 = axes[0, 1]\n",
    "ax2.hist(loaded_data['X_ctx'].flatten(), bins=100, color='coral', alpha=0.7, edgecolor='black')\n",
    "ax2.set_xlabel('Value')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('X_ctx (Cell Contexts, INT Normalized) Distribution', fontweight='bold')\n",
    "\n",
    "# 3. è¯ç‰©æ ‡ç­¾åˆ†å¸ƒ\n",
    "ax3 = axes[0, 2]\n",
    "drug_counts = pd.Series(loaded_data['y']).value_counts().sort_index()\n",
    "ax3.hist(drug_counts.values, bins=50, color='forestgreen', alpha=0.7, edgecolor='black')\n",
    "ax3.set_xlabel('Samples per Drug')\n",
    "ax3.set_ylabel('Number of Drugs')\n",
    "ax3.set_title('Drug Label Distribution', fontweight='bold')\n",
    "\n",
    "# 4. ç»†èƒç³»IDåˆ†å¸ƒ\n",
    "ax4 = axes[1, 0]\n",
    "cell_counts = pd.Series(loaded_data['cell_ids']).value_counts().sort_index()\n",
    "ax4.bar(range(len(cell_counts)), cell_counts.values, color='mediumpurple', alpha=0.7)\n",
    "ax4.set_xlabel('Cell Line Index')\n",
    "ax4.set_ylabel('Number of Samples')\n",
    "ax4.set_title('Samples per Cell Line', fontweight='bold')\n",
    "\n",
    "# 5. X_pert vs X_ctxç›¸å…³æ€§ï¼ˆéšæœºé‡‡æ ·ï¼‰\n",
    "ax5 = axes[1, 1]\n",
    "sample_idx = np.random.choice(len(loaded_data['X_pert']), size=1000, replace=False)\n",
    "pert_sample = loaded_data['X_pert'][sample_idx].flatten()\n",
    "ctx_sample = loaded_data['X_ctx'][sample_idx].flatten()\n",
    "ax5.hexbin(pert_sample, ctx_sample, gridsize=50, cmap='YlOrRd', mincnt=1)\n",
    "ax5.set_xlabel('X_pert Value')\n",
    "ax5.set_ylabel('X_ctx Value')\n",
    "ax5.set_title('X_pert vs X_ctx (Sampled)', fontweight='bold')\n",
    "plt.colorbar(ax5.collections[0], ax=ax5, label='Count')\n",
    "\n",
    "# 6. äº¤å‰éªŒè¯æŠ˜åˆ†å¸ƒ\n",
    "ax6 = axes[1, 2]\n",
    "fold_counts = pd.Series(loaded_data['folds']).value_counts().sort_index()\n",
    "bars = ax6.bar(fold_counts.index, fold_counts.values, color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "ax6.set_xlabel('Fold')\n",
    "ax6.set_ylabel('Number of Samples')\n",
    "ax6.set_title('Cross-Validation Fold Distribution', fontweight='bold')\n",
    "for bar, val in zip(bars, fold_counts.values):\n",
    "    ax6.text(bar.get_x() + bar.get_width()/2, val, f'{val:,}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(VIZ_DIR / 'hca_dr_final_data_summary.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ¯ HCA-DR Data Preprocessing Complete!\")\n",
    "print(f\"   Output: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. ä½¿ç”¨ç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å±•ç¤ºå¦‚ä½•ä½¿ç”¨æ•°æ®è¿›è¡Œè®­ç»ƒ\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ“ Usage Example for HCA-DR Training\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "example_code = '''\n",
    "# åŠ è½½HCA-DRæ•°æ®\n",
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "with open(\"hca_dr_training_data.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# åˆ›å»ºPyTorch Dataset\n",
    "class HCADRDataset(Dataset):\n",
    "    def __init__(self, data, fold_ids, mode=\"train\"):\n",
    "        \"\"\"\n",
    "        HCA-DRæ•°æ®é›†\n",
    "        \n",
    "        å‚æ•°ï¼š\n",
    "            data: HCA-DRæ•°æ®å­—å…¸\n",
    "            fold_ids: ç”¨äºè®­ç»ƒ/éªŒè¯çš„foldåˆ—è¡¨\n",
    "            mode: \"train\" æˆ– \"val\"\n",
    "        \"\"\"\n",
    "        mask = np.isin(data[\"folds\"], fold_ids)\n",
    "        \n",
    "        self.X_pert = torch.FloatTensor(data[\"X_pert\"][mask])\n",
    "        self.X_ctx = torch.FloatTensor(data[\"X_ctx\"][mask])\n",
    "        self.y = torch.LongTensor(data[\"y\"][mask])\n",
    "        self.cell_ids = torch.LongTensor(data[\"cell_ids\"][mask])\n",
    "        \n",
    "        self.context_dropout = 0.15 if mode == \"train\" else 0.0\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x_pert = self.X_pert[idx]\n",
    "        x_ctx = self.X_ctx[idx]\n",
    "        \n",
    "        # Context Dropoutï¼ˆè®­ç»ƒæ—¶ï¼‰\n",
    "        if np.random.random() < self.context_dropout:\n",
    "            x_ctx = torch.zeros_like(x_ctx)\n",
    "            is_dropout = 1\n",
    "        else:\n",
    "            is_dropout = 0\n",
    "        \n",
    "        return {\n",
    "            \"x_pert\": x_pert,\n",
    "            \"x_ctx\": x_ctx,\n",
    "            \"y\": self.y[idx],\n",
    "            \"cell_id\": self.cell_ids[idx],\n",
    "            \"is_dropout\": is_dropout\n",
    "        }\n",
    "\n",
    "# åˆ›å»ºDataLoader\n",
    "train_dataset = HCADRDataset(data, fold_ids=[0, 1], mode=\"train\")\n",
    "val_dataset = HCADRDataset(data, fold_ids=[2], mode=\"val\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Val samples: {len(val_dataset)}\")\n",
    "'''\n",
    "\n",
    "print(example_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… ALL PREPROCESSING COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nğŸ“ Output files:\")\n",
    "print(f\"   â€¢ Data: {OUTPUT_DIR / OUTPUT_FILE}\")\n",
    "print(f\"   â€¢ Visualizations: {VIZ_DIR}\")\n",
    "print(f\"\\nğŸ¯ Ready for HCA-DR model training!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
