"""
HCA-DR Evaluation Script
å…¨é¢è¯„ä¼°è„šæœ¬ï¼ŒåŒ…æ‹¬ï¼š
1. Top-k% Recall
2. LOCO (Leave-One-Cell-Out) è¯„ä¼°
3. OOD (Out-of-Distribution) è¯„ä¼°
4. Alphaå€¼åˆ†æ

ç”¨æ³•ï¼š
    python eval.py --checkpoint <path_to_checkpoint> --data_path <path_to_data>
"""

import argparse
import torch
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from collections import defaultdict
from tqdm import tqdm
import json
import matplotlib.pyplot as plt
import seaborn as sns

from model import HCADR, HCADROutput
from dataset import (
    load_data, HCADRDataset, LOCODataset, OODDataset, collate_fn
)
from torch.utils.data import DataLoader
from config import HCADRConfig


class HCADREvaluator:
    """
    HCA-DRè¯„ä¼°å™¨
    
    å®ç°å¤šç§è¯„ä¼°æŒ‡æ ‡
    """
    
    def __init__(self,
                 model: HCADR,
                 device: str = 'cuda'):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        å‚æ•°ï¼š
            model: è®­ç»ƒå¥½çš„HCA-DRæ¨¡å‹
            device: è®¡ç®—è®¾å¤‡
        """
        self.model = model.to(device)
        self.model.eval()
        self.device = device
    
    @torch.no_grad()
    def compute_top_k_recall(self,
                             dataloader: DataLoader,
                             k_percentages: List[float] = [0.01, 0.05, 0.1]) -> Dict:
        """
        è®¡ç®—Top-k% Recall
        
        å¯¹äºæ¯ä¸ªåŒ–åˆç‰©ï¼Œè®¡ç®—å…¶æ ·æœ¬è¢«æ­£ç¡®é¢„æµ‹åœ¨top-k%çš„æ¯”ä¾‹
        
        å‚æ•°ï¼š
            dataloader: æ•°æ®åŠ è½½å™¨
            k_percentages: kçš„ç™¾åˆ†æ¯”åˆ—è¡¨
        
        è¿”å›ï¼š
            åŒ…å«å„kå€¼recallçš„å­—å…¸
        """
        print("\nğŸ“Š Computing Top-k% Recall...")
        
        all_logits = []
        all_labels = []
        all_compounds = []
        
        for batch in tqdm(dataloader, desc="   Evaluating"):
            x_pert = batch['x_pert'].to(self.device)
            x_ctx = batch['x_ctx'].to(self.device)
            y = batch['y']
            
            output = self.model(x_pert, x_ctx)
            
            all_logits.append(output.logits.cpu())
            all_labels.append(y)
            all_compounds.append(y)
        
        logits = torch.cat(all_logits, dim=0)
        labels = torch.cat(all_labels, dim=0)
        
        n_classes = logits.shape[1]
        results = {}
        
        for k_pct in k_percentages:
            k = max(1, int(k_pct * n_classes))
            
            # è·å–top-ké¢„æµ‹
            _, top_k_pred = torch.topk(logits, k, dim=1)
            
            # è®¡ç®—æ¯ä¸ªæ ·æœ¬æ˜¯å¦å‘½ä¸­
            hits = (top_k_pred == labels.unsqueeze(1)).any(dim=1).float()
            
            # æŒ‰åŒ–åˆç‰©åˆ†ç»„è®¡ç®—recall
            compound_recalls = defaultdict(list)
            for i, (label, hit) in enumerate(zip(labels.numpy(), hits.numpy())):
                compound_recalls[label].append(hit)
            
            # è®¡ç®—å¹³å‡recall
            per_compound_recall = {
                c: np.mean(hits) for c, hits in compound_recalls.items()
            }
            mean_recall = np.mean(list(per_compound_recall.values()))
            
            results[f'recall@{k_pct*100:.0f}%'] = mean_recall
            results[f'k@{k_pct*100:.0f}%'] = k
            
            print(f"   Top-{k_pct*100:.0f}% (k={k}): Recall = {mean_recall:.4f}")
        
        # æ•´ä½“å‡†ç¡®ç‡
        pred = logits.argmax(dim=1)
        accuracy = (pred == labels).float().mean().item()
        results['accuracy'] = accuracy
        print(f"   Accuracy: {accuracy:.4f}")
        
        return results
    
    @torch.no_grad()
    def analyze_alpha_values(self,
                             dataloader: DataLoader,
                             ood_dataloader: Optional[DataLoader] = None) -> Dict:
        """
        åˆ†æAlphaå€¼åˆ†å¸ƒ
        
        éªŒè¯æ¨¡å‹æ˜¯å¦å­¦ä¼šäº†åœ¨OODæƒ…å†µä¸‹å›é€€åˆ°å…¨å±€æ¨¡å‹
        
        å‚æ•°ï¼š
            dataloader: æ­£å¸¸æ•°æ®åŠ è½½å™¨
            ood_dataloader: OODæ•°æ®åŠ è½½å™¨ï¼ˆä¸Šä¸‹æ–‡å…¨ä¸º0ï¼‰
        
        è¿”å›ï¼š
            Alphaåˆ†æç»“æœ
        """
        print("\nğŸ“Š Analyzing Alpha Values...")
        
        # æ”¶é›†æ­£å¸¸æ ·æœ¬çš„alpha
        seen_alphas = []
        for batch in tqdm(dataloader, desc="   Normal samples"):
            x_pert = batch['x_pert'].to(self.device)
            x_ctx = batch['x_ctx'].to(self.device)
            
            output = self.model(x_pert, x_ctx)
            seen_alphas.append(output.alpha.cpu().numpy())
        
        seen_alphas = np.concatenate(seen_alphas, axis=0).flatten()
        
        results = {
            'seen_alpha_mean': float(np.mean(seen_alphas)),
            'seen_alpha_std': float(np.std(seen_alphas)),
            'seen_alpha_median': float(np.median(seen_alphas)),
            'seen_alpha_min': float(np.min(seen_alphas)),
            'seen_alpha_max': float(np.max(seen_alphas)),
        }
        
        print(f"   Seen samples Î±: mean={results['seen_alpha_mean']:.4f}, "
              f"std={results['seen_alpha_std']:.4f}")
        
        # å¦‚æœæä¾›äº†OODæ•°æ®
        if ood_dataloader is not None:
            ood_alphas = []
            for batch in tqdm(ood_dataloader, desc="   OOD samples"):
                x_pert = batch['x_pert'].to(self.device)
                x_ctx = batch['x_ctx'].to(self.device)  # å…¨ä¸º0
                
                output = self.model(x_pert, x_ctx)
                ood_alphas.append(output.alpha.cpu().numpy())
            
            ood_alphas = np.concatenate(ood_alphas, axis=0).flatten()
            
            results['ood_alpha_mean'] = float(np.mean(ood_alphas))
            results['ood_alpha_std'] = float(np.std(ood_alphas))
            results['ood_alpha_median'] = float(np.median(ood_alphas))
            
            # OOD Fallback Quality
            fallback_quality = (results['seen_alpha_mean'] - results['ood_alpha_mean']) / \
                               max(results['seen_alpha_mean'], 1e-6)
            results['ood_fallback_quality'] = fallback_quality
            
            print(f"   OOD samples Î±: mean={results['ood_alpha_mean']:.4f}, "
                  f"std={results['ood_alpha_std']:.4f}")
            print(f"   OOD Fallback Quality: {fallback_quality:.4f}")
        
        return results
    
    @torch.no_grad()
    def loco_evaluation(self,
                        data: Dict,
                        cell_ids_to_test: Optional[List[int]] = None,
                        n_cells: int = 10,
                        batch_size: int = 256) -> Dict:
        """
        Leave-One-Cell-Out (LOCO) è¯„ä¼°
        
        å¯¹äºæ¯ä¸ªè¢«ç•™å‡ºçš„ç»†èƒç³»ï¼Œæµ‹è¯•æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›
        
        å‚æ•°ï¼š
            data: HCA-DRæ•°æ®å­—å…¸
            cell_ids_to_test: è¦æµ‹è¯•çš„ç»†èƒç³»IDåˆ—è¡¨
            n_cells: å¦‚æœæœªæŒ‡å®šcell_ids_to_testï¼Œéšæœºé€‰æ‹©çš„ç»†èƒç³»æ•°é‡
            batch_size: æ‰¹å¤§å°
        
        è¿”å›ï¼š
            LOCOè¯„ä¼°ç»“æœ
        """
        print("\nğŸ“Š LOCO (Leave-One-Cell-Out) Evaluation...")
        
        # è·å–æ‰€æœ‰ç»†èƒç³»
        unique_cells = np.unique(data['cell_ids'])
        unique_cells = unique_cells[unique_cells >= 0]  # æ’é™¤-1
        
        if cell_ids_to_test is None:
            if len(unique_cells) > n_cells:
                cell_ids_to_test = np.random.choice(unique_cells, n_cells, replace=False)
            else:
                cell_ids_to_test = unique_cells
        
        print(f"   Testing {len(cell_ids_to_test)} cell lines")
        
        loco_results = {}
        all_recalls = []
        all_alphas = []
        
        for cell_id in tqdm(cell_ids_to_test, desc="   LOCO"):
            # åˆ›å»ºæµ‹è¯•æ•°æ®é›†ï¼ˆåªåŒ…å«è¯¥ç»†èƒç³»ï¼‰
            test_dataset = LOCODataset(data, held_out_cell=int(cell_id), mode="test")
            
            if len(test_dataset) < 10:
                print(f"      Cell {cell_id}: too few samples ({len(test_dataset)}), skipping")
                continue
            
            test_loader = DataLoader(
                test_dataset,
                batch_size=batch_size,
                shuffle=False,
                collate_fn=collate_fn
            )
            
            # è¯„ä¼°
            correct = 0
            total = 0
            alphas = []
            
            for batch in test_loader:
                x_pert = batch['x_pert'].to(self.device)
                x_ctx = batch['x_ctx'].to(self.device)
                y = batch['y'].to(self.device)
                
                output = self.model(x_pert, x_ctx)
                pred = output.logits.argmax(dim=1)
                
                correct += (pred == y).sum().item()
                total += len(y)
                alphas.append(output.alpha.cpu().numpy())
            
            accuracy = correct / total
            mean_alpha = np.mean(np.concatenate(alphas))
            
            loco_results[int(cell_id)] = {
                'accuracy': accuracy,
                'n_samples': total,
                'mean_alpha': float(mean_alpha)
            }
            
            all_recalls.append(accuracy)
            all_alphas.append(mean_alpha)
        
        # æ±‡æ€»
        results = {
            'per_cell_results': loco_results,
            'mean_accuracy': float(np.mean(all_recalls)),
            'std_accuracy': float(np.std(all_recalls)),
            'mean_alpha': float(np.mean(all_alphas)),
            'n_cells_tested': len(cell_ids_to_test)
        }
        
        print(f"\n   LOCO Results:")
        print(f"      Mean Accuracy: {results['mean_accuracy']:.4f} Â± {results['std_accuracy']:.4f}")
        print(f"      Mean Alpha: {results['mean_alpha']:.4f}")
        
        return results
    
    @torch.no_grad()
    def ood_evaluation(self,
                       data: Dict,
                       fold_ids: List[int],
                       batch_size: int = 256) -> Dict:
        """
        OOD (Out-of-Distribution) è¯„ä¼°
        
        å°†æ‰€æœ‰ä¸Šä¸‹æ–‡è®¾ä¸ºé›¶å‘é‡ï¼Œæµ‹è¯•æ¨¡å‹çš„å›é€€èƒ½åŠ›
        
        å‚æ•°ï¼š
            data: HCA-DRæ•°æ®å­—å…¸
            fold_ids: ä½¿ç”¨çš„foldåˆ—è¡¨
            batch_size: æ‰¹å¤§å°
        
        è¿”å›ï¼š
            OODè¯„ä¼°ç»“æœ
        """
        print("\nğŸ“Š OOD (Zero-Context) Evaluation...")
        
        # åˆ›å»ºOODæ•°æ®é›†
        ood_dataset = OODDataset(data, fold_ids=fold_ids)
        ood_loader = DataLoader(
            ood_dataset,
            batch_size=batch_size,
            shuffle=False,
            collate_fn=collate_fn
        )
        
        # åŒæ—¶åˆ›å»ºæ­£å¸¸æ•°æ®é›†è¿›è¡Œå¯¹æ¯”
        normal_dataset = HCADRDataset(
            data, fold_ids=fold_ids, mode="val", context_dropout_prob=0.0
        )
        normal_loader = DataLoader(
            normal_dataset,
            batch_size=batch_size,
            shuffle=False,
            collate_fn=collate_fn
        )
        
        # è¯„ä¼°æ­£å¸¸æ ·æœ¬
        print("   Evaluating normal samples...")
        normal_correct = 0
        normal_total = 0
        normal_alphas = []
        
        for batch in tqdm(normal_loader, desc="      Normal"):
            x_pert = batch['x_pert'].to(self.device)
            x_ctx = batch['x_ctx'].to(self.device)
            y = batch['y'].to(self.device)
            
            output = self.model(x_pert, x_ctx)
            pred = output.logits.argmax(dim=1)
            
            normal_correct += (pred == y).sum().item()
            normal_total += len(y)
            normal_alphas.append(output.alpha.cpu().numpy())
        
        normal_accuracy = normal_correct / normal_total
        normal_alpha_mean = np.mean(np.concatenate(normal_alphas))
        
        # è¯„ä¼°OODæ ·æœ¬
        print("   Evaluating OOD samples (zero context)...")
        ood_correct = 0
        ood_total = 0
        ood_alphas = []
        
        for batch in tqdm(ood_loader, desc="      OOD"):
            x_pert = batch['x_pert'].to(self.device)
            x_ctx = batch['x_ctx'].to(self.device)  # å…¨ä¸º0
            y = batch['y'].to(self.device)
            
            output = self.model(x_pert, x_ctx)
            pred = output.logits.argmax(dim=1)
            
            ood_correct += (pred == y).sum().item()
            ood_total += len(y)
            ood_alphas.append(output.alpha.cpu().numpy())
        
        ood_accuracy = ood_correct / ood_total
        ood_alpha_mean = np.mean(np.concatenate(ood_alphas))
        
        # è®¡ç®—æ€§èƒ½ä¸‹é™
        accuracy_drop = normal_accuracy - ood_accuracy
        alpha_drop = normal_alpha_mean - ood_alpha_mean
        
        results = {
            'normal_accuracy': normal_accuracy,
            'normal_alpha': float(normal_alpha_mean),
            'ood_accuracy': ood_accuracy,
            'ood_alpha': float(ood_alpha_mean),
            'accuracy_drop': accuracy_drop,
            'alpha_drop': float(alpha_drop),
            'ood_fallback_quality': float(alpha_drop / max(normal_alpha_mean, 1e-6))
        }
        
        print(f"\n   OOD Results:")
        print(f"      Normal: Accuracy={normal_accuracy:.4f}, Î±={normal_alpha_mean:.4f}")
        print(f"      OOD: Accuracy={ood_accuracy:.4f}, Î±={ood_alpha_mean:.4f}")
        print(f"      Accuracy Drop: {accuracy_drop:.4f}")
        print(f"      Alpha Drop: {alpha_drop:.4f}")
        print(f"      OOD Fallback Quality: {results['ood_fallback_quality']:.4f}")
        
        return results
    
    def full_evaluation(self,
                        data: Dict,
                        val_folds: List[int] = [2],
                        batch_size: int = 256,
                        output_dir: Optional[str] = None) -> Dict:
        """
        å®Œæ•´è¯„ä¼°
        
        è¿è¡Œæ‰€æœ‰è¯„ä¼°æŒ‡æ ‡
        """
        print("\n" + "=" * 80)
        print("ğŸ¯ Full HCA-DR Evaluation")
        print("=" * 80)
        
        # åˆ›å»ºéªŒè¯æ•°æ®é›†
        val_dataset = HCADRDataset(
            data, fold_ids=val_folds, mode="val", context_dropout_prob=0.0
        )
        val_loader = DataLoader(
            val_dataset,
            batch_size=batch_size,
            shuffle=False,
            collate_fn=collate_fn
        )
        
        results = {}
        
        # 1. Top-k Recall
        results['top_k_recall'] = self.compute_top_k_recall(
            val_loader, 
            k_percentages=[0.01, 0.05, 0.1]
        )
        
        # 2. Alphaåˆ†æ
        ood_dataset = OODDataset(data, fold_ids=val_folds)
        ood_loader = DataLoader(
            ood_dataset,
            batch_size=batch_size,
            shuffle=False,
            collate_fn=collate_fn
        )
        results['alpha_analysis'] = self.analyze_alpha_values(val_loader, ood_loader)
        
        # 3. LOCOè¯„ä¼°
        results['loco'] = self.loco_evaluation(data, n_cells=10, batch_size=batch_size)
        
        # 4. OODè¯„ä¼°
        results['ood'] = self.ood_evaluation(data, val_folds, batch_size)
        
        # ä¿å­˜ç»“æœ
        if output_dir:
            output_path = Path(output_dir)
            output_path.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜JSON
            results_path = output_path / 'evaluation_results.json'
            
            # è½¬æ¢numpyç±»å‹
            def convert_to_python(obj):
                if isinstance(obj, np.floating):
                    return float(obj)
                elif isinstance(obj, np.integer):
                    return int(obj)
                elif isinstance(obj, dict):
                    return {k: convert_to_python(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [convert_to_python(v) for v in obj]
                return obj
            
            results_serializable = convert_to_python(results)
            
            with open(results_path, 'w') as f:
                json.dump(results_serializable, f, indent=2)
            
            print(f"\nâœ“ Results saved to: {results_path}")
        
        # æ‰“å°æ‘˜è¦
        print("\n" + "=" * 80)
        print("ğŸ“Š Evaluation Summary")
        print("=" * 80)
        print(f"Top-1% Recall: {results['top_k_recall']['recall@1%']:.4f}")
        print(f"Top-5% Recall: {results['top_k_recall']['recall@5%']:.4f}")
        print(f"Accuracy: {results['top_k_recall']['accuracy']:.4f}")
        print(f"LOCO Accuracy: {results['loco']['mean_accuracy']:.4f}")
        print(f"OOD Accuracy: {results['ood']['ood_accuracy']:.4f}")
        print(f"OOD Fallback Quality: {results['ood']['ood_fallback_quality']:.4f}")
        
        return results


def load_model_from_checkpoint(checkpoint_path: str, device: str = 'cuda') -> HCADR:
    """ä»æ£€æŸ¥ç‚¹åŠ è½½æ¨¡å‹"""
    print(f"ğŸ“¥ Loading model from: {checkpoint_path}")
    
    checkpoint = torch.load(checkpoint_path, map_location=device)
    config = checkpoint.get('config', None)
    
    if config is None:
        # ä½¿ç”¨é»˜è®¤é…ç½®
        from config import get_default_config
        config = get_default_config()
    
    # ä»state_dictæ¨æ–­n_compounds
    state_dict = checkpoint['model_state_dict']
    n_compounds = state_dict['classifier.classifier.weight'].shape[0]
    config.model.n_compounds = n_compounds
    
    # æ„å»ºæ¨¡å‹
    model = HCADR(
        n_genes=config.model.n_genes,
        n_compounds=config.model.n_compounds,
        n_cell_lines=config.model.n_cell_lines,
        encoder_hidden_dims=config.model.encoder_hidden_dims,
        encoder_dropout=config.model.encoder_dropout,
        context_hidden_dim=config.model.context_hidden_dim
    )
    
    model.load_state_dict(state_dict)
    print(f"âœ“ Model loaded successfully")
    
    return model


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='HCA-DR Evaluation')
    
    parser.add_argument('--checkpoint', type=str, required=True,
                        help='Path to model checkpoint')
    parser.add_argument('--data_path', type=str, required=True,
                        help='Path to HCA-DR data')
    parser.add_argument('--output_dir', type=str, default=None,
                        help='Output directory for results')
    parser.add_argument('--batch_size', type=int, default=256,
                        help='Batch size')
    parser.add_argument('--val_folds', type=int, nargs='+', default=[2],
                        help='Validation folds')
    parser.add_argument('--device', type=str, default='cuda',
                        help='Device')
    parser.add_argument('--loco_n_cells', type=int, default=10,
                        help='Number of cells for LOCO evaluation')
    
    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    print("=" * 80)
    print("ğŸ¯ HCA-DR Model Evaluation")
    print("=" * 80)
    
    # è®¾ç½®è®¾å¤‡
    if args.device == 'cuda' and not torch.cuda.is_available():
        print("âš ï¸  CUDA not available, using CPU")
        args.device = 'cpu'
    device = torch.device(args.device)
    
    # åŠ è½½æ¨¡å‹
    model = load_model_from_checkpoint(args.checkpoint, args.device)
    
    # åŠ è½½æ•°æ®
    data = load_data(args.data_path)
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = HCADREvaluator(model, device=args.device)
    
    # è¿è¡Œå®Œæ•´è¯„ä¼°
    results = evaluator.full_evaluation(
        data=data,
        val_folds=args.val_folds,
        batch_size=args.batch_size,
        output_dir=args.output_dir
    )
    
    print("\nâœ… Evaluation completed!")
    
    return results


if __name__ == "__main__":
    main()