"""
HCA-DR Dataset
æ•°æ®é›†å®šä¹‰ï¼Œæ”¯æŒContext Dropoutå’Œåˆ†å±‚é‡‡æ ·
"""

import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader, Sampler
import pickle
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from collections import defaultdict
import random


class HCADRDataset(Dataset):
    """
    HCA-DRæ•°æ®é›†
    
    ç‰¹æ€§ï¼š
    1. æ”¯æŒContext Dropoutï¼ˆè®­ç»ƒæ—¶ä»¥æ¦‚ç‡p_dropå°†ä¸Šä¸‹æ–‡ç½®é›¶ï¼‰
    2. è¿”å›dropoutæ ‡å¿—ç”¨äºÎ±-penaltyè®¡ç®—
    3. æ”¯æŒæŒ‰foldåˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†
    """
    
    def __init__(self,
                 data: Dict,
                 fold_ids: List[int],
                 mode: str = "train",
                 context_dropout_prob: float = 0.15):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        
        å‚æ•°ï¼š
            data: HCA-DRæ•°æ®å­—å…¸
            fold_ids: ä½¿ç”¨çš„foldåˆ—è¡¨
            mode: "train" æˆ– "val"/"test"
            context_dropout_prob: ä¸Šä¸‹æ–‡dropoutæ¦‚ç‡ï¼ˆä»…è®­ç»ƒæ—¶ç”Ÿæ•ˆï¼‰
        """
        self.mode = mode
        self.context_dropout_prob = context_dropout_prob if mode == "train" else 0.0
        
        # æ ¹æ®foldç­›é€‰æ•°æ®
        mask = np.isin(data['folds'], fold_ids)
        
        self.X_pert = torch.FloatTensor(data['X_pert'][mask])
        self.X_ctx = torch.FloatTensor(data['X_ctx'][mask])
        self.y = torch.LongTensor(data['y'][mask])
        self.cell_ids = torch.LongTensor(data['cell_ids'][mask])
        
        # ä¿å­˜åŸå§‹ç´¢å¼•ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        self.original_indices = np.where(mask)[0]
        
        # å…ƒæ•°æ®
        self.n_samples = len(self.y)
        self.n_genes = self.X_pert.shape[1]
        
        # è·å–å”¯ä¸€çš„åŒ–åˆç‰©å’Œç»†èƒç³»
        self.unique_compounds = torch.unique(self.y).numpy()
        self.unique_cell_lines = torch.unique(self.cell_ids).numpy()
        self.n_compounds = len(self.unique_compounds)
        self.n_cell_lines = len(self.unique_cell_lines)
        
        # æ„å»ºåŒ–åˆç‰©->æ ·æœ¬ç´¢å¼•æ˜ å°„ï¼ˆç”¨äºåˆ†å±‚é‡‡æ ·ï¼‰
        self.compound_to_indices = defaultdict(list)
        for idx, compound in enumerate(self.y.numpy()):
            self.compound_to_indices[compound].append(idx)
        
        # æ„å»ºç»†èƒç³»->æ ·æœ¬ç´¢å¼•æ˜ å°„ï¼ˆç”¨äºå¯¹æ¯”å­¦ä¹ ï¼‰
        self.cell_to_indices = defaultdict(list)
        for idx, cell in enumerate(self.cell_ids.numpy()):
            self.cell_to_indices[cell].append(idx)
        
        print(f"âœ“ Dataset created ({mode}): {self.n_samples:,} samples, "
              f"{self.n_compounds} compounds, {self.n_cell_lines} cell lines")
    
    def __len__(self) -> int:
        return self.n_samples
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        """
        è·å–å•ä¸ªæ ·æœ¬
        
        è¿”å›ï¼š
            x_pert: æ‰°åŠ¨ç­¾å (978,)
            x_ctx: ç»†èƒç³»ä¸Šä¸‹æ–‡ (978,)ï¼Œå¯èƒ½è¢«dropout
            y: è¯ç‰©æ ‡ç­¾
            cell_id: ç»†èƒç³»ID
            is_ctx_dropout: æ˜¯å¦è¿›è¡Œäº†context dropout (0æˆ–1)
        """
        x_pert = self.X_pert[idx]
        x_ctx = self.X_ctx[idx].clone()  # å…‹éš†ä»¥é¿å…ä¿®æ”¹åŸæ•°æ®
        y = self.y[idx]
        cell_id = self.cell_ids[idx]
        
        # Context Dropout
        is_ctx_dropout = 0
        if self.mode == "train" and random.random() < self.context_dropout_prob:
            x_ctx = torch.zeros_like(x_ctx)
            is_ctx_dropout = 1
        
        return {
            'x_pert': x_pert,
            'x_ctx': x_ctx,
            'y': y,
            'cell_id': cell_id,
            'is_ctx_dropout': torch.tensor(is_ctx_dropout, dtype=torch.float32)
        }
    
    def get_sample_weights(self, alpha: float = 0.7) -> torch.Tensor:
        """
        è®¡ç®—åˆ†å±‚é‡‡æ ·æƒé‡
        
        ä½¿ç”¨åŠ æƒé‡‡æ ·ç¡®ä¿æ¯ä¸ªç»†èƒç³»å’ŒåŒ–åˆç‰©éƒ½æœ‰ä»£è¡¨æ€§
        
        å‚æ•°ï¼š
            alpha: å¹³è¡¡å› å­
        
        è¿”å›ï¼š
            é‡‡æ ·æƒé‡
        """
        # è®¡ç®—æ¯ä¸ªåŒ–åˆç‰©çš„é¢‘ç‡
        compound_counts = torch.bincount(self.y)
        compound_weights = 1.0 / (compound_counts[self.y].float() ** alpha)
        
        # å½’ä¸€åŒ–
        compound_weights = compound_weights / compound_weights.sum() * len(self.y)
        
        return compound_weights


class StratifiedBatchSampler(Sampler):
    """
    åˆ†å±‚æ‰¹é‡‡æ ·å™¨
    
    ç¡®ä¿æ¯ä¸ªbatchä¸­ï¼š
    1. æ¯ä¸ªç»†èƒç³»è‡³å°‘æœ‰min_samples_per_cellä¸ªæ ·æœ¬
    2. å°½å¯èƒ½å¹³è¡¡åŒ–åˆç‰©åˆ†å¸ƒ
    """
    
    def __init__(self,
                 dataset: HCADRDataset,
                 batch_size: int,
                 min_samples_per_cell: int = 1,
                 drop_last: bool = False):
        """
        åˆå§‹åŒ–é‡‡æ ·å™¨
        
        å‚æ•°ï¼š
            dataset: HCADRDatasetå®ä¾‹
            batch_size: æ‰¹å¤§å°
            min_samples_per_cell: æ¯ä¸ªbatchä¸­æ¯ä¸ªç»†èƒç³»çš„æœ€å°æ ·æœ¬æ•°
            drop_last: æ˜¯å¦ä¸¢å¼ƒæœ€åä¸å®Œæ•´çš„batch
        """
        self.dataset = dataset
        self.batch_size = batch_size
        self.min_samples_per_cell = min_samples_per_cell
        self.drop_last = drop_last
        
        # é¢„è®¡ç®—
        self.cell_to_indices = dataset.cell_to_indices
        self.n_cells = len(self.cell_to_indices)
        self.n_samples = len(dataset)
    
    def __iter__(self):
        """ç”Ÿæˆbatchç´¢å¼•"""
        # æ‰“ä¹±æ¯ä¸ªç»†èƒç³»çš„æ ·æœ¬é¡ºåº
        cell_indices = {
            cell: list(indices) 
            for cell, indices in self.cell_to_indices.items()
        }
        for indices in cell_indices.values():
            random.shuffle(indices)
        
        # åˆ›å»ºbatch
        batches = []
        current_batch = []
        cell_pointers = {cell: 0 for cell in cell_indices}
        
        while True:
            # ä¸ºæ¯ä¸ªç»†èƒç³»æ·»åŠ æ ·æœ¬
            for cell in cell_indices:
                pointer = cell_pointers[cell]
                indices = cell_indices[cell]
                
                if pointer < len(indices):
                    # æ·»åŠ min_samples_per_cellä¸ªæ ·æœ¬
                    for _ in range(self.min_samples_per_cell):
                        if pointer < len(indices) and len(current_batch) < self.batch_size:
                            current_batch.append(indices[pointer])
                            pointer += 1
                    cell_pointers[cell] = pointer
            
            # å¦‚æœè¿˜æ²¡å¡«æ»¡batchï¼Œéšæœºæ·»åŠ æ ·æœ¬
            all_remaining = []
            for cell, indices in cell_indices.items():
                pointer = cell_pointers[cell]
                all_remaining.extend(indices[pointer:])
            
            random.shuffle(all_remaining)
            
            while len(current_batch) < self.batch_size and all_remaining:
                idx = all_remaining.pop()
                if idx not in current_batch:
                    current_batch.append(idx)
            
            # å®Œæˆä¸€ä¸ªbatch
            if len(current_batch) == self.batch_size:
                batches.append(current_batch)
                current_batch = []
            
            # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æ ·æœ¬éƒ½è¢«ä½¿ç”¨
            total_used = sum(cell_pointers.values())
            if total_used >= self.n_samples * 0.99:  # å…è®¸1%çš„è¯¯å·®
                break
        
        # å¤„ç†æœ€åçš„batch
        if current_batch and not self.drop_last:
            batches.append(current_batch)
        
        # æ‰“ä¹±batché¡ºåº
        random.shuffle(batches)
        
        for batch in batches:
            yield batch
    
    def __len__(self) -> int:
        if self.drop_last:
            return self.n_samples // self.batch_size
        else:
            return (self.n_samples + self.batch_size - 1) // self.batch_size


def collate_fn(batch: List[Dict]) -> Dict[str, torch.Tensor]:
    """
    æ‰¹å¤„ç†æ•´ç†å‡½æ•°
    
    å‚æ•°ï¼š
        batch: æ ·æœ¬åˆ—è¡¨
    
    è¿”å›ï¼š
        æ•´ç†åçš„æ‰¹æ•°æ®
    """
    return {
        'x_pert': torch.stack([item['x_pert'] for item in batch]),
        'x_ctx': torch.stack([item['x_ctx'] for item in batch]),
        'y': torch.stack([item['y'] for item in batch]),
        'cell_id': torch.stack([item['cell_id'] for item in batch]),
        'is_ctx_dropout': torch.stack([item['is_ctx_dropout'] for item in batch])
    }


def load_data(data_path: str) -> Dict:
    """
    åŠ è½½HCA-DRæ•°æ®
    
    å‚æ•°ï¼š
        data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
    
    è¿”å›ï¼š
        æ•°æ®å­—å…¸
    """
    print(f"ğŸ“– Loading data from: {data_path}")
    
    with open(data_path, 'rb') as f:
        data = pickle.load(f)
    
    print(f"   âœ“ Samples: {data['n_samples']:,}")
    print(f"   âœ“ Compounds: {data['n_compounds']:,}")
    print(f"   âœ“ Cell lines: {data['n_cell_lines']}")
    print(f"   âœ“ Genes: {data['n_genes']}")
    
    return data


def create_dataloaders(data: Dict,
                       train_folds: List[int],
                       val_folds: List[int],
                       batch_size: int = 256,
                       num_workers: int = 4,
                       context_dropout_prob: float = 0.15,
                       use_stratified_sampling: bool = True) -> Tuple[DataLoader, DataLoader]:
    """
    åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯DataLoader
    
    å‚æ•°ï¼š
        data: HCA-DRæ•°æ®å­—å…¸
        train_folds: è®­ç»ƒfoldåˆ—è¡¨
        val_folds: éªŒè¯foldåˆ—è¡¨
        batch_size: æ‰¹å¤§å°
        num_workers: æ•°æ®åŠ è½½çº¿ç¨‹æ•°
        context_dropout_prob: Context Dropoutæ¦‚ç‡
        use_stratified_sampling: æ˜¯å¦ä½¿ç”¨åˆ†å±‚é‡‡æ ·
    
    è¿”å›ï¼š
        train_loader, val_loader
    """
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = HCADRDataset(
        data=data,
        fold_ids=train_folds,
        mode="train",
        context_dropout_prob=context_dropout_prob
    )
    
    val_dataset = HCADRDataset(
        data=data,
        fold_ids=val_folds,
        mode="val",
        context_dropout_prob=0.0  # éªŒè¯æ—¶ä¸åšdropout
    )
    
    # åˆ›å»ºDataLoader
    if use_stratified_sampling:
        train_sampler = StratifiedBatchSampler(
            train_dataset,
            batch_size=batch_size,
            min_samples_per_cell=1,
            drop_last=True
        )
        train_loader = DataLoader(
            train_dataset,
            batch_sampler=train_sampler,
            num_workers=num_workers,
            collate_fn=collate_fn,
            pin_memory=True
        )
    else:
        train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=num_workers,
            collate_fn=collate_fn,
            pin_memory=True,
            drop_last=True
        )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        collate_fn=collate_fn,
        pin_memory=True
    )
    
    return train_loader, val_loader


class LOCODataset(Dataset):
    """
    Leave-One-Cell-Out (LOCO) è¯„ä¼°æ•°æ®é›†
    
    ç”¨äºè¯„ä¼°æ¨¡å‹åœ¨æœªè§ç»†èƒç³»ä¸Šçš„æ³›åŒ–èƒ½åŠ›
    """
    
    def __init__(self,
                 data: Dict,
                 held_out_cell: int,
                 mode: str = "test"):
        """
        åˆå§‹åŒ–LOCOæ•°æ®é›†
        
        å‚æ•°ï¼š
            data: HCA-DRæ•°æ®å­—å…¸
            held_out_cell: ç•™å‡ºçš„ç»†èƒç³»ID
            mode: "train" (æ’é™¤è¯¥ç»†èƒç³») æˆ– "test" (åªåŒ…å«è¯¥ç»†èƒç³»)
        """
        self.mode = mode
        self.held_out_cell = held_out_cell
        
        # æ ¹æ®ç»†èƒç³»ç­›é€‰
        if mode == "train":
            mask = data['cell_ids'] != held_out_cell
        else:
            mask = data['cell_ids'] == held_out_cell
        
        self.X_pert = torch.FloatTensor(data['X_pert'][mask])
        self.X_ctx = torch.FloatTensor(data['X_ctx'][mask])
        self.y = torch.LongTensor(data['y'][mask])
        self.cell_ids = torch.LongTensor(data['cell_ids'][mask])
        
        self.n_samples = len(self.y)
        
        print(f"âœ“ LOCO Dataset ({mode}, cell={held_out_cell}): {self.n_samples:,} samples")
    
    def __len__(self) -> int:
        return self.n_samples
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        return {
            'x_pert': self.X_pert[idx],
            'x_ctx': self.X_ctx[idx],
            'y': self.y[idx],
            'cell_id': self.cell_ids[idx],
            'is_ctx_dropout': torch.tensor(0, dtype=torch.float32)
        }


class OODDataset(Dataset):
    """
    Out-of-Distribution (OOD) æµ‹è¯•æ•°æ®é›†
    
    å°†æ‰€æœ‰ä¸Šä¸‹æ–‡è®¾ä¸ºé›¶å‘é‡ï¼Œæµ‹è¯•æ¨¡å‹çš„å›é€€èƒ½åŠ›
    """
    
    def __init__(self, data: Dict, fold_ids: List[int]):
        """
        åˆå§‹åŒ–OODæ•°æ®é›†
        
        å‚æ•°ï¼š
            data: HCA-DRæ•°æ®å­—å…¸
            fold_ids: ä½¿ç”¨çš„foldåˆ—è¡¨
        """
        mask = np.isin(data['folds'], fold_ids)
        
        self.X_pert = torch.FloatTensor(data['X_pert'][mask])
        self.X_ctx = torch.zeros_like(self.X_pert)  # å…¨éƒ¨è®¾ä¸ºé›¶
        self.y = torch.LongTensor(data['y'][mask])
        self.cell_ids = torch.LongTensor(data['cell_ids'][mask])
        
        self.n_samples = len(self.y)
        
        print(f"âœ“ OOD Dataset: {self.n_samples:,} samples (all contexts zeroed)")
    
    def __len__(self) -> int:
        return self.n_samples
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        return {
            'x_pert': self.X_pert[idx],
            'x_ctx': self.X_ctx[idx],
            'y': self.y[idx],
            'cell_id': self.cell_ids[idx],
            'is_ctx_dropout': torch.tensor(1, dtype=torch.float32)  # æ ‡è®°ä¸ºOOD
        }


if __name__ == "__main__":
    # æµ‹è¯•æ•°æ®é›†
    print("Testing HCA-DR Dataset...")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    n_samples = 1000
    n_genes = 978
    n_compounds = 100
    n_cells = 10
    
    test_data = {
        'X_pert': np.random.randn(n_samples, n_genes).astype(np.float32),
        'X_ctx': np.random.randn(n_samples, n_genes).astype(np.float32),
        'y': np.random.randint(0, n_compounds, n_samples).astype(np.int64),
        'cell_ids': np.random.randint(0, n_cells, n_samples).astype(np.int64),
        'folds': np.random.randint(0, 3, n_samples).astype(np.int32),
        'n_samples': n_samples,
        'n_compounds': n_compounds,
        'n_genes': n_genes,
        'n_cell_lines': n_cells
    }
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = HCADRDataset(test_data, fold_ids=[0, 1], mode="train")
    
    # æµ‹è¯•å•ä¸ªæ ·æœ¬
    sample = dataset[0]
    print(f"\nâœ“ Sample shapes:")
    for key, value in sample.items():
        print(f"  {key}: {value.shape if hasattr(value, 'shape') else value}")
    
    # æµ‹è¯•DataLoader
    train_loader, val_loader = create_dataloaders(
        test_data,
        train_folds=[0, 1],
        val_folds=[2],
        batch_size=32,
        num_workers=0
    )
    
    batch = next(iter(train_loader))
    print(f"\nâœ“ Batch shapes:")
    for key, value in batch.items():
        print(f"  {key}: {value.shape}")
    
    # æµ‹è¯•Context Dropout
    dropout_count = 0
    for i in range(100):
        sample = dataset[i % len(dataset)]
        dropout_count += sample['is_ctx_dropout'].item()
    
    print(f"\nâœ“ Context Dropout rate: {dropout_count/100:.2f} (expected: ~0.15)")