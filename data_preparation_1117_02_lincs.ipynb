{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a656975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ§¬ DRUGREFLECTOR DATA PREPROCESSING\n",
      "   Dataset: Expanded CMap LINCS Resource 2020\n",
      "   Method: Science 2025 Supplementary Materials\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "ğŸ”¬ LINCS 2020 Data Loader Initialized\n",
      "================================================================================\n",
      "Data directory: E:\\ç§‘ç ”\\Models\\drugreflector\\datasets\\LINCS2020\n",
      "Dataset: Expanded CMap LINCS Resource 2020\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "STEP 1: Loading metadata\n",
      "================================================================================\n",
      "ğŸ“– Loading gene information...\n",
      "   File: geneinfo_beta.txt\n",
      "   âœ“ Loaded 12,328 genes\n",
      "   âœ“ Landmark genes: 978\n",
      "   âœ“ Expected: 978\n",
      "\n",
      "ğŸ“– Loading cell information...\n",
      "   File: cellinfo_beta.txt\n",
      "   âœ“ Loaded 240 cell lines\n",
      "\n",
      "ğŸ“– Loading compound information...\n",
      "   File: compoundinfo_beta.txt\n",
      "   âœ“ Loaded 39,321 compounds\n",
      "   âœ“ Unique perturbagens: 34419\n",
      "\n",
      "================================================================================\n",
      "STEP 2: Loading Level 4 signatures (Z-scores)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "ğŸ“– Loading Level 4 Signatures\n",
      "================================================================================\n",
      "File: level4_beta_trt_cp_n1805898x12328.gctx\n",
      "\n",
      "ğŸ“– Reading GCTX file: level4_beta_trt_cp_n1805898x12328.gctx\n",
      "   File size: 82.94 GB\n",
      "   âš ï¸  Large file detected. This may take 10-30 minutes...\n",
      "ğŸ“Š Loading matrix from HDF5...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "LINCS 2020æ•°æ®åŠ è½½å’Œé¢„å¤„ç†è„šæœ¬ - DrugReflectorè®ºæ–‡ä¸¥æ ¼å¤ç°ç‰ˆ\n",
    "é€‚é… Expanded CMap LINCS Resource 2020 æ•°æ®é›†\n",
    "å®Œå…¨æŒ‰ç…§Science 2025è¡¥å……ææ–™çš„é¢„å¤„ç†æµç¨‹å®ç°\n",
    "ä¼˜åŒ–ç‰ˆï¼šæå‡è¿è¡Œæ•ˆç‡ï¼Œä½¿ç”¨å‘é‡åŒ–æ“ä½œ\n",
    "\n",
    "æ•°æ®æ¥æºï¼šExpanded CMap LINCS Resource 2020\n",
    "- Dataset: CMAP LINCS 2020\n",
    "- Updated: 11/23/2021\n",
    "- Source: https://clue.io/data/CMap2020#LINCS2020\n",
    "- Level 4 (Z-scores): level4_beta_trt_cp_n1805898x12328.gctx\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import gzip\n",
    "import shutil\n",
    "import glob\n",
    "from scipy.spatial.distance import cosine, pdist, squareform\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class LINCS2020DataLoader:\n",
    "    \"\"\"\n",
    "    åŠ è½½å’Œé¢„å¤„ç†LINCS 2020æ•°æ® - ä¸¥æ ¼éµå¾ªDrugReflectorè®ºæ–‡æµç¨‹\n",
    "    \n",
    "    æ•°æ®é›†ä¿¡æ¯ï¼š\n",
    "    - åŸºå› ä¿¡æ¯: geneinfo_beta.txt (1.09 MB)\n",
    "    - ç»†èƒä¿¡æ¯: cellinfo_beta.txt (0.04 MB)  \n",
    "    - åŒ–åˆç‰©ä¿¡æ¯: compoundinfo_beta.txt (4.42 MB)\n",
    "    - Level 4æ•°æ®: level4_beta_trt_cp_n1805898x12328.gctx (82.94 GB)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.gene_info = None\n",
    "        self.cell_info = None\n",
    "        self.compound_info = None\n",
    "        self.signatures = None\n",
    "        self.decompressed_files = []\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ğŸ”¬ LINCS 2020 Data Loader Initialized\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Data directory: {self.data_dir}\")\n",
    "        print(f\"Dataset: Expanded CMap LINCS Resource 2020\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    def load_gene_info(self):\n",
    "        \"\"\"\n",
    "        åŠ è½½åŸºå› ä¿¡æ¯ï¼Œè·å–978ä¸ªlandmark genes\n",
    "        \n",
    "        æ–‡ä»¶: geneinfo_beta.txt\n",
    "        å…³é”®åˆ—:\n",
    "        - pr_gene_id: åŸºå› ID\n",
    "        - pr_gene_symbol: åŸºå› ç¬¦å·\n",
    "        - pr_is_lm: landmarkåŸºå› æ ‡è®° (1è¡¨ç¤ºlandmark)\n",
    "        - pr_is_bing: bingåŸºå› æ ‡è®°\n",
    "        \"\"\"\n",
    "        gene_file = self.data_dir / \"geneinfo_beta.txt\"\n",
    "        \n",
    "        if not gene_file.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"âŒ Gene info file not found: {gene_file}\\n\"\n",
    "                f\"   Please download 'geneinfo_beta.txt' from:\\n\"\n",
    "                f\"   https://clue.io/data/CMap2020#LINCS2020\"\n",
    "            )\n",
    "        \n",
    "        print(f\"ğŸ“– Loading gene information...\")\n",
    "        print(f\"   File: {gene_file.name}\")\n",
    "        \n",
    "        # LINCS 2020ä½¿ç”¨åˆ¶è¡¨ç¬¦åˆ†éš”\n",
    "        gene_info = pd.read_csv(gene_file, sep='\\t')\n",
    "        \n",
    "        print(f\"   âœ“ Loaded {len(gene_info):,} genes\")\n",
    "        \n",
    "        # ç­›é€‰landmark genes (pr_is_lm == 1)\n",
    "        if 'pr_is_lm' in gene_info.columns:\n",
    "            landmark_genes = gene_info[gene_info['pr_is_lm'] == 1].copy()\n",
    "        elif 'feature_space' in gene_info.columns:\n",
    "            # å¤‡ç”¨å­—æ®µ\n",
    "            landmark_genes = gene_info[gene_info['feature_space'] == 'landmark'].copy()\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Cannot identify landmark genes. \"\n",
    "                \"Expected 'pr_is_lm' or 'feature_space' column.\"\n",
    "            )\n",
    "        \n",
    "        print(f\"   âœ“ Landmark genes: {len(landmark_genes):,}\")\n",
    "        print(f\"   âœ“ Expected: 978\")\n",
    "        \n",
    "        if len(landmark_genes) != 978:\n",
    "            print(f\"   âš ï¸  Warning: Found {len(landmark_genes)} landmark genes, expected 978\")\n",
    "        \n",
    "        self.gene_info = landmark_genes\n",
    "        return landmark_genes\n",
    "    \n",
    "    def load_cell_info(self):\n",
    "        \"\"\"\n",
    "        åŠ è½½ç»†èƒç³»ä¿¡æ¯\n",
    "        \n",
    "        æ–‡ä»¶: cellinfo_beta.txt\n",
    "        å…³é”®åˆ—:\n",
    "        - cell_id: ç»†èƒç³»ID\n",
    "        - cell_lineage: ç»†èƒè°±ç³»\n",
    "        - primary_disease: ä¸»è¦ç–¾ç—…\n",
    "        \"\"\"\n",
    "        cell_file = self.data_dir / \"cellinfo_beta.txt\"\n",
    "        \n",
    "        if not cell_file.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"âŒ Cell info file not found: {cell_file}\\n\"\n",
    "                f\"   Please download 'cellinfo_beta.txt' from:\\n\"\n",
    "                f\"   https://clue.io/data/CMap2020#LINCS2020\"\n",
    "            )\n",
    "        \n",
    "        print(f\"\\nğŸ“– Loading cell information...\")\n",
    "        print(f\"   File: {cell_file.name}\")\n",
    "        \n",
    "        cell_info = pd.read_csv(cell_file, sep='\\t')\n",
    "        \n",
    "        print(f\"   âœ“ Loaded {len(cell_info):,} cell lines\")\n",
    "        \n",
    "        if 'cell_id' in cell_info.columns:\n",
    "            print(f\"   âœ“ Unique cell IDs: {cell_info['cell_id'].nunique()}\")\n",
    "        \n",
    "        self.cell_info = cell_info\n",
    "        return cell_info\n",
    "    \n",
    "    def load_compound_info(self):\n",
    "        \"\"\"\n",
    "        åŠ è½½åŒ–åˆç‰©ä¿¡æ¯\n",
    "        \n",
    "        æ–‡ä»¶: compoundinfo_beta.txt\n",
    "        å…³é”®åˆ—:\n",
    "        - pert_id: æ‰°åŠ¨ID (BRD-XXXXXXXXX)\n",
    "        - pert_iname: åŒ–åˆç‰©åç§°\n",
    "        - pert_type: æ‰°åŠ¨ç±»å‹\n",
    "        - canonical_smiles: SMILESç»“æ„\n",
    "        \"\"\"\n",
    "        compound_file = self.data_dir / \"compoundinfo_beta.txt\"\n",
    "        \n",
    "        if not compound_file.exists():\n",
    "            print(f\"âš ï¸  Compound info file not found: {compound_file}\")\n",
    "            print(f\"   This file is optional but recommended.\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"\\nğŸ“– Loading compound information...\")\n",
    "        print(f\"   File: {compound_file.name}\")\n",
    "        \n",
    "        compound_info = pd.read_csv(compound_file, sep='\\t')\n",
    "        \n",
    "        print(f\"   âœ“ Loaded {len(compound_info):,} compounds\")\n",
    "        \n",
    "        if 'pert_id' in compound_info.columns:\n",
    "            print(f\"   âœ“ Unique perturbagens: {compound_info['pert_id'].nunique()}\")\n",
    "        \n",
    "        # ç»Ÿè®¡åŒ–åˆç‰©ç±»å‹\n",
    "        if 'pert_type' in compound_info.columns:\n",
    "            print(f\"\\n   Perturbagen types:\")\n",
    "            for ptype, count in compound_info['pert_type'].value_counts().head(5).items():\n",
    "                print(f\"     - {ptype}: {count:,}\")\n",
    "        \n",
    "        self.compound_info = compound_info\n",
    "        return compound_info\n",
    "    \n",
    "    def decompress_gctx_file(self, gctx_file):\n",
    "        \"\"\"\n",
    "        å¦‚æœGCTXæ–‡ä»¶è¢«å‹ç¼©ï¼Œåˆ™è§£å‹\n",
    "        æ³¨æ„ï¼šLINCS 2020çš„GCTXæ–‡ä»¶é€šå¸¸ä¸å‹ç¼©ï¼ˆå·²ç»æ˜¯HDF5äºŒè¿›åˆ¶æ ¼å¼ï¼‰\n",
    "        \"\"\"\n",
    "        gctx_file = Path(gctx_file)\n",
    "        \n",
    "        if not gctx_file.exists():\n",
    "            raise FileNotFoundError(f\"GCTX file not found: {gctx_file}\")\n",
    "        \n",
    "        # å¦‚æœæ˜¯.gzæ–‡ä»¶ï¼Œéœ€è¦è§£å‹\n",
    "        if str(gctx_file).endswith('.gz'):\n",
    "            print(f\"âš ï¸  Detected compressed GCTX file: {gctx_file.name}\")\n",
    "            \n",
    "            decompressed_dir = self.data_dir / \"_decompressed\"\n",
    "            decompressed_dir.mkdir(exist_ok=True)\n",
    "            \n",
    "            output_path = decompressed_dir / gctx_file.stem\n",
    "            \n",
    "            if output_path.exists():\n",
    "                print(f\"âœ“ Found existing decompressed file: {output_path.name}\")\n",
    "                return str(output_path)\n",
    "            \n",
    "            print(f\"ğŸ“¦ Decompressing...\")\n",
    "            with gzip.open(gctx_file, 'rb') as f_in:\n",
    "                with open(output_path, 'wb') as f_out:\n",
    "                    shutil.copyfileobj(f_in, f_out, length=8*1024*1024)\n",
    "            \n",
    "            print(f\"âœ“ Decompressed to: {output_path}\")\n",
    "            return str(output_path)\n",
    "        \n",
    "        return str(gctx_file)\n",
    "    \n",
    "    def read_gctx(self, gctx_file, use_landmark_only=True):\n",
    "        \"\"\"\n",
    "        è¯»å–GCTXæ–‡ä»¶ (HDF5æ ¼å¼)\n",
    "        \n",
    "        LINCS 2020 GCTXç»“æ„ï¼š\n",
    "        - /0/DATA/0/matrix: æ•°æ®çŸ©é˜µ (samples Ã— genes)\n",
    "        - /0/META/ROW: æ ·æœ¬å…ƒæ•°æ®\n",
    "        - /0/META/COL: åŸºå› å…ƒæ•°æ®\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        gctx_file: str or Path\n",
    "            GCTXæ–‡ä»¶è·¯å¾„\n",
    "        use_landmark_only: bool\n",
    "            æ˜¯å¦åªä¿ç•™landmark genes\n",
    "        \"\"\"\n",
    "        gctx_file = str(gctx_file)\n",
    "        print(f\"\\nğŸ“– Reading GCTX file: {Path(gctx_file).name}\")\n",
    "        \n",
    "        # æ£€æŸ¥æ–‡ä»¶å¤§å°\n",
    "        file_size_gb = Path(gctx_file).stat().st_size / (1024**3)\n",
    "        print(f\"   File size: {file_size_gb:.2f} GB\")\n",
    "        \n",
    "        if file_size_gb > 50:\n",
    "            print(f\"   âš ï¸  Large file detected. This may take 10-30 minutes...\")\n",
    "        \n",
    "        # è§£å‹ï¼ˆå¦‚æœéœ€è¦ï¼‰\n",
    "        gctx_file = self.decompress_gctx_file(gctx_file)\n",
    "        \n",
    "        with h5py.File(gctx_file, 'r') as f:\n",
    "            print(f\"ğŸ“Š Loading matrix from HDF5...\")\n",
    "            \n",
    "            # LINCS 2020ä½¿ç”¨æ ‡å‡†GCTX v1.0ç»“æ„\n",
    "            if '/0/DATA/0/matrix' in f:\n",
    "                matrix = f['/0/DATA/0/matrix'][:]\n",
    "            elif '/matrix' in f:\n",
    "                matrix = f['/matrix'][:]\n",
    "            else:\n",
    "                raise ValueError(f\"Cannot find matrix in GCTX file. Available keys: {list(f.keys())}\")\n",
    "            \n",
    "            print(f\"âœ“ Matrix shape: {matrix.shape} (samples Ã— genes)\")\n",
    "            \n",
    "            print(f\"ğŸ“‹ Loading metadata...\")\n",
    "            \n",
    "            # è¯»å–æ ·æœ¬å…ƒæ•°æ®ï¼ˆROWï¼‰\n",
    "            sample_meta = {}\n",
    "            row_path = '/0/META/ROW' if '/0/META/ROW' in f else '/row'\n",
    "            \n",
    "            for key in f[row_path].keys():\n",
    "                data = f[f'{row_path}/{key}'][:]\n",
    "                if data.dtype.kind in ['S', 'O', 'U']:\n",
    "                    # å­—ç¬¦ä¸²ç±»å‹\n",
    "                    try:\n",
    "                        sample_meta[key] = np.char.decode(data.astype('S'), 'utf-8')\n",
    "                    except:\n",
    "                        sample_meta[key] = data.astype(str)\n",
    "                else:\n",
    "                    sample_meta[key] = data\n",
    "            \n",
    "            # è¯»å–åŸºå› å…ƒæ•°æ®ï¼ˆCOLï¼‰\n",
    "            gene_meta = {}\n",
    "            col_path = '/0/META/COL' if '/0/META/COL' in f else '/col'\n",
    "            \n",
    "            for key in f[col_path].keys():\n",
    "                data = f[f'{col_path}/{key}'][:]\n",
    "                if data.dtype.kind in ['S', 'O', 'U']:\n",
    "                    try:\n",
    "                        gene_meta[key] = np.char.decode(data.astype('S'), 'utf-8')\n",
    "                    except:\n",
    "                        gene_meta[key] = data.astype(str)\n",
    "                else:\n",
    "                    gene_meta[key] = data\n",
    "        \n",
    "        sample_df = pd.DataFrame(sample_meta)\n",
    "        gene_df = pd.DataFrame(gene_meta)\n",
    "        \n",
    "        print(f\"\\nâœ“ Data loaded successfully:\")\n",
    "        print(f\"  Matrix: {matrix.shape} (samples Ã— genes)\")\n",
    "        print(f\"  Samples: {len(sample_df):,}\")\n",
    "        print(f\"  Genes: {len(gene_df)}\")\n",
    "        \n",
    "        # è¿‡æ»¤landmark genes\n",
    "        if use_landmark_only:\n",
    "            print(f\"\\nğŸ”¬ Filtering to landmark genes...\")\n",
    "            \n",
    "            if self.gene_info is None:\n",
    "                print(f\"   Loading gene info...\")\n",
    "                self.load_gene_info()\n",
    "            \n",
    "            landmark_ids = set(self.gene_info['pr_gene_id'].astype(str).values)\n",
    "            print(f\"   Landmark genes to find: {len(landmark_ids)}\")\n",
    "            \n",
    "            # LINCS 2020ä¸­åŸºå› IDå¯èƒ½åœ¨'id'æˆ–'pr_gene_id'åˆ—\n",
    "            if 'id' in gene_df.columns:\n",
    "                gene_id_col = 'id'\n",
    "            elif 'pr_gene_id' in gene_df.columns:\n",
    "                gene_id_col = 'pr_gene_id'\n",
    "            else:\n",
    "                raise ValueError(f\"Cannot find gene ID column. Available: {gene_df.columns.tolist()}\")\n",
    "            \n",
    "            gene_mask = gene_df[gene_id_col].astype(str).isin(landmark_ids)\n",
    "            n_matched = gene_mask.sum()\n",
    "            print(f\"   âœ“ Matched: {n_matched} genes\")\n",
    "            \n",
    "            if n_matched == 0:\n",
    "                raise ValueError(\"No landmark genes matched!\")\n",
    "            \n",
    "            print(f\"\\nğŸ¯ Applying filter...\")\n",
    "            matrix = matrix[:, gene_mask]\n",
    "            gene_df = gene_df[gene_mask].reset_index(drop=True)\n",
    "            \n",
    "            print(f\"   âœ“ Final matrix shape: {matrix.shape}\")\n",
    "        \n",
    "        return matrix, sample_df, gene_df\n",
    "    \n",
    "    def load_level4_signatures(self):\n",
    "        \"\"\"\n",
    "        åŠ è½½Level 4æ•°æ® (Z-scores)\n",
    "        \n",
    "        æ–‡ä»¶: level4_beta_trt_cp_n1805898x12328.gctx\n",
    "        - trt: treatment (å¤„ç†)\n",
    "        - cp: compound perturbation (åŒ–åˆç‰©æ‰°åŠ¨)\n",
    "        - n1805898: 180ä¸‡+æ ·æœ¬\n",
    "        - x12328: 12,328ä¸ªåŸºå›  (978 landmark + æ¨æ–­åŸºå› )\n",
    "        \"\"\"\n",
    "        # æŸ¥æ‰¾Level 4åŒ–åˆç‰©å¤„ç†æ–‡ä»¶\n",
    "        level4_file = self.data_dir / \"level4_beta_trt_cp_n1805898x12328.gctx\"\n",
    "        \n",
    "        if not level4_file.exists():\n",
    "            # å°è¯•é€šé…ç¬¦åŒ¹é…\n",
    "            pattern = self.data_dir / \"level4_beta_trt_cp*.gctx\"\n",
    "            files = glob.glob(str(pattern))\n",
    "            \n",
    "            if not files:\n",
    "                raise FileNotFoundError(\n",
    "                    f\"âŒ Level 4 file not found: {level4_file}\\n\"\n",
    "                    f\"   Please download 'level4_beta_trt_cp_n1805898x12328.gctx' from:\\n\"\n",
    "                    f\"   https://clue.io/data/CMap2020#LINCS2020\\n\"\n",
    "                    f\"   File size: 82.94 GB\"\n",
    "                )\n",
    "            \n",
    "            level4_file = files[0]\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ğŸ“– Loading Level 4 Signatures\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"File: {level4_file.name}\")\n",
    "        \n",
    "        # è¯»å–GCTXï¼ˆåªä¿ç•™landmark genesï¼‰\n",
    "        matrix, sample_meta, gene_meta = self.read_gctx(\n",
    "            level4_file, \n",
    "            use_landmark_only=True\n",
    "        )\n",
    "        \n",
    "        self.signatures = {\n",
    "            'matrix': matrix,\n",
    "            'row_meta': sample_meta,\n",
    "            'col_meta': gene_meta\n",
    "        }\n",
    "        \n",
    "        return matrix, sample_meta, gene_meta\n",
    "    \n",
    "    def calculate_cosine_similarity_to_nearest_replicate(\n",
    "        self, \n",
    "        matrix: np.ndarray, \n",
    "        pert_ids: pd.Series\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        è®¡ç®—æ¯ä¸ªæ ·æœ¬ä¸å…¶æœ€è¿‘replicateçš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆå‘é‡åŒ–ä¼˜åŒ–ï¼‰\n",
    "        \n",
    "        åŸæ–‡æ–¹æ³•ï¼ˆSIç¬¬2é¡µï¼‰ï¼š\n",
    "        \"For each compound, remove any observations with a cosine similarity <0.12 \n",
    "        to the closest replicate\"\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        matrix: np.ndarray\n",
    "            è¡¨è¾¾çŸ©é˜µ (n_samples, n_genes)\n",
    "        pert_ids: pd.Series\n",
    "            æ¯ä¸ªæ ·æœ¬çš„åŒ–åˆç‰©ID\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        np.ndarray: æ¯ä¸ªæ ·æœ¬ä¸å…¶æœ€è¿‘replicateçš„ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "        \"\"\"\n",
    "        print(f\"\\nğŸ“Š Calculating cosine similarity to nearest replicate...\")\n",
    "        n_samples = len(pert_ids)\n",
    "        nearest_similarities = np.zeros(n_samples)\n",
    "        \n",
    "        unique_perts = pert_ids.unique()\n",
    "        print(f\"   Processing {len(unique_perts):,} compounds...\")\n",
    "        \n",
    "        # è¿›åº¦æ¡\n",
    "        from tqdm import tqdm\n",
    "        \n",
    "        for pert_id in tqdm(unique_perts, desc=\"   Computing similarities\"):\n",
    "            pert_mask = pert_ids == pert_id\n",
    "            pert_indices = np.where(pert_mask)[0]\n",
    "            \n",
    "            if len(pert_indices) < 2:\n",
    "                # åªæœ‰ä¸€ä¸ªæ ·æœ¬ï¼Œè®¾ä¸º0ï¼ˆä¼šè¢«è¿‡æ»¤ï¼‰\n",
    "                nearest_similarities[pert_indices] = 0.0\n",
    "                continue\n",
    "            \n",
    "            pert_data = matrix[pert_mask]\n",
    "            \n",
    "            # ä½¿ç”¨sklearnå¿«é€Ÿè®¡ç®—æˆå¯¹ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "            sim_matrix = cosine_similarity(pert_data)\n",
    "            \n",
    "            # å°†å¯¹è§’çº¿è®¾ä¸º-infï¼Œé¿å…æ ·æœ¬ä¸è‡ªå·±æ¯”è¾ƒ\n",
    "            np.fill_diagonal(sim_matrix, -np.inf)\n",
    "            \n",
    "            # å¯¹æ¯ä¸ªæ ·æœ¬æ‰¾åˆ°æœ€å¤§ç›¸ä¼¼åº¦ï¼ˆæœ€è¿‘çš„replicateï¼‰\n",
    "            max_sims = np.max(sim_matrix, axis=1)\n",
    "            \n",
    "            nearest_similarities[pert_indices] = max_sims\n",
    "        \n",
    "        print(f\"   âœ“ Calculated similarities for {n_samples:,} samples\")\n",
    "        print(f\"   Mean similarity: {nearest_similarities.mean():.4f}\")\n",
    "        print(f\"   Median similarity: {np.median(nearest_similarities):.4f}\")\n",
    "        print(f\"   Min similarity: {nearest_similarities.min():.4f}\")\n",
    "        print(f\"   Max similarity: {nearest_similarities.max():.4f}\")\n",
    "        \n",
    "        return nearest_similarities\n",
    "    \n",
    "    def prepare_training_data(\n",
    "        self, \n",
    "        min_observations_per_compound=5,      # åŸæ–‡filter 2\n",
    "        min_replicate_similarity=0.12,        # åŸæ–‡filter 3\n",
    "        dose_range=(1.0, 20.0),               # åŸæ–‡filter 4: 1-20ÂµM\n",
    "        valid_timepoints=['6 h', '24 h'],     # åŸæ–‡filter 5 (LINCS 2020æ ¼å¼)\n",
    "        min_cell_lines=5,                     # åŸæ–‡filter 6\n",
    "        max_cell_lines=40,                    # åŸæ–‡filter 6\n",
    "        remove_dos=True                       # åŸæ–‡filter 1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        å‡†å¤‡è®­ç»ƒæ•°æ®ï¼Œä¸¥æ ¼æŒ‰ç…§è®ºæ–‡SIç¬¬2é¡µçš„è¿‡æ»¤æµç¨‹\n",
    "        \n",
    "        åŸæ–‡è¿‡æ»¤é¡ºåºï¼š\n",
    "        1. Remove DOS compounds\n",
    "        2. Remove compounds with <5 observations\n",
    "        3. Remove observations with cosine similarity <0.12 to closest replicate\n",
    "        4. Select most frequent dose between 1-20ÂµM for each compound\n",
    "        5. Keep only 6h or 24h measurements\n",
    "        6. Remove compounds in <5 or >40 cell lines\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        min_observations_per_compound: int\n",
    "            æ¯ä¸ªåŒ–åˆç‰©çš„æœ€å°è§‚æµ‹æ•° (åŸæ–‡: 5)\n",
    "        min_replicate_similarity: float\n",
    "            ä¸æœ€è¿‘replicateçš„æœ€å°ä½™å¼¦ç›¸ä¼¼åº¦ (åŸæ–‡: 0.12)\n",
    "        dose_range: tuple\n",
    "            æœ‰æ•ˆå‰‚é‡èŒƒå›´ ÂµM (åŸæ–‡: 1-20)\n",
    "        valid_timepoints: list\n",
    "            æœ‰æ•ˆæ—¶é—´ç‚¹ (åŸæ–‡: 6h, 24h)\n",
    "            æ³¨æ„: LINCS 2020ä½¿ç”¨ '6 h', '24 h' æ ¼å¼\n",
    "        min_cell_lines: int\n",
    "            æ¯ä¸ªåŒ–åˆç‰©çš„æœ€å°ç»†èƒç³»æ•° (åŸæ–‡: 5)\n",
    "        max_cell_lines: int\n",
    "            æ¯ä¸ªåŒ–åˆç‰©çš„æœ€å¤§ç»†èƒç³»æ•° (åŸæ–‡: 40)\n",
    "        remove_dos: bool\n",
    "            æ˜¯å¦ç§»é™¤DOSåŒ–åˆç‰© (åŸæ–‡: True)\n",
    "        \"\"\"\n",
    "        if self.signatures is None:\n",
    "            raise ValueError(\"Please load signatures first using load_level4_signatures()\")\n",
    "        \n",
    "        matrix = self.signatures['matrix']\n",
    "        row_meta = self.signatures['row_meta'].copy()\n",
    "        col_meta = self.signatures['col_meta']\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"ğŸ” DRUGREFLECTOR QUALITY CONTROL PIPELINE\")\n",
    "        print(\"   Following Science 2025 Supplementary Materials (Page 2)\")\n",
    "        print(\"   Dataset: LINCS 2020\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Initial samples: {len(row_meta):,}\")\n",
    "        \n",
    "        # æ‰“å°å¯ç”¨çš„å…ƒæ•°æ®åˆ—\n",
    "        print(f\"\\nğŸ“‹ Available metadata columns:\")\n",
    "        for col in row_meta.columns[:20]:  # æ˜¾ç¤ºå‰20åˆ—\n",
    "            print(f\"   - {col}\")\n",
    "        if len(row_meta.columns) > 20:\n",
    "            print(f\"   ... and {len(row_meta.columns) - 20} more\")\n",
    "        \n",
    "        # LINCS 2020å…³é”®å­—æ®µæ˜ å°„\n",
    "        # å¸¸è§å­—æ®µå: pert_id, cell_id, pert_time, pert_dose, pert_type\n",
    "        \n",
    "        working_matrix = matrix.copy()\n",
    "        working_meta = row_meta.copy()\n",
    "        initial_compounds = working_meta['pert_id'].nunique() if 'pert_id' in working_meta.columns else 0\n",
    "        \n",
    "        print(f\"\\nInitial compounds: {initial_compounds:,}\")\n",
    "        \n",
    "        # ========== Filter 1: Remove DOS compounds ==========\n",
    "        if remove_dos:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"FILTER 1: Remove DOS (Diversity-Oriented Synthesis) compounds\")\n",
    "            print(f\"{'='*80}\")\n",
    "            \n",
    "            # LINCS 2020ä¸­ï¼Œpert_typeå­—æ®µæ ‡è¯†åŒ–åˆç‰©ç±»å‹\n",
    "            if 'pert_type' in working_meta.columns:\n",
    "                # ä¿ç•™ 'trt_cp' (åŒ–åˆç‰©å¤„ç†)ï¼Œç§»é™¤DOSç­‰å…¶ä»–ç±»å‹\n",
    "                dos_mask = working_meta['pert_type'] == 'trt_cp'\n",
    "                n_dos = (~dos_mask).sum()\n",
    "            else:\n",
    "                # é€šè¿‡pert_idåˆ¤æ–­ï¼ˆBRD-DOSå¼€å¤´æˆ–åŒ…å«DOSï¼‰\n",
    "                dos_mask = ~working_meta['pert_id'].str.contains('DOS', case=False, na=False)\n",
    "                n_dos = (~dos_mask).sum()\n",
    "            \n",
    "            working_matrix = working_matrix[dos_mask]\n",
    "            working_meta = working_meta[dos_mask].reset_index(drop=True)\n",
    "            \n",
    "            print(f\"  Removed {n_dos:,} DOS observations\")\n",
    "            print(f\"  Remaining samples: {len(working_meta):,}\")\n",
    "            print(f\"  Remaining compounds: {working_meta['pert_id'].nunique():,}\")\n",
    "        \n",
    "        # ========== Filter 2: Minimum observations per compound ==========\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FILTER 2: Remove compounds with <{min_observations_per_compound} observations\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        obs_counts = working_meta.groupby('pert_id').size()\n",
    "        valid_perts = obs_counts[obs_counts >= min_observations_per_compound].index\n",
    "        \n",
    "        print(f\"  Compounds with â‰¥{min_observations_per_compound} observations: \"\n",
    "              f\"{len(valid_perts):,}/{obs_counts.nunique():,}\")\n",
    "        \n",
    "        obs_mask = working_meta['pert_id'].isin(valid_perts)\n",
    "        working_matrix = working_matrix[obs_mask]\n",
    "        working_meta = working_meta[obs_mask].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"  Remaining samples: {len(working_meta):,}\")\n",
    "        print(f\"  Remaining compounds: {working_meta['pert_id'].nunique():,}\")\n",
    "        \n",
    "        # ========== Filter 3: Cosine similarity to closest replicate ==========\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FILTER 3: Remove observations with cosine similarity <{min_replicate_similarity}\")\n",
    "        print(f\"         to closest replicate\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # è®¡ç®—æ¯ä¸ªæ ·æœ¬ä¸å…¶æœ€è¿‘replicateçš„ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "        nearest_similarities = self.calculate_cosine_similarity_to_nearest_replicate(\n",
    "            working_matrix, \n",
    "            working_meta['pert_id']\n",
    "        )\n",
    "        \n",
    "        # è¿‡æ»¤ä½ç›¸ä¼¼åº¦æ ·æœ¬\n",
    "        sim_mask = nearest_similarities >= min_replicate_similarity\n",
    "        n_removed_sim = (~sim_mask).sum()\n",
    "        \n",
    "        working_matrix = working_matrix[sim_mask]\n",
    "        working_meta = working_meta[sim_mask].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"  Removed {n_removed_sim:,} low-similarity observations\")\n",
    "        print(f\"  Remaining samples: {len(working_meta):,}\")\n",
    "        print(f\"  Remaining compounds: {working_meta['pert_id'].nunique():,}\")\n",
    "        \n",
    "        # ========== Filter 4: Dose selection ==========\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FILTER 4: Select most frequent dose in range {dose_range[0]}-{dose_range[1]} ÂµM\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        if 'pert_dose' in working_meta.columns:\n",
    "            # LINCS 2020å‰‚é‡æ ¼å¼: \"10 um\", \"1 um\"ç­‰\n",
    "            # è§£æå‰‚é‡å€¼å’Œå•ä½\n",
    "            working_meta['dose_value'] = pd.to_numeric(\n",
    "                working_meta['pert_dose'].astype(str).str.extract(r'(\\d+\\.?\\d*)')[0], \n",
    "                errors='coerce'\n",
    "            )\n",
    "            \n",
    "            # è§£æå‰‚é‡å•ä½\n",
    "            working_meta['dose_unit'] = working_meta['pert_dose'].astype(str).str.extract(r'([a-zA-Z]+)')[0]\n",
    "            \n",
    "            # è½¬æ¢ä¸ºÂµM\n",
    "            def convert_to_uM(row):\n",
    "                if pd.isna(row['dose_value']):\n",
    "                    return np.nan\n",
    "                value = row['dose_value']\n",
    "                unit = str(row['dose_unit']).lower() if pd.notna(row['dose_unit']) else 'um'\n",
    "                \n",
    "                if 'nm' in unit:\n",
    "                    return value / 1000  # nM to ÂµM\n",
    "                elif 'mm' in unit:\n",
    "                    return value * 1000  # mM to ÂµM\n",
    "                else:  # assume ÂµM\n",
    "                    return value\n",
    "            \n",
    "            working_meta['dose_uM'] = working_meta.apply(convert_to_uM, axis=1)\n",
    "            \n",
    "            # ç­›é€‰æœ‰æ•ˆå‰‚é‡èŒƒå›´\n",
    "            dose_mask = (\n",
    "                (working_meta['dose_uM'] >= dose_range[0]) & \n",
    "                (working_meta['dose_uM'] <= dose_range[1])\n",
    "            )\n",
    "            \n",
    "            print(f\"  Samples in valid dose range: {dose_mask.sum():,}\")\n",
    "            \n",
    "            # å¯¹æ¯ä¸ªåŒ–åˆç‰©ï¼Œé€‰æ‹©æœ€å¸¸è§çš„å‰‚é‡\n",
    "            valid_doses = []\n",
    "            for pert_id in working_meta['pert_id'].unique():\n",
    "                pert_mask = (working_meta['pert_id'] == pert_id) & dose_mask\n",
    "                if pert_mask.sum() == 0:\n",
    "                    continue\n",
    "                \n",
    "                # æ‰¾å‡ºæœ€å¸¸è§çš„å‰‚é‡\n",
    "                dose_counts = working_meta.loc[pert_mask, 'dose_uM'].value_counts()\n",
    "                if len(dose_counts) > 0:\n",
    "                    most_common_dose = dose_counts.index[0]\n",
    "                    valid_doses.append(\n",
    "                        (working_meta['pert_id'] == pert_id) & \n",
    "                        (working_meta['dose_uM'] == most_common_dose)\n",
    "                    )\n",
    "            \n",
    "            if valid_doses:\n",
    "                dose_final_mask = pd.concat([pd.Series(mask) for mask in valid_doses], axis=1).any(axis=1)\n",
    "                n_removed_dose = (~dose_final_mask).sum()\n",
    "                \n",
    "                working_matrix = working_matrix[dose_final_mask]\n",
    "                working_meta = working_meta[dose_final_mask].reset_index(drop=True)\n",
    "                \n",
    "                print(f\"  Removed {n_removed_dose:,} observations (invalid or non-modal dose)\")\n",
    "                print(f\"  Remaining samples: {len(working_meta):,}\")\n",
    "                print(f\"  Remaining compounds: {working_meta['pert_id'].nunique():,}\")\n",
    "            else:\n",
    "                print(f\"  âš ï¸  Warning: No valid doses found in range, skipping dose filter\")\n",
    "        else:\n",
    "            print(f\"  âš ï¸  Warning: 'pert_dose' column not found, skipping dose filter\")\n",
    "        \n",
    "        # ========== Filter 5: Timepoint selection ==========\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FILTER 5: Keep only measurements at {valid_timepoints}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        if 'pert_time' in working_meta.columns:\n",
    "            # LINCS 2020æ—¶é—´æ ¼å¼: \"6 h\", \"24 h\"\n",
    "            # æ ‡å‡†åŒ–æ—¶é—´ç‚¹æ ¼å¼\n",
    "            working_meta['time_normalized'] = working_meta['pert_time'].astype(str).str.lower().str.strip()\n",
    "            \n",
    "            # è§„èŒƒåŒ–valid_timepoints\n",
    "            normalized_valid_times = []\n",
    "            for t in valid_timepoints:\n",
    "                normalized_valid_times.extend([\n",
    "                    t.lower().strip(),\n",
    "                    t.lower().strip().replace(' ', ''),\n",
    "                    t.lower().strip().replace('h', ' h'),\n",
    "                    t.lower().strip().replace('h', 'hr')\n",
    "                ])\n",
    "            \n",
    "            time_mask = working_meta['time_normalized'].isin(normalized_valid_times)\n",
    "            \n",
    "            if time_mask.sum() == 0:\n",
    "                # å¦‚æœæ²¡æœ‰åŒ¹é…ï¼Œæ˜¾ç¤ºå¯ç”¨çš„æ—¶é—´ç‚¹\n",
    "                print(f\"  âš ï¸  Warning: No samples match timepoints {valid_timepoints}\")\n",
    "                print(f\"  Available timepoints:\")\n",
    "                for tp, count in working_meta['pert_time'].value_counts().head(10).items():\n",
    "                    print(f\"    - {tp}: {count:,} samples\")\n",
    "                print(f\"  Skipping timepoint filter...\")\n",
    "            else:\n",
    "                n_removed_time = (~time_mask).sum()\n",
    "                working_matrix = working_matrix[time_mask]\n",
    "                working_meta = working_meta[time_mask].reset_index(drop=True)\n",
    "                \n",
    "                print(f\"  Removed {n_removed_time:,} observations (invalid timepoint)\")\n",
    "                print(f\"  Remaining samples: {len(working_meta):,}\")\n",
    "                print(f\"  Remaining compounds: {working_meta['pert_id'].nunique():,}\")\n",
    "        else:\n",
    "            print(f\"  âš ï¸  Warning: 'pert_time' column not found, skipping timepoint filter\")\n",
    "        \n",
    "        # ========== Filter 6: Cell line count per compound ==========\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FILTER 6: Remove compounds in <{min_cell_lines} or >{max_cell_lines} cell lines\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        cell_line_counts = working_meta.groupby('pert_id')['cell_id'].nunique()\n",
    "        valid_perts_cell = cell_line_counts[\n",
    "            (cell_line_counts >= min_cell_lines) & \n",
    "            (cell_line_counts <= max_cell_lines)\n",
    "        ].index\n",
    "        \n",
    "        print(f\"  Compounds in {min_cell_lines}-{max_cell_lines} cell lines: \"\n",
    "              f\"{len(valid_perts_cell):,}/{len(cell_line_counts):,}\")\n",
    "        \n",
    "        cell_mask = working_meta['pert_id'].isin(valid_perts_cell)\n",
    "        working_matrix = working_matrix[cell_mask]\n",
    "        working_meta = working_meta[cell_mask].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"  Removed {(~cell_mask).sum():,} observations\")\n",
    "        print(f\"  Remaining samples: {len(working_meta):,}\")\n",
    "        print(f\"  Remaining compounds: {working_meta['pert_id'].nunique():,}\")\n",
    "        \n",
    "        # ========== Create final dataset ==========\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"âœ… FINAL DATASET (After all filters)\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # åˆ›å»ºåŒ–åˆç‰©æ ‡ç­¾ç¼–ç \n",
    "        unique_perts = sorted(working_meta['pert_id'].unique())\n",
    "        pert_to_idx = {pert: idx for idx, pert in enumerate(unique_perts)}\n",
    "        labels = np.array([pert_to_idx[p] for p in working_meta['pert_id']])\n",
    "        \n",
    "        final_compounds = len(unique_perts)\n",
    "        final_samples = len(working_matrix)\n",
    "        final_cells = working_meta['cell_id'].nunique()\n",
    "        \n",
    "        print(f\"  Total samples: {final_samples:,}\")\n",
    "        print(f\"  Total compounds: {final_compounds:,}\")\n",
    "        print(f\"  Cell lines: {final_cells}\")\n",
    "        print(f\"  Gene features: {working_matrix.shape[1]}\")\n",
    "        print(f\"  Samples per compound (mean): {final_samples / final_compounds:.1f}\")\n",
    "        print(f\"  Samples per compound (median): \"\n",
    "              f\"{working_meta.groupby('pert_id').size().median():.0f}\")\n",
    "        \n",
    "        # ç»Ÿè®¡åŒ–åˆç‰©åˆ†å¸ƒ\n",
    "        compound_obs = working_meta.groupby('pert_id').size()\n",
    "        n_compounds_100plus = (compound_obs > 100).sum()\n",
    "        print(f\"  Compounds with >100 observations: {n_compounds_100plus:,}\")\n",
    "        \n",
    "        # ä¸åŸæ–‡å¯¹æ¯”\n",
    "        print(f\"\\nğŸ“Š Comparison with paper results:\")\n",
    "        print(f\"  Paper: 425,242 obs, 9,597 compounds, 52 cell lines\")\n",
    "        print(f\"  Ours:  {final_samples:,} obs, {final_compounds:,} compounds, {final_cells} cell lines\")\n",
    "        \n",
    "        if initial_compounds > 0:\n",
    "            retention_rate = (final_compounds / initial_compounds) * 100\n",
    "            print(f\"  Compound retention rate: {retention_rate:.1f}%\")\n",
    "        \n",
    "        training_data = {\n",
    "            'X': working_matrix,\n",
    "            'y': labels,\n",
    "            'folds': np.zeros(len(working_matrix), dtype=int),  # ä¸´æ—¶å€¼ï¼Œç¨åä¼šè¢«create_3fold_splitsè¦†ç›–\n",
    "            'sample_meta': working_meta,\n",
    "            'metadata': working_meta,  # æ·»åŠ æ­¤è¡Œ - å…¼å®¹è®­ç»ƒä»£ç \n",
    "            'gene_names': list(col_meta['id'].values if 'id' in col_meta.columns else col_meta['pr_gene_id'].values),  # è½¬ä¸ºlist\n",
    "            'compound_names': list(unique_perts),  # ç¡®ä¿æ˜¯list\n",
    "            'pert_to_idx': pert_to_idx\n",
    "        }\n",
    "                \n",
    "        return training_data\n",
    "    \n",
    "    def create_3fold_splits(self, training_data, random_state=42):\n",
    "        \"\"\"\n",
    "        åˆ›å»º3æŠ˜äº¤å‰éªŒè¯åˆ’åˆ†\n",
    "        \n",
    "        åŸæ–‡ï¼ˆSIç¬¬2é¡µï¼‰ï¼š\n",
    "        \"The training data was divided randomly into three folds, \n",
    "        with perturbation replicates balanced across the folds.\"\n",
    "        \"\"\"\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        sample_meta = training_data['sample_meta']\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ğŸ² Creating 3-fold cross-validation splits\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        folds = np.zeros(len(sample_meta), dtype=int)\n",
    "        \n",
    "        # æŒ‰åŒ–åˆç‰©åˆ†é…foldï¼Œç¡®ä¿åŒä¸€åŒ–åˆç‰©çš„æ‰€æœ‰æ ·æœ¬åœ¨åŒä¸€fold\n",
    "        for pert_id in sample_meta['pert_id'].unique():\n",
    "            pert_mask = sample_meta['pert_id'] == pert_id\n",
    "            pert_indices = np.where(pert_mask)[0]\n",
    "            \n",
    "            # éšæœºæ‰“ä¹±\n",
    "            np.random.shuffle(pert_indices)\n",
    "            n_samples = len(pert_indices)\n",
    "            \n",
    "            # å°½å¯èƒ½å‡åŒ€åˆ†é…åˆ°3ä¸ªfold\n",
    "            fold_sizes = [n_samples // 3] * 3\n",
    "            for i in range(n_samples % 3):\n",
    "                fold_sizes[i] += 1\n",
    "            \n",
    "            start_idx = 0\n",
    "            for fold_id, size in enumerate(fold_sizes):\n",
    "                end_idx = start_idx + size\n",
    "                folds[pert_indices[start_idx:end_idx]] = fold_id\n",
    "                start_idx = end_idx\n",
    "        \n",
    "        training_data['folds'] = folds\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Fold statistics:\")\n",
    "        for fold_id in range(3):\n",
    "            fold_mask = folds == fold_id\n",
    "            n_samples = fold_mask.sum()\n",
    "            n_compounds = sample_meta[fold_mask]['pert_id'].nunique()\n",
    "            print(f\"   Fold {fold_id}: {n_samples:,} samples, {n_compounds:,} compounds\")\n",
    "        \n",
    "        return training_data\n",
    "\n",
    "\n",
    "# ========== ä¸»ç¨‹åº ==========\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    ä¸»ç¨‹åº - ä¸¥æ ¼æŒ‰ç…§DrugReflectorè®ºæ–‡æµç¨‹\n",
    "    ä½¿ç”¨LINCS 2020æ•°æ®é›†\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ğŸ§¬ DRUGREFLECTOR DATA PREPROCESSING\")\n",
    "    print(\"   Dataset: Expanded CMap LINCS Resource 2020\")\n",
    "    print(\"   Method: Science 2025 Supplementary Materials\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # åˆå§‹åŒ–åŠ è½½å™¨\n",
    "    data_dir = \"E:/ç§‘ç ”/Models/drugreflector/datasets/LINCS2020\"\n",
    "    loader = LINCS2020DataLoader(data_dir)\n",
    "    \n",
    "    try:\n",
    "        # Step 1: åŠ è½½å…ƒæ•°æ®\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"STEP 1: Loading metadata\")\n",
    "        print(\"=\" * 80)\n",
    "        gene_info = loader.load_gene_info()\n",
    "        cell_info = loader.load_cell_info()\n",
    "        compound_info = loader.load_compound_info()\n",
    "        \n",
    "        # Step 2: åŠ è½½Level 4 signatures\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"STEP 2: Loading Level 4 signatures (Z-scores)\")\n",
    "        print(\"=\" * 80)\n",
    "        matrix, row_meta, col_meta = loader.load_level4_signatures()\n",
    "        \n",
    "        # Step 3: å‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆä¸¥æ ¼æŒ‰ç…§è®ºæ–‡æµç¨‹ï¼‰\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"STEP 3: Preparing training data (Paper-compliant pipeline)\")\n",
    "        print(\"=\" * 80)\n",
    "        training_data = loader.prepare_training_data(\n",
    "            min_observations_per_compound=5,    # Filter 2\n",
    "            min_replicate_similarity=0.12,      # Filter 3\n",
    "            dose_range=(1.0, 20.0),             # Filter 4\n",
    "            valid_timepoints=['6 h', '24 h',\n",
    "                              '6h', '24h',\n",
    "                              '6hr', '24hr'\n",
    "                              '6 hr, 24 hr'],   # Filter 5\n",
    "            min_cell_lines=5,                   # Filter 6\n",
    "            max_cell_lines=40,                  # Filter 6\n",
    "            remove_dos=True                     # Filter 1\n",
    "        )\n",
    "        \n",
    "        # Step 4: åˆ›å»º3æŠ˜åˆ’åˆ†\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"STEP 4: Creating 3-fold splits\")\n",
    "        print(\"=\" * 80)\n",
    "        training_data = loader.create_3fold_splits(training_data)\n",
    "        \n",
    "        # Step 5: ä¿å­˜å¤„ç†åçš„æ•°æ®\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"STEP 5: Saving processed data\")\n",
    "        print(\"=\" * 80)\n",
    "        output_dir = Path(\"E:/ç§‘ç ”/Models/drugreflector/processed_data\")\n",
    "        output_dir.mkdir(exist_ok=True, parents=True)\n",
    "        \n",
    "        output_file = output_dir / \"training_data_lincs2020_paper_compliant.pkl\"\n",
    "        print(f\"ğŸ’¾ Saving to: {output_file}\")\n",
    "        \n",
    "        with open(output_file, 'wb') as f:\n",
    "            pickle.dump(training_data, f)\n",
    "        \n",
    "        print(f\"âœ“ Saved successfully!\")\n",
    "        print(f\"   File size: {output_file.stat().st_size / (1024**2):.1f} MB\")\n",
    "        \n",
    "        # æ‰“å°æœ€ç»ˆæ‘˜è¦\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"âœ… DATA PREPARATION COMPLETE!\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"ğŸ“ Output file: {output_file}\")\n",
    "        print(f\"\\nğŸ“Š Final dataset summary:\")\n",
    "        print(f\"   â€¢ Total samples: {len(training_data['X']):,}\")\n",
    "        print(f\"   â€¢ Total compounds: {len(training_data['compound_names']):,}\")\n",
    "        print(f\"   â€¢ Gene features: {training_data['X'].shape[1]}\")\n",
    "        print(f\"   â€¢ Data shape: {training_data['X'].shape}\")\n",
    "        print(f\"   â€¢ Average samples per compound: \"\n",
    "              f\"{len(training_data['X']) / len(training_data['compound_names']):.1f}\")\n",
    "        \n",
    "        print(f\"\\nğŸ¯ Next steps:\")\n",
    "        print(f\"   1. Run training: python drugreflector_training.py\")\n",
    "        print(f\"   2. Or use quick start: python quick_train.py\")\n",
    "        \n",
    "        return training_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n\" + \"=\" * 80)\n",
    "        print(\"âŒ ERROR DURING DATA PREPARATION\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"   Type: {type(e).__name__}\")\n",
    "        print(f\"   Message: {e}\")\n",
    "        print(f\"\\nğŸ“‹ Full traceback:\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        print(f\"\\nğŸ’¡ Troubleshooting tips:\")\n",
    "        print(f\"   1. Check if all LINCS 2020 files are downloaded\")\n",
    "        print(f\"   2. Verify file paths and names\")\n",
    "        print(f\"   3. Ensure sufficient disk space (>100 GB)\")\n",
    "        print(f\"   4. Check RAM availability (>32 GB recommended)\")\n",
    "        \n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    training_data = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemprop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
