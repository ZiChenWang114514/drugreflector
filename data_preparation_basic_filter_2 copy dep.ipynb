{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a656975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ§¬ DRUGREFLECTOR DATA PREPROCESSING - Optimized Version\n",
      "   Fixed DOS filtering and timepoint matching\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "ğŸ”¬ LINCS 2020 Data Loader - Optimized Version\n",
      "================================================================================\n",
      "Data directory: E:\\ç§‘ç ”\\Models\\drugreflector\\datasets\\LINCS2020\n",
      "Dataset: Expanded CMap LINCS Resource 2020\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "STEP 1: Loading metadata\n",
      "================================================================================\n",
      "ğŸ“– Loading gene information...\n",
      "   File: geneinfo_beta.txt\n",
      "   âœ“ Loaded 12,328 genes\n",
      "   âœ“ Columns: ['gene_id', 'gene_symbol', 'ensembl_id', 'gene_title', 'gene_type', 'src', 'feature_space']\n",
      "   âœ“ Using 'feature_space' column to identify landmarks\n",
      "   âœ“ Landmark genes: 978\n",
      "   âœ“ Expected: 978\n",
      "\n",
      "   Landmark column indices (first 10): [2154 2155 2156 2157 2158 2159 2160 2161 2162 2163]\n",
      "   Sample IDs: ['7319', '9915', '55127', '57178', '9695']\n",
      "   Sample symbols: ['GNAI1', 'HSPA8', 'TXLNA', 'HOMER2', 'FSD1']\n",
      "\n",
      "ğŸ“– Loading cell information...\n",
      "   File: cellinfo_beta.txt\n",
      "   âœ“ Loaded 240 cell lines\n",
      "   âœ“ Columns: ['cell_iname', 'cellosaurus_id', 'donor_age', 'donor_age_death', 'donor_disease_age_onset', 'doubling_time', 'growth_medium', 'provider_catalog_id', 'feature_id', 'cell_type']...\n",
      "   âœ“ Unique cell lines (cell_iname): 240\n",
      "\n",
      "   Sample cell lines:\n",
      "     - 1HAE\n",
      "     - AALE\n",
      "     - AG06263_2\n",
      "     - AG06840_A\n",
      "     - AG078N1_1\n",
      "\n",
      "ğŸ“– Loading compound information...\n",
      "   File: compoundinfo_beta.txt\n",
      "   âœ“ Loaded 39,321 compounds\n",
      "   âœ“ Columns: ['pert_id', 'cmap_name', 'target', 'moa', 'canonical_smiles', 'inchi_key', 'compound_aliases']\n",
      "   âœ“ Unique perturbagens: 34419\n",
      "\n",
      "   Sample compounds:\n",
      "     - BRD-A08715367: L-theanine\n",
      "     - BRD-A12237696: L-citrulline\n",
      "     - BRD-A18795974: BRD-A18795974\n",
      "\n",
      "================================================================================\n",
      "STEP 2: Loading Level 4 signatures\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "ğŸ“– Loading Level 4 Signatures\n",
      "================================================================================\n",
      "File: level4_beta_trt_cp_n1805898x12328.gctx\n",
      "\n",
      "ğŸ“– Reading GCTX file: level4_beta_trt_cp_n1805898x12328.gctx\n",
      "   File size: 82.94 GB\n",
      "   âš ï¸  Large file detected. Using memory-optimized loading...\n",
      "ğŸ“Š Inspecting HDF5 structure...\n",
      "   Available keys: ['0']\n",
      "   âœ“ Matrix shape: (1805898, 12328) (samples Ã— genes)\n",
      "\n",
      "ğŸ“‹ Loading metadata...\n",
      "   Reading sample metadata from: /0/META/ROW\n",
      "   Available row fields: ['id']\n",
      "   âœ“ Loaded 1 sample metadata fields\n",
      "   Reading gene metadata from: /0/META/COL\n",
      "   Available col fields: ['id']\n",
      "   âœ“ Loaded 1 gene metadata fields\n",
      "\n",
      "   âš ï¸ Detected ROW/COL swap. Correcting...\n",
      "\n",
      "   Sample metadata columns: ['id']...\n",
      "   Gene metadata columns: ['id']\n",
      "\n",
      "ğŸ”¬ Filtering to landmark genes...\n",
      "   âœ“ Using 978 landmark features out of 12328 total\n",
      "\n",
      "ğŸ¯ Loading data (memory-optimized)...\n",
      "   Reading 978 columns out of 12328...\n",
      "   Loading rows 1,220,000 to 1,805,898... (100.0%)\n",
      "   Finalizing matrix...\n",
      "   âœ“ Final matrix shape: (1805898, 978)\n",
      "   âœ“ Memory usage: 6.58 GB\n",
      "   âœ“ Data type: float32\n",
      "\n",
      "ğŸ“– Loading instance information...\n",
      "   File: instinfo_beta.txt\n",
      "   âœ“ Loaded 3,026,460 instances\n",
      "   âœ“ Columns: ['bead_batch', 'nearest_dose', 'pert_dose', 'pert_dose_unit', 'pert_idose', 'pert_time', 'pert_itime', 'pert_time_unit', 'cell_mfc_name', 'pert_mfc_id']...\n",
      "   âœ“ Using 'sample_id' as join key\n",
      "   ğŸ”— Merging GCTX metadata with instinfo on 'sample_id'...\n",
      "\n",
      "================================================================================\n",
      "STEP 3: Preparing training data\n",
      "================================================================================\n",
      "\n",
      "[DEBUG] row_meta shape: (1805898, 30)\n",
      "[DEBUG] columns (first 15): ['sample_id', 'bead_batch', 'nearest_dose', 'pert_dose', 'pert_dose_unit', 'pert_idose', 'pert_time', 'pert_itime', 'pert_time_unit', 'cell_mfc_name', 'pert_mfc_id', 'det_plate', 'det_well', 'rna_plate', 'rna_well']\n",
      "[DEBUG] Example row:\n",
      "sample_id     ABY001_A375_XH_X1_B15:A13\n",
      "pert_id                   BRD-K66175015\n",
      "pert_type                        trt_cp\n",
      "cell_iname                         A375\n",
      "pert_time                          24.0\n",
      "pert_dose                          10.0\n",
      "Name: 0, dtype: object\n",
      "\n",
      "================================================================================\n",
      "ğŸ” DRUGREFLECTOR QUALITY CONTROL PIPELINE - Optimized\n",
      "================================================================================\n",
      "Initial samples: 1,805,898\n",
      "Initial memory: 6.58 GB\n",
      "\n",
      "ğŸ“‹ Available metadata columns:\n",
      "   - sample_id: ABY001_A375_XH_X1_B15:A13\n",
      "   - bead_batch: b15\n",
      "   - nearest_dose: 10.0\n",
      "   - pert_dose: 10.0\n",
      "   - pert_dose_unit: uM\n",
      "   - pert_idose: 10 uM\n",
      "   - pert_time: 24.0\n",
      "   - pert_itime: 24 h\n",
      "   - pert_time_unit: h\n",
      "   - cell_mfc_name: A375\n",
      "   - pert_mfc_id: BRD-K66175015\n",
      "   - det_plate: ABY001_A375_XH_X1_B15\n",
      "   - det_well: A13\n",
      "   - rna_plate: ABY001_A375_XH_X1\n",
      "   - rna_well: A13\n",
      "   ... and 15 more\n",
      "\n",
      "âœ“ Using 'cell_iname' as cell line identifier\n",
      "\n",
      "Initial compounds: 34,419\n",
      "\n",
      "================================================================================\n",
      "FILTER 1: Remove DOS compounds (keep trt_cp only)\n",
      "================================================================================\n",
      "   pert_type value counts:\n",
      "     - trt_cp: 1,805,898 samples\n",
      "\n",
      "  âœ“ Keeping only 'trt_cp' perturbations\n",
      "  Removed 0 non-trt_cp observations\n",
      "  Remaining samples: 1,805,898\n",
      "  Remaining compounds: 34,419\n",
      "\n",
      "================================================================================\n",
      "FILTER 2: Remove compounds with <5 observations\n",
      "================================================================================\n",
      "  Compounds with â‰¥5 observations: 22,731/874\n",
      "  Remaining samples: 1,777,129\n",
      "  Remaining compounds: 22,731\n",
      "\n",
      "================================================================================\n",
      "FILTER 3: Remove observations with cosine similarity <0.12\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Calculating similarities (Numba-accelerated)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Computing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22731/22731 [12:45<00:00, 29.70it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ“ Mean similarity: 0.3843\n",
      "  Removed 37,549 low-similarity observations\n",
      "  Remaining samples: 1,739,580\n",
      "\n",
      "================================================================================\n",
      "FILTER 4: Select most frequent dose in range 1-20.0 ÂµM\n",
      "         (Improved: Using intelligent dose binning)\n",
      "================================================================================\n",
      "  Step 1/6: Parsing dose values...\n",
      "  âœ“ Successfully parsed: 1,805,392 entries\n",
      "  âš ï¸  Missing/invalid doses: 506 entries\n",
      "\n",
      "  Step 2/6: Converting to ÂµM...\n",
      "  Dose unit distribution:\n",
      "    - nan: 506 samples\n",
      "    - e: 30 samples\n",
      "\n",
      "  Step 3/6: Applying intelligent dose binning...\n",
      "            (Grouping similar doses like 4.99, 5.00, 5.01 â†’ 5.0)\n",
      "  âœ“ Original unique doses: 10,752\n",
      "  âœ“ Standardized unique doses: 1\n",
      "  âœ“ Compression ratio: 10752.0x\n",
      "\n",
      "  ğŸ“‹ Dose binning examples:\n",
      "    4.9900 ÂµM â†’ 10.0000 ÂµM\n",
      "    5.0000 ÂµM â†’ 10.0000 ÂµM\n",
      "    5.0100 ÂµM â†’ 10.0000 ÂµM\n",
      "    5.0200 ÂµM â†’ 10.0000 ÂµM\n",
      "    10.0000 ÂµM â†’ 10.0000 ÂµM\n",
      "    9.9800 ÂµM â†’ 10.0000 ÂµM\n",
      "    10.0200 ÂµM â†’ 10.0000 ÂµM\n",
      "    3.3300 ÂµM â†’ 10.0000 ÂµM\n",
      "    3.3333 ÂµM â†’ 10.0000 ÂµM\n",
      "    1.1100 ÂµM â†’ 10.0000 ÂµM\n",
      "    1.1111 ÂµM â†’ 10.0000 ÂµM\n",
      "\n",
      "  Step 4/6: Dose distribution analysis...\n",
      "\n",
      "  ğŸ“Š Dose statistics (before filtering):\n",
      "    Total samples with valid dose: 1,739,075\n",
      "    Original dose range: 0.0001 - 316228.00 ÂµM\n",
      "    Mean dose: 9.2557 ÂµM\n",
      "    Median dose: 3.3333 ÂµM\n",
      "\n",
      "  Dose distribution by range:\n",
      "          <0.1 ÂµM:  258,999 ( 14.9%)\n",
      "       0.1-0.5 ÂµM:  249,334 ( 14.3%)\n",
      "         0.5-1 ÂµM:   89,293 (  5.1%)\n",
      "           1-2 ÂµM:  137,117 (  7.9%)\n",
      "           2-5 ÂµM:  260,518 ( 15.0%)\n",
      "          5-10 ÂµM:  611,972 ( 35.2%)\n",
      "         10-20 ÂµM:  102,426 (  5.9%)\n",
      "         20-50 ÂµM:   16,959 (  1.0%)\n",
      "        50-100 ÂµM:    6,555 (  0.4%)\n",
      "       100-500 ÂµM:    5,394 (  0.3%)\n",
      "          >500 ÂµM:      508 (  0.0%)\n",
      "\n",
      "  Top 20 most frequent STANDARDIZED doses:\n",
      "     10.0000 ÂµM: 1,739,075 samples (100.00%)\n",
      "\n",
      "  Step 5/6: Filtering dose range 1-20.0 ÂµM...\n",
      "  âœ“ Samples in target range: 1,142,310\n",
      "  âœ“ Samples outside range: 597,270\n",
      "  âœ“ Removed 597,270 samples outside dose range\n",
      "\n",
      "  Step 6/6: Finding most frequent STANDARDIZED dose per compound...\n",
      "  Processing 22,484 compounds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Finding modal doses (standardized): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22484/22484 [00:01<00:00, 11547.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ“ Identified modal doses for 22,484 compounds\n",
      "\n",
      "  ğŸ“Š Modal dose distribution (STANDARDIZED):\n",
      "     10.0000 ÂµM: 22,484 compounds (100.00%)\n",
      "\n",
      "  âœ… Filter 4 Results (with intelligent dose binning):\n",
      "  âœ“ Removed 0 samples with non-modal dose\n",
      "  âœ“ Remaining samples: 1,142,310\n",
      "  âœ“ Remaining compounds: 22,484\n",
      "  âš ï¸  'pert_dose' not found, skipping dose filter\n",
      "\n",
      "================================================================================\n",
      "FILTER 5: Keep only measurements at [6, 24] hours\n",
      "================================================================================\n",
      "   Available timepoints (numeric):\n",
      "     - 24.0 hours: 1,338,831 samples\n",
      "     - 6.0 hours: 398,943 samples\n",
      "     - 3.0 hours: 31,925 samples\n",
      "     - 48.0 hours: 18,614 samples\n",
      "     - 4.0 hours: 11,372 samples\n",
      "     - 72.0 hours: 2,308 samples\n",
      "     - 2.0 hours: 1,135 samples\n",
      "     - 12.0 hours: 1,083 samples\n",
      "     - -666.0 hours: 581 samples\n",
      "     - 120.0 hours: 367 samples\n",
      "  âœ“ Kept samples at [6, 24] hours\n",
      "  Removed 33,712 observations (invalid timepoint)\n",
      "  Remaining samples: 1,108,598\n",
      "\n",
      "================================================================================\n",
      "FILTER 6: Remove cell lines with <200 compounds\n",
      "================================================================================\n",
      "  Total cell lines before filtering: 221\n",
      "  Compounds per cell line (mean): 807.5\n",
      "  Compounds per cell line (median): 236\n",
      "  Compounds per cell line (min): 1\n",
      "  Compounds per cell line (max): 15171\n",
      "\n",
      "  Cell lines with â‰¥200 compounds: 120/221\n",
      "\n",
      "  Sample removed cell lines (showing up to 10):\n",
      "    - 5637: 3 compounds\n",
      "    - AN3CA: 3 compounds\n",
      "    - BICR6: 117 compounds\n",
      "    - BJAB: 124 compounds\n",
      "    - BT20: 198 compounds\n",
      "    - BT474: 119 compounds\n",
      "    - C42: 5 compounds\n",
      "    - COV434: 117 compounds\n",
      "    - CW2: 3 compounds\n",
      "    - DU145: 5 compounds\n",
      "\n",
      "  Removed 56,105 observations from 101 cell lines\n",
      "  Remaining samples: 1,052,493\n",
      "  Remaining compounds: 22,406\n",
      "  Remaining cell lines: 120\n",
      "\n",
      "================================================================================\n",
      "FILTER 7: Remove compounds in <5 or >40 cell lines\n",
      "================================================================================\n",
      "  Compounds in 5-40 cell lines: 10,755/22,406\n",
      "  Removed 282,428 observations\n",
      "  Remaining samples: 770,065\n",
      "  Remaining compounds: 10,755\n",
      "\n",
      "================================================================================\n",
      "âœ… FINAL DATASET\n",
      "================================================================================\n",
      "  Extracted 770,065 samples\n",
      "  Memory usage: 2.81 GB\n",
      "\n",
      "  Total samples: 770,065\n",
      "  Total compounds: 10,755\n",
      "  Cell lines: 114\n",
      "  Gene features: 978\n",
      "  Samples per compound (mean): 71.6\n",
      "  Samples per compound (median): 52\n",
      "  Compounds with >100 observations: 2,076\n",
      "\n",
      "ğŸ“Š Comparison with paper (SI page 2):\n",
      "  Paper: 425,242 obs, 9,597 compounds, 52 cell lines\n",
      "  Ours:  770,065 obs, 10,755 compounds, 114 cell lines\n",
      "  Compound retention rate: 31.2%\n",
      "  âœ“ Using geneinfo_beta.txt for gene names\n",
      "\n",
      "================================================================================\n",
      "STEP 4: Creating 3-fold splits\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "ğŸ² Creating 3-fold cross-validation splits\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Fold statistics:\n",
      "   Fold 0: 260,152 samples, 10,755 compounds\n",
      "   Fold 1: 256,800 samples, 10,755 compounds\n",
      "   Fold 2: 253,113 samples, 10,755 compounds\n",
      "\n",
      "================================================================================\n",
      "STEP 5: Saving processed data\n",
      "================================================================================\n",
      "ğŸ’¾ Saving to: E:\\ç§‘ç ”\\Models\\drugreflector\\processed_data\\training_data_lincs2020_optimized_1212_s.pkl\n",
      "âœ“ Saved successfully!\n",
      "   File size: 3086.7 MB\n",
      "\n",
      "================================================================================\n",
      "âœ… DATA PREPARATION COMPLETE!\n",
      "================================================================================\n",
      "ğŸ“ Output: E:\\ç§‘ç ”\\Models\\drugreflector\\processed_data\\training_data_lincs2020_optimized_1212_s.pkl\n",
      "\n",
      "ğŸ“Š Final dataset:\n",
      "   â€¢ Samples: 770,065\n",
      "   â€¢ Compounds: 10,755\n",
      "   â€¢ Genes: 978\n",
      "   â€¢ Memory: 2.81 GB\n",
      "\n",
      "ğŸ“ˆ Comparison with paper:\n",
      "   Samples: 770,065 / 425,242 (181.1%)\n",
      "   Compounds: 10,755 / 9,597 (112.1%)\n",
      "\n",
      "ğŸ¯ Ready for training!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "LINCS 2020æ•°æ®åŠ è½½å’Œé¢„å¤„ç†è„šæœ¬ - ä¼˜åŒ–ç‰ˆ\n",
    "ä¿®æ­£DOSè¿‡æ»¤å’Œæ—¶é—´ç‚¹åŒ¹é…é—®é¢˜\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import gzip\n",
    "import shutil\n",
    "import glob\n",
    "from scipy.spatial.distance import cosine, pdist, squareform\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from numba import jit, prange\n",
    "from sklearn.preprocessing import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, Tuple, Optional\n",
    "import warnings\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def assign_standard_dose(dose_values, method='standard_doses', tolerance_pct=0.1, round_precision=0.5):\n",
    "    \"\"\"\n",
    "    å°†ç²¾ç¡®å‰‚é‡å€¼æ˜ å°„åˆ°æ ‡å‡†å‰‚é‡\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dose_values : array-like\n",
    "        åŸå§‹å‰‚é‡å€¼\n",
    "    method : str\n",
    "        'standard_doses' - æ˜ å°„åˆ°é¢„å®šä¹‰çš„æ ‡å‡†å‰‚é‡ (0.01, 0.03, 0.1, 0.3, 1, 3, 10 ç­‰)\n",
    "        'rounding' - å››èˆäº”å…¥åˆ°æŒ‡å®šç²¾åº¦\n",
    "        'hybrid' - æ··åˆæ–¹æ³•ï¼šå…ˆå°è¯•æ ‡å‡†å‰‚é‡ï¼Œå¤±è´¥åˆ™å››èˆäº”å…¥\n",
    "    tolerance_pct : float\n",
    "        æ ‡å‡†å‰‚é‡æ˜ å°„çš„å®¹å·®ç™¾åˆ†æ¯” (é»˜è®¤ 10%)\n",
    "    round_precision : float\n",
    "        å››èˆäº”å…¥ç²¾åº¦ (é»˜è®¤ 0.5 ÂµM)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    np.array : æ ‡å‡†åŒ–åçš„å‰‚é‡å€¼\n",
    "    \"\"\"\n",
    "    dose_values = np.array(dose_values, dtype=np.float64)\n",
    "    \n",
    "    # å®šä¹‰è¯ç†å­¦å¸¸ç”¨çš„æ ‡å‡†å‰‚é‡ï¼ˆåŠå¯¹æ•°çº§åˆ«ï¼‰\n",
    "    # è¿™äº›æ˜¯å®éªŒä¸­æœ€å¸¸ç”¨çš„å‰‚é‡ç‚¹\n",
    "    # standard_doses = np.array([\n",
    "    #    0.001, 0.003, 0.01, 0.03, 0.04, 0.05, 0.08, 0.1, 0.12, 0.15, 0.2, 0.25, 0.3, 0.37, 0.4, 0.5, \n",
    "    #    0.75, 0.77, 0.8, 1.0, 1.1, 1.11, 1.2, 1.5, 2.0, 2.3, 2.5, 3.0, 3.3, 3.33, 4.0, 5.0, \n",
    "    #    6.0, 7.0, 7.5, 8.0, 10.0, 12.0, 15.0, 20.0, 25.0, 30.0, 50.0, 100.0\n",
    "    #])\n",
    "    standard_doses = np.array([1.0, 2.5, 5.0, 10, 15, 20])\n",
    "        \n",
    "    \n",
    "    standardized = np.full_like(dose_values, np.nan)\n",
    "    \n",
    "    for i, dose in enumerate(dose_values):\n",
    "        if np.isnan(dose):\n",
    "            continue\n",
    "            \n",
    "        if method == 'rounding':\n",
    "            # æ–¹æ³•1ï¼šç®€å•å››èˆäº”å…¥\n",
    "            standardized[i] = round(dose / round_precision) * round_precision\n",
    "            \n",
    "        elif method == 'standard_doses':\n",
    "            # æ–¹æ³•2ï¼šæ˜ å°„åˆ°æœ€è¿‘çš„æ ‡å‡†å‰‚é‡\n",
    "            # è®¡ç®—ä¸æ¯ä¸ªæ ‡å‡†å‰‚é‡çš„ç›¸å¯¹å·®å¼‚\n",
    "            relative_diff = np.abs(standard_doses - dose) / np.maximum(dose, 0.001)\n",
    "            min_idx = np.argmin(relative_diff)\n",
    "            \n",
    "            if relative_diff[min_idx] <= tolerance_pct:\n",
    "                standardized[i] = standard_doses[min_idx]\n",
    "            else:\n",
    "                # å¦‚æœå·®å¼‚å¤ªå¤§ï¼Œä½¿ç”¨å››èˆäº”å…¥\n",
    "                standardized[i] = round(dose / round_precision) * round_precision\n",
    "                \n",
    "        elif method == 'hybrid':\n",
    "            # æ–¹æ³•3ï¼šæ··åˆæ–¹æ³•\n",
    "            # é¦–å…ˆå°è¯•æ˜ å°„åˆ°æ ‡å‡†å‰‚é‡\n",
    "            relative_diff = np.abs(standard_doses - dose) / np.maximum(dose, 0.001)\n",
    "            min_idx = np.argmin(relative_diff)\n",
    "            \n",
    "            if relative_diff[min_idx] <= tolerance_pct:\n",
    "                standardized[i] = standard_doses[min_idx]\n",
    "            else:\n",
    "                # å¯¹äºéæ ‡å‡†å‰‚é‡ï¼Œä½¿ç”¨æ™ºèƒ½å››èˆäº”å…¥\n",
    "                # å°å‰‚é‡ç”¨æ›´ç»†çš„ç²¾åº¦ï¼Œå¤§å‰‚é‡ç”¨æ›´ç²—çš„ç²¾åº¦\n",
    "                if dose < 0.1:\n",
    "                    precision = 0.01\n",
    "                elif dose < 1:\n",
    "                    precision = 0.1\n",
    "                elif dose < 10:\n",
    "                    precision = 0.5\n",
    "                else:\n",
    "                    precision = 1.0\n",
    "                standardized[i] = round(dose / precision) * precision\n",
    "    \n",
    "    return standardized\n",
    "\n",
    "def assign_standard_dose(dose_values, method='standard_doses', tolerance_pct=0.1, round_precision=0.5):\n",
    "    \"\"\"\n",
    "    å°†1-20 ÂµMèŒƒå›´å†…çš„å‰‚é‡ç›´æ¥æ˜ å°„åˆ°æœ€è¿‘çš„æ ‡å‡†å‰‚é‡\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dose_values : array-like\n",
    "        åŸå§‹å‰‚é‡å€¼ï¼ˆå•ä½ï¼šÂµMï¼‰\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    np.array : æ ‡å‡†åŒ–åçš„å‰‚é‡å€¼\n",
    "    \n",
    "    æ˜ å°„è§„åˆ™:\n",
    "    - 1-20 ÂµMèŒƒå›´å†…ï¼šæ˜ å°„åˆ°æœ€è¿‘çš„æ ‡å‡†å‰‚é‡ç‚¹\n",
    "    - <1 ÂµM æˆ– >20 ÂµMï¼šæ˜ å°„åˆ°è¾¹ç•Œå€¼ (1.0 æˆ– 20.0)\n",
    "    - NaNï¼šä¿æŒä¸ºNaN\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    dose_values = np.array(dose_values, dtype=np.float64)\n",
    "    \n",
    "    # å®šä¹‰æ ‡å‡†å‰‚é‡ç‚¹ï¼ˆ1-20 ÂµMèŒƒå›´ï¼Œ7ä¸ªç‚¹ï¼‰\n",
    "    standard_doses = np.array([10.0])\n",
    "    \n",
    "    standardized = np.full_like(dose_values, np.nan)\n",
    "    \n",
    "    for i, dose in enumerate(dose_values):\n",
    "        if np.isnan(dose):\n",
    "            continue\n",
    "        \n",
    "        # å°†å‰‚é‡é™åˆ¶åœ¨1-20èŒƒå›´å†…\n",
    "        dose_clipped = np.clip(dose, 1, 20)\n",
    "        \n",
    "        # æ‰¾åˆ°æœ€è¿‘çš„æ ‡å‡†å‰‚é‡ï¼ˆä½¿ç”¨ç»å¯¹å·®å€¼ï¼‰\n",
    "        distances = np.abs(standard_doses - dose_clipped)\n",
    "        min_idx = np.argmin(distances)\n",
    "        \n",
    "        standardized[i] = standard_doses[min_idx]\n",
    "    \n",
    "    return standardized\n",
    "\n",
    "class LINCS2020DataLoader:\n",
    "    \"\"\"\n",
    "    åŠ è½½å’Œé¢„å¤„ç†LINCS 2020æ•°æ® - ä¼˜åŒ–ç‰ˆ\n",
    "    ä¿®æ­£DOSè¿‡æ»¤å’Œæ—¶é—´ç‚¹åŒ¹é…\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.gene_info = None\n",
    "        self.cell_info = None\n",
    "        self.compound_info = None\n",
    "        self.inst_info = None  \n",
    "        self.signatures = None\n",
    "        self.decompressed_files = []\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ğŸ”¬ LINCS 2020 Data Loader - Optimized Version\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Data directory: {self.data_dir}\")\n",
    "        print(f\"Dataset: Expanded CMap LINCS Resource 2020\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    def load_gene_info(self):\n",
    "        \"\"\"åŠ è½½åŸºå› ä¿¡æ¯ï¼Œè·å–978ä¸ªlandmark genes\"\"\"\n",
    "        gene_file = self.data_dir / \"geneinfo_beta.txt\"\n",
    "        \n",
    "        if not gene_file.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"âŒ Gene info file not found: {gene_file}\\n\"\n",
    "                f\"   Please download 'geneinfo_beta.txt' from:\\n\"\n",
    "                f\"   https://clue.io/data/CMap2020#LINCS2020\"\n",
    "            )\n",
    "        \n",
    "        print(f\"ğŸ“– Loading gene information...\")\n",
    "        print(f\"   File: {gene_file.name}\")\n",
    "        \n",
    "        gene_info = pd.read_csv(gene_file, sep='\\t')\n",
    "        \n",
    "        print(f\"   âœ“ Loaded {len(gene_info):,} genes\")\n",
    "        print(f\"   âœ“ Columns: {list(gene_info.columns)}\")\n",
    "        \n",
    "        if 'feature_space' in gene_info.columns:\n",
    "            landmark_mask = gene_info['feature_space'] == 'landmark'\n",
    "            landmark_genes = gene_info[landmark_mask].copy()\n",
    "            print(f\"   âœ“ Using 'feature_space' column to identify landmarks\")\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Cannot identify landmark genes. 'feature_space' column not found.\\n\"\n",
    "                f\"Available columns: {list(gene_info.columns)}\"\n",
    "            )\n",
    "\n",
    "        print(f\"   âœ“ Landmark genes: {len(landmark_genes):,}\")\n",
    "        print(f\"   âœ“ Expected: 978\")\n",
    "\n",
    "        self.landmark_col_indices = np.where(landmark_mask.values)[0]\n",
    "        print(f\"\\n   Landmark column indices (first 10): {self.landmark_col_indices[:10]}\")\n",
    "\n",
    "        self.landmark_gene_ids = set(landmark_genes['gene_id'].astype(str).values)\n",
    "        self.landmark_gene_symbols = set(landmark_genes['gene_symbol'].astype(str).values)\n",
    "\n",
    "        print(f\"   Sample IDs: {list(self.landmark_gene_ids)[:5]}\")\n",
    "        print(f\"   Sample symbols: {list(self.landmark_gene_symbols)[:5]}\")\n",
    "\n",
    "        self.gene_info = gene_info\n",
    "        return gene_info\n",
    "    \n",
    "    def load_cell_info(self):\n",
    "        \"\"\"åŠ è½½ç»†èƒç³»ä¿¡æ¯\"\"\"\n",
    "        cell_file = self.data_dir / \"cellinfo_beta.txt\"\n",
    "        \n",
    "        if not cell_file.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"âŒ Cell info file not found: {cell_file}\\n\"\n",
    "                f\"   Please download 'cellinfo_beta.txt' from:\\n\"\n",
    "                f\"   https://clue.io/data/CMap2020#LINCS2020\"\n",
    "            )\n",
    "        \n",
    "        print(f\"\\nğŸ“– Loading cell information...\")\n",
    "        print(f\"   File: {cell_file.name}\")\n",
    "        \n",
    "        cell_info = pd.read_csv(cell_file, sep='\\t')\n",
    "        \n",
    "        print(f\"   âœ“ Loaded {len(cell_info):,} cell lines\")\n",
    "        print(f\"   âœ“ Columns: {list(cell_info.columns[:10])}...\")\n",
    "        \n",
    "        if 'cell_iname' in cell_info.columns:\n",
    "            print(f\"   âœ“ Unique cell lines (cell_iname): {cell_info['cell_iname'].nunique()}\")\n",
    "        \n",
    "        print(f\"\\n   Sample cell lines:\")\n",
    "        for cell in cell_info['cell_iname'].head(5).values:\n",
    "            print(f\"     - {cell}\")\n",
    "        \n",
    "        self.cell_info = cell_info\n",
    "        return cell_info\n",
    "    \n",
    "    def load_compound_info(self):\n",
    "        \"\"\"åŠ è½½åŒ–åˆç‰©ä¿¡æ¯\"\"\"\n",
    "        compound_file = self.data_dir / \"compoundinfo_beta.txt\"\n",
    "        \n",
    "        if not compound_file.exists():\n",
    "            print(f\"âš ï¸  Compound info file not found: {compound_file}\")\n",
    "            print(f\"   This file is optional but recommended.\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"\\nğŸ“– Loading compound information...\")\n",
    "        print(f\"   File: {compound_file.name}\")\n",
    "        \n",
    "        compound_info = pd.read_csv(compound_file, sep='\\t')\n",
    "        \n",
    "        print(f\"   âœ“ Loaded {len(compound_info):,} compounds\")\n",
    "        print(f\"   âœ“ Columns: {list(compound_info.columns)}\")\n",
    "        \n",
    "        if 'pert_id' in compound_info.columns:\n",
    "            print(f\"   âœ“ Unique perturbagens: {compound_info['pert_id'].nunique()}\")\n",
    "        \n",
    "        print(f\"\\n   Sample compounds:\")\n",
    "        for _, row in compound_info.head(3).iterrows():\n",
    "            name = row.get('cmap_name', 'Unknown')\n",
    "            print(f\"     - {row['pert_id']}: {name}\")\n",
    "        \n",
    "        self.compound_info = compound_info\n",
    "        return compound_info\n",
    "    \n",
    "    def load_instance_info(self):\n",
    "        \"\"\"åŠ è½½å®ä¾‹ä¿¡æ¯\"\"\"\n",
    "        inst_file = self.data_dir / \"instinfo_beta.txt\"\n",
    "\n",
    "        if not inst_file.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"âŒ Instance info file not found: {inst_file}\\n\"\n",
    "                f\"   Please download 'instinfo_beta.txt' from:\\n\"\n",
    "                f\"   https://clue.io/data/CMap2020#LINCS2020\"\n",
    "            )\n",
    "\n",
    "        print(f\"\\nğŸ“– Loading instance information...\")\n",
    "        print(f\"   File: {inst_file.name}\")\n",
    "\n",
    "        inst_info = pd.read_csv(inst_file, sep='\\t')\n",
    "\n",
    "        print(f\"   âœ“ Loaded {len(inst_info):,} instances\")\n",
    "        print(f\"   âœ“ Columns: {list(inst_info.columns[:10])}...\")\n",
    "\n",
    "        if 'inst_id' in inst_info.columns:\n",
    "            self.instance_join_col = 'inst_id'\n",
    "        elif 'sample_id' in inst_info.columns:\n",
    "            self.instance_join_col = 'sample_id'\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Neither 'inst_id' nor 'sample_id' found in instinfo_beta.txt. \"\n",
    "                f\"Available columns: {list(inst_info.columns)}\"\n",
    "            )\n",
    "\n",
    "        print(f\"   âœ“ Using '{self.instance_join_col}' as join key\")\n",
    "\n",
    "        self.inst_info = inst_info\n",
    "        return inst_info\n",
    "\n",
    "    def decompress_gctx_file(self, gctx_file):\n",
    "        \"\"\"å¦‚æœGCTXæ–‡ä»¶è¢«å‹ç¼©ï¼Œåˆ™è§£å‹\"\"\"\n",
    "        gctx_file = Path(gctx_file)\n",
    "        \n",
    "        if not gctx_file.exists():\n",
    "            raise FileNotFoundError(f\"GCTX file not found: {gctx_file}\")\n",
    "        \n",
    "        if str(gctx_file).endswith('.gz'):\n",
    "            print(f\"âš ï¸  Detected compressed GCTX file: {gctx_file.name}\")\n",
    "            \n",
    "            decompressed_dir = self.data_dir / \"_decompressed\"\n",
    "            decompressed_dir.mkdir(exist_ok=True)\n",
    "            \n",
    "            output_path = decompressed_dir / gctx_file.stem\n",
    "            \n",
    "            if output_path.exists():\n",
    "                print(f\"âœ“ Found existing decompressed file: {output_path.name}\")\n",
    "                return str(output_path)\n",
    "            \n",
    "            print(f\"ğŸ“¦ Decompressing...\")\n",
    "            with gzip.open(gctx_file, 'rb') as f_in:\n",
    "                with open(output_path, 'wb') as f_out:\n",
    "                    shutil.copyfileobj(f_in, f_out, length=8*1024*1024)\n",
    "            \n",
    "            print(f\"âœ“ Decompressed to: {output_path}\")\n",
    "            return str(output_path)\n",
    "        \n",
    "        return str(gctx_file)\n",
    "    \n",
    "    def read_gctx(self, gctx_file, use_landmark_only=True):\n",
    "        \"\"\"è¯»å–GCTXæ–‡ä»¶ - å†…å­˜ä¼˜åŒ–ç‰ˆ\"\"\"\n",
    "        gctx_file = str(gctx_file)\n",
    "        print(f\"\\nğŸ“– Reading GCTX file: {Path(gctx_file).name}\")\n",
    "        \n",
    "        file_size_gb = Path(gctx_file).stat().st_size / (1024**3)\n",
    "        print(f\"   File size: {file_size_gb:.2f} GB\")\n",
    "        \n",
    "        if file_size_gb > 50:\n",
    "            print(f\"   âš ï¸  Large file detected. Using memory-optimized loading...\")\n",
    "        \n",
    "        gctx_file = self.decompress_gctx_file(gctx_file)\n",
    "        \n",
    "        with h5py.File(gctx_file, 'r') as f:\n",
    "            print(f\"ğŸ“Š Inspecting HDF5 structure...\")\n",
    "            print(f\"   Available keys: {list(f.keys())}\")\n",
    "            \n",
    "            if '/0/DATA/0/matrix' in f:\n",
    "                matrix_dataset = f['/0/DATA/0/matrix']\n",
    "                row_path = '/0/META/ROW'\n",
    "                col_path = '/0/META/COL'\n",
    "            elif '/matrix' in f:\n",
    "                matrix_dataset = f['/matrix']\n",
    "                row_path = '/row'\n",
    "                col_path = '/col'\n",
    "            else:\n",
    "                raise ValueError(f\"Cannot find matrix in GCTX file. Available keys: {list(f.keys())}\")\n",
    "            \n",
    "            matrix_shape = matrix_dataset.shape\n",
    "            print(f\"   âœ“ Matrix shape: {matrix_shape} (samples Ã— genes)\")\n",
    "            \n",
    "            print(f\"\\nğŸ“‹ Loading metadata...\")\n",
    "            \n",
    "            # è¯»å–æ ·æœ¬å…ƒæ•°æ®\n",
    "            sample_meta = {}\n",
    "            if row_path in f:\n",
    "                print(f\"   Reading sample metadata from: {row_path}\")\n",
    "                print(f\"   Available row fields: {list(f[row_path].keys())}\")\n",
    "                \n",
    "                for key in f[row_path].keys():\n",
    "                    data = f[f'{row_path}/{key}'][:]\n",
    "                    if data.dtype.kind in ['S', 'O', 'U']:\n",
    "                        try:\n",
    "                            sample_meta[key] = np.char.decode(data.astype('S'), 'utf-8')\n",
    "                        except:\n",
    "                            sample_meta[key] = data.astype(str)\n",
    "                    else:\n",
    "                        sample_meta[key] = data\n",
    "                \n",
    "                print(f\"   âœ“ Loaded {len(sample_meta)} sample metadata fields\")\n",
    "            else:\n",
    "                raise ValueError(f\"Cannot find row metadata at: {row_path}\")\n",
    "            \n",
    "            # è¯»å–åŸºå› å…ƒæ•°æ®\n",
    "            gene_meta = {}\n",
    "            if col_path in f:\n",
    "                print(f\"   Reading gene metadata from: {col_path}\")\n",
    "                print(f\"   Available col fields: {list(f[col_path].keys())}\")\n",
    "                \n",
    "                for key in f[col_path].keys():\n",
    "                    data = f[f'{col_path}/{key}'][:]\n",
    "                    if data.dtype.kind in ['S', 'O', 'U']:\n",
    "                        try:\n",
    "                            gene_meta[key] = np.char.decode(data.astype('S'), 'utf-8')\n",
    "                        except:\n",
    "                            gene_meta[key] = data.astype(str)\n",
    "                    else:\n",
    "                        gene_meta[key] = data\n",
    "                \n",
    "                print(f\"   âœ“ Loaded {len(gene_meta)} gene metadata fields\")\n",
    "            else:\n",
    "                raise ValueError(f\"Cannot find col metadata at: {col_path}\")\n",
    "            \n",
    "            sample_df = pd.DataFrame(sample_meta)\n",
    "            gene_df = pd.DataFrame(gene_meta)\n",
    "\n",
    "            # è‡ªåŠ¨æ£€æµ‹å¹¶äº¤æ¢ROW/COL\n",
    "            if self.gene_info is not None:\n",
    "                n_features_expected = len(self.gene_info)\n",
    "                if len(sample_df) == n_features_expected and len(gene_df) != n_features_expected:\n",
    "                    print(f\"\\n   âš ï¸ Detected ROW/COL swap. Correcting...\")\n",
    "                    sample_df, gene_df = gene_df, sample_df\n",
    "\n",
    "            print(f\"\\n   Sample metadata columns: {list(sample_df.columns[:10])}...\")\n",
    "            print(f\"   Gene metadata columns: {list(gene_df.columns)}\")\n",
    "            \n",
    "            # ç¡®å®šlandmarkåŸºå› çš„åˆ—ç´¢å¼•\n",
    "            landmark_col_indices = None\n",
    "            if use_landmark_only:\n",
    "                print(f\"\\nğŸ”¬ Filtering to landmark genes...\")\n",
    "\n",
    "                if (self.gene_info is None) or (not hasattr(self, \"landmark_col_indices\")):\n",
    "                    print(f\"   Loading gene info to get landmark indices...\")\n",
    "                    self.load_gene_info()\n",
    "\n",
    "                if len(self.gene_info) != matrix_shape[1]:\n",
    "                    print(f\"   âš ï¸  Warning: geneinfo rows ({len(self.gene_info)}) \"\n",
    "                        f\"!= GCTX feature count ({matrix_shape[1]})\")\n",
    "                \n",
    "                landmark_col_indices = np.array(self.landmark_col_indices, dtype=int)\n",
    "                print(f\"   âœ“ Using {len(landmark_col_indices)} landmark features \"\n",
    "                    f\"out of {matrix_shape[1]} total\")\n",
    "            \n",
    "            # å†…å­˜ä¼˜åŒ–ï¼šåˆ†å—è¯»å–\n",
    "            print(f\"\\nğŸ¯ Loading data (memory-optimized)...\")\n",
    "            if landmark_col_indices is not None:\n",
    "                print(f\"   Reading {len(landmark_col_indices)} columns out of {matrix_shape[1]}...\")\n",
    "                chunk_size = 610000\n",
    "                chunks = []\n",
    "                \n",
    "                for start_idx in range(0, matrix_shape[0], chunk_size):\n",
    "                    end_idx = min(start_idx + chunk_size, matrix_shape[0])\n",
    "                    print(f\"   Loading rows {start_idx:,} to {end_idx:,}... ({end_idx/matrix_shape[0]*100:.1f}%)\", end='\\r')\n",
    "                    \n",
    "                    chunk = matrix_dataset[start_idx:end_idx, landmark_col_indices].astype(np.float32)\n",
    "                    chunks.append(chunk)\n",
    "                    \n",
    "                    if len(chunks) >= 10:\n",
    "                        print(f\"\\n   Consolidating chunks...\")\n",
    "                        merged = np.vstack(chunks)\n",
    "                        chunks = [merged]\n",
    "                        gc.collect()\n",
    "                \n",
    "                print(f\"\\n   Finalizing matrix...\")\n",
    "                matrix = np.vstack(chunks) if len(chunks) > 1 else chunks[0]\n",
    "                del chunks\n",
    "                gc.collect()\n",
    "            else:\n",
    "                print(f\"   Reading full matrix...\")\n",
    "                matrix = matrix_dataset[:].astype(np.float32)\n",
    "            \n",
    "            print(f\"   âœ“ Final matrix shape: {matrix.shape}\")\n",
    "            print(f\"   âœ“ Memory usage: {matrix.nbytes / (1024**3):.2f} GB\")\n",
    "            print(f\"   âœ“ Data type: {matrix.dtype}\")\n",
    "        \n",
    "        return matrix, sample_df, gene_df\n",
    "    \n",
    "    def load_level4_signatures(self):\n",
    "        \"\"\"åŠ è½½Level 4æ•°æ®\"\"\"\n",
    "        level4_file = self.data_dir / \"level4_beta_trt_cp_n1805898x12328.gctx\"\n",
    "        \n",
    "        if not level4_file.exists():\n",
    "            pattern = self.data_dir / \"level4_beta_trt_cp*.gctx\"\n",
    "            files = glob.glob(str(pattern))\n",
    "            \n",
    "            if not files:\n",
    "                raise FileNotFoundError(\n",
    "                    f\"âŒ Level 4 file not found: {level4_file}\\n\"\n",
    "                    f\"   Please download from: https://clue.io/data/CMap2020#LINCS2020\"\n",
    "                )\n",
    "            \n",
    "            level4_file = files[0]\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ğŸ“– Loading Level 4 Signatures\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"File: {level4_file.name}\")\n",
    "        \n",
    "        matrix, sample_meta, gene_meta = self.read_gctx(\n",
    "            level4_file, \n",
    "            use_landmark_only=True\n",
    "        )\n",
    "\n",
    "        # ç”¨instinfoè¡¥é½metadata\n",
    "        if 'id' in sample_meta.columns:\n",
    "            sample_meta = sample_meta.rename(columns={'id': 'sample_id'})\n",
    "        elif 'sample_id' not in sample_meta.columns:\n",
    "            raise ValueError(\"Cannot find 'id' or 'sample_id' in GCTX ROW metadata\")\n",
    "\n",
    "        if self.inst_info is None:\n",
    "            self.load_instance_info()\n",
    "\n",
    "        inst_info = self.inst_info\n",
    "        join_col = getattr(self, \"instance_join_col\", None)\n",
    "        \n",
    "        if join_col is None:\n",
    "            join_col = 'sample_id' if 'sample_id' in inst_info.columns else 'inst_id'\n",
    "\n",
    "        if join_col not in sample_meta.columns:\n",
    "            if join_col == 'inst_id' and 'sample_id' in sample_meta.columns:\n",
    "                print(\"   âš ï¸ Using 'sample_id' instead of 'inst_id'\")\n",
    "                join_col = 'sample_id'\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Join column '{join_col}' not found in GCTX metadata. \"\n",
    "                    f\"Available: {list(sample_meta.columns)}\"\n",
    "                )\n",
    "\n",
    "        print(f\"   ğŸ”— Merging GCTX metadata with instinfo on '{join_col}'...\")\n",
    "        merged_meta = sample_meta.merge(inst_info, on=join_col, how='left')\n",
    "\n",
    "        if 'pert_id' not in merged_meta.columns:\n",
    "            raise ValueError(\"After merging, 'pert_id' is still missing\")\n",
    "\n",
    "        n_missing = merged_meta['pert_id'].isna().sum()\n",
    "        if n_missing > 0:\n",
    "            print(f\"   âš ï¸ {n_missing:,} rows have missing pert_id\")\n",
    "\n",
    "        self.signatures = {\n",
    "            'matrix': matrix,\n",
    "            'row_meta': merged_meta,\n",
    "            'col_meta': gene_meta\n",
    "        }\n",
    "\n",
    "        return matrix, merged_meta, gene_meta\n",
    "    \n",
    "    def calculate_cosine_similarity_to_nearest_replicate(\n",
    "        self, \n",
    "        matrix: np.ndarray, \n",
    "        pert_ids: pd.Series\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        ä½¿ç”¨Numba JITç¼–è¯‘çš„è¶…é«˜æ•ˆç‰ˆæœ¬\n",
    "        \n",
    "        è®¡ç®—æ¯ä¸ªæ ·æœ¬ä¸å…¶æœ€è¿‘åŒåŒ–åˆç‰©å¤åˆ¶å“çš„ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"\\nğŸ“Š Calculating similarities (Numba-accelerated)...\")\n",
    "        \n",
    "        # é¢„å½’ä¸€åŒ–\n",
    "        matrix_norm = normalize(matrix, norm='l2', axis=1).astype(np.float32)\n",
    "        \n",
    "        @jit(nopython=True, parallel=True, fastmath=True)\n",
    "        def compute_max_similarities(data, indices, n_total):\n",
    "            \"\"\"NumbaåŠ é€Ÿçš„æ ¸å¿ƒè®¡ç®—\"\"\"\n",
    "            n = len(indices)\n",
    "            result = np.full(n_total, -np.inf, dtype=np.float32)\n",
    "            \n",
    "            for i in prange(n):\n",
    "                idx_i = indices[i]\n",
    "                vec_i = data[i]\n",
    "                max_sim = -np.inf\n",
    "                \n",
    "                for j in range(n):\n",
    "                    if i != j:\n",
    "                        # ç‚¹ç§¯ï¼ˆå·²å½’ä¸€åŒ– = ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰\n",
    "                        sim = np.dot(vec_i, data[j])\n",
    "                        if sim > max_sim:\n",
    "                            max_sim = sim\n",
    "                \n",
    "                result[idx_i] = max_sim\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        nearest_similarities = np.zeros(len(pert_ids), dtype=np.float32)\n",
    "        \n",
    "        # æŒ‰åŒ–åˆç‰©åˆ†ç»„\n",
    "        pert_ids_array = pert_ids.values\n",
    "        unique_perts = np.unique(pert_ids_array)\n",
    "        \n",
    "        from tqdm import tqdm\n",
    "        for pert_id in tqdm(unique_perts, desc=\"   Computing\"):\n",
    "            mask = pert_ids_array == pert_id\n",
    "            indices = np.where(mask)[0]\n",
    "            \n",
    "            if len(indices) < 2:\n",
    "                nearest_similarities[indices] = 0.0\n",
    "                continue\n",
    "            \n",
    "            pert_data = matrix_norm[mask]\n",
    "            sims = compute_max_similarities(pert_data, indices, len(pert_ids))\n",
    "            nearest_similarities[indices] = sims[indices]\n",
    "        \n",
    "        print(f\"   âœ“ Mean similarity: {nearest_similarities.mean():.4f}\")\n",
    "        return nearest_similarities\n",
    "    \n",
    "    def prepare_training_data(\n",
    "        self, \n",
    "        min_observations_per_compound=5,\n",
    "        min_replicate_similarity=0.12,\n",
    "        dose_range=(0.1, 20.0),\n",
    "        valid_timepoints=[6, 24],  # ä¿®æ”¹ä¸ºæ•°å€¼åˆ—è¡¨\n",
    "        min_cell_lines=5,\n",
    "        max_cell_lines=40,\n",
    "        min_compounds_per_cell=200,\n",
    "        remove_dos=True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        å‡†å¤‡è®­ç»ƒæ•°æ®\n",
    "        ä¿®æ­£DOSè¿‡æ»¤å’Œæ—¶é—´ç‚¹åŒ¹é…\n",
    "        \"\"\"\n",
    "        if self.signatures is None:\n",
    "            raise ValueError(\"Please load signatures first\")\n",
    "        \n",
    "        matrix = self.signatures['matrix']\n",
    "        row_meta = self.signatures['row_meta'].copy()\n",
    "        col_meta = self.signatures['col_meta']\n",
    "        \n",
    "        print(f\"\\n[DEBUG] row_meta shape: {row_meta.shape}\")\n",
    "        print(f\"[DEBUG] columns (first 15): {list(row_meta.columns[:15])}\")\n",
    "        print(f\"[DEBUG] Example row:\")\n",
    "        print(row_meta.iloc[0][['sample_id', 'pert_id', 'pert_type', 'cell_iname', 'pert_time', 'pert_dose']])\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"ğŸ” DRUGREFLECTOR QUALITY CONTROL PIPELINE - Optimized\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Initial samples: {len(row_meta):,}\")\n",
    "        print(f\"Initial memory: {matrix.nbytes / (1024**3):.2f} GB\")\n",
    "        \n",
    "        print(f\"\\nğŸ“‹ Available metadata columns:\")\n",
    "        for col in row_meta.columns[:15]:\n",
    "            sample_val = row_meta[col].iloc[0] if len(row_meta) > 0 else 'N/A'\n",
    "            print(f\"   - {col}: {sample_val}\")\n",
    "        if len(row_meta.columns) > 15:\n",
    "            print(f\"   ... and {len(row_meta.columns) - 15} more\")\n",
    "        \n",
    "        # æ£€æŸ¥å¿…éœ€å­—æ®µ\n",
    "        required_fields = ['pert_id']\n",
    "        missing_fields = [f for f in required_fields if f not in row_meta.columns]\n",
    "        if missing_fields:\n",
    "            raise ValueError(f\"Missing required fields: {missing_fields}\")\n",
    "        \n",
    "        # ç¡®å®šç»†èƒç³»IDå­—æ®µ\n",
    "        cell_id_col = None\n",
    "        for possible_col in ['cell_id', 'cell_iname', 'cell_mfc_name']:\n",
    "            if possible_col in row_meta.columns:\n",
    "                cell_id_col = possible_col\n",
    "                print(f\"\\nâœ“ Using '{cell_id_col}' as cell line identifier\")\n",
    "                break\n",
    "        \n",
    "        if cell_id_col is None:\n",
    "            print(f\"\\nâš ï¸  Warning: Cannot find cell line identifier\")\n",
    "        \n",
    "        valid_mask = np.ones(len(row_meta), dtype=bool)\n",
    "        \n",
    "        initial_compounds = row_meta['pert_id'].nunique()\n",
    "        print(f\"\\nInitial compounds: {initial_compounds:,}\")\n",
    "        \n",
    "        # ========== Filter 1: Remove DOS compounds (ä¼˜åŒ–) ==========\n",
    "        if remove_dos:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"FILTER 1: Remove DOS compounds (keep trt_cp only)\")\n",
    "            print(f\"{'='*80}\")\n",
    "            \n",
    "            # ä½¿ç”¨pert_typeå­—æ®µï¼Œåªä¿ç•™trt_cp\n",
    "            if 'pert_type' in row_meta.columns:\n",
    "                # æ£€æŸ¥pert_typeçš„å€¼åˆ†å¸ƒ\n",
    "                print(f\"   pert_type value counts:\")\n",
    "                pert_type_counts = row_meta['pert_type'].value_counts()\n",
    "                for ptype, count in pert_type_counts.items():\n",
    "                    print(f\"     - {ptype}: {count:,} samples\")\n",
    "                \n",
    "                # åªä¿ç•™trt_cpç±»å‹\n",
    "                dos_mask = row_meta['pert_type'] == 'trt_cp'\n",
    "                n_removed = (~dos_mask).sum()\n",
    "                \n",
    "                print(f\"\\n  âœ“ Keeping only 'trt_cp' perturbations\")\n",
    "                print(f\"  Removed {n_removed:,} non-trt_cp observations\")\n",
    "            else:\n",
    "                # é™çº§æ–¹æ¡ˆï¼šå¦‚æœæ²¡æœ‰pert_typeï¼Œä½¿ç”¨æ›´ä¿å®ˆçš„BRD-Kè§„åˆ™\n",
    "                print(f\"   âš ï¸ 'pert_type' not found, using fallback method\")\n",
    "                # åªè¿‡æ»¤æ˜ç¡®æ ‡è®°ä¸ºDOSçš„\n",
    "                dos_mask = ~row_meta['pert_id'].str.contains('DOS', case=False, na=False)\n",
    "                n_removed = (~dos_mask).sum()\n",
    "                print(f\"  Removed {n_removed:,} DOS observations\")\n",
    "            \n",
    "            valid_mask &= dos_mask\n",
    "            print(f\"  Remaining samples: {valid_mask.sum():,}\")\n",
    "            print(f\"  Remaining compounds: {row_meta.loc[valid_mask, 'pert_id'].nunique():,}\")\n",
    "        \n",
    "        # ========== Filter 2: Minimum observations ==========\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FILTER 2: Remove compounds with <{min_observations_per_compound} observations\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        valid_meta = row_meta[valid_mask]\n",
    "        obs_counts = valid_meta.groupby('pert_id').size()\n",
    "        valid_perts = obs_counts[obs_counts >= min_observations_per_compound].index\n",
    "        \n",
    "        print(f\"  Compounds with â‰¥{min_observations_per_compound} observations: \"\n",
    "              f\"{len(valid_perts):,}/{obs_counts.nunique():,}\")\n",
    "        \n",
    "        obs_mask = row_meta['pert_id'].isin(valid_perts)\n",
    "        valid_mask &= obs_mask\n",
    "        \n",
    "        print(f\"  Remaining samples: {valid_mask.sum():,}\")\n",
    "        print(f\"  Remaining compounds: {row_meta.loc[valid_mask, 'pert_id'].nunique():,}\")\n",
    "        \n",
    "        del valid_meta, obs_counts, obs_mask\n",
    "        gc.collect()\n",
    "            \n",
    "        # ========== Filter 3: Cosine similarity ==========\n",
    "\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FILTER 3: Remove observations with cosine similarity <{min_replicate_similarity}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        valid_indices = np.where(valid_mask)[0]\n",
    "        valid_matrix = matrix[valid_mask]\n",
    "        valid_pert_ids = row_meta.loc[valid_mask, 'pert_id'].reset_index(drop=True)\n",
    "        \n",
    "        \n",
    "        nearest_similarities = self.calculate_cosine_similarity_to_nearest_replicate(\n",
    "            valid_matrix, \n",
    "            valid_pert_ids\n",
    "        )\n",
    "        \n",
    "        full_similarities = np.zeros(len(row_meta), dtype=np.float32)\n",
    "        full_similarities[valid_indices] = nearest_similarities\n",
    "        \n",
    "        sim_mask = (full_similarities >= min_replicate_similarity) | (~valid_mask)\n",
    "        n_removed_sim = (~sim_mask & valid_mask).sum()\n",
    "        valid_mask &= sim_mask\n",
    "        \n",
    "        print(f\"  Removed {n_removed_sim:,} low-similarity observations\")\n",
    "        print(f\"  Remaining samples: {valid_mask.sum():,}\")\n",
    "        \n",
    "        del valid_matrix, valid_pert_ids, nearest_similarities, full_similarities, sim_mask\n",
    "        gc.collect()\n",
    "        \n",
    "        # ========== Filter 4: Dose selection (ä¿®æ­£ç‰ˆ + è¯¦ç»†ç»Ÿè®¡) ==========\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FILTER 4: Select most frequent dose in range {dose_range[0]}-{dose_range[1]} ÂµM\")\n",
    "        print(f\"         (Improved: Using intelligent dose binning)\")\n",
    "        print(f\"{'='*80}\")\n",
    "\n",
    "        if 'pert_dose' in row_meta.columns:\n",
    "            \n",
    "            # ===== Step 1: è§£æå‰‚é‡å€¼ =====\n",
    "            print(f\"  Step 1/6: Parsing dose values...\")\n",
    "            dose_str = row_meta['pert_dose'].astype(str)\n",
    "            row_meta['dose_value'] = pd.to_numeric(\n",
    "                dose_str.str.replace(r'[^\\d.]', '', regex=True), \n",
    "                errors='coerce'\n",
    "            )\n",
    "            row_meta['dose_unit'] = dose_str.str.extract(r'([a-zA-Z]+)', expand=False).str.lower()\n",
    "            \n",
    "            valid_dose_count = row_meta['dose_value'].notna().sum()\n",
    "            invalid_dose_count = row_meta['dose_value'].isna().sum()\n",
    "            print(f\"  âœ“ Successfully parsed: {valid_dose_count:,} entries\")\n",
    "            print(f\"  âš ï¸  Missing/invalid doses: {invalid_dose_count:,} entries\")\n",
    "            \n",
    "            # ===== Step 2: è½¬æ¢å•ä½åˆ° ÂµM =====\n",
    "            print(f\"\\n  Step 2/6: Converting to ÂµM...\")\n",
    "            dose_value = row_meta['dose_value'].values\n",
    "            dose_unit = row_meta['dose_unit'].values\n",
    "            \n",
    "            conditions = [\n",
    "                dose_unit == 'nm',\n",
    "                dose_unit == 'mm'\n",
    "            ]\n",
    "            choices = [\n",
    "                dose_value / 1000,\n",
    "                dose_value * 1000\n",
    "            ]\n",
    "            row_meta['dose_uM'] = np.select(conditions, choices, default=dose_value)\n",
    "            \n",
    "            print(f\"  Dose unit distribution:\")\n",
    "            unit_counts = row_meta['dose_unit'].value_counts()\n",
    "            for unit, count in unit_counts.head(5).items():\n",
    "                print(f\"    - {unit}: {count:,} samples\")\n",
    "            \n",
    "            # ===== Step 3: æ™ºèƒ½å‰‚é‡åˆ†ç»„ (æ ¸å¿ƒæ”¹è¿›) =====\n",
    "            print(f\"\\n  Step 3/6: Applying intelligent dose binning...\")\n",
    "            print(f\"            (Grouping similar doses like 4.99, 5.00, 5.01 â†’ 5.0)\")\n",
    "            \n",
    "            # ä½¿ç”¨æ··åˆæ–¹æ³•è¿›è¡Œå‰‚é‡æ ‡å‡†åŒ–\n",
    "            row_meta['dose_standardized'] = assign_standard_dose(\n",
    "                row_meta['dose_uM'].values,\n",
    "                method='hybrid',\n",
    "                tolerance_pct=0.1,  # 10% å®¹å·®\n",
    "                round_precision=0.5\n",
    "            )\n",
    "            \n",
    "            # ç»Ÿè®¡åˆ†ç»„æ•ˆæœ\n",
    "            original_unique_doses = row_meta.loc[valid_mask, 'dose_uM'].nunique()\n",
    "            standardized_unique_doses = row_meta.loc[valid_mask, 'dose_standardized'].nunique()\n",
    "            print(f\"  âœ“ Original unique doses: {original_unique_doses:,}\")\n",
    "            print(f\"  âœ“ Standardized unique doses: {standardized_unique_doses:,}\")\n",
    "            print(f\"  âœ“ Compression ratio: {original_unique_doses/standardized_unique_doses:.1f}x\")\n",
    "            \n",
    "            # å±•ç¤ºåˆ†ç»„ç¤ºä¾‹\n",
    "            print(f\"\\n  ğŸ“‹ Dose binning examples:\")\n",
    "            sample_doses = [4.99, 5.00, 5.01, 5.02, 10.0, 9.98, 10.02, 3.33, 3.3333, 1.11, 1.1111]\n",
    "            for d in sample_doses:\n",
    "                std_d = assign_standard_dose([d], method='hybrid')[0]\n",
    "                print(f\"    {d:.4f} ÂµM â†’ {std_d:.4f} ÂµM\")\n",
    "            \n",
    "            # ===== Step 4: å‰‚é‡åˆ†å¸ƒç»Ÿè®¡ =====\n",
    "            print(f\"\\n  Step 4/6: Dose distribution analysis...\")\n",
    "            \n",
    "            valid_doses = row_meta.loc[valid_mask & (~pd.isna(row_meta['dose_uM'])), 'dose_uM']\n",
    "            valid_doses_std = row_meta.loc[valid_mask & (~pd.isna(row_meta['dose_standardized'])), 'dose_standardized']\n",
    "            \n",
    "            if len(valid_doses) > 0:\n",
    "                print(f\"\\n  ğŸ“Š Dose statistics (before filtering):\")\n",
    "                print(f\"    Total samples with valid dose: {len(valid_doses):,}\")\n",
    "                print(f\"    Original dose range: {valid_doses.min():.4f} - {valid_doses.max():.2f} ÂµM\")\n",
    "                print(f\"    Mean dose: {valid_doses.mean():.4f} ÂµM\")\n",
    "                print(f\"    Median dose: {valid_doses.median():.4f} ÂµM\")\n",
    "                \n",
    "                # å‰‚é‡åˆ†å¸ƒï¼ˆæŒ‰åŒºé—´ï¼‰\n",
    "                dose_bins = [0, 0.1, 0.5, 1, 2, 5, 10, 20, 50, 100, 500, np.inf]\n",
    "                dose_labels = ['<0.1', '0.1-0.5', '0.5-1', '1-2', '2-5', '5-10', \n",
    "                            '10-20', '20-50', '50-100', '100-500', '>500']\n",
    "                dose_binned = pd.cut(valid_doses, bins=dose_bins, labels=dose_labels, include_lowest=True)\n",
    "                dose_dist = dose_binned.value_counts().sort_index()\n",
    "                \n",
    "                print(f\"\\n  Dose distribution by range:\")\n",
    "                for dose_range_label, count in dose_dist.items():\n",
    "                    pct = count / len(valid_doses) * 100\n",
    "                    print(f\"    {dose_range_label:>10} ÂµM: {count:>8,} ({pct:>5.1f}%)\")\n",
    "                \n",
    "                # ç»Ÿè®¡æœ€å¸¸è§çš„æ ‡å‡†åŒ–å‰‚é‡å€¼\n",
    "                print(f\"\\n  Top 20 most frequent STANDARDIZED doses:\")\n",
    "                top_doses_std = valid_doses_std.value_counts().head(20)\n",
    "                for dose_val, count in top_doses_std.items():\n",
    "                    pct = count / len(valid_doses_std) * 100\n",
    "                    print(f\"    {dose_val:>8.4f} ÂµM: {count:>8,} samples ({pct:>5.2f}%)\")\n",
    "            \n",
    "            # ===== Step 5: åº”ç”¨å‰‚é‡èŒƒå›´è¿‡æ»¤ =====\n",
    "            print(f\"\\n  Step 5/6: Filtering dose range {dose_range[0]}-{dose_range[1]} ÂµM...\")\n",
    "            dose_range_mask = (\n",
    "                (row_meta['dose_uM'] >= dose_range[0]) & \n",
    "                (row_meta['dose_uM'] <= dose_range[1]) &\n",
    "                (~pd.isna(row_meta['dose_uM']))\n",
    "            )\n",
    "            \n",
    "            n_in_range = (valid_mask & dose_range_mask).sum()\n",
    "            n_out_range = (valid_mask & ~dose_range_mask).sum()\n",
    "            print(f\"  âœ“ Samples in target range: {n_in_range:,}\")\n",
    "            print(f\"  âœ“ Samples outside range: {n_out_range:,}\")\n",
    "            \n",
    "            n_before_dose = valid_mask.sum()\n",
    "            valid_mask &= dose_range_mask\n",
    "            n_removed_range = n_before_dose - valid_mask.sum()\n",
    "            print(f\"  âœ“ Removed {n_removed_range:,} samples outside dose range\")\n",
    "            \n",
    "            # ===== Step 6: åŸºäºæ ‡å‡†åŒ–å‰‚é‡æ‰¾æœ€å¸¸è§å‰‚é‡ =====\n",
    "            print(f\"\\n  Step 6/6: Finding most frequent STANDARDIZED dose per compound...\")\n",
    "            valid_meta = row_meta[valid_mask].copy()\n",
    "            unique_compounds = valid_meta['pert_id'].nunique()\n",
    "            print(f\"  Processing {unique_compounds:,} compounds...\")\n",
    "            \n",
    "            # ä½¿ç”¨æ ‡å‡†åŒ–åçš„å‰‚é‡æ¥è®¡ç®—æœ€å¸¸è§å‰‚é‡\n",
    "            tqdm.pandas(desc=\"    Finding modal doses (standardized)\")\n",
    "            most_common_doses_std = (\n",
    "                valid_meta.groupby('pert_id')['dose_standardized']\n",
    "                .progress_apply(lambda x: x.mode()[0] if len(x.mode()) > 0 else np.nan)\n",
    "            )\n",
    "            \n",
    "            print(f\"  âœ“ Identified modal doses for {len(most_common_doses_std):,} compounds\")\n",
    "            \n",
    "            # ç»Ÿè®¡ modal dose åˆ†å¸ƒ\n",
    "            print(f\"\\n  ğŸ“Š Modal dose distribution (STANDARDIZED):\")\n",
    "            modal_dose_counts_std = most_common_doses_std.value_counts().head(15)\n",
    "            for dose_val, count in modal_dose_counts_std.items():\n",
    "                pct = count / len(most_common_doses_std) * 100\n",
    "                print(f\"    {dose_val:>8.4f} ÂµM: {count:>5,} compounds ({pct:>5.2f}%)\")\n",
    "            \n",
    "            # æ˜ å°„åˆ°æ‰€æœ‰è¡Œ\n",
    "            row_meta['most_common_dose_std'] = row_meta['pert_id'].map(most_common_doses_std)\n",
    "            \n",
    "            # ä½¿ç”¨æ ‡å‡†åŒ–å‰‚é‡è¿›è¡ŒåŒ¹é…\n",
    "            dose_modal_mask = (\n",
    "                (row_meta['dose_standardized'] == row_meta['most_common_dose_std']) &\n",
    "                (~pd.isna(row_meta['most_common_dose_std']))\n",
    "            )\n",
    "            \n",
    "            n_before_modal = valid_mask.sum()\n",
    "            valid_mask &= dose_modal_mask\n",
    "            n_removed_modal = n_before_modal - valid_mask.sum()\n",
    "            \n",
    "            print(f\"\\n  âœ… Filter 4 Results (with intelligent dose binning):\")\n",
    "            print(f\"  âœ“ Removed {n_removed_modal:,} samples with non-modal dose\")\n",
    "            print(f\"  âœ“ Remaining samples: {valid_mask.sum():,}\")\n",
    "            print(f\"  âœ“ Remaining compounds: {row_meta.loc[valid_mask, 'pert_id'].nunique():,}\")\n",
    "            \n",
    "            # ========== å¯è§†åŒ–éƒ¨åˆ†ï¼ˆä¼˜åŒ–ç‰ˆï¼‰ ==========\n",
    "            if False:\n",
    "                print(f\"\\n  ğŸ“Š Generating dose distribution visualizations...\")\n",
    "                \n",
    "                viz_dir = Path(\"E:/ç§‘ç ”/Models/drugreflector/visualizations\")\n",
    "                viz_dir.mkdir(exist_ok=True, parents=True)\n",
    "                \n",
    "                # ===== å›¾1: ç»¼åˆå‰‚é‡åˆ†æ (2x3å¸ƒå±€) =====\n",
    "                fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "                \n",
    "                # å­å›¾1: åŸå§‹å‰‚é‡åˆ†å¸ƒ (å¯¹æ•°åˆ»åº¦)\n",
    "                ax1 = axes[0, 0]\n",
    "                valid_doses_before = row_meta.loc[\n",
    "                    row_meta.index[valid_indices] if 'valid_indices' in locals() else slice(None), \n",
    "                    'dose_uM'\n",
    "                ].dropna()\n",
    "                \n",
    "                if len(valid_doses_before) > 0:\n",
    "                    log_bins = np.logspace(np.log10(0.001), np.log10(1000), 50)\n",
    "                    ax1.hist(valid_doses_before, bins=log_bins, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "                    ax1.axvline(dose_range[0], color='red', linestyle='--', linewidth=2, \n",
    "                            label=f'Range: {dose_range[0]}-{dose_range[1]} ÂµM')\n",
    "                    ax1.axvline(dose_range[1], color='red', linestyle='--', linewidth=2)\n",
    "                    ax1.set_xscale('log')\n",
    "                    ax1.set_xlabel('Dose (ÂµM, log scale)', fontsize=11)\n",
    "                    ax1.set_ylabel('Number of samples', fontsize=11)\n",
    "                    ax1.set_title('Original Dose Distribution\\n(Before Filtering)', fontsize=12, fontweight='bold')\n",
    "                    ax1.legend()\n",
    "                    ax1.grid(True, alpha=0.3)\n",
    "                \n",
    "                # å­å›¾2: æ ‡å‡†åŒ–åçš„å‰‚é‡åˆ†å¸ƒ\n",
    "                ax2 = axes[0, 1]\n",
    "                valid_doses_std_before = row_meta.loc[\n",
    "                    row_meta.index[valid_indices] if 'valid_indices' in locals() else slice(None), \n",
    "                    'dose_standardized'\n",
    "                ].dropna()\n",
    "                \n",
    "                if len(valid_doses_std_before) > 0:\n",
    "                    log_bins = np.logspace(np.log10(0.001), np.log10(1000), 50)\n",
    "                    ax2.hist(valid_doses_std_before, bins=log_bins, edgecolor='black', alpha=0.7, color='darkorange')\n",
    "                    ax2.axvline(dose_range[0], color='red', linestyle='--', linewidth=2)\n",
    "                    ax2.axvline(dose_range[1], color='red', linestyle='--', linewidth=2)\n",
    "                    ax2.set_xscale('log')\n",
    "                    ax2.set_xlabel('Standardized Dose (ÂµM, log scale)', fontsize=11)\n",
    "                    ax2.set_ylabel('Number of samples', fontsize=11)\n",
    "                    ax2.set_title('Standardized Dose Distribution\\n(After Binning)', fontsize=12, fontweight='bold')\n",
    "                    ax2.grid(True, alpha=0.3)\n",
    "                \n",
    "                # å­å›¾3: è¿‡æ»¤åçš„æœ€ç»ˆåˆ†å¸ƒ\n",
    "                ax3 = axes[0, 2]\n",
    "                final_doses = row_meta.loc[valid_mask, 'dose_uM'].dropna()\n",
    "                \n",
    "                if len(final_doses) > 0:\n",
    "                    log_bins = np.logspace(np.log10(dose_range[0]), np.log10(dose_range[1]), 30)\n",
    "                    ax3.hist(final_doses, bins=log_bins, edgecolor='black', alpha=0.7, color='forestgreen')\n",
    "                    ax3.set_xscale('log')\n",
    "                    ax3.set_xlabel('Dose (ÂµM, log scale)', fontsize=11)\n",
    "                    ax3.set_ylabel('Number of samples', fontsize=11)\n",
    "                    ax3.set_title('Final Dose Distribution\\n(After All Filtering)', fontsize=12, fontweight='bold')\n",
    "                    ax3.grid(True, alpha=0.3)\n",
    "                \n",
    "                # å­å›¾4: åŸå§‹ vs æ ‡å‡†åŒ–å‰‚é‡å¯¹æ¯” (Top 15)\n",
    "                ax4 = axes[1, 0]\n",
    "                \n",
    "                # åŸå§‹å‰‚é‡ top 15\n",
    "                top_original = valid_doses.value_counts().head(15)\n",
    "                top_std = valid_doses_std.value_counts().head(15)\n",
    "                \n",
    "                x = np.arange(min(15, len(top_original)))\n",
    "                width = 0.35\n",
    "                \n",
    "                bars1 = ax4.bar(x - width/2, top_original.values[:len(x)], width, \n",
    "                            label='Original doses', alpha=0.8, color='steelblue', edgecolor='black')\n",
    "                bars2 = ax4.bar(x + width/2, top_std.values[:len(x)], width, \n",
    "                            label='Standardized doses', alpha=0.8, color='darkorange', edgecolor='black')\n",
    "                \n",
    "                ax4.set_ylabel('Sample count', fontsize=11)\n",
    "                ax4.set_title('Top 15 Doses: Original vs Standardized', fontsize=12, fontweight='bold')\n",
    "                ax4.set_xticks(x)\n",
    "                \n",
    "                # åˆ›å»ºæ··åˆæ ‡ç­¾\n",
    "                labels = []\n",
    "                for i in range(len(x)):\n",
    "                    orig_label = f'{top_original.index[i]:.2f}' if i < len(top_original) else ''\n",
    "                    std_label = f'{top_std.index[i]:.2f}' if i < len(top_std) else ''\n",
    "                    labels.append(f'{orig_label}\\n({std_label})')\n",
    "                ax4.set_xticklabels(labels, fontsize=8, rotation=45, ha='right')\n",
    "                ax4.legend()\n",
    "                ax4.grid(axis='y', alpha=0.3)\n",
    "                \n",
    "                # å­å›¾5: Modal dose åˆ†å¸ƒ (æ ‡å‡†åŒ–å)\n",
    "                ax5 = axes[1, 1]\n",
    "                modal_doses_final = most_common_doses_std.value_counts().head(20)\n",
    "                \n",
    "                if len(modal_doses_final) > 0:\n",
    "                    bars = ax5.barh(range(len(modal_doses_final)), modal_doses_final.values, \n",
    "                                color='mediumpurple', edgecolor='black')\n",
    "                    ax5.set_yticks(range(len(modal_doses_final)))\n",
    "                    ax5.set_yticklabels([f'{d:.2f} ÂµM' for d in modal_doses_final.index])\n",
    "                    ax5.set_xlabel('Number of compounds', fontsize=11)\n",
    "                    ax5.set_ylabel('Modal Dose (ÂµM)', fontsize=11)\n",
    "                    ax5.set_title('Top 20 Modal Doses\\n(Standardized, Per Compound)', fontsize=12, fontweight='bold')\n",
    "                    ax5.grid(axis='x', alpha=0.3)\n",
    "                    \n",
    "                    for i, (bar, val) in enumerate(zip(bars, modal_doses_final.values)):\n",
    "                        ax5.text(val, i, f' {val:,}', va='center', fontsize=8)\n",
    "                \n",
    "                # å­å›¾6: å‰‚é‡æ ‡å‡†åŒ–æ•ˆæœç»Ÿè®¡\n",
    "                ax6 = axes[1, 2]\n",
    "                \n",
    "                # ç»Ÿè®¡5 ÂµMé™„è¿‘çš„åˆ†ç»„æ•ˆæœ\n",
    "                dose_5um_range = (valid_doses >= 4.5) & (valid_doses <= 5.5)\n",
    "                doses_near_5 = valid_doses[dose_5um_range].value_counts().head(10)\n",
    "                doses_near_5_std = valid_doses_std[dose_5um_range].value_counts()\n",
    "                \n",
    "                # åˆ›å»ºåˆ†ç»„æ•ˆæœçš„æ¡å½¢å›¾\n",
    "                categories = ['4.5-5.5 ÂµM\\nåŸå§‹å”¯ä¸€å€¼', '4.5-5.5 ÂµM\\næ ‡å‡†åŒ–åå”¯ä¸€å€¼', \n",
    "                            'å…¨å±€\\nåŸå§‹å”¯ä¸€å€¼', 'å…¨å±€\\næ ‡å‡†åŒ–åå”¯ä¸€å€¼']\n",
    "                values = [len(doses_near_5), len(doses_near_5_std),\n",
    "                        original_unique_doses, standardized_unique_doses]\n",
    "                colors = ['steelblue', 'darkorange', 'steelblue', 'darkorange']\n",
    "                \n",
    "                bars = ax6.bar(categories, values, color=colors, edgecolor='black', alpha=0.8)\n",
    "                ax6.set_ylabel('Number of unique dose values', fontsize=11)\n",
    "                ax6.set_title('Dose Binning Effect\\n(Unique Doses Reduction)', fontsize=12, fontweight='bold')\n",
    "                ax6.grid(axis='y', alpha=0.3)\n",
    "                \n",
    "                for bar, val in zip(bars, values):\n",
    "                    ax6.text(bar.get_x() + bar.get_width()/2, val, f'{val:,}', \n",
    "                            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                output_path = viz_dir / \"dose_distribution_analysis_improved.png\"\n",
    "                plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "                print(f\"  âœ“ Saved figure: {output_path}\")\n",
    "                plt.close()\n",
    "                \n",
    "                # ===== å›¾2: å‰‚é‡èŒƒå›´å¯¹æ¯” =====\n",
    "                plt.figure(figsize=(14, 8))\n",
    "                \n",
    "                dose_bins_viz = [0, 0.1, 0.5, 1, 2, 5, 10, 20, 50, 100, 500, np.inf]\n",
    "                dose_labels_viz = ['<0.1', '0.1-0.5', '0.5-1', '1-2', '2-5', '5-10', \n",
    "                            '10-20', '20-50', '50-100', '100-500', '>500']\n",
    "                \n",
    "                # è¿‡æ»¤å‰ï¼ˆåŸå§‹ï¼‰\n",
    "                doses_before = row_meta.loc[\n",
    "                    row_meta.index[valid_indices] if 'valid_indices' in locals() else slice(None), \n",
    "                    'dose_uM'\n",
    "                ].dropna()\n",
    "                binned_before = pd.cut(doses_before, bins=dose_bins_viz, labels=dose_labels_viz, include_lowest=True)\n",
    "                dist_before = binned_before.value_counts().sort_index()\n",
    "                \n",
    "                # è¿‡æ»¤å\n",
    "                doses_after = row_meta.loc[valid_mask, 'dose_uM'].dropna()\n",
    "                binned_after = pd.cut(doses_after, bins=dose_bins_viz, labels=dose_labels_viz, include_lowest=True)\n",
    "                dist_after = binned_after.value_counts().sort_index()\n",
    "                \n",
    "                x = np.arange(len(dose_labels_viz))\n",
    "                width = 0.35\n",
    "                \n",
    "                plt.bar(x - width/2, dist_before.values, width, label='Before filtering', \n",
    "                        alpha=0.8, color='steelblue', edgecolor='black')\n",
    "                plt.bar(x + width/2, dist_after.values, width, label='After filtering (with dose binning)', \n",
    "                        alpha=0.8, color='forestgreen', edgecolor='black')\n",
    "                \n",
    "                plt.xlabel('Dose Range (ÂµM)', fontsize=13, fontweight='bold')\n",
    "                plt.ylabel('Number of Samples', fontsize=13, fontweight='bold')\n",
    "                plt.title('Sample Distribution by Dose Range\\n(Before vs After Filtering with Intelligent Dose Binning)', \n",
    "                        fontsize=14, fontweight='bold')\n",
    "                plt.xticks(x, dose_labels_viz, rotation=45, ha='right', fontsize=11)\n",
    "                plt.legend(fontsize=11)\n",
    "                plt.grid(axis='y', alpha=0.3)\n",
    "                \n",
    "                # æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "                for i, (v_before, v_after) in enumerate(zip(dist_before.values, dist_after.values)):\n",
    "                    if v_before > 0:\n",
    "                        plt.text(i - width/2, v_before, f'{v_before:,}', ha='center', va='bottom', fontsize=8, rotation=90)\n",
    "                    if v_after > 0:\n",
    "                        plt.text(i + width/2, v_after, f'{v_after:,}', ha='center', va='bottom', fontsize=8, rotation=90)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                output_path2 = viz_dir / \"dose_range_comparison_improved.png\"\n",
    "                plt.savefig(output_path2, dpi=300, bbox_inches='tight')\n",
    "                print(f\"  âœ“ Saved figure: {output_path2}\")\n",
    "                plt.close()\n",
    "                \n",
    "                # ===== å›¾3: å‰‚é‡åˆ†ç»„è¯¦ç»†åˆ†æ (å±•ç¤º5ÂµMé™„è¿‘çš„èšåˆæ•ˆæœ) =====\n",
    "                fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "                \n",
    "                # å­å›¾1: 5 ÂµM é™„è¿‘çš„åŸå§‹å‰‚é‡åˆ†å¸ƒ\n",
    "                ax1 = axes[0]\n",
    "                dose_5um_mask = (valid_doses >= 4.0) & (valid_doses <= 6.0)\n",
    "                doses_near_5_detail = valid_doses[dose_5um_mask].value_counts().head(20)\n",
    "                \n",
    "                if len(doses_near_5_detail) > 0:\n",
    "                    bars = ax1.barh(range(len(doses_near_5_detail)), doses_near_5_detail.values, \n",
    "                                color='steelblue', edgecolor='black', alpha=0.8)\n",
    "                    ax1.set_yticks(range(len(doses_near_5_detail)))\n",
    "                    ax1.set_yticklabels([f'{d:.4f}' for d in doses_near_5_detail.index])\n",
    "                    ax1.set_xlabel('Sample Count', fontsize=11)\n",
    "                    ax1.set_ylabel('Original Dose (ÂµM)', fontsize=11)\n",
    "                    ax1.set_title('Original Doses Near 5 ÂµM\\n(Before Standardization)', fontsize=12, fontweight='bold')\n",
    "                    ax1.grid(axis='x', alpha=0.3)\n",
    "                    \n",
    "                    for i, val in enumerate(doses_near_5_detail.values):\n",
    "                        ax1.text(val, i, f' {val:,}', va='center', fontsize=8)\n",
    "                \n",
    "                # å­å›¾2: æ ‡å‡†åŒ–åçš„å‰‚é‡åˆ†å¸ƒ\n",
    "                ax2 = axes[1]\n",
    "                dose_5um_mask_std = (valid_doses_std >= 4.0) & (valid_doses_std <= 6.0)\n",
    "                doses_near_5_std_detail = valid_doses_std[dose_5um_mask_std].value_counts().head(20)\n",
    "                \n",
    "                if len(doses_near_5_std_detail) > 0:\n",
    "                    bars = ax2.barh(range(len(doses_near_5_std_detail)), doses_near_5_std_detail.values, \n",
    "                                color='darkorange', edgecolor='black', alpha=0.8)\n",
    "                    ax2.set_yticks(range(len(doses_near_5_std_detail)))\n",
    "                    ax2.set_yticklabels([f'{d:.4f}' for d in doses_near_5_std_detail.index])\n",
    "                    ax2.set_xlabel('Sample Count', fontsize=11)\n",
    "                    ax2.set_ylabel('Standardized Dose (ÂµM)', fontsize=11)\n",
    "                    ax2.set_title('Standardized Doses Near 5 ÂµM\\n(After Intelligent Binning)', fontsize=12, fontweight='bold')\n",
    "                    ax2.grid(axis='x', alpha=0.3)\n",
    "                    \n",
    "                    for i, val in enumerate(doses_near_5_std_detail.values):\n",
    "                        ax2.text(val, i, f' {val:,}', va='center', fontsize=8)\n",
    "                \n",
    "                plt.suptitle('Dose Binning Effect: Grouping Similar Doses Together\\n'\n",
    "                            '(e.g., 4.99, 5.00, 5.01, 5.02 â†’ 5.0 ÂµM)', fontsize=14, fontweight='bold', y=1.02)\n",
    "                plt.tight_layout()\n",
    "                output_path3 = viz_dir / \"dose_binning_detail.png\"\n",
    "                plt.savefig(output_path3, dpi=300, bbox_inches='tight')\n",
    "                print(f\"  âœ“ Saved figure: {output_path3}\")\n",
    "                plt.close()\n",
    "                \n",
    "                print(f\"\\n  âœ… Visualization complete!\")\n",
    "                print(f\"     Figures saved to: {viz_dir}\")\n",
    "                print(f\"     - dose_distribution_analysis_improved.png\")\n",
    "                print(f\"     - dose_range_comparison_improved.png\")\n",
    "                print(f\"     - dose_binning_detail.png\")\n",
    "                \n",
    "                # æ¸…ç†ä¸´æ—¶åˆ—\n",
    "                cols_to_drop = ['dose_value', 'dose_unit', 'most_common_dose_std']\n",
    "                for col in cols_to_drop:\n",
    "                    if col in row_meta.columns:\n",
    "                        row_meta.drop(col, axis=1, inplace=True, errors='ignore')\n",
    "                \n",
    "                del valid_meta, dose_range_mask, dose_modal_mask, most_common_doses_std\n",
    "                gc.collect()\n",
    "\n",
    "            else:\n",
    "                print(f\"  âš ï¸  'pert_dose' not found, skipping dose filter\")\n",
    "            \n",
    "        \n",
    "        # ========== Filter 5: Timepoint selection (ä¼˜åŒ–) ==========\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FILTER 5: Keep only measurements at {valid_timepoints} hours\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        if 'pert_time' in row_meta.columns:\n",
    "            # è½¬æ¢ä¸ºæ•°å€¼ç±»å‹\n",
    "            row_meta['time_numeric'] = pd.to_numeric(row_meta['pert_time'], errors='coerce')\n",
    "            \n",
    "            # æ˜¾ç¤ºæ—¶é—´ç‚¹åˆ†å¸ƒ\n",
    "            print(f\"   Available timepoints (numeric):\")\n",
    "            time_counts = row_meta['time_numeric'].value_counts().head(10)\n",
    "            for time_val, count in time_counts.items():\n",
    "                print(f\"     - {time_val} hours: {count:,} samples\")\n",
    "            \n",
    "            # åŒ¹é…valid_timepointsä¸­çš„æ•°å€¼\n",
    "            time_mask = row_meta['time_numeric'].isin(valid_timepoints)\n",
    "            \n",
    "            if time_mask.sum() == 0:\n",
    "                print(f\"  âš ï¸  No samples match timepoints {valid_timepoints}\")\n",
    "                print(f\"  Skipping timepoint filter...\")\n",
    "            else:\n",
    "                n_before = valid_mask.sum()\n",
    "                valid_mask &= time_mask\n",
    "                n_removed = n_before - valid_mask.sum()\n",
    "                \n",
    "                print(f\"  âœ“ Kept samples at {valid_timepoints} hours\")\n",
    "                print(f\"  Removed {n_removed:,} observations (invalid timepoint)\")\n",
    "                print(f\"  Remaining samples: {valid_mask.sum():,}\")\n",
    "            \n",
    "            del time_mask\n",
    "            gc.collect()\n",
    "        else:\n",
    "            print(f\"  âš ï¸  'pert_time' not found, skipping timepoint filter\")\n",
    "\n",
    "\n",
    "        # ========== Filter 6: Remove cell lines with <200 compounds ==========\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FILTER 6: Remove cell lines with <{min_compounds_per_cell} compounds\")\n",
    "        print(f\"{'='*80}\")\n",
    "\n",
    "        if cell_id_col is not None:\n",
    "            valid_meta = row_meta[valid_mask]\n",
    "            \n",
    "            # ç»Ÿè®¡æ¯ä¸ªç»†èƒç³»çš„åŒ–åˆç‰©æ•°é‡\n",
    "            compounds_per_cell = valid_meta.groupby(cell_id_col)['pert_id'].nunique()\n",
    "            \n",
    "            print(f\"  Total cell lines before filtering: {len(compounds_per_cell):,}\")\n",
    "            print(f\"  Compounds per cell line (mean): {compounds_per_cell.mean():.1f}\")\n",
    "            print(f\"  Compounds per cell line (median): {compounds_per_cell.median():.0f}\")\n",
    "            print(f\"  Compounds per cell line (min): {compounds_per_cell.min():.0f}\")\n",
    "            print(f\"  Compounds per cell line (max): {compounds_per_cell.max():.0f}\")\n",
    "            \n",
    "            # ç­›é€‰å‡ºåŒ–åˆç‰©æ•°â‰¥200çš„ç»†èƒç³»\n",
    "            valid_cells = compounds_per_cell[compounds_per_cell >= min_compounds_per_cell].index\n",
    "            \n",
    "            print(f\"\\n  Cell lines with â‰¥{min_compounds_per_cell} compounds: \"\n",
    "                f\"{len(valid_cells):,}/{len(compounds_per_cell):,}\")\n",
    "            \n",
    "            # æ˜¾ç¤ºè¢«è¿‡æ»¤æ‰çš„ç»†èƒç³»ç¤ºä¾‹\n",
    "            removed_cells = compounds_per_cell[compounds_per_cell < min_compounds_per_cell]\n",
    "            if len(removed_cells) > 0:\n",
    "                print(f\"\\n  Sample removed cell lines (showing up to 10):\")\n",
    "                for cell, n_compounds in removed_cells.head(10).items():\n",
    "                    print(f\"    - {cell}: {n_compounds} compounds\")\n",
    "            \n",
    "            # åº”ç”¨è¿‡æ»¤\n",
    "            cell_filter_mask = row_meta[cell_id_col].isin(valid_cells)\n",
    "            n_before = valid_mask.sum()\n",
    "            valid_mask &= cell_filter_mask\n",
    "            n_removed = n_before - valid_mask.sum()\n",
    "            \n",
    "            print(f\"\\n  Removed {n_removed:,} observations from {len(removed_cells):,} cell lines\")\n",
    "            print(f\"  Remaining samples: {valid_mask.sum():,}\")\n",
    "            print(f\"  Remaining compounds: {row_meta.loc[valid_mask, 'pert_id'].nunique():,}\")\n",
    "            print(f\"  Remaining cell lines: {len(valid_cells):,}\")\n",
    "            \n",
    "            del valid_meta, compounds_per_cell, cell_filter_mask, removed_cells\n",
    "            gc.collect()\n",
    "        else:\n",
    "            print(f\"  âš ï¸  Cell line column not found, skipping filter\")\n",
    "        \n",
    "        # ========== Filter 7: Cell line count ==========\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FILTER 7: Remove compounds in <{min_cell_lines} or >{max_cell_lines} cell lines\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        if cell_id_col is not None:\n",
    "            valid_meta = row_meta[valid_mask]\n",
    "            cell_line_counts = valid_meta.groupby('pert_id')[cell_id_col].nunique()\n",
    "            valid_perts_cell = cell_line_counts[\n",
    "                (cell_line_counts >= min_cell_lines) & \n",
    "                (cell_line_counts <= max_cell_lines)\n",
    "            ].index\n",
    "            \n",
    "            print(f\"  Compounds in {min_cell_lines}-{max_cell_lines} cell lines: \"\n",
    "                  f\"{len(valid_perts_cell):,}/{len(cell_line_counts):,}\")\n",
    "            \n",
    "            cell_mask = row_meta['pert_id'].isin(valid_perts_cell)\n",
    "            n_before = valid_mask.sum()\n",
    "            valid_mask &= cell_mask\n",
    "            n_removed = n_before - valid_mask.sum()\n",
    "            \n",
    "            print(f\"  Removed {n_removed:,} observations\")\n",
    "            print(f\"  Remaining samples: {valid_mask.sum():,}\")\n",
    "            print(f\"  Remaining compounds: {row_meta.loc[valid_mask, 'pert_id'].nunique():,}\")\n",
    "            \n",
    "            del valid_meta, cell_line_counts, cell_mask\n",
    "            gc.collect()\n",
    "        else:\n",
    "            print(f\"  âš ï¸  Cell line column not found, skipping\")\n",
    "        \n",
    "        # ========== æœ€ç»ˆæå–æ•°æ® ==========\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"âœ… FINAL DATASET\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        final_matrix = matrix[valid_mask].copy()\n",
    "        final_meta = row_meta[valid_mask].reset_index(drop=True)\n",
    "        \n",
    "        del matrix\n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"  Extracted {len(final_matrix):,} samples\")\n",
    "        print(f\"  Memory usage: {final_matrix.nbytes / (1024**3):.2f} GB\")\n",
    "        \n",
    "        # åˆ›å»ºæ ‡ç­¾\n",
    "        unique_perts = sorted(final_meta['pert_id'].unique())\n",
    "        pert_to_idx = {pert: idx for idx, pert in enumerate(unique_perts)}\n",
    "        labels = np.array([pert_to_idx[p] for p in final_meta['pert_id']], dtype=np.int32)\n",
    "        \n",
    "        final_compounds = len(unique_perts)\n",
    "        final_samples = len(final_matrix)\n",
    "        \n",
    "        if cell_id_col:\n",
    "            final_cells = final_meta[cell_id_col].nunique()\n",
    "        else:\n",
    "            final_cells = 'Unknown'\n",
    "        \n",
    "        print(f\"\\n  Total samples: {final_samples:,}\")\n",
    "        print(f\"  Total compounds: {final_compounds:,}\")\n",
    "        print(f\"  Cell lines: {final_cells}\")\n",
    "        print(f\"  Gene features: {final_matrix.shape[1]}\")\n",
    "        \n",
    "        compound_obs = final_meta.groupby('pert_id').size()\n",
    "        print(f\"  Samples per compound (mean): {final_samples / final_compounds:.1f}\")\n",
    "        print(f\"  Samples per compound (median): {compound_obs.median():.0f}\")\n",
    "        \n",
    "        n_compounds_100plus = (compound_obs > 100).sum()\n",
    "        print(f\"  Compounds with >100 observations: {n_compounds_100plus:,}\")\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Comparison with paper (SI page 2):\")\n",
    "        print(f\"  Paper: 425,242 obs, 9,597 compounds, 52 cell lines\")\n",
    "        print(f\"  Ours:  {final_samples:,} obs, {final_compounds:,} compounds, {final_cells} cell lines\")\n",
    "        \n",
    "        if initial_compounds > 0:\n",
    "            retention_rate = (final_compounds / initial_compounds) * 100\n",
    "            print(f\"  Compound retention rate: {retention_rate:.1f}%\")\n",
    "        \n",
    "        # åŸºå› å\n",
    "        if (self.gene_info is not None) and hasattr(self, \"landmark_col_indices\"):\n",
    "            gi = self.gene_info\n",
    "            landmark_mask = gi['feature_space'] == 'landmark'\n",
    "            landmark_geneinfo = gi[landmark_mask]\n",
    "            gene_names = list(landmark_geneinfo['gene_symbol'].values)\n",
    "            print(f\"  âœ“ Using geneinfo_beta.txt for gene names\")\n",
    "        else:\n",
    "            gene_name_col = None\n",
    "            for possible_col in ['gene_symbol', 'pr_gene_symbol', 'symbol', 'id']:\n",
    "                if possible_col in col_meta.columns:\n",
    "                    gene_name_col = possible_col\n",
    "                    break\n",
    "            if gene_name_col is None:\n",
    "                gene_name_col = col_meta.columns[0]\n",
    "            gene_names = list(col_meta[gene_name_col].values)\n",
    "            print(f\"  âš ï¸ Using '{gene_name_col}' from col_meta for gene names\")\n",
    "\n",
    "        training_data = {\n",
    "            'X': final_matrix,\n",
    "            'y': labels,\n",
    "            'folds': np.zeros(len(final_matrix), dtype=np.int32),\n",
    "            'sample_meta': final_meta,\n",
    "            'metadata': final_meta,\n",
    "            'gene_names': gene_names,\n",
    "            'compound_names': list(unique_perts),\n",
    "            'pert_to_idx': pert_to_idx\n",
    "        }\n",
    "\n",
    "        gc.collect()\n",
    "        \n",
    "        return training_data\n",
    "    \n",
    "    def create_3fold_splits(self, training_data, random_state=42):\n",
    "        \"\"\"åˆ›å»º3æŠ˜äº¤å‰éªŒè¯åˆ’åˆ†\"\"\"\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        sample_meta = training_data['sample_meta']\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ğŸ² Creating 3-fold cross-validation splits\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        folds = np.zeros(len(sample_meta), dtype=np.int32)\n",
    "        \n",
    "        for pert_id in sample_meta['pert_id'].unique():\n",
    "            pert_mask = sample_meta['pert_id'] == pert_id\n",
    "            pert_indices = np.where(pert_mask)[0]\n",
    "            \n",
    "            np.random.shuffle(pert_indices)\n",
    "            n_samples = len(pert_indices)\n",
    "            \n",
    "            fold_sizes = [n_samples // 3] * 3\n",
    "            for i in range(n_samples % 3):\n",
    "                fold_sizes[i] += 1\n",
    "            \n",
    "            start_idx = 0\n",
    "            for fold_id, size in enumerate(fold_sizes):\n",
    "                end_idx = start_idx + size\n",
    "                folds[pert_indices[start_idx:end_idx]] = fold_id\n",
    "                start_idx = end_idx\n",
    "        \n",
    "        training_data['folds'] = folds\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Fold statistics:\")\n",
    "        for fold_id in range(3):\n",
    "            fold_mask = folds == fold_id\n",
    "            n_samples = fold_mask.sum()\n",
    "            n_compounds = sample_meta[fold_mask]['pert_id'].nunique()\n",
    "            print(f\"   Fold {fold_id}: {n_samples:,} samples, {n_compounds:,} compounds\")\n",
    "        \n",
    "        return training_data\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"ä¸»ç¨‹åº - ä¼˜åŒ–ç‰ˆ\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ğŸ§¬ DRUGREFLECTOR DATA PREPROCESSING - Optimized Version\")\n",
    "    print(\"   Fixed DOS filtering and timepoint matching\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    data_dir = \"E:/ç§‘ç ”/Models/drugreflector/datasets/LINCS2020\"\n",
    "    loader = LINCS2020DataLoader(data_dir)\n",
    "    \n",
    "    try:\n",
    "        # Step 1: åŠ è½½å…ƒæ•°æ®\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"STEP 1: Loading metadata\")\n",
    "        print(\"=\" * 80)\n",
    "        gene_info = loader.load_gene_info()\n",
    "        cell_info = loader.load_cell_info()\n",
    "        compound_info = loader.load_compound_info()\n",
    "        \n",
    "        # Step 2: åŠ è½½Level 4 signatures\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"STEP 2: Loading Level 4 signatures\")\n",
    "        print(\"=\" * 80)\n",
    "        matrix, row_meta, col_meta = loader.load_level4_signatures()\n",
    "        \n",
    "        # Step 3: å‡†å¤‡è®­ç»ƒæ•°æ®\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"STEP 3: Preparing training data\")\n",
    "        print(\"=\" * 80)\n",
    "        training_data = loader.prepare_training_data(\n",
    "            min_observations_per_compound=5,\n",
    "            min_replicate_similarity=0.12,\n",
    "            dose_range=(1, 20.0),\n",
    "            valid_timepoints=[6, 24], \n",
    "            min_cell_lines=5,\n",
    "            max_cell_lines=40,\n",
    "            min_compounds_per_cell=200,\n",
    "            remove_dos=True\n",
    "        )\n",
    "        \n",
    "        # Step 4: åˆ›å»º3æŠ˜åˆ’åˆ†\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"STEP 4: Creating 3-fold splits\")\n",
    "        print(\"=\" * 80)\n",
    "        training_data = loader.create_3fold_splits(training_data)\n",
    "        \n",
    "        # Step 5: ä¿å­˜\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"STEP 5: Saving processed data\")\n",
    "        print(\"=\" * 80)\n",
    "        output_dir = Path(\"E:/ç§‘ç ”/Models/drugreflector/processed_data\")\n",
    "        output_dir.mkdir(exist_ok=True, parents=True)\n",
    "        \n",
    "        output_file = output_dir / \"training_data_lincs2020_optimized_1212_s.pkl\"\n",
    "        print(f\"ğŸ’¾ Saving to: {output_file}\")\n",
    "        \n",
    "        with open(output_file, 'wb') as f:\n",
    "            pickle.dump(training_data, f, protocol=4)\n",
    "        \n",
    "        print(f\"âœ“ Saved successfully!\")\n",
    "        print(f\"   File size: {output_file.stat().st_size / (1024**2):.1f} MB\")\n",
    "        \n",
    "        # æœ€ç»ˆæ‘˜è¦\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"âœ… DATA PREPARATION COMPLETE!\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"ğŸ“ Output: {output_file}\")\n",
    "        print(f\"\\nğŸ“Š Final dataset:\")\n",
    "        print(f\"   â€¢ Samples: {len(training_data['X']):,}\")\n",
    "        print(f\"   â€¢ Compounds: {len(training_data['compound_names']):,}\")\n",
    "        print(f\"   â€¢ Genes: {training_data['X'].shape[1]}\")\n",
    "        print(f\"   â€¢ Memory: {training_data['X'].nbytes / (1024**3):.2f} GB\")\n",
    "        \n",
    "        # ä¸è®ºæ–‡å¯¹æ¯”\n",
    "        paper_samples = 425242\n",
    "        paper_compounds = 9597\n",
    "        our_samples = len(training_data['X'])\n",
    "        our_compounds = len(training_data['compound_names'])\n",
    "        \n",
    "        print(f\"\\nğŸ“ˆ Comparison with paper:\")\n",
    "        print(f\"   Samples: {our_samples:,} / {paper_samples:,} ({our_samples/paper_samples*100:.1f}%)\")\n",
    "        print(f\"   Compounds: {our_compounds:,} / {paper_compounds:,} ({our_compounds/paper_compounds*100:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nğŸ¯ Ready for training!\")\n",
    "        \n",
    "        return training_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n\" + \"=\" * 80)\n",
    "        print(\"âŒ ERROR\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"   Type: {type(e).__name__}\")\n",
    "        print(f\"   Message: {e}\")\n",
    "        \n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    training_data = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e863973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸš€ å¼€å§‹ç”Ÿæˆ scGPT ä¸“ç”¨æ•°æ®é›†\n",
      "   è¾“å…¥æ–‡ä»¶: D:\\ç§‘ç ”\\Models\\drugreflector\\processed_data\\training_data_lincs2020_optimized_1201_l.pkl\n",
      "============================================================\n",
      "\n",
      "ğŸ“– æ­£åœ¨åŠ è½½ Pickle æ–‡ä»¶ (å¯èƒ½éœ€è¦å‡ ç§’é’Ÿ)...\n",
      "   âœ“ åŠ è½½å®Œæˆã€‚åŸå§‹ X ç±»å‹: float32\n",
      "ğŸ”„ æ­£åœ¨è½¬æ¢çŸ©é˜µå½¢çŠ¶: (945908, 978)...\n",
      "   ğŸ“Š è½¬æ¢ç»Ÿè®¡:\n",
      "      - ç¨€ç–åŒ–ç‡ (Bin 0): 83.6% (åŸ Z-score ç»å¯¹å€¼ < 1.5)\n",
      "      - æ¨¡æ‹Ÿé«˜è¡¨è¾¾ (Bin 30-50): 9.2%\n",
      "      - æ¨¡æ‹Ÿä½è¡¨è¾¾ (Bin 1-10): 90.8% (å« Bin 0)\n",
      "\n",
      "âš¡ æ­£åœ¨æ›¿æ¢ data['X'] ä¸º scGPT Bins æ ¼å¼...\n",
      "\n",
      "ğŸ’¾ æ­£åœ¨ä¿å­˜æ–°æ•°æ®é›†åˆ°: D:\\ç§‘ç ”\\Models\\drugreflector\\processed_data\\training_data_lincs2020_optimized_1201_l_scgpt.pkl\n",
      "   âœ“ ä¿å­˜æˆåŠŸ! æ–‡ä»¶å¤§å°: 3790.5 MB\n",
      "\n",
      "âœ… å®Œæˆ! ç°åœ¨ä½ å¯ä»¥ä½¿ç”¨ 'training_data_lincs2020_optimized_1201_l_scgpt.pkl' ç›´æ¥è¾“å…¥åˆ° scGPT æ¨¡å‹äº†ã€‚\n",
      "   æ³¨æ„: æ­¤æ—¶ data['X'] æ˜¯ int32 ç±»å‹ (LongTensor)ã€‚\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "# ==========================================\n",
    "# 1. å®šä¹‰è½¬æ¢å‡½æ•° (Z-score -> scGPT Bins)\n",
    "# ==========================================\n",
    "def zscore_to_scgpt_bins(z_scores, threshold=1.5, n_bins=51):\n",
    "    \"\"\"\n",
    "    å°†è¿ç»­çš„ Z-scores è½¬æ¢ä¸º scGPT é¢„è®­ç»ƒæ¨¡å‹å¯ç†è§£çš„ç¦»æ•£ Bins (0-50)ã€‚\n",
    "    \n",
    "    è½¬æ¢é€»è¾‘:\n",
    "    - èƒŒæ™¯å™ªéŸ³ (Zero-shot sparsity): |Z| < 1.5 -> Bin 0 (æ¨¡æ‹Ÿæœªè¡¨è¾¾/æ— æ‰°åŠ¨)\n",
    "    - ä¸Šè°ƒåŸºå›  (Up-regulated):       Z > 1.5  -> Bin 30-50 (æ¨¡æ‹Ÿä¸­é«˜è¡¨è¾¾)\n",
    "    - ä¸‹è°ƒåŸºå›  (Down-regulated):     Z < -1.5 -> Bin 1-10  (æ¨¡æ‹Ÿä½è¡¨è¾¾/æŠ‘åˆ¶)\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ”„ æ­£åœ¨è½¬æ¢çŸ©é˜µå½¢çŠ¶: {z_scores.shape}...\")\n",
    "    \n",
    "    # 1. åˆå§‹åŒ–å…¨ 0 çŸ©é˜µ (int32æ ¼å¼)\n",
    "    binned = np.zeros_like(z_scores, dtype=np.int32)\n",
    "    \n",
    "    # 2. å¤„ç†ä¸Šè°ƒåŸºå›  (Z > Threshold) -> æ˜ å°„åˆ° [30, 50]\n",
    "    # Z=1.5 å¯¹åº” Bin 30ï¼ŒZ>=10 å¯¹åº” Bin 50\n",
    "    up_mask = z_scores > threshold\n",
    "    if up_mask.any():\n",
    "        z_up = z_scores[up_mask]\n",
    "        # æˆªæ–­å¹¶åœ¨ [1.5, 10] èŒƒå›´å†…å½’ä¸€åŒ–\n",
    "        z_up_clipped = np.clip(z_up, threshold, 10.0)\n",
    "        norm_up = (z_up_clipped - threshold) / (10.0 - threshold)\n",
    "        # çº¿æ€§æ˜ å°„\n",
    "        bins_up = 30 + (norm_up * 20)\n",
    "        binned[up_mask] = np.round(bins_up).astype(np.int32)\n",
    "    \n",
    "    # 3. å¤„ç†ä¸‹è°ƒåŸºå›  (Z < -Threshold) -> æ˜ å°„åˆ° [1, 10]\n",
    "    # Z=-1.5 å¯¹åº” Bin 10ï¼ŒZ<=-10 å¯¹åº” Bin 1\n",
    "    # æ³¨æ„ï¼šåœ¨ scGPT è¯­ä¹‰ä¸­ï¼ŒBin å€¼è¶Šå°ä»£è¡¨è¡¨è¾¾é‡è¶Šä½\n",
    "    down_mask = z_scores < -threshold\n",
    "    if down_mask.any():\n",
    "        z_down = np.abs(z_scores[down_mask]) # å–ç»å¯¹å€¼å¤„ç†\n",
    "        z_down_clipped = np.clip(z_down, threshold, 10.0)\n",
    "        norm_down = (z_down_clipped - threshold) / (10.0 - threshold)\n",
    "        # çº¿æ€§æ˜ å°„ (åå‘ï¼šZ ç»å¯¹å€¼è¶Šå¤§ -> Bin è¶Šå°)\n",
    "        bins_down = 10 - (norm_down * 9)\n",
    "        binned[down_mask] = np.round(bins_down).astype(np.int32)\n",
    "        \n",
    "    # 4. å†æ¬¡ç¡®ä¿èŒƒå›´å®‰å…¨\n",
    "    binned = np.clip(binned, 0, n_bins-1)\n",
    "    \n",
    "    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯å¸®åŠ©æ£€æŸ¥\n",
    "    n_total = z_scores.size\n",
    "    n_zeros = (binned == 0).sum()\n",
    "    print(f\"   ğŸ“Š è½¬æ¢ç»Ÿè®¡:\")\n",
    "    print(f\"      - ç¨€ç–åŒ–ç‡ (Bin 0): {n_zeros/n_total*100:.1f}% (åŸ Z-score ç»å¯¹å€¼ < {threshold})\")\n",
    "    print(f\"      - æ¨¡æ‹Ÿé«˜è¡¨è¾¾ (Bin 30-50): {(binned >= 30).sum()/n_total*100:.1f}%\")\n",
    "    print(f\"      - æ¨¡æ‹Ÿä½è¡¨è¾¾ (Bin 1-10): {(binned <= 10).sum()/n_total*100:.1f}% (å« Bin 0)\")\n",
    "    \n",
    "    return binned\n",
    "\n",
    "# ==========================================\n",
    "# 2. æ‰§è¡Œè½¬æ¢æµç¨‹\n",
    "# ==========================================\n",
    "def main():\n",
    "    # ğŸ“‚ è·¯å¾„é…ç½® (è¯·ç¡®è®¤ä½ çš„è¾“å…¥è·¯å¾„)\n",
    "    input_path = Path(\"D:\\\\ç§‘ç ”\\\\Models\\\\drugreflector\\\\processed_data\\\\training_data_lincs2020_optimized_1201_l.pkl\")\n",
    "    output_path = input_path.parent / f\"{input_path.stem}_scgpt.pkl\"\n",
    "    \n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"ğŸš€ å¼€å§‹ç”Ÿæˆ scGPT ä¸“ç”¨æ•°æ®é›†\")\n",
    "    print(f\"   è¾“å…¥æ–‡ä»¶: {input_path}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    if not input_path.exists():\n",
    "        print(f\"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {input_path}\")\n",
    "        return\n",
    "\n",
    "    # 1. åŠ è½½æ•°æ®\n",
    "    print(\"ğŸ“– æ­£åœ¨åŠ è½½ Pickle æ–‡ä»¶ (å¯èƒ½éœ€è¦å‡ ç§’é’Ÿ)...\")\n",
    "    with open(input_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    print(f\"   âœ“ åŠ è½½å®Œæˆã€‚åŸå§‹ X ç±»å‹: {data['X'].dtype}\")\n",
    "    \n",
    "    # 2. è½¬æ¢ X\n",
    "    # å¦‚æœæ•°æ®æ˜¯ floatï¼Œåˆ™è¿›è¡Œè½¬æ¢ï¼›å¦‚æœæ˜¯ int è¯´æ˜å¯èƒ½å·²ç»è½¬æ¢è¿‡äº†\n",
    "    if np.issubdtype(data['X'].dtype, np.floating):\n",
    "        X_scgpt = zscore_to_scgpt_bins(data['X'], threshold=1.5)\n",
    "        \n",
    "        # ğŸ”¥ å…³é”®æ­¥éª¤ï¼šç›´æ¥æ›¿æ¢ X\n",
    "        print(\"\\nâš¡ æ­£åœ¨æ›¿æ¢ data['X'] ä¸º scGPT Bins æ ¼å¼...\")\n",
    "        data['X'] = X_scgpt\n",
    "        \n",
    "        # é‡Šæ”¾å†…å­˜\n",
    "        gc.collect()\n",
    "    else:\n",
    "        print(\"\\nâš ï¸ è­¦å‘Š: data['X'] ä¼¼ä¹å·²ç»æ˜¯æ•´æ•°æ ¼å¼ï¼Œè·³è¿‡è½¬æ¢æ­¥éª¤ã€‚\")\n",
    "\n",
    "    # 3. ä¿å­˜æ–°æ•°æ®é›†\n",
    "    print(f\"\\nğŸ’¾ æ­£åœ¨ä¿å­˜æ–°æ•°æ®é›†åˆ°: {output_path}\")\n",
    "    with open(output_path, 'wb') as f:\n",
    "        pickle.dump(data, f, protocol=4)\n",
    "        \n",
    "    print(f\"   âœ“ ä¿å­˜æˆåŠŸ! æ–‡ä»¶å¤§å°: {output_path.stat().st_size / (1024**2):.1f} MB\")\n",
    "    print(f\"\\nâœ… å®Œæˆ! ç°åœ¨ä½ å¯ä»¥ä½¿ç”¨ '{output_path.name}' ç›´æ¥è¾“å…¥åˆ° scGPT æ¨¡å‹äº†ã€‚\")\n",
    "    print(f\"   æ³¨æ„: æ­¤æ—¶ data['X'] æ˜¯ int32 ç±»å‹ (LongTensor)ã€‚\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abace059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ”¬ APPLYING CHEMICAL FILTERS TO TRAINING DATA\n",
      "================================================================================\n",
      "\n",
      "ğŸ“– Loading training data...\n",
      "   âœ“ Loaded: 904,569 samples, 11,116 compounds\n",
      "   âœ“ Training data contains: 11,116 unique compounds\n",
      "ğŸ“– Loading NIBR filters from: SubstructureFilter_HitTriaging_wPubChemExamples.csv\n",
      "   âœ“ Loaded 444 NIBR filter patterns\n",
      "   âœ“ Successfully added 444 NIBR filters to catalog\n",
      "================================================================================\n",
      "ğŸ§ª Chemical Filter Initialized\n",
      "================================================================================\n",
      "Based on DrugReflector SI (page 2)\n",
      "================================================================================\n",
      "\n",
      "ğŸ“– Loading compound information...\n",
      "   âœ“ Loaded 39,321 compounds\n",
      "   âœ“ Columns: ['pert_id', 'cmap_name', 'target', 'moa', 'canonical_smiles', 'inchi_key', 'compound_aliases']\n",
      "   âœ“ Using 'canonical_smiles' for molecular structures\n",
      "   âœ“ Compounds with valid SMILES: 33,531\n",
      "\n",
      "================================================================================\n",
      "ğŸ§ª APPLYING ALL CHEMICAL FILTERS\n",
      "================================================================================\n",
      "\n",
      "âš ï¸  Filtering only 11,116 compounds present in training data\n",
      "   Compounds to check: 15,449\n",
      "Initial compounds: 15,449\n",
      "\n",
      "================================================================================\n",
      "CHEMICAL FILTER 1: Molecular Weight (60-1000 Da)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:26:25] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:26:25] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:26:25] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:26:25] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:26:25] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:26:25] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:26:26] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:26:26] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:26:26] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:26:26] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:26:26] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:26:26] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:26:26] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:26:26] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:26:26] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:26:26] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:26:26] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:26:26] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:26:26] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:26:26] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:26:26] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:26:26] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:26:26] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:26:26] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:26:26] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:26:26] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:26:26] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:26:26] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:26:26] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:26:26] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:26:26] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:26:26] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ“ Passed: 10,757 compounds\n",
      "  âœ— Failed (out of MW range): 65 compounds\n",
      "  âš ï¸  No SMILES: 297 compounds\n",
      "  âš ï¸  Invalid SMILES: 16 compounds\n",
      "\n",
      "  MW distribution of failed compounds:\n",
      "    â€¢ Mean: 1402.8 Da\n",
      "    â€¢ Min: 59.1 Da\n",
      "    â€¢ Max: 2430.9 Da\n",
      "\n",
      "================================================================================\n",
      "CHEMICAL FILTER 2: Covalent Motifs (â‰¤1)\n",
      "================================================================================\n",
      "Checking 20 covalent SMARTS patterns...\n",
      "  âœ“ Successfully compiled 20 SMARTS patterns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:26:27] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:26:27] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:26:27] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:26:27] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:26:27] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:26:27] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:26:29] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:26:29] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:26:29] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:26:29] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:26:29] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:26:29] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:26:29] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:26:29] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:26:29] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:26:29] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:26:29] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:26:29] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:26:29] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:26:29] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:26:29] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:26:29] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:26:29] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:26:29] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:26:29] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:26:29] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:26:29] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:26:29] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:26:29] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:26:29] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:26:29] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:26:29] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ“ Passed: 10,463 compounds\n",
      "  âœ— Failed (>1 motifs): 591 compounds\n",
      "\n",
      "  Distribution of failed compounds:\n",
      "    â€¢ Mean motifs: 2.2\n",
      "    â€¢ Max motifs: 4\n",
      "\n",
      "================================================================================\n",
      "CHEMICAL FILTER 3: NIBR Structure Flags (â‰¤9)\n",
      "================================================================================\n",
      "Using official NIBR filter catalog (Schuffenhauer et al. 2020)\n",
      "  âœ“ Catalog contains 444 filter patterns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:26:31] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:26:31] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:26:31] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:26:31] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:26:32] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:26:32] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:26:54] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:26:54] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:26:54] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:26:54] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:26:54] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:26:54] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:26:54] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:26:54] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:26:56] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:26:56] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:26:56] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:26:56] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:26:56] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:26:56] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:26:57] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:26:57] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:26:57] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:26:57] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:26:57] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:26:57] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:26:57] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:26:57] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:26:57] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:26:57] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:26:57] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:26:57] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ“ Passed: 9,737 compounds\n",
      "  âœ— Failed (>9 flags): 1,910 compounds\n",
      "\n",
      "  Flag distribution (all compounds):\n",
      "    â€¢ Mean flags: 2.57\n",
      "    â€¢ Median flags: 0\n",
      "    â€¢ Max flags: 10\n",
      "\n",
      "  Flag count histogram:\n",
      "    â€¢ 0 flags: 4,872 compounds\n",
      "    â€¢ 1 flags: 1,044 compounds\n",
      "    â€¢ 2 flags: 110 compounds\n",
      "    â€¢ 3 flags: 12 compounds\n",
      "    â€¢ 10 flags: 1,910 compounds\n",
      "\n",
      "  Failed compounds statistics:\n",
      "    â€¢ Mean flags: 10.00\n",
      "    â€¢ Max flags: 10\n",
      "\n",
      "================================================================================\n",
      "CHEMICAL FILTER 4: BRENK Criteria\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:27:18] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:27:18] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:27:18] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:27:18] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:27:18] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:27:18] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:27:22] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:27:22] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:27:22] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:27:22] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:27:22] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:27:22] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:27:22] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:27:22] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:27:22] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:27:22] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:27:22] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:27:22] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:27:22] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:27:22] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:27:22] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:27:22] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:27:22] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:27:22] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:27:22] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:27:22] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:27:22] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:27:22] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:27:22] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:27:22] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:27:22] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:27:22] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ“ Passed: 7,586 compounds\n",
      "  âœ— Failed (BRENK violations): 4,958 compounds\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š CHEMICAL FILTER SUMMARY\n",
      "================================================================================\n",
      "  Initial compounds: 15,449\n",
      "  Passed Filter 1 (MW): 10,757\n",
      "  Passed Filter 2 (Covalent): 10,463\n",
      "  Passed Filter 3 (NIBR): 9,737\n",
      "  Passed Filter 4 (BRENK): 7,586\n",
      "  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  âœ… Passed ALL filters: 7,191\n",
      "  âŒ Removed: 8,258\n",
      "  Retention rate: 46.5%\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š FILTERING TRAINING DATA\n",
      "================================================================================\n",
      "  Compounds in training data: 11,116\n",
      "  Compounds passed filters: 7,191\n",
      "  Compounds to remove: 3,925\n",
      "  Removing 424,860 samples from 3,925 compounds...\n",
      "\n",
      "================================================================================\n",
      "âœ… FILTERING COMPLETE\n",
      "================================================================================\n",
      "  Before chemical filters:\n",
      "    â€¢ Samples: 904,569\n",
      "    â€¢ Compounds: 11,116\n",
      "  After chemical filters:\n",
      "    â€¢ Samples: 479,709 (53.0%)\n",
      "    â€¢ Compounds: 7,191 (64.7%)\n",
      "    â€¢ Cell lines: 56\n",
      "  Removed:\n",
      "    â€¢ Samples: 424,860\n",
      "    â€¢ Compounds: 3,925\n",
      "\n",
      "ğŸ“Š Comparison with paper (SI page 2):\n",
      "  Paper: 425,242 obs, 9,597 compounds, 52 cells\n",
      "  Ours:  479,709 obs, 7,191 compounds, 56 cells\n",
      "  Match rate:\n",
      "    â€¢ Samples: 112.8%\n",
      "    â€¢ Compounds: 74.9%\n",
      "    â€¢ Cell lines: 107.7%\n",
      "\n",
      "ğŸ’¾ Saving filtered data to: D:\\ç§‘ç ”\\Models\\drugreflector\\processed_data\\training_data_lincs2020_chemfiltered_1212_s.pkl\n",
      "   âœ“ Saved successfully! (2032.8 MB)\n",
      "\n",
      "================================================================================\n",
      "âœ… CHEMICAL FILTERING COMPLETE!\n",
      "================================================================================\n",
      "ğŸ“ Final output: D:\\ç§‘ç ”\\Models\\drugreflector\\processed_data\\training_data_lincs2020_chemfiltered_1212_s.pkl\n",
      "ğŸ¯ Ready for training!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "LINCS 2020 Chemical Filters\n",
    "æ ¹æ®DrugReflectorè®ºæ–‡SIç¬¬2é¡µå®ç°åŒ–å­¦è¿‡æ»¤\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# RDKit imports\n",
    "try:\n",
    "    from rdkit import Chem\n",
    "    from rdkit.Chem import Descriptors, AllChem, FilterCatalog\n",
    "    from rdkit.Chem.FilterCatalog import FilterCatalogParams\n",
    "    RDKIT_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"âš ï¸  RDKit not installed. Install with: conda install -c conda-forge rdkit\")\n",
    "    RDKIT_AVAILABLE = False\n",
    "\n",
    "\n",
    "class ChemicalFilter:\n",
    "    \"\"\"\n",
    "    åŒ–å­¦è¿‡æ»¤å™¨ - æ ¹æ®DrugReflector SIå®ç°\n",
    "    \n",
    "    Filters applied (SI page 2):\n",
    "    1. Molecular weight: 60-1000 Da (inclusive)\n",
    "    2. No more than 1 covalent motif (SMARTS-defined)\n",
    "    3. No more than 9 NIBR structure flags\n",
    "    4. Pass BRENK criteria\n",
    "    5. Must not match 30 SMARTS patterns (not disclosed)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, compound_info_path: str, nibr_filter_csv: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–åŒ–å­¦è¿‡æ»¤å™¨\n",
    "        \n",
    "        å‚æ•°ï¼š\n",
    "            compound_info_path: compoundinfo_beta.txtè·¯å¾„\n",
    "            nibr_filter_csv: NIBRè¿‡æ»¤å™¨CSVæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰\n",
    "        \"\"\"\n",
    "        if not RDKIT_AVAILABLE:\n",
    "            raise ImportError(\"RDKit is required for chemical filtering\")\n",
    "        \n",
    "        self.compound_info_path = Path(compound_info_path)\n",
    "        self.compound_info = None\n",
    "        self.filtered_compounds = set()\n",
    "        \n",
    "        # åˆå§‹åŒ–BRENKè¿‡æ»¤å™¨\n",
    "        self.brenk_catalog = self._init_brenk_filter()\n",
    "        \n",
    "        # åˆå§‹åŒ–å…±ä»·åŸºå›¢SMARTS\n",
    "        self.covalent_smarts = self._init_covalent_smarts()\n",
    "        \n",
    "        # ğŸ”¥ æ–°å¢ï¼šåˆå§‹åŒ–NIBRè¿‡æ»¤å™¨ç›®å½•\n",
    "        self.nibr_catalog = self._init_nibr_filter(nibr_filter_csv)\n",
    "        \n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"ğŸ§ª Chemical Filter Initialized\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Based on DrugReflector SI (page 2)\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    def _init_brenk_filter(self):\n",
    "        \"\"\"åˆå§‹åŒ–BRENKè¿‡æ»¤å™¨\"\"\"\n",
    "        params = FilterCatalogParams()\n",
    "        params.AddCatalog(FilterCatalogParams.FilterCatalogs.BRENK)\n",
    "        return FilterCatalog.FilterCatalog(params)\n",
    "    \n",
    "    def _init_covalent_smarts(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        å…±ä»·åŸºå›¢SMARTSæ¨¡å¼\n",
    "        \n",
    "        å‚è€ƒï¼š\n",
    "        - Backman et al. 2019 (ChEMBL structural alerts)\n",
    "        - Common covalent warheads in drug discovery\n",
    "        \"\"\"\n",
    "        return [\n",
    "            # Michael acceptors\n",
    "            'C=CC(=O)',  # Î±,Î²-unsaturated carbonyl\n",
    "            'C=CC(=O)N',  # acrylamide\n",
    "            'C=CC#N',  # acrylonitrile\n",
    "            \n",
    "            # Electrophilic carbonyls\n",
    "            '[C;!R](=O)Cl',  # acyl chloride\n",
    "            '[C;!R](=O)O[C;!R](=O)',  # anhydride\n",
    "            'C(=O)N=[N+]=[N-]',  # acyl azide\n",
    "            \n",
    "            # Epoxides and aziridines\n",
    "            'C1OC1',  # epoxide\n",
    "            'C1NC1',  # aziridine\n",
    "            \n",
    "            # Haloacetamides\n",
    "            'ClCC(=O)N',  # chloroacetamide\n",
    "            'BrCC(=O)N',  # bromoacetamide\n",
    "            \n",
    "            # Aldehydes (reactive)\n",
    "            '[CH;!R]=O',  # aldehyde\n",
    "            \n",
    "            # Isocyanates/isothiocyanates\n",
    "            'N=C=O',  # isocyanate\n",
    "            'N=C=S',  # isothiocyanate\n",
    "            \n",
    "            # Sulfonyl halides\n",
    "            'S(=O)(=O)Cl',  # sulfonyl chloride\n",
    "            'S(=O)(=O)F',  # sulfonyl fluoride\n",
    "            \n",
    "            # Nitriles (activated)\n",
    "            '[C;!R]#N',  # nitrile (non-aromatic)\n",
    "            \n",
    "            # Peroxides\n",
    "            'OO',  # peroxide\n",
    "            \n",
    "            # Beta-lactams (strained)\n",
    "            'C1(=O)NCC1',  # beta-lactam\n",
    "            \n",
    "            # Activated esters\n",
    "            'C(=O)OC(=O)',  # activated ester\n",
    "            \n",
    "            # Quinones\n",
    "            'C1=CC(=O)C=CC1=O',  # quinone\n",
    "        ]\n",
    "    \n",
    "    def _init_nibr_filter(self, csv_path: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–NIBRè¿‡æ»¤å™¨ç›®å½•ï¼ˆåŸºäºå®˜æ–¹444ä¸ªSMARTSï¼‰\n",
    "        \n",
    "        å‚æ•°ï¼š\n",
    "            csv_path: NIBRè¿‡æ»¤å™¨CSVæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰\n",
    "            \n",
    "        è¿”å›ï¼š\n",
    "            FilterCatalogå¯¹è±¡\n",
    "        \"\"\"\n",
    "        # ğŸ”¥ ç¡®å®šCSVæ–‡ä»¶è·¯å¾„\n",
    "        if csv_path is None:\n",
    "            # é»˜è®¤è·¯å¾„ï¼šå½“å‰è„šæœ¬ç›®å½•ä¸‹çš„chem_filteræ–‡ä»¶å¤¹\n",
    "            try:\n",
    "                # å°è¯•ä½¿ç”¨ __file__ï¼ˆåœ¨è„šæœ¬æ¨¡å¼ä¸‹ï¼‰\n",
    "                script_dir = Path(__file__).parent\n",
    "            except NameError:\n",
    "                # åœ¨äº¤äº’å¼ç¯å¢ƒä¸­ä½¿ç”¨å½“å‰å·¥ä½œç›®å½•\n",
    "                script_dir = Path(os.getcwd())\n",
    "            csv_path = script_dir / \"chem_filter\" / \"SubstructureFilter_HitTriaging_wPubChemExamples.csv\"\n",
    "        else:\n",
    "            csv_path = Path(csv_path)\n",
    "        \n",
    "        if not csv_path.exists():\n",
    "            print(f\"âš ï¸  NIBR filter CSV not found: {csv_path}\")\n",
    "            print(f\"   Using fallback: empty NIBR catalog\")\n",
    "            return FilterCatalog.FilterCatalog()\n",
    "        \n",
    "        print(f\"ğŸ“– Loading NIBR filters from: {csv_path.name}\")\n",
    "        \n",
    "        # è¯»å–CSVå¹¶æ„å»ºFilterCatalog\n",
    "        try:\n",
    "            nibr_df = pd.read_csv(csv_path)\n",
    "            print(f\"   âœ“ Loaded {len(nibr_df)} NIBR filter patterns\")\n",
    "            \n",
    "            nibr_catalog = FilterCatalog.FilterCatalog()\n",
    "            \n",
    "            n_added = 0\n",
    "            for idx, row in nibr_df.iterrows():\n",
    "                try:\n",
    "                    # æå–å‚æ•°\n",
    "                    pattern_name = row['PATTERN_NAME']\n",
    "                    smarts = row['SMARTS']\n",
    "                    min_count = 1 if row['MIN_COUNT'] == 0 else int(row['MIN_COUNT'])\n",
    "                    severity = int(row['SEVERITY_SCORE'])\n",
    "                    covalent = int(row['COVALENT'])\n",
    "                    special_mol = int(row['SPECIAL_MOL'])\n",
    "                    set_name = row['SET_NAME']\n",
    "                    \n",
    "                    # æ„å»ºè¿‡æ»¤å™¨åç§°ï¼ˆä¸å®˜æ–¹ä»£ç ä¸€è‡´ï¼‰\n",
    "                    filter_name = f\"{pattern_name}_min({min_count})__{severity}__{covalent}__{special_mol}\"\n",
    "                    \n",
    "                    # åˆ›å»ºSMARTSåŒ¹é…å™¨\n",
    "                    matcher = FilterCatalog.SmartsMatcher(filter_name, smarts, min_count)\n",
    "                    \n",
    "                    # æ·»åŠ åˆ°ç›®å½•\n",
    "                    entry = FilterCatalog.FilterCatalogEntry(filter_name, matcher)\n",
    "                    entry.SetProp('Scope', set_name)\n",
    "                    entry.SetProp('Severity', str(severity))\n",
    "                    nibr_catalog.AddEntry(entry)\n",
    "                    \n",
    "                    n_added += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"   âš ï¸  Failed to add filter {idx}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            print(f\"   âœ“ Successfully added {n_added} NIBR filters to catalog\")\n",
    "            return nibr_catalog\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Error loading NIBR filters: {e}\")\n",
    "            print(f\"   Using fallback: empty NIBR catalog\")\n",
    "            return FilterCatalog.FilterCatalog()\n",
    "    \n",
    "    def load_compound_info(self):\n",
    "        \"\"\"åŠ è½½åŒ–åˆç‰©ä¿¡æ¯\"\"\"\n",
    "        print(f\"ğŸ“– Loading compound information...\")\n",
    "        \n",
    "        if not self.compound_info_path.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"Compound info file not found: {self.compound_info_path}\"\n",
    "            )\n",
    "        \n",
    "        self.compound_info = pd.read_csv(self.compound_info_path, sep='\\t')\n",
    "        \n",
    "        print(f\"   âœ“ Loaded {len(self.compound_info):,} compounds\")\n",
    "        print(f\"   âœ“ Columns: {list(self.compound_info.columns)}\")\n",
    "        \n",
    "        # æ£€æŸ¥SMILESåˆ—\n",
    "        smiles_col = None\n",
    "        for col in ['canonical_smiles', 'smiles', 'SMILES']:\n",
    "            if col in self.compound_info.columns:\n",
    "                smiles_col = col\n",
    "                break\n",
    "        \n",
    "        if smiles_col is None:\n",
    "            raise ValueError(\n",
    "                f\"SMILES column not found. Available columns: {list(self.compound_info.columns)}\"\n",
    "            )\n",
    "        \n",
    "        self.smiles_col = smiles_col\n",
    "        print(f\"   âœ“ Using '{smiles_col}' for molecular structures\")\n",
    "        \n",
    "        # æ£€æŸ¥æœ‰æ•ˆSMILESæ•°é‡\n",
    "        valid_smiles = self.compound_info[smiles_col].notna().sum()\n",
    "        print(f\"   âœ“ Compounds with valid SMILES: {valid_smiles:,}\")\n",
    "        \n",
    "        return self.compound_info\n",
    "    \n",
    "    def filter_1_molecular_weight(self, min_mw=60, max_mw=1000) -> set:\n",
    "        \"\"\"\n",
    "        Filter 1: åˆ†å­é‡è¿‡æ»¤ (60-1000 Da)\n",
    "        \n",
    "        å‚æ•°ï¼š\n",
    "            min_mw: æœ€å°åˆ†å­é‡ï¼ˆé»˜è®¤60ï¼‰\n",
    "            max_mw: æœ€å¤§åˆ†å­é‡ï¼ˆé»˜è®¤1000ï¼‰\n",
    "        \n",
    "        è¿”å›ï¼š\n",
    "            é€šè¿‡è¿‡æ»¤çš„åŒ–åˆç‰©IDé›†åˆ\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"CHEMICAL FILTER 1: Molecular Weight ({min_mw}-{max_mw} Da)\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        passed = set()\n",
    "        failed_mw = []\n",
    "        no_smiles = 0\n",
    "        invalid_smiles = 0\n",
    "        \n",
    "        for idx, row in self.compound_info.iterrows():\n",
    "            pert_id = row['pert_id']\n",
    "            smiles = row[self.smiles_col]\n",
    "            \n",
    "            # æ£€æŸ¥SMILESæ˜¯å¦å­˜åœ¨\n",
    "            if pd.isna(smiles) or smiles == '':\n",
    "                no_smiles += 1\n",
    "                continue\n",
    "            \n",
    "            # è§£æåˆ†å­\n",
    "            mol = Chem.MolFromSmiles(str(smiles))\n",
    "            if mol is None:\n",
    "                invalid_smiles += 1\n",
    "                continue\n",
    "            \n",
    "            # è®¡ç®—åˆ†å­é‡\n",
    "            mw = Descriptors.MolWt(mol)\n",
    "            \n",
    "            if min_mw <= mw <= max_mw:\n",
    "                passed.add(pert_id)\n",
    "            else:\n",
    "                failed_mw.append((pert_id, mw))\n",
    "        \n",
    "        print(f\"  âœ“ Passed: {len(passed):,} compounds\")\n",
    "        print(f\"  âœ— Failed (out of MW range): {len(failed_mw):,} compounds\")\n",
    "        print(f\"  âš ï¸  No SMILES: {no_smiles:,} compounds\")\n",
    "        print(f\"  âš ï¸  Invalid SMILES: {invalid_smiles:,} compounds\")\n",
    "        \n",
    "        if failed_mw:\n",
    "            failed_df = pd.DataFrame(failed_mw, columns=['pert_id', 'MW'])\n",
    "            print(f\"\\n  MW distribution of failed compounds:\")\n",
    "            print(f\"    â€¢ Mean: {failed_df['MW'].mean():.1f} Da\")\n",
    "            print(f\"    â€¢ Min: {failed_df['MW'].min():.1f} Da\")\n",
    "            print(f\"    â€¢ Max: {failed_df['MW'].max():.1f} Da\")\n",
    "        \n",
    "        return passed\n",
    "    \n",
    "    def filter_2_covalent_motifs(self, max_motifs=1) -> set:\n",
    "        \"\"\"\n",
    "        Filter 2: å…±ä»·åŸºå›¢è¿‡æ»¤ (â‰¤1ä¸ª)\n",
    "        \n",
    "        å‚æ•°ï¼š\n",
    "            max_motifs: å…è®¸çš„æœ€å¤§å…±ä»·åŸºå›¢æ•°ï¼ˆé»˜è®¤1ï¼‰\n",
    "        \n",
    "        è¿”å›ï¼š\n",
    "            é€šè¿‡è¿‡æ»¤çš„åŒ–åˆç‰©IDé›†åˆ\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"CHEMICAL FILTER 2: Covalent Motifs (â‰¤{max_motifs})\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Checking {len(self.covalent_smarts)} covalent SMARTS patterns...\")\n",
    "        \n",
    "        passed = set()\n",
    "        failed_counts = []\n",
    "        \n",
    "        # é¢„ç¼–è¯‘SMARTS\n",
    "        smarts_mols = []\n",
    "        for smarts in self.covalent_smarts:\n",
    "            try:\n",
    "                smarts_mol = Chem.MolFromSmarts(smarts)\n",
    "                if smarts_mol:\n",
    "                    smarts_mols.append(smarts_mol)\n",
    "            except:\n",
    "                print(f\"  âš ï¸  Invalid SMARTS: {smarts}\")\n",
    "        \n",
    "        print(f\"  âœ“ Successfully compiled {len(smarts_mols)} SMARTS patterns\")\n",
    "        \n",
    "        for idx, row in self.compound_info.iterrows():\n",
    "            pert_id = row['pert_id']\n",
    "            smiles = row[self.smiles_col]\n",
    "            \n",
    "            if pd.isna(smiles):\n",
    "                continue\n",
    "            \n",
    "            mol = Chem.MolFromSmiles(str(smiles))\n",
    "            if mol is None:\n",
    "                continue\n",
    "            \n",
    "            # è®¡æ•°åŒ¹é…çš„å…±ä»·åŸºå›¢\n",
    "            motif_count = 0\n",
    "            for smarts_mol in smarts_mols:\n",
    "                if mol.HasSubstructMatch(smarts_mol):\n",
    "                    motif_count += 1\n",
    "            \n",
    "            if motif_count <= max_motifs:\n",
    "                passed.add(pert_id)\n",
    "            else:\n",
    "                failed_counts.append(motif_count)\n",
    "        \n",
    "        print(f\"  âœ“ Passed: {len(passed):,} compounds\")\n",
    "        print(f\"  âœ— Failed (>{max_motifs} motifs): {len(failed_counts):,} compounds\")\n",
    "        \n",
    "        if failed_counts:\n",
    "            print(f\"\\n  Distribution of failed compounds:\")\n",
    "            print(f\"    â€¢ Mean motifs: {np.mean(failed_counts):.1f}\")\n",
    "            print(f\"    â€¢ Max motifs: {max(failed_counts)}\")\n",
    "        \n",
    "        return passed\n",
    "    \n",
    "    def filter_3_nibr_flags(self, max_flags=9) -> set:\n",
    "        \"\"\"\n",
    "        Filter 3: NIBRç»“æ„æ ‡è®° (â‰¤9ä¸ª)\n",
    "        \n",
    "        ä½¿ç”¨å®˜æ–¹NIBR 444ä¸ªSMARTSè¿‡æ»¤å™¨\n",
    "        å‚è€ƒï¼šSchuffenhauer et al. 2020, J. Med. Chem.\n",
    "        \n",
    "        å‚æ•°ï¼š\n",
    "            max_flags: å…è®¸çš„æœ€å¤§æ ‡è®°æ•°ï¼ˆé»˜è®¤9ï¼Œç¬¦åˆSIè¦æ±‚ï¼‰\n",
    "        \n",
    "        è¿”å›ï¼š\n",
    "            é€šè¿‡è¿‡æ»¤çš„åŒ–åˆç‰©IDé›†åˆ\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"CHEMICAL FILTER 3: NIBR Structure Flags (â‰¤{max_flags})\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Using official NIBR filter catalog (Schuffenhauer et al. 2020)\")\n",
    "        \n",
    "        if self.nibr_catalog is None or self.nibr_catalog.GetNumEntries() == 0:\n",
    "            print(f\"  âš ï¸  NIBR catalog is empty, skipping this filter\")\n",
    "            # è¿”å›æ‰€æœ‰åŒ–åˆç‰©ï¼ˆç›¸å½“äºä¸è¿‡æ»¤ï¼‰\n",
    "            return set(self.compound_info['pert_id'].values)\n",
    "        \n",
    "        print(f\"  âœ“ Catalog contains {self.nibr_catalog.GetNumEntries()} filter patterns\")\n",
    "        \n",
    "        passed = set()\n",
    "        failed_counts = []\n",
    "        flag_distribution = []\n",
    "        \n",
    "        for idx, row in self.compound_info.iterrows():\n",
    "            pert_id = row['pert_id']\n",
    "            smiles = row[self.smiles_col]\n",
    "            \n",
    "            if pd.isna(smiles):\n",
    "                continue\n",
    "            \n",
    "            mol = Chem.MolFromSmiles(str(smiles))\n",
    "            if mol is None:\n",
    "                continue\n",
    "            \n",
    "            # ğŸ”¥ ä½¿ç”¨FilterCatalogè·å–æ‰€æœ‰åŒ¹é…\n",
    "            matches = self.nibr_catalog.GetMatches(mol)\n",
    "            \n",
    "            if len(matches) == 0:\n",
    "                # æ²¡æœ‰åŒ¹é…ä»»ä½•flagï¼Œé€šè¿‡\n",
    "                flag_count = 0\n",
    "                passed.add(pert_id)\n",
    "            else:\n",
    "                # ğŸ”¥ è®¡ç®—flagæ•°é‡ï¼ˆå‚è€ƒå®˜æ–¹ä»£ç é€»è¾‘ï¼‰\n",
    "                # åªç»Ÿè®¡severity=1çš„FLAGï¼Œseverity=2çš„EXCLUDEå•ç‹¬å¤„ç†\n",
    "                severity_scores = []\n",
    "                for entry in matches:\n",
    "                    # æå–severityä¿¡æ¯\n",
    "                    desc = entry.GetDescription()\n",
    "                    # æ ¼å¼ï¼špattern_name_min(X)__severity__covalent__special_mol\n",
    "                    parts = desc.split('__')\n",
    "                    if len(parts) >= 2:\n",
    "                        severity = int(parts[1])\n",
    "                        severity_scores.append(severity)\n",
    "                \n",
    "                # ğŸ”¥ å…³é”®é€»è¾‘ï¼ˆä¸å®˜æ–¹ä»£ç ä¸€è‡´ï¼‰ï¼š\n",
    "                # å¦‚æœæœ‰severity=2ï¼ˆEXCLUDEï¼‰ï¼Œç›´æ¥æ ‡è®°ä¸º10ï¼ˆå¿…é¡»æ’é™¤ï¼‰\n",
    "                if 2 in severity_scores:\n",
    "                    flag_count = 10  # è¶…è¿‡é˜ˆå€¼ï¼Œå¿…é¡»æ’é™¤\n",
    "                else:\n",
    "                    # å¦åˆ™ï¼Œflag_count = severity=1çš„æ•°é‡\n",
    "                    flag_count = sum(1 for s in severity_scores if s == 1)\n",
    "                \n",
    "                flag_distribution.append(flag_count)\n",
    "                \n",
    "                if flag_count <= max_flags:\n",
    "                    passed.add(pert_id)\n",
    "                else:\n",
    "                    failed_counts.append(flag_count)\n",
    "        \n",
    "        print(f\"  âœ“ Passed: {len(passed):,} compounds\")\n",
    "        print(f\"  âœ— Failed (>{max_flags} flags): {len(failed_counts):,} compounds\")\n",
    "        \n",
    "        if flag_distribution:\n",
    "            print(f\"\\n  Flag distribution (all compounds):\")\n",
    "            print(f\"    â€¢ Mean flags: {np.mean(flag_distribution):.2f}\")\n",
    "            print(f\"    â€¢ Median flags: {np.median(flag_distribution):.0f}\")\n",
    "            print(f\"    â€¢ Max flags: {max(flag_distribution)}\")\n",
    "            \n",
    "            # ç»Ÿè®¡å„flagæ•°é‡çš„åˆ†å¸ƒ\n",
    "            from collections import Counter\n",
    "            flag_counts = Counter(flag_distribution)\n",
    "            print(f\"\\n  Flag count histogram:\")\n",
    "            for count in sorted(flag_counts.keys())[:15]:  # åªæ˜¾ç¤ºå‰15ä¸ª\n",
    "                n_compounds = flag_counts[count]\n",
    "                print(f\"    â€¢ {count} flags: {n_compounds:,} compounds\")\n",
    "        \n",
    "        if failed_counts:\n",
    "            print(f\"\\n  Failed compounds statistics:\")\n",
    "            print(f\"    â€¢ Mean flags: {np.mean(failed_counts):.2f}\")\n",
    "            print(f\"    â€¢ Max flags: {max(failed_counts)}\")\n",
    "        \n",
    "        return passed\n",
    "    \n",
    "    def filter_4_brenk(self) -> set:\n",
    "        \"\"\"\n",
    "        Filter 4: BRENKè¿‡æ»¤å™¨\n",
    "        \n",
    "        BRENKè¿‡æ»¤å™¨è¯†åˆ«ä¸è‰¯çš„åŒ–å­¦åŸºå›¢å’Œååº”æ€§å®˜èƒ½å›¢\n",
    "        \n",
    "        è¿”å›ï¼š\n",
    "            é€šè¿‡è¿‡æ»¤çš„åŒ–åˆç‰©IDé›†åˆ\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"CHEMICAL FILTER 4: BRENK Criteria\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        passed = set()\n",
    "        failed = []\n",
    "        \n",
    "        for idx, row in self.compound_info.iterrows():\n",
    "            pert_id = row['pert_id']\n",
    "            smiles = row[self.smiles_col]\n",
    "            \n",
    "            if pd.isna(smiles):\n",
    "                continue\n",
    "            \n",
    "            mol = Chem.MolFromSmiles(str(smiles))\n",
    "            if mol is None:\n",
    "                continue\n",
    "            \n",
    "            # æ£€æŸ¥BRENKè¿‡æ»¤å™¨\n",
    "            matches = self.brenk_catalog.GetMatches(mol)\n",
    "            \n",
    "            if len(matches) == 0:\n",
    "                passed.add(pert_id)\n",
    "            else:\n",
    "                failed.append((pert_id, len(matches)))\n",
    "        \n",
    "        print(f\"  âœ“ Passed: {len(passed):,} compounds\")\n",
    "        print(f\"  âœ— Failed (BRENK violations): {len(failed):,} compounds\")\n",
    "        \n",
    "        return passed\n",
    "    \n",
    "    def apply_all_filters(self, target_compounds: Optional[set] = None) -> set:\n",
    "        \"\"\"\n",
    "        åº”ç”¨æ‰€æœ‰åŒ–å­¦è¿‡æ»¤å™¨\n",
    "        \n",
    "        è¿”å›ï¼š\n",
    "            target_compounds: éœ€è¦è¿‡æ»¤çš„åŒ–åˆç‰©é›†åˆï¼ˆå¦‚æœæä¾›ï¼Œåˆ™åªè¿‡æ»¤è¿™äº›åŒ–åˆç‰©ï¼‰\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ğŸ§ª APPLYING ALL CHEMICAL FILTERS\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        if self.compound_info is None:\n",
    "            self.load_compound_info()\n",
    "        \n",
    "        #  å¦‚æœæŒ‡å®šäº†ç›®æ ‡åŒ–åˆç‰©ï¼Œåªè¿‡æ»¤è¿™äº›\n",
    "        if target_compounds is not None:\n",
    "            print(f\"\\nâš ï¸  Filtering only {len(target_compounds):,} compounds present in training data\")\n",
    "            # ä¸´æ—¶ä¿å­˜åŸå§‹æ•°æ®\n",
    "            original_compound_info = self.compound_info.copy()\n",
    "            # åªä¿ç•™ç›®æ ‡åŒ–åˆç‰©\n",
    "            self.compound_info = self.compound_info[\n",
    "                self.compound_info['pert_id'].isin(target_compounds)\n",
    "            ]\n",
    "            print(f\"   Compounds to check: {len(self.compound_info):,}\")\n",
    "        \n",
    "        initial_compounds = len(self.compound_info)\n",
    "        print(f\"Initial compounds: {initial_compounds:,}\")\n",
    "        \n",
    "        # åº”ç”¨å„ä¸ªè¿‡æ»¤å™¨\n",
    "        passed_mw = self.filter_1_molecular_weight()\n",
    "        passed_covalent = self.filter_2_covalent_motifs()\n",
    "        passed_nibr = self.filter_3_nibr_flags()\n",
    "        passed_brenk = self.filter_4_brenk()\n",
    "        \n",
    "        # å–äº¤é›†\n",
    "        passed_all = passed_mw & passed_covalent & passed_nibr & passed_brenk\n",
    "        \n",
    "        # å¦‚æœæ˜¯é’ˆå¯¹ç›®æ ‡åŒ–åˆç‰©çš„è¿‡æ»¤ï¼Œæ¢å¤åŸå§‹æ•°æ®\n",
    "        if target_compounds is not None:\n",
    "            self.compound_info = original_compound_info\n",
    "        \n",
    "        # ç»Ÿè®¡\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ğŸ“Š CHEMICAL FILTER SUMMARY\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"  Initial compounds: {initial_compounds:,}\")\n",
    "        print(f\"  Passed Filter 1 (MW): {len(passed_mw):,}\")\n",
    "        print(f\"  Passed Filter 2 (Covalent): {len(passed_covalent):,}\")\n",
    "        print(f\"  Passed Filter 3 (NIBR): {len(passed_nibr):,}\")\n",
    "        print(f\"  Passed Filter 4 (BRENK): {len(passed_brenk):,}\")\n",
    "        print(f\"  {'â”€'*80}\")\n",
    "        print(f\"  âœ… Passed ALL filters: {len(passed_all):,}\")\n",
    "        print(f\"  âŒ Removed: {initial_compounds - len(passed_all):,}\")\n",
    "        print(f\"  Retention rate: {len(passed_all)/initial_compounds*100:.1f}%\")\n",
    "        \n",
    "        self.filtered_compounds = passed_all\n",
    "        return passed_all\n",
    "\n",
    "\n",
    "def apply_chemical_filters_to_training_data(\n",
    "    training_data_path: str,\n",
    "    compound_info_path: str,\n",
    "    output_path: Optional[str] = None,\n",
    "    nibr_filter_csv: Optional[str] = None\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    å¯¹è®­ç»ƒæ•°æ®åº”ç”¨åŒ–å­¦è¿‡æ»¤å™¨\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        training_data_path: è®­ç»ƒæ•°æ®pklæ–‡ä»¶è·¯å¾„\n",
    "        compound_info_path: compoundinfo_beta.txtè·¯å¾„\n",
    "        output_path: è¾“å‡ºè·¯å¾„ï¼ˆå¯é€‰ï¼‰\n",
    "        nibr_filter_csv: NIBRè¿‡æ»¤å™¨CSVè·¯å¾„ï¼ˆå¯é€‰ï¼‰\n",
    "    \n",
    "    è¿”å›ï¼š\n",
    "        è¿‡æ»¤åçš„è®­ç»ƒæ•°æ®å­—å…¸\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ğŸ”¬ APPLYING CHEMICAL FILTERS TO TRAINING DATA\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # 1. åŠ è½½è®­ç»ƒæ•°æ®\n",
    "    print(f\"ğŸ“– Loading training data...\")\n",
    "    with open(training_data_path, 'rb') as f:\n",
    "        training_data = pickle.load(f)\n",
    "    \n",
    "    initial_samples = len(training_data['X'])\n",
    "    initial_compounds = len(training_data['compound_names'])\n",
    "    \n",
    "    print(f\"   âœ“ Loaded: {initial_samples:,} samples, {initial_compounds:,} compounds\")\n",
    "    \n",
    "    # è·å–è®­ç»ƒæ•°æ®ä¸­çš„åŒ–åˆç‰©\n",
    "    metadata = training_data['sample_meta']\n",
    "    training_compounds = set(metadata['pert_id'].unique())\n",
    "    print(f\"   âœ“ Training data contains: {len(training_compounds):,} unique compounds\")\n",
    "    \n",
    "    # 2. åº”ç”¨åŒ–å­¦è¿‡æ»¤å™¨ï¼ˆåªé’ˆå¯¹è®­ç»ƒæ•°æ®ä¸­çš„åŒ–åˆç‰©ï¼‰\n",
    "    filter_obj = ChemicalFilter(compound_info_path, nibr_filter_csv=nibr_filter_csv)  # ğŸ”¥ ä¼ é€’å‚æ•°\n",
    "    filter_obj.load_compound_info()\n",
    "    valid_compounds = filter_obj.apply_all_filters(target_compounds=training_compounds)\n",
    "    \n",
    "    # 3. è¿‡æ»¤è®­ç»ƒæ•°æ®\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ğŸ“Š FILTERING TRAINING DATA\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # valid_compoundsç°åœ¨æ˜¯training_compoundsçš„å­é›†\n",
    "    print(f\"  Compounds in training data: {len(training_compounds):,}\")\n",
    "    print(f\"  Compounds passed filters: {len(valid_compounds):,}\")\n",
    "    print(f\"  Compounds to remove: {len(training_compounds - valid_compounds):,}\")\n",
    "    \n",
    "    valid_mask = metadata['pert_id'].isin(valid_compounds)\n",
    "    \n",
    "    n_removed_samples = (~valid_mask).sum()\n",
    "    n_removed_compounds = len(training_compounds - valid_compounds)  # ä»è®­ç»ƒæ•°æ®ä¸­ç§»é™¤çš„\n",
    "\n",
    "    print(f\"  Removing {n_removed_samples:,} samples from {n_removed_compounds:,} compounds...\")\n",
    "    \n",
    "    # åˆ›å»ºæ–°çš„è®­ç»ƒæ•°æ®\n",
    "    filtered_data = {\n",
    "        'X': training_data['X'][valid_mask],\n",
    "        'y': None,  # éœ€è¦é‡æ–°ç¼–ç \n",
    "        'folds': training_data['folds'][valid_mask],\n",
    "        'sample_meta': metadata[valid_mask].reset_index(drop=True),\n",
    "        'metadata': metadata[valid_mask].reset_index(drop=True),\n",
    "        'gene_names': training_data['gene_names'],\n",
    "        'compound_names': None,  # éœ€è¦æ›´æ–°\n",
    "        'pert_to_idx': None  # éœ€è¦é‡æ–°æ„å»º\n",
    "    }\n",
    "    \n",
    "    # é‡æ–°æ„å»ºåŒ–åˆç‰©æ˜ å°„å’Œæ ‡ç­¾\n",
    "    new_compound_names = sorted(list(valid_compounds))\n",
    "    new_pert_to_idx = {pert: idx for idx, pert in enumerate(new_compound_names)}\n",
    "    new_labels = np.array([\n",
    "        new_pert_to_idx[pert] \n",
    "        for pert in filtered_data['sample_meta']['pert_id']\n",
    "    ], dtype=np.int32)\n",
    "    \n",
    "    filtered_data['compound_names'] = new_compound_names\n",
    "    filtered_data['pert_to_idx'] = new_pert_to_idx\n",
    "    filtered_data['y'] = new_labels\n",
    "    \n",
    "    # ç»Ÿè®¡\n",
    "    final_samples = len(filtered_data['X'])\n",
    "    final_compounds = len(new_compound_names)\n",
    "    \n",
    "    if 'cell_iname' in filtered_data['sample_meta'].columns:\n",
    "        final_cells = filtered_data['sample_meta']['cell_iname'].nunique()\n",
    "    else:\n",
    "        final_cells = 'Unknown'\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"âœ… FILTERING COMPLETE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"  Before chemical filters:\")\n",
    "    print(f\"    â€¢ Samples: {initial_samples:,}\")\n",
    "    print(f\"    â€¢ Compounds: {initial_compounds:,}\")\n",
    "    print(f\"  After chemical filters:\")\n",
    "    print(f\"    â€¢ Samples: {final_samples:,} ({final_samples/initial_samples*100:.1f}%)\")\n",
    "    print(f\"    â€¢ Compounds: {final_compounds:,} ({final_compounds/initial_compounds*100:.1f}%)\")\n",
    "    print(f\"    â€¢ Cell lines: {final_cells}\")\n",
    "    print(f\"  Removed:\")\n",
    "    print(f\"    â€¢ Samples: {n_removed_samples:,}\")\n",
    "    print(f\"    â€¢ Compounds: {n_removed_compounds:,}\")\n",
    "    \n",
    "    # ä¸è®ºæ–‡å¯¹æ¯”\n",
    "    paper_samples = 425242\n",
    "    paper_compounds = 9597\n",
    "    paper_cells = 52\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Comparison with paper (SI page 2):\")\n",
    "    print(f\"  Paper: {paper_samples:,} obs, {paper_compounds:,} compounds, {paper_cells} cells\")\n",
    "    print(f\"  Ours:  {final_samples:,} obs, {final_compounds:,} compounds, {final_cells} cells\")\n",
    "    print(f\"  Match rate:\")\n",
    "    print(f\"    â€¢ Samples: {final_samples/paper_samples*100:.1f}%\")\n",
    "    print(f\"    â€¢ Compounds: {final_compounds/paper_compounds*100:.1f}%\")\n",
    "    if isinstance(final_cells, int):\n",
    "        print(f\"    â€¢ Cell lines: {final_cells/paper_cells*100:.1f}%\")\n",
    "    \n",
    "    # 4. ä¿å­˜\n",
    "    if output_path is None:\n",
    "        input_path = Path(training_data_path)\n",
    "        output_path = input_path.parent / f\"{input_path.stem}_chemfiltered.pkl\"\n",
    "    else:\n",
    "        output_path = Path(output_path)\n",
    "    \n",
    "    print(f\"\\nğŸ’¾ Saving filtered data to: {output_path}\")\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(output_path, 'wb') as f:\n",
    "        pickle.dump(filtered_data, f, protocol=4)\n",
    "    \n",
    "    file_size_mb = output_path.stat().st_size / (1024**2)\n",
    "    print(f\"   âœ“ Saved successfully! ({file_size_mb:.1f} MB)\")\n",
    "    \n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"ä¸»ç¨‹åº - åº”ç”¨åŒ–å­¦è¿‡æ»¤å™¨\"\"\"\n",
    "    \n",
    "    # é…ç½®è·¯å¾„\n",
    "    data_dir = \"D:/ç§‘ç ”/Models/drugreflector/datasets/LINCS2020\"\n",
    "    processed_dir = \"D:/ç§‘ç ”/Models/drugreflector/processed_data\"\n",
    "    \n",
    "    training_data_path = Path(processed_dir) / \"training_data_lincs2020_optimized_1212_s.pkl\"\n",
    "    compound_info_path = Path(data_dir) / \"compoundinfo_beta.txt\"\n",
    "    output_path = Path(processed_dir) / \"training_data_lincs2020_chemfiltered_1212_s.pkl\"\n",
    "    \n",
    "    # NIBRè¿‡æ»¤å™¨CSVè·¯å¾„\n",
    "    # CSVæ–‡ä»¶åœ¨å½“å‰è„šæœ¬åŒçº§çš„chem_filteræ–‡ä»¶å¤¹ä¸‹\n",
    "    try:\n",
    "        # å°è¯•ä½¿ç”¨ __file__ï¼ˆåœ¨è„šæœ¬æ¨¡å¼ä¸‹ï¼‰\n",
    "        script_dir = Path(__file__).parent\n",
    "    except NameError:\n",
    "        # åœ¨äº¤äº’å¼ç¯å¢ƒä¸­ä½¿ç”¨å½“å‰å·¥ä½œç›®å½•\n",
    "        script_dir = Path(os.getcwd())\n",
    "    nibr_csv_path = script_dir / \"chem_filter\" / \"SubstructureFilter_HitTriaging_wPubChemExamples.csv\"\n",
    "    \n",
    "    # æ£€æŸ¥æ–‡ä»¶\n",
    "    if not training_data_path.exists():\n",
    "        print(f\"âŒ Training data not found: {training_data_path}\")\n",
    "        print(f\"   Please run data preprocessing first!\")\n",
    "        return\n",
    "    \n",
    "    if not compound_info_path.exists():\n",
    "        print(f\"âŒ Compound info not found: {compound_info_path}\")\n",
    "        return\n",
    "    \n",
    "    # æ£€æŸ¥NIBR CSVæ–‡ä»¶\n",
    "    if not nibr_csv_path.exists():\n",
    "        print(f\"âš ï¸  NIBR filter CSV not found: {nibr_csv_path}\")\n",
    "        print(f\"   Will use fallback NIBR filters (less accurate)\")\n",
    "        nibr_csv_path = None\n",
    "        \n",
    "    # åº”ç”¨è¿‡æ»¤å™¨\n",
    "    try:\n",
    "        filtered_data = apply_chemical_filters_to_training_data(\n",
    "            training_data_path=str(training_data_path),\n",
    "            compound_info_path=str(compound_info_path),\n",
    "            output_path=str(output_path)\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"âœ… CHEMICAL FILTERING COMPLETE!\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"ğŸ“ Final output: {output_path}\")\n",
    "        print(f\"ğŸ¯ Ready for training!\")\n",
    "        \n",
    "        return filtered_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"âŒ ERROR\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"   Type: {type(e).__name__}\")\n",
    "        print(f\"   Message: {e}\")\n",
    "        \n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    filtered_data = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0357564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ”¬ APPLYING CHEMICAL FILTERS TO TRAINING DATA\n",
      "================================================================================\n",
      "\n",
      "ğŸ“– Loading training data...\n",
      "   âœ“ Loaded: 945,908 samples, 14,669 compounds\n",
      "   âœ“ Training data contains: 14,669 unique compounds\n",
      "ğŸ“– Loading NIBR filters from: SubstructureFilter_HitTriaging_wPubChemExamples.csv\n",
      "   âœ“ Loaded 444 NIBR filter patterns\n",
      "   âœ“ Successfully added 444 NIBR filters to catalog\n",
      "================================================================================\n",
      "ğŸ§ª Chemical Filter Initialized\n",
      "================================================================================\n",
      "Based on DrugReflector SI (page 2)\n",
      "================================================================================\n",
      "\n",
      "ğŸ“– Loading compound information...\n",
      "   âœ“ Loaded 39,321 compounds\n",
      "   âœ“ Columns: ['pert_id', 'cmap_name', 'target', 'moa', 'canonical_smiles', 'inchi_key', 'compound_aliases']\n",
      "   âœ“ Using 'canonical_smiles' for molecular structures\n",
      "   âœ“ Compounds with valid SMILES: 33,531\n",
      "\n",
      "================================================================================\n",
      "ğŸ§ª APPLYING ALL CHEMICAL FILTERS\n",
      "================================================================================\n",
      "\n",
      "âš ï¸  Filtering only 14,669 compounds present in training data\n",
      "   Compounds to check: 19,100\n",
      "Initial compounds: 19,100\n",
      "\n",
      "================================================================================\n",
      "CHEMICAL FILTER 1: Molecular Weight (60-1000 Da)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:30:50] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:30:50] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:30:50] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:30:50] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:30:50] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:30:50] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:30:52] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:30:52] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:30:52] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:30:52] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:30:52] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:30:52] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:30:52] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:30:52] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:30:52] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:30:52] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:30:52] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:30:52] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:30:52] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:30:52] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:30:52] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:30:52] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:30:52] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:30:52] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:30:52] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:30:52] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:30:52] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:30:52] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:30:52] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:30:52] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:30:52] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:30:52] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ“ Passed: 14,212 compounds\n",
      "  âœ— Failed (out of MW range): 66 compounds\n",
      "  âš ï¸  No SMILES: 394 compounds\n",
      "  âš ï¸  Invalid SMILES: 16 compounds\n",
      "\n",
      "  MW distribution of failed compounds:\n",
      "    â€¢ Mean: 1400.6 Da\n",
      "    â€¢ Min: 59.1 Da\n",
      "    â€¢ Max: 2430.9 Da\n",
      "\n",
      "================================================================================\n",
      "CHEMICAL FILTER 2: Covalent Motifs (â‰¤1)\n",
      "================================================================================\n",
      "Checking 20 covalent SMARTS patterns...\n",
      "  âœ“ Successfully compiled 20 SMARTS patterns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:30:53] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:30:53] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:30:53] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:30:53] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:30:53] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:30:53] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:30:56] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:30:56] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:30:56] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:30:56] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:30:56] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:30:56] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:30:56] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:30:56] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:30:56] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:30:56] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:30:56] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:30:56] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:30:56] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:30:56] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:30:56] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:30:56] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:30:56] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:30:56] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:30:56] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:30:56] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:30:56] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:30:56] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:30:56] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:30:56] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:30:56] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:30:56] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ“ Passed: 13,908 compounds\n",
      "  âœ— Failed (>1 motifs): 602 compounds\n",
      "\n",
      "  Distribution of failed compounds:\n",
      "    â€¢ Mean motifs: 2.2\n",
      "    â€¢ Max motifs: 4\n",
      "\n",
      "================================================================================\n",
      "CHEMICAL FILTER 3: NIBR Structure Flags (â‰¤9)\n",
      "================================================================================\n",
      "Using official NIBR filter catalog (Schuffenhauer et al. 2020)\n",
      "  âœ“ Catalog contains 444 filter patterns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:30:58] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:30:58] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:30:58] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:30:58] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:30:59] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:30:59] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:31:39] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:31:39] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:31:39] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:31:39] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:31:39] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:31:39] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:31:39] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:31:39] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:31:41] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:31:41] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:31:41] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:31:41] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:31:41] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:31:41] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:31:42] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:31:42] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:31:42] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:31:42] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:31:43] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:31:43] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:31:43] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:31:43] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:31:43] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:31:43] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:31:43] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:31:43] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ“ Passed: 13,148 compounds\n",
      "  âœ— Failed (>9 flags): 1,972 compounds\n",
      "\n",
      "  Flag distribution (all compounds):\n",
      "    â€¢ Mean flags: 2.27\n",
      "    â€¢ Median flags: 0\n",
      "    â€¢ Max flags: 10\n",
      "\n",
      "  Flag count histogram:\n",
      "    â€¢ 0 flags: 6,103 compounds\n",
      "    â€¢ 1 flags: 1,070 compounds\n",
      "    â€¢ 2 flags: 111 compounds\n",
      "    â€¢ 3 flags: 12 compounds\n",
      "    â€¢ 10 flags: 1,972 compounds\n",
      "\n",
      "  Failed compounds statistics:\n",
      "    â€¢ Mean flags: 10.00\n",
      "    â€¢ Max flags: 10\n",
      "\n",
      "================================================================================\n",
      "CHEMICAL FILTER 4: BRENK Criteria\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:32:10] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:32:10] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:32:10] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:32:10] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:32:10] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:32:10] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:32:15] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:32:15] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:32:15] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:32:15] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:32:15] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:32:15] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:32:15] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:32:15] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:32:15] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:32:15] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:32:15] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:32:15] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:32:15] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:32:15] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:32:16] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:32:16] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:32:16] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:32:16] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:32:16] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:32:16] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:32:16] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:32:16] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:32:16] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:32:16] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n",
      "[11:32:16] SMILES Parse Error: syntax error while parsing: restricted\n",
      "[11:32:16] SMILES Parse Error: Failed parsing SMILES 'restricted' for input: 'restricted'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ“ Passed: 10,518 compounds\n",
      "  âœ— Failed (BRENK violations): 5,539 compounds\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š CHEMICAL FILTER SUMMARY\n",
      "================================================================================\n",
      "  Initial compounds: 19,100\n",
      "  Passed Filter 1 (MW): 14,212\n",
      "  Passed Filter 2 (Covalent): 13,908\n",
      "  Passed Filter 3 (NIBR): 13,148\n",
      "  Passed Filter 4 (BRENK): 10,518\n",
      "  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  âœ… Passed ALL filters: 10,107\n",
      "  âŒ Removed: 8,993\n",
      "  Retention rate: 52.9%\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š FILTERING TRAINING DATA\n",
      "================================================================================\n",
      "  Compounds in training data: 14,669\n",
      "  Compounds passed filters: 10,107\n",
      "  Compounds to remove: 4,562\n",
      "  Removing 436,902 samples from 4,562 compounds...\n",
      "\n",
      "================================================================================\n",
      "âœ… FILTERING COMPLETE\n",
      "================================================================================\n",
      "  Before chemical filters:\n",
      "    â€¢ Samples: 945,908\n",
      "    â€¢ Compounds: 14,669\n",
      "  After chemical filters:\n",
      "    â€¢ Samples: 509,006 (53.8%)\n",
      "    â€¢ Compounds: 10,107 (68.9%)\n",
      "    â€¢ Cell lines: 57\n",
      "  Removed:\n",
      "    â€¢ Samples: 436,902\n",
      "    â€¢ Compounds: 4,562\n",
      "\n",
      "ğŸ“Š Comparison with paper (SI page 2):\n",
      "  Paper: 425,242 obs, 9,597 compounds, 52 cells\n",
      "  Ours:  509,006 obs, 10,107 compounds, 57 cells\n",
      "  Match rate:\n",
      "    â€¢ Samples: 119.7%\n",
      "    â€¢ Compounds: 105.3%\n",
      "    â€¢ Cell lines: 109.6%\n",
      "\n",
      "ğŸ’¾ Saving filtered data to: D:\\ç§‘ç ”\\Models\\drugreflector\\processed_data\\training_data_lincs2020_chemfiltered_1201_l_scgpt.pkl\n",
      "   âœ“ Saved successfully! (2157.2 MB)\n",
      "\n",
      "================================================================================\n",
      "âœ… CHEMICAL FILTERING COMPLETE!\n",
      "================================================================================\n",
      "ğŸ“ Final output: D:\\ç§‘ç ”\\Models\\drugreflector\\processed_data\\training_data_lincs2020_chemfiltered_1201_l_scgpt.pkl\n",
      "ğŸ¯ Ready for training!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "LINCS 2020 Chemical Filters\n",
    "æ ¹æ®DrugReflectorè®ºæ–‡SIç¬¬2é¡µå®ç°åŒ–å­¦è¿‡æ»¤\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# RDKit imports\n",
    "try:\n",
    "    from rdkit import Chem\n",
    "    from rdkit.Chem import Descriptors, AllChem, FilterCatalog\n",
    "    from rdkit.Chem.FilterCatalog import FilterCatalogParams\n",
    "    RDKIT_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"âš ï¸  RDKit not installed. Install with: conda install -c conda-forge rdkit\")\n",
    "    RDKIT_AVAILABLE = False\n",
    "\n",
    "\n",
    "class ChemicalFilter:\n",
    "    \"\"\"\n",
    "    åŒ–å­¦è¿‡æ»¤å™¨ - æ ¹æ®DrugReflector SIå®ç°\n",
    "    \n",
    "    Filters applied (SI page 2):\n",
    "    1. Molecular weight: 60-1000 Da (inclusive)\n",
    "    2. No more than 1 covalent motif (SMARTS-defined)\n",
    "    3. No more than 9 NIBR structure flags\n",
    "    4. Pass BRENK criteria\n",
    "    5. Must not match 30 SMARTS patterns (not disclosed)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, compound_info_path: str, nibr_filter_csv: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–åŒ–å­¦è¿‡æ»¤å™¨\n",
    "        \n",
    "        å‚æ•°ï¼š\n",
    "            compound_info_path: compoundinfo_beta.txtè·¯å¾„\n",
    "            nibr_filter_csv: NIBRè¿‡æ»¤å™¨CSVæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰\n",
    "        \"\"\"\n",
    "        if not RDKIT_AVAILABLE:\n",
    "            raise ImportError(\"RDKit is required for chemical filtering\")\n",
    "        \n",
    "        self.compound_info_path = Path(compound_info_path)\n",
    "        self.compound_info = None\n",
    "        self.filtered_compounds = set()\n",
    "        \n",
    "        # åˆå§‹åŒ–BRENKè¿‡æ»¤å™¨\n",
    "        self.brenk_catalog = self._init_brenk_filter()\n",
    "        \n",
    "        # åˆå§‹åŒ–å…±ä»·åŸºå›¢SMARTS\n",
    "        self.covalent_smarts = self._init_covalent_smarts()\n",
    "        \n",
    "        # ğŸ”¥ æ–°å¢ï¼šåˆå§‹åŒ–NIBRè¿‡æ»¤å™¨ç›®å½•\n",
    "        self.nibr_catalog = self._init_nibr_filter(nibr_filter_csv)\n",
    "        \n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"ğŸ§ª Chemical Filter Initialized\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Based on DrugReflector SI (page 2)\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    def _init_brenk_filter(self):\n",
    "        \"\"\"åˆå§‹åŒ–BRENKè¿‡æ»¤å™¨\"\"\"\n",
    "        params = FilterCatalogParams()\n",
    "        params.AddCatalog(FilterCatalogParams.FilterCatalogs.BRENK)\n",
    "        return FilterCatalog.FilterCatalog(params)\n",
    "    \n",
    "    def _init_covalent_smarts(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        å…±ä»·åŸºå›¢SMARTSæ¨¡å¼\n",
    "        \n",
    "        å‚è€ƒï¼š\n",
    "        - Backman et al. 2019 (ChEMBL structural alerts)\n",
    "        - Common covalent warheads in drug discovery\n",
    "        \"\"\"\n",
    "        return [\n",
    "            # Michael acceptors\n",
    "            'C=CC(=O)',  # Î±,Î²-unsaturated carbonyl\n",
    "            'C=CC(=O)N',  # acrylamide\n",
    "            'C=CC#N',  # acrylonitrile\n",
    "            \n",
    "            # Electrophilic carbonyls\n",
    "            '[C;!R](=O)Cl',  # acyl chloride\n",
    "            '[C;!R](=O)O[C;!R](=O)',  # anhydride\n",
    "            'C(=O)N=[N+]=[N-]',  # acyl azide\n",
    "            \n",
    "            # Epoxides and aziridines\n",
    "            'C1OC1',  # epoxide\n",
    "            'C1NC1',  # aziridine\n",
    "            \n",
    "            # Haloacetamides\n",
    "            'ClCC(=O)N',  # chloroacetamide\n",
    "            'BrCC(=O)N',  # bromoacetamide\n",
    "            \n",
    "            # Aldehydes (reactive)\n",
    "            '[CH;!R]=O',  # aldehyde\n",
    "            \n",
    "            # Isocyanates/isothiocyanates\n",
    "            'N=C=O',  # isocyanate\n",
    "            'N=C=S',  # isothiocyanate\n",
    "            \n",
    "            # Sulfonyl halides\n",
    "            'S(=O)(=O)Cl',  # sulfonyl chloride\n",
    "            'S(=O)(=O)F',  # sulfonyl fluoride\n",
    "            \n",
    "            # Nitriles (activated)\n",
    "            '[C;!R]#N',  # nitrile (non-aromatic)\n",
    "            \n",
    "            # Peroxides\n",
    "            'OO',  # peroxide\n",
    "            \n",
    "            # Beta-lactams (strained)\n",
    "            'C1(=O)NCC1',  # beta-lactam\n",
    "            \n",
    "            # Activated esters\n",
    "            'C(=O)OC(=O)',  # activated ester\n",
    "            \n",
    "            # Quinones\n",
    "            'C1=CC(=O)C=CC1=O',  # quinone\n",
    "        ]\n",
    "    \n",
    "    def _init_nibr_filter(self, csv_path: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–NIBRè¿‡æ»¤å™¨ç›®å½•ï¼ˆåŸºäºå®˜æ–¹444ä¸ªSMARTSï¼‰\n",
    "        \n",
    "        å‚æ•°ï¼š\n",
    "            csv_path: NIBRè¿‡æ»¤å™¨CSVæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰\n",
    "            \n",
    "        è¿”å›ï¼š\n",
    "            FilterCatalogå¯¹è±¡\n",
    "        \"\"\"\n",
    "        # ğŸ”¥ ç¡®å®šCSVæ–‡ä»¶è·¯å¾„\n",
    "        if csv_path is None:\n",
    "            # é»˜è®¤è·¯å¾„ï¼šå½“å‰è„šæœ¬ç›®å½•ä¸‹çš„chem_filteræ–‡ä»¶å¤¹\n",
    "            try:\n",
    "                # å°è¯•ä½¿ç”¨ __file__ï¼ˆåœ¨è„šæœ¬æ¨¡å¼ä¸‹ï¼‰\n",
    "                script_dir = Path(__file__).parent\n",
    "            except NameError:\n",
    "                # åœ¨äº¤äº’å¼ç¯å¢ƒä¸­ä½¿ç”¨å½“å‰å·¥ä½œç›®å½•\n",
    "                script_dir = Path(os.getcwd())\n",
    "            csv_path = script_dir / \"chem_filter\" / \"SubstructureFilter_HitTriaging_wPubChemExamples.csv\"\n",
    "        else:\n",
    "            csv_path = Path(csv_path)\n",
    "        \n",
    "        if not csv_path.exists():\n",
    "            print(f\"âš ï¸  NIBR filter CSV not found: {csv_path}\")\n",
    "            print(f\"   Using fallback: empty NIBR catalog\")\n",
    "            return FilterCatalog.FilterCatalog()\n",
    "        \n",
    "        print(f\"ğŸ“– Loading NIBR filters from: {csv_path.name}\")\n",
    "        \n",
    "        # è¯»å–CSVå¹¶æ„å»ºFilterCatalog\n",
    "        try:\n",
    "            nibr_df = pd.read_csv(csv_path)\n",
    "            print(f\"   âœ“ Loaded {len(nibr_df)} NIBR filter patterns\")\n",
    "            \n",
    "            nibr_catalog = FilterCatalog.FilterCatalog()\n",
    "            \n",
    "            n_added = 0\n",
    "            for idx, row in nibr_df.iterrows():\n",
    "                try:\n",
    "                    # æå–å‚æ•°\n",
    "                    pattern_name = row['PATTERN_NAME']\n",
    "                    smarts = row['SMARTS']\n",
    "                    min_count = 1 if row['MIN_COUNT'] == 0 else int(row['MIN_COUNT'])\n",
    "                    severity = int(row['SEVERITY_SCORE'])\n",
    "                    covalent = int(row['COVALENT'])\n",
    "                    special_mol = int(row['SPECIAL_MOL'])\n",
    "                    set_name = row['SET_NAME']\n",
    "                    \n",
    "                    # æ„å»ºè¿‡æ»¤å™¨åç§°ï¼ˆä¸å®˜æ–¹ä»£ç ä¸€è‡´ï¼‰\n",
    "                    filter_name = f\"{pattern_name}_min({min_count})__{severity}__{covalent}__{special_mol}\"\n",
    "                    \n",
    "                    # åˆ›å»ºSMARTSåŒ¹é…å™¨\n",
    "                    matcher = FilterCatalog.SmartsMatcher(filter_name, smarts, min_count)\n",
    "                    \n",
    "                    # æ·»åŠ åˆ°ç›®å½•\n",
    "                    entry = FilterCatalog.FilterCatalogEntry(filter_name, matcher)\n",
    "                    entry.SetProp('Scope', set_name)\n",
    "                    entry.SetProp('Severity', str(severity))\n",
    "                    nibr_catalog.AddEntry(entry)\n",
    "                    \n",
    "                    n_added += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"   âš ï¸  Failed to add filter {idx}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            print(f\"   âœ“ Successfully added {n_added} NIBR filters to catalog\")\n",
    "            return nibr_catalog\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Error loading NIBR filters: {e}\")\n",
    "            print(f\"   Using fallback: empty NIBR catalog\")\n",
    "            return FilterCatalog.FilterCatalog()\n",
    "    \n",
    "    def load_compound_info(self):\n",
    "        \"\"\"åŠ è½½åŒ–åˆç‰©ä¿¡æ¯\"\"\"\n",
    "        print(f\"ğŸ“– Loading compound information...\")\n",
    "        \n",
    "        if not self.compound_info_path.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"Compound info file not found: {self.compound_info_path}\"\n",
    "            )\n",
    "        \n",
    "        self.compound_info = pd.read_csv(self.compound_info_path, sep='\\t')\n",
    "        \n",
    "        print(f\"   âœ“ Loaded {len(self.compound_info):,} compounds\")\n",
    "        print(f\"   âœ“ Columns: {list(self.compound_info.columns)}\")\n",
    "        \n",
    "        # æ£€æŸ¥SMILESåˆ—\n",
    "        smiles_col = None\n",
    "        for col in ['canonical_smiles', 'smiles', 'SMILES']:\n",
    "            if col in self.compound_info.columns:\n",
    "                smiles_col = col\n",
    "                break\n",
    "        \n",
    "        if smiles_col is None:\n",
    "            raise ValueError(\n",
    "                f\"SMILES column not found. Available columns: {list(self.compound_info.columns)}\"\n",
    "            )\n",
    "        \n",
    "        self.smiles_col = smiles_col\n",
    "        print(f\"   âœ“ Using '{smiles_col}' for molecular structures\")\n",
    "        \n",
    "        # æ£€æŸ¥æœ‰æ•ˆSMILESæ•°é‡\n",
    "        valid_smiles = self.compound_info[smiles_col].notna().sum()\n",
    "        print(f\"   âœ“ Compounds with valid SMILES: {valid_smiles:,}\")\n",
    "        \n",
    "        return self.compound_info\n",
    "    \n",
    "    def filter_1_molecular_weight(self, min_mw=60, max_mw=1000) -> set:\n",
    "        \"\"\"\n",
    "        Filter 1: åˆ†å­é‡è¿‡æ»¤ (60-1000 Da)\n",
    "        \n",
    "        å‚æ•°ï¼š\n",
    "            min_mw: æœ€å°åˆ†å­é‡ï¼ˆé»˜è®¤60ï¼‰\n",
    "            max_mw: æœ€å¤§åˆ†å­é‡ï¼ˆé»˜è®¤1000ï¼‰\n",
    "        \n",
    "        è¿”å›ï¼š\n",
    "            é€šè¿‡è¿‡æ»¤çš„åŒ–åˆç‰©IDé›†åˆ\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"CHEMICAL FILTER 1: Molecular Weight ({min_mw}-{max_mw} Da)\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        passed = set()\n",
    "        failed_mw = []\n",
    "        no_smiles = 0\n",
    "        invalid_smiles = 0\n",
    "        \n",
    "        for idx, row in self.compound_info.iterrows():\n",
    "            pert_id = row['pert_id']\n",
    "            smiles = row[self.smiles_col]\n",
    "            \n",
    "            # æ£€æŸ¥SMILESæ˜¯å¦å­˜åœ¨\n",
    "            if pd.isna(smiles) or smiles == '':\n",
    "                no_smiles += 1\n",
    "                continue\n",
    "            \n",
    "            # è§£æåˆ†å­\n",
    "            mol = Chem.MolFromSmiles(str(smiles))\n",
    "            if mol is None:\n",
    "                invalid_smiles += 1\n",
    "                continue\n",
    "            \n",
    "            # è®¡ç®—åˆ†å­é‡\n",
    "            mw = Descriptors.MolWt(mol)\n",
    "            \n",
    "            if min_mw <= mw <= max_mw:\n",
    "                passed.add(pert_id)\n",
    "            else:\n",
    "                failed_mw.append((pert_id, mw))\n",
    "        \n",
    "        print(f\"  âœ“ Passed: {len(passed):,} compounds\")\n",
    "        print(f\"  âœ— Failed (out of MW range): {len(failed_mw):,} compounds\")\n",
    "        print(f\"  âš ï¸  No SMILES: {no_smiles:,} compounds\")\n",
    "        print(f\"  âš ï¸  Invalid SMILES: {invalid_smiles:,} compounds\")\n",
    "        \n",
    "        if failed_mw:\n",
    "            failed_df = pd.DataFrame(failed_mw, columns=['pert_id', 'MW'])\n",
    "            print(f\"\\n  MW distribution of failed compounds:\")\n",
    "            print(f\"    â€¢ Mean: {failed_df['MW'].mean():.1f} Da\")\n",
    "            print(f\"    â€¢ Min: {failed_df['MW'].min():.1f} Da\")\n",
    "            print(f\"    â€¢ Max: {failed_df['MW'].max():.1f} Da\")\n",
    "        \n",
    "        return passed\n",
    "    \n",
    "    def filter_2_covalent_motifs(self, max_motifs=1) -> set:\n",
    "        \"\"\"\n",
    "        Filter 2: å…±ä»·åŸºå›¢è¿‡æ»¤ (â‰¤1ä¸ª)\n",
    "        \n",
    "        å‚æ•°ï¼š\n",
    "            max_motifs: å…è®¸çš„æœ€å¤§å…±ä»·åŸºå›¢æ•°ï¼ˆé»˜è®¤1ï¼‰\n",
    "        \n",
    "        è¿”å›ï¼š\n",
    "            é€šè¿‡è¿‡æ»¤çš„åŒ–åˆç‰©IDé›†åˆ\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"CHEMICAL FILTER 2: Covalent Motifs (â‰¤{max_motifs})\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Checking {len(self.covalent_smarts)} covalent SMARTS patterns...\")\n",
    "        \n",
    "        passed = set()\n",
    "        failed_counts = []\n",
    "        \n",
    "        # é¢„ç¼–è¯‘SMARTS\n",
    "        smarts_mols = []\n",
    "        for smarts in self.covalent_smarts:\n",
    "            try:\n",
    "                smarts_mol = Chem.MolFromSmarts(smarts)\n",
    "                if smarts_mol:\n",
    "                    smarts_mols.append(smarts_mol)\n",
    "            except:\n",
    "                print(f\"  âš ï¸  Invalid SMARTS: {smarts}\")\n",
    "        \n",
    "        print(f\"  âœ“ Successfully compiled {len(smarts_mols)} SMARTS patterns\")\n",
    "        \n",
    "        for idx, row in self.compound_info.iterrows():\n",
    "            pert_id = row['pert_id']\n",
    "            smiles = row[self.smiles_col]\n",
    "            \n",
    "            if pd.isna(smiles):\n",
    "                continue\n",
    "            \n",
    "            mol = Chem.MolFromSmiles(str(smiles))\n",
    "            if mol is None:\n",
    "                continue\n",
    "            \n",
    "            # è®¡æ•°åŒ¹é…çš„å…±ä»·åŸºå›¢\n",
    "            motif_count = 0\n",
    "            for smarts_mol in smarts_mols:\n",
    "                if mol.HasSubstructMatch(smarts_mol):\n",
    "                    motif_count += 1\n",
    "            \n",
    "            if motif_count <= max_motifs:\n",
    "                passed.add(pert_id)\n",
    "            else:\n",
    "                failed_counts.append(motif_count)\n",
    "        \n",
    "        print(f\"  âœ“ Passed: {len(passed):,} compounds\")\n",
    "        print(f\"  âœ— Failed (>{max_motifs} motifs): {len(failed_counts):,} compounds\")\n",
    "        \n",
    "        if failed_counts:\n",
    "            print(f\"\\n  Distribution of failed compounds:\")\n",
    "            print(f\"    â€¢ Mean motifs: {np.mean(failed_counts):.1f}\")\n",
    "            print(f\"    â€¢ Max motifs: {max(failed_counts)}\")\n",
    "        \n",
    "        return passed\n",
    "    \n",
    "    def filter_3_nibr_flags(self, max_flags=9) -> set:\n",
    "        \"\"\"\n",
    "        Filter 3: NIBRç»“æ„æ ‡è®° (â‰¤9ä¸ª)\n",
    "        \n",
    "        ä½¿ç”¨å®˜æ–¹NIBR 444ä¸ªSMARTSè¿‡æ»¤å™¨\n",
    "        å‚è€ƒï¼šSchuffenhauer et al. 2020, J. Med. Chem.\n",
    "        \n",
    "        å‚æ•°ï¼š\n",
    "            max_flags: å…è®¸çš„æœ€å¤§æ ‡è®°æ•°ï¼ˆé»˜è®¤9ï¼Œç¬¦åˆSIè¦æ±‚ï¼‰\n",
    "        \n",
    "        è¿”å›ï¼š\n",
    "            é€šè¿‡è¿‡æ»¤çš„åŒ–åˆç‰©IDé›†åˆ\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"CHEMICAL FILTER 3: NIBR Structure Flags (â‰¤{max_flags})\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Using official NIBR filter catalog (Schuffenhauer et al. 2020)\")\n",
    "        \n",
    "        if self.nibr_catalog is None or self.nibr_catalog.GetNumEntries() == 0:\n",
    "            print(f\"  âš ï¸  NIBR catalog is empty, skipping this filter\")\n",
    "            # è¿”å›æ‰€æœ‰åŒ–åˆç‰©ï¼ˆç›¸å½“äºä¸è¿‡æ»¤ï¼‰\n",
    "            return set(self.compound_info['pert_id'].values)\n",
    "        \n",
    "        print(f\"  âœ“ Catalog contains {self.nibr_catalog.GetNumEntries()} filter patterns\")\n",
    "        \n",
    "        passed = set()\n",
    "        failed_counts = []\n",
    "        flag_distribution = []\n",
    "        \n",
    "        for idx, row in self.compound_info.iterrows():\n",
    "            pert_id = row['pert_id']\n",
    "            smiles = row[self.smiles_col]\n",
    "            \n",
    "            if pd.isna(smiles):\n",
    "                continue\n",
    "            \n",
    "            mol = Chem.MolFromSmiles(str(smiles))\n",
    "            if mol is None:\n",
    "                continue\n",
    "            \n",
    "            # ğŸ”¥ ä½¿ç”¨FilterCatalogè·å–æ‰€æœ‰åŒ¹é…\n",
    "            matches = self.nibr_catalog.GetMatches(mol)\n",
    "            \n",
    "            if len(matches) == 0:\n",
    "                # æ²¡æœ‰åŒ¹é…ä»»ä½•flagï¼Œé€šè¿‡\n",
    "                flag_count = 0\n",
    "                passed.add(pert_id)\n",
    "            else:\n",
    "                # ğŸ”¥ è®¡ç®—flagæ•°é‡ï¼ˆå‚è€ƒå®˜æ–¹ä»£ç é€»è¾‘ï¼‰\n",
    "                # åªç»Ÿè®¡severity=1çš„FLAGï¼Œseverity=2çš„EXCLUDEå•ç‹¬å¤„ç†\n",
    "                severity_scores = []\n",
    "                for entry in matches:\n",
    "                    # æå–severityä¿¡æ¯\n",
    "                    desc = entry.GetDescription()\n",
    "                    # æ ¼å¼ï¼špattern_name_min(X)__severity__covalent__special_mol\n",
    "                    parts = desc.split('__')\n",
    "                    if len(parts) >= 2:\n",
    "                        severity = int(parts[1])\n",
    "                        severity_scores.append(severity)\n",
    "                \n",
    "                # ğŸ”¥ å…³é”®é€»è¾‘ï¼ˆä¸å®˜æ–¹ä»£ç ä¸€è‡´ï¼‰ï¼š\n",
    "                # å¦‚æœæœ‰severity=2ï¼ˆEXCLUDEï¼‰ï¼Œç›´æ¥æ ‡è®°ä¸º10ï¼ˆå¿…é¡»æ’é™¤ï¼‰\n",
    "                if 2 in severity_scores:\n",
    "                    flag_count = 10  # è¶…è¿‡é˜ˆå€¼ï¼Œå¿…é¡»æ’é™¤\n",
    "                else:\n",
    "                    # å¦åˆ™ï¼Œflag_count = severity=1çš„æ•°é‡\n",
    "                    flag_count = sum(1 for s in severity_scores if s == 1)\n",
    "                \n",
    "                flag_distribution.append(flag_count)\n",
    "                \n",
    "                if flag_count <= max_flags:\n",
    "                    passed.add(pert_id)\n",
    "                else:\n",
    "                    failed_counts.append(flag_count)\n",
    "        \n",
    "        print(f\"  âœ“ Passed: {len(passed):,} compounds\")\n",
    "        print(f\"  âœ— Failed (>{max_flags} flags): {len(failed_counts):,} compounds\")\n",
    "        \n",
    "        if flag_distribution:\n",
    "            print(f\"\\n  Flag distribution (all compounds):\")\n",
    "            print(f\"    â€¢ Mean flags: {np.mean(flag_distribution):.2f}\")\n",
    "            print(f\"    â€¢ Median flags: {np.median(flag_distribution):.0f}\")\n",
    "            print(f\"    â€¢ Max flags: {max(flag_distribution)}\")\n",
    "            \n",
    "            # ç»Ÿè®¡å„flagæ•°é‡çš„åˆ†å¸ƒ\n",
    "            from collections import Counter\n",
    "            flag_counts = Counter(flag_distribution)\n",
    "            print(f\"\\n  Flag count histogram:\")\n",
    "            for count in sorted(flag_counts.keys())[:15]:  # åªæ˜¾ç¤ºå‰15ä¸ª\n",
    "                n_compounds = flag_counts[count]\n",
    "                print(f\"    â€¢ {count} flags: {n_compounds:,} compounds\")\n",
    "        \n",
    "        if failed_counts:\n",
    "            print(f\"\\n  Failed compounds statistics:\")\n",
    "            print(f\"    â€¢ Mean flags: {np.mean(failed_counts):.2f}\")\n",
    "            print(f\"    â€¢ Max flags: {max(failed_counts)}\")\n",
    "        \n",
    "        return passed\n",
    "    \n",
    "    def filter_4_brenk(self) -> set:\n",
    "        \"\"\"\n",
    "        Filter 4: BRENKè¿‡æ»¤å™¨\n",
    "        \n",
    "        BRENKè¿‡æ»¤å™¨è¯†åˆ«ä¸è‰¯çš„åŒ–å­¦åŸºå›¢å’Œååº”æ€§å®˜èƒ½å›¢\n",
    "        \n",
    "        è¿”å›ï¼š\n",
    "            é€šè¿‡è¿‡æ»¤çš„åŒ–åˆç‰©IDé›†åˆ\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"CHEMICAL FILTER 4: BRENK Criteria\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        passed = set()\n",
    "        failed = []\n",
    "        \n",
    "        for idx, row in self.compound_info.iterrows():\n",
    "            pert_id = row['pert_id']\n",
    "            smiles = row[self.smiles_col]\n",
    "            \n",
    "            if pd.isna(smiles):\n",
    "                continue\n",
    "            \n",
    "            mol = Chem.MolFromSmiles(str(smiles))\n",
    "            if mol is None:\n",
    "                continue\n",
    "            \n",
    "            # æ£€æŸ¥BRENKè¿‡æ»¤å™¨\n",
    "            matches = self.brenk_catalog.GetMatches(mol)\n",
    "            \n",
    "            if len(matches) == 0:\n",
    "                passed.add(pert_id)\n",
    "            else:\n",
    "                failed.append((pert_id, len(matches)))\n",
    "        \n",
    "        print(f\"  âœ“ Passed: {len(passed):,} compounds\")\n",
    "        print(f\"  âœ— Failed (BRENK violations): {len(failed):,} compounds\")\n",
    "        \n",
    "        return passed\n",
    "    \n",
    "    def apply_all_filters(self, target_compounds: Optional[set] = None) -> set:\n",
    "        \"\"\"\n",
    "        åº”ç”¨æ‰€æœ‰åŒ–å­¦è¿‡æ»¤å™¨\n",
    "        \n",
    "        è¿”å›ï¼š\n",
    "            target_compounds: éœ€è¦è¿‡æ»¤çš„åŒ–åˆç‰©é›†åˆï¼ˆå¦‚æœæä¾›ï¼Œåˆ™åªè¿‡æ»¤è¿™äº›åŒ–åˆç‰©ï¼‰\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ğŸ§ª APPLYING ALL CHEMICAL FILTERS\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        if self.compound_info is None:\n",
    "            self.load_compound_info()\n",
    "        \n",
    "        #  å¦‚æœæŒ‡å®šäº†ç›®æ ‡åŒ–åˆç‰©ï¼Œåªè¿‡æ»¤è¿™äº›\n",
    "        if target_compounds is not None:\n",
    "            print(f\"\\nâš ï¸  Filtering only {len(target_compounds):,} compounds present in training data\")\n",
    "            # ä¸´æ—¶ä¿å­˜åŸå§‹æ•°æ®\n",
    "            original_compound_info = self.compound_info.copy()\n",
    "            # åªä¿ç•™ç›®æ ‡åŒ–åˆç‰©\n",
    "            self.compound_info = self.compound_info[\n",
    "                self.compound_info['pert_id'].isin(target_compounds)\n",
    "            ]\n",
    "            print(f\"   Compounds to check: {len(self.compound_info):,}\")\n",
    "        \n",
    "        initial_compounds = len(self.compound_info)\n",
    "        print(f\"Initial compounds: {initial_compounds:,}\")\n",
    "        \n",
    "        # åº”ç”¨å„ä¸ªè¿‡æ»¤å™¨\n",
    "        passed_mw = self.filter_1_molecular_weight()\n",
    "        passed_covalent = self.filter_2_covalent_motifs()\n",
    "        passed_nibr = self.filter_3_nibr_flags()\n",
    "        passed_brenk = self.filter_4_brenk()\n",
    "        \n",
    "        # å–äº¤é›†\n",
    "        passed_all = passed_mw & passed_covalent & passed_nibr & passed_brenk\n",
    "        \n",
    "        # å¦‚æœæ˜¯é’ˆå¯¹ç›®æ ‡åŒ–åˆç‰©çš„è¿‡æ»¤ï¼Œæ¢å¤åŸå§‹æ•°æ®\n",
    "        if target_compounds is not None:\n",
    "            self.compound_info = original_compound_info\n",
    "        \n",
    "        # ç»Ÿè®¡\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ğŸ“Š CHEMICAL FILTER SUMMARY\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"  Initial compounds: {initial_compounds:,}\")\n",
    "        print(f\"  Passed Filter 1 (MW): {len(passed_mw):,}\")\n",
    "        print(f\"  Passed Filter 2 (Covalent): {len(passed_covalent):,}\")\n",
    "        print(f\"  Passed Filter 3 (NIBR): {len(passed_nibr):,}\")\n",
    "        print(f\"  Passed Filter 4 (BRENK): {len(passed_brenk):,}\")\n",
    "        print(f\"  {'â”€'*80}\")\n",
    "        print(f\"  âœ… Passed ALL filters: {len(passed_all):,}\")\n",
    "        print(f\"  âŒ Removed: {initial_compounds - len(passed_all):,}\")\n",
    "        print(f\"  Retention rate: {len(passed_all)/initial_compounds*100:.1f}%\")\n",
    "        \n",
    "        self.filtered_compounds = passed_all\n",
    "        return passed_all\n",
    "\n",
    "\n",
    "def apply_chemical_filters_to_training_data(\n",
    "    training_data_path: str,\n",
    "    compound_info_path: str,\n",
    "    output_path: Optional[str] = None,\n",
    "    nibr_filter_csv: Optional[str] = None\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    å¯¹è®­ç»ƒæ•°æ®åº”ç”¨åŒ–å­¦è¿‡æ»¤å™¨\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        training_data_path: è®­ç»ƒæ•°æ®pklæ–‡ä»¶è·¯å¾„\n",
    "        compound_info_path: compoundinfo_beta.txtè·¯å¾„\n",
    "        output_path: è¾“å‡ºè·¯å¾„ï¼ˆå¯é€‰ï¼‰\n",
    "        nibr_filter_csv: NIBRè¿‡æ»¤å™¨CSVè·¯å¾„ï¼ˆå¯é€‰ï¼‰\n",
    "    \n",
    "    è¿”å›ï¼š\n",
    "        è¿‡æ»¤åçš„è®­ç»ƒæ•°æ®å­—å…¸\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ğŸ”¬ APPLYING CHEMICAL FILTERS TO TRAINING DATA\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # 1. åŠ è½½è®­ç»ƒæ•°æ®\n",
    "    print(f\"ğŸ“– Loading training data...\")\n",
    "    with open(training_data_path, 'rb') as f:\n",
    "        training_data = pickle.load(f)\n",
    "    \n",
    "    initial_samples = len(training_data['X'])\n",
    "    initial_compounds = len(training_data['compound_names'])\n",
    "    \n",
    "    print(f\"   âœ“ Loaded: {initial_samples:,} samples, {initial_compounds:,} compounds\")\n",
    "    \n",
    "    # è·å–è®­ç»ƒæ•°æ®ä¸­çš„åŒ–åˆç‰©\n",
    "    metadata = training_data['sample_meta']\n",
    "    training_compounds = set(metadata['pert_id'].unique())\n",
    "    print(f\"   âœ“ Training data contains: {len(training_compounds):,} unique compounds\")\n",
    "    \n",
    "    # 2. åº”ç”¨åŒ–å­¦è¿‡æ»¤å™¨ï¼ˆåªé’ˆå¯¹è®­ç»ƒæ•°æ®ä¸­çš„åŒ–åˆç‰©ï¼‰\n",
    "    filter_obj = ChemicalFilter(compound_info_path, nibr_filter_csv=nibr_filter_csv)  # ä¼ é€’å‚æ•°\n",
    "    filter_obj.load_compound_info()\n",
    "    valid_compounds = filter_obj.apply_all_filters(target_compounds=training_compounds)\n",
    "    \n",
    "    # 3. è¿‡æ»¤è®­ç»ƒæ•°æ®\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ğŸ“Š FILTERING TRAINING DATA\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # valid_compoundsç°åœ¨æ˜¯training_compoundsçš„å­é›†\n",
    "    print(f\"  Compounds in training data: {len(training_compounds):,}\")\n",
    "    print(f\"  Compounds passed filters: {len(valid_compounds):,}\")\n",
    "    print(f\"  Compounds to remove: {len(training_compounds - valid_compounds):,}\")\n",
    "    \n",
    "    valid_mask = metadata['pert_id'].isin(valid_compounds)\n",
    "    \n",
    "    n_removed_samples = (~valid_mask).sum()\n",
    "    n_removed_compounds = len(training_compounds - valid_compounds)  # ä»è®­ç»ƒæ•°æ®ä¸­ç§»é™¤çš„\n",
    "\n",
    "    print(f\"  Removing {n_removed_samples:,} samples from {n_removed_compounds:,} compounds...\")\n",
    "    \n",
    "    # åˆ›å»ºæ–°çš„è®­ç»ƒæ•°æ®\n",
    "    filtered_data = {\n",
    "        'X': training_data['X'][valid_mask],\n",
    "        'y': None,  # éœ€è¦é‡æ–°ç¼–ç \n",
    "        'folds': training_data['folds'][valid_mask],\n",
    "        'sample_meta': metadata[valid_mask].reset_index(drop=True),\n",
    "        'metadata': metadata[valid_mask].reset_index(drop=True),\n",
    "        'gene_names': training_data['gene_names'],\n",
    "        'compound_names': None,  # éœ€è¦æ›´æ–°\n",
    "        'pert_to_idx': None  # éœ€è¦é‡æ–°æ„å»º\n",
    "    }\n",
    "    \n",
    "    # é‡æ–°æ„å»ºåŒ–åˆç‰©æ˜ å°„å’Œæ ‡ç­¾\n",
    "    new_compound_names = sorted(list(valid_compounds))\n",
    "    new_pert_to_idx = {pert: idx for idx, pert in enumerate(new_compound_names)}\n",
    "    new_labels = np.array([\n",
    "        new_pert_to_idx[pert] \n",
    "        for pert in filtered_data['sample_meta']['pert_id']\n",
    "    ], dtype=np.int32)\n",
    "    \n",
    "    filtered_data['compound_names'] = new_compound_names\n",
    "    filtered_data['pert_to_idx'] = new_pert_to_idx\n",
    "    filtered_data['y'] = new_labels\n",
    "    \n",
    "    # ç»Ÿè®¡\n",
    "    final_samples = len(filtered_data['X'])\n",
    "    final_compounds = len(new_compound_names)\n",
    "    \n",
    "    if 'cell_iname' in filtered_data['sample_meta'].columns:\n",
    "        final_cells = filtered_data['sample_meta']['cell_iname'].nunique()\n",
    "    else:\n",
    "        final_cells = 'Unknown'\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"âœ… FILTERING COMPLETE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"  Before chemical filters:\")\n",
    "    print(f\"    â€¢ Samples: {initial_samples:,}\")\n",
    "    print(f\"    â€¢ Compounds: {initial_compounds:,}\")\n",
    "    print(f\"  After chemical filters:\")\n",
    "    print(f\"    â€¢ Samples: {final_samples:,} ({final_samples/initial_samples*100:.1f}%)\")\n",
    "    print(f\"    â€¢ Compounds: {final_compounds:,} ({final_compounds/initial_compounds*100:.1f}%)\")\n",
    "    print(f\"    â€¢ Cell lines: {final_cells}\")\n",
    "    print(f\"  Removed:\")\n",
    "    print(f\"    â€¢ Samples: {n_removed_samples:,}\")\n",
    "    print(f\"    â€¢ Compounds: {n_removed_compounds:,}\")\n",
    "    \n",
    "    # ä¸è®ºæ–‡å¯¹æ¯”\n",
    "    paper_samples = 425242\n",
    "    paper_compounds = 9597\n",
    "    paper_cells = 52\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Comparison with paper (SI page 2):\")\n",
    "    print(f\"  Paper: {paper_samples:,} obs, {paper_compounds:,} compounds, {paper_cells} cells\")\n",
    "    print(f\"  Ours:  {final_samples:,} obs, {final_compounds:,} compounds, {final_cells} cells\")\n",
    "    print(f\"  Match rate:\")\n",
    "    print(f\"    â€¢ Samples: {final_samples/paper_samples*100:.1f}%\")\n",
    "    print(f\"    â€¢ Compounds: {final_compounds/paper_compounds*100:.1f}%\")\n",
    "    if isinstance(final_cells, int):\n",
    "        print(f\"    â€¢ Cell lines: {final_cells/paper_cells*100:.1f}%\")\n",
    "    \n",
    "    # 4. ä¿å­˜\n",
    "    if output_path is None:\n",
    "        input_path = Path(training_data_path)\n",
    "        output_path = input_path.parent / f\"{input_path.stem}_chemfiltered.pkl\"\n",
    "    else:\n",
    "        output_path = Path(output_path)\n",
    "    \n",
    "    print(f\"\\nğŸ’¾ Saving filtered data to: {output_path}\")\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(output_path, 'wb') as f:\n",
    "        pickle.dump(filtered_data, f, protocol=4)\n",
    "    \n",
    "    file_size_mb = output_path.stat().st_size / (1024**2)\n",
    "    print(f\"   âœ“ Saved successfully! ({file_size_mb:.1f} MB)\")\n",
    "    \n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"ä¸»ç¨‹åº - åº”ç”¨åŒ–å­¦è¿‡æ»¤å™¨\"\"\"\n",
    "    \n",
    "    # é…ç½®è·¯å¾„\n",
    "    data_dir = \"D:/ç§‘ç ”/Models/drugreflector/datasets/LINCS2020\"\n",
    "    processed_dir = \"D:/ç§‘ç ”/Models/drugreflector/processed_data\"\n",
    "    \n",
    "    training_data_path = Path(processed_dir) / \"training_data_lincs2020_optimized_1201_l_scgpt.pkl\"\n",
    "    compound_info_path = Path(data_dir) / \"compoundinfo_beta.txt\"\n",
    "    output_path = Path(processed_dir) / \"training_data_lincs2020_chemfiltered_1201_l_scgpt.pkl\"\n",
    "    \n",
    "    # NIBRè¿‡æ»¤å™¨CSVè·¯å¾„\n",
    "    # CSVæ–‡ä»¶åœ¨å½“å‰è„šæœ¬åŒçº§çš„chem_filteræ–‡ä»¶å¤¹ä¸‹\n",
    "    try:\n",
    "        # å°è¯•ä½¿ç”¨ __file__ï¼ˆåœ¨è„šæœ¬æ¨¡å¼ä¸‹ï¼‰\n",
    "        script_dir = Path(__file__).parent\n",
    "    except NameError:\n",
    "        # åœ¨äº¤äº’å¼ç¯å¢ƒä¸­ä½¿ç”¨å½“å‰å·¥ä½œç›®å½•\n",
    "        script_dir = Path(os.getcwd())\n",
    "    nibr_csv_path = script_dir / \"chem_filter\" / \"SubstructureFilter_HitTriaging_wPubChemExamples.csv\"\n",
    "    \n",
    "    # æ£€æŸ¥æ–‡ä»¶\n",
    "    if not training_data_path.exists():\n",
    "        print(f\"âŒ Training data not found: {training_data_path}\")\n",
    "        print(f\"   Please run data preprocessing first!\")\n",
    "        return\n",
    "    \n",
    "    if not compound_info_path.exists():\n",
    "        print(f\"âŒ Compound info not found: {compound_info_path}\")\n",
    "        return\n",
    "    \n",
    "    # æ£€æŸ¥NIBR CSVæ–‡ä»¶\n",
    "    if not nibr_csv_path.exists():\n",
    "        print(f\"âš ï¸  NIBR filter CSV not found: {nibr_csv_path}\")\n",
    "        print(f\"   Will use fallback NIBR filters (less accurate)\")\n",
    "        nibr_csv_path = None\n",
    "        \n",
    "    # åº”ç”¨è¿‡æ»¤å™¨\n",
    "    try:\n",
    "        filtered_data = apply_chemical_filters_to_training_data(\n",
    "            training_data_path=str(training_data_path),\n",
    "            compound_info_path=str(compound_info_path),\n",
    "            output_path=str(output_path)\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"âœ… CHEMICAL FILTERING COMPLETE!\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"ğŸ“ Final output: {output_path}\")\n",
    "        print(f\"ğŸ¯ Ready for training!\")\n",
    "        \n",
    "        return filtered_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"âŒ ERROR\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"   Type: {type(e).__name__}\")\n",
    "        print(f\"   Message: {e}\")\n",
    "        \n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    filtered_data = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bebe7c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ§¬ scGPT Data Reconstruction - Post-processing\n",
      "================================================================================\n",
      "\n",
      "ğŸ“– Loading training data...\n",
      "   File: D:/ç§‘ç ”/Models/drugreflector/processed_data/training_data_lincs2020_chemfiltered_1201_l.pkl\n",
      "   âœ“ Loaded successfully\n",
      "   Current data shape: (509006, 978)\n",
      "   Current data type: float32\n",
      "   Samples: 509,006\n",
      "   Genes: 978\n",
      "\n",
      "================================================================================\n",
      "ğŸ”„ scGPT Reconstruction: Building Plate Background Model\n",
      "================================================================================\n",
      "   ğŸ¯ Target plates: 4,583\n",
      "   ğŸ“‹ Sample target plates: ['CRCGN015_MCF10A.TP53.M_24H_X1', 'REP.B025_MCF7_24H_X1', 'REP.B028_MCF7_24H_X1', 'REP.A018_YAPC_24H_X2', 'LJP007_ASC_24H_X3.A2']\n",
      "   ğŸ“– Reading Level 3 Control data (landmarks only)...\n",
      "ğŸ“– Loading gene information...\n",
      "   âœ“ Loaded 12328 genes\n",
      "   âœ“ Landmark genes: 978\n",
      "    ğŸ” Detected matrix shape: (188708, 12328)\n",
      "    âœ“ Format: Samples x Genes (Transposed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Loading chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:03<00:00,  5.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ“ Loaded control matrix: (188708, 978)\n",
      "   ğŸ”— Mapping control samples to plates...\n",
      "ğŸ“– Loading instance information...\n",
      "   âœ“ Loaded 3026460 instances\n",
      "   ğŸ“Š Plate mapping diagnostics:\n",
      "      Control samples: 188,708\n",
      "      Mapped to plates: 188,708\n",
      "      Unmapped (NaN): 0\n",
      "      Unique control plates: 8027\n",
      "      Target plates needed: 4583\n",
      "\n",
      "   ğŸ” Sample mappings (first 5):\n",
      "      ABY001_A375_XH_X1_B15:A03 â†’ ABY001_A375_XH_X1\n",
      "      ABY001_A375_XH_X1_B15:A04 â†’ ABY001_A375_XH_X1\n",
      "      ABY001_A375_XH_X1_B15:A05 â†’ ABY001_A375_XH_X1\n",
      "      ABY001_A375_XH_X1_B15:A06 â†’ ABY001_A375_XH_X1\n",
      "      ABY001_A375_XH_X1_B15:A07 â†’ ABY001_A375_XH_X1\n",
      "\n",
      "   ğŸ¯ Plate overlap:\n",
      "      Control plates: 8027\n",
      "      Target plates: 4583\n",
      "      Overlap: 4576\n",
      "      Example overlap plates: ['CRCGN015_MCF10A.TP53.M_24H_X1', 'REP.B025_MCF7_24H_X1', 'REP.B028_MCF7_24H_X1', 'REP.A018_YAPC_24H_X2', 'LJP007_ASC_24H_X3.A2']\n",
      "   ğŸ“Š Computing plate-level statistics (Median & MAD)...\n",
      "   Valid control samples: 89,779 / 188,708\n",
      "   âœ“ Cached statistics for 4576 plates\n",
      "\n",
      "================================================================================\n",
      "âš¡ scGPT Reconstruction: Level 4 â†’ Level 3 â†’ Bins\n",
      "================================================================================\n",
      "   Strategy: Z-score â†’ Expression â†’ Rank-based Binning (0-50)\n",
      "   Noise threshold: < 4.0 â†’ Bin 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Processing Plates: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4583/4583 [00:59<00:00, 76.57it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âš ï¸  Warning: 7 plates missing from control data\n",
      "       Used fallback Z-score binning for those plates\n",
      "   âœ“ Noise masking: 16,378,822 values set to bin 0\n",
      "   âœ“ Bin distribution:\n",
      "       Bin 0 (noise/padding): 16,378,822\n",
      "       Bins 1-50 (expression): 481,429,046\n",
      "   âœ“ Updating training_data...\n",
      "\n",
      "================================================================================\n",
      "ğŸ’¾ Saving reconstructed data...\n",
      "================================================================================\n",
      "   Output: D:\\ç§‘ç ”\\Models\\drugreflector\\processed_data\\training_data_lincs2020_chemfiltered_1201_l_scgpt.pkl\n",
      "   âœ“ Saved successfully! (5955.2 MB)\n",
      "\n",
      "================================================================================\n",
      "âœ… scGPT RECONSTRUCTION COMPLETE\n",
      "================================================================================\n",
      "ğŸ“Š Summary:\n",
      "   â€¢ Input:  Level 4 (Z-score)\n",
      "   â€¢ Output: scGPT Bins (0-50)\n",
      "   â€¢ Samples: 509,006\n",
      "   â€¢ Genes: 978\n",
      "   â€¢ File: D:\\ç§‘ç ”\\Models\\drugreflector\\processed_data\\training_data_lincs2020_chemfiltered_1201_l_scgpt.pkl\n",
      "\n",
      "ğŸ” Reconstruction Details:\n",
      "   â€¢ Bins: 51\n",
      "   â€¢ Noise threshold: 4.0\n",
      "   â€¢ Noise-masked values: 16,378,822\n",
      "   â€¢ Missing plates (fallback): 7\n",
      "\n",
      "ğŸ¯ Ready for scGPT training!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "scGPT Data Reconstruction - Post-processing Script\n",
    "ç”¨äºå°†å·²å¤„ç†çš„Level 4æ•°æ®é‡æ„ä¸ºscGPTæ ¼å¼\n",
    "\n",
    "ä½¿ç”¨æ–¹æ³•:\n",
    "1. ç¡®ä¿å·²è¿è¡Œå®Œæ•´çš„æ•°æ®é¢„å¤„ç†æµç¨‹ï¼ˆç”Ÿæˆ.pklæ–‡ä»¶ï¼‰\n",
    "2. ä¿®æ”¹main()ä¸­çš„è·¯å¾„é…ç½®\n",
    "3. è¿è¡Œæ­¤è„šæœ¬\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import median_abs_deviation\n",
    "\n",
    "\n",
    "class Level3Reconstructor:\n",
    "    \"\"\"\n",
    "    scGPT æ•°æ®åå¤„ç†å™¨ï¼š\n",
    "    ä»å·²ä¿å­˜çš„ training_data.pkl å¼€å§‹\n",
    "    1. åŠ è½½ Level 3 Control æ•°æ®å¹¶è®¡ç®—æ¿çº§ç»Ÿè®¡é‡\n",
    "    2. å°† Level 4 (Z-score) é‡æ„ä¸º Level 3\n",
    "    3. è½¬æ¢ä¸º scGPT Bins\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir: LINCS2020æ•°æ®ç›®å½•è·¯å¾„\n",
    "        \"\"\"\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.medians = None\n",
    "        self.mads = None\n",
    "        self.gene_info = None\n",
    "        self.inst_info = None\n",
    "        \n",
    "    def load_gene_info(self):\n",
    "        \"\"\"åŠ è½½åŸºå› ä¿¡æ¯ä»¥è·å–landmarkç´¢å¼•\"\"\"\n",
    "        print(\"ğŸ“– Loading gene information...\")\n",
    "        gene_info_path = self.data_dir / \"geneinfo_beta.txt\"\n",
    "        self.gene_info = pd.read_csv(gene_info_path, sep='\\t', low_memory=False)\n",
    "        \n",
    "        # æ‰¾åˆ°landmarkåŸºå› \n",
    "        landmark_mask = self.gene_info['feature_space'] == 'landmark'\n",
    "        self.landmark_indices = np.where(landmark_mask)[0]\n",
    "        \n",
    "        print(f\"   âœ“ Loaded {len(self.gene_info)} genes\")\n",
    "        print(f\"   âœ“ Landmark genes: {landmark_mask.sum()}\")\n",
    "        \n",
    "        return self.landmark_indices\n",
    "    \n",
    "    def load_instance_info(self):\n",
    "        \"\"\"åŠ è½½å®ä¾‹ä¿¡æ¯ä»¥è·å–æ¿å·æ˜ å°„\"\"\"\n",
    "        print(\"ğŸ“– Loading instance information...\")\n",
    "        inst_info_path = self.data_dir / \"instinfo_beta.txt\"\n",
    "        self.inst_info = pd.read_csv(inst_info_path, sep='\\t', low_memory=False)\n",
    "        \n",
    "        print(f\"   âœ“ Loaded {len(self.inst_info)} instances\")\n",
    "        \n",
    "        return self.inst_info\n",
    "    \n",
    "    def load_control_stats(self, sample_meta):\n",
    "        \"\"\"\n",
    "        æ­¥éª¤ 1: åŠ è½½ Control æ•°æ®å¹¶è®¡ç®—æ¿çº§ç»Ÿè®¡é‡ (Median, MAD)\n",
    "        \n",
    "        Args:\n",
    "            sample_meta: è®­ç»ƒæ•°æ®çš„æ ·æœ¬å…ƒæ•°æ® (DataFrame)\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ğŸ”„ scGPT Reconstruction: Building Plate Background Model\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # 1. è·å–éœ€è¦çš„æ¿å·åˆ—è¡¨\n",
    "        if 'rna_plate' not in sample_meta.columns:\n",
    "            print(\"   âš ï¸  'rna_plate' not found in sample_meta, loading from instinfo...\")\n",
    "            if self.inst_info is None:\n",
    "                self.load_instance_info()\n",
    "            \n",
    "            # åˆ›å»ºæ˜ å°„\n",
    "            plate_map = dict(zip(\n",
    "                self.inst_info['sample_id'], \n",
    "                self.inst_info['rna_plate']\n",
    "            ))\n",
    "            sample_meta = sample_meta.copy()\n",
    "            sample_meta['rna_plate'] = sample_meta['sample_id'].map(plate_map)\n",
    "            \n",
    "            # æ·»åŠ è°ƒè¯•ä¿¡æ¯\n",
    "            print(f\"      Samples in training data: {len(sample_meta):,}\")\n",
    "            print(f\"      Samples mapped to plates: {sample_meta['rna_plate'].notna().sum():,}\")\n",
    "            print(f\"      Unmapped samples: {sample_meta['rna_plate'].isna().sum():,}\")\n",
    "\n",
    "        target_plates = set(sample_meta['rna_plate'].dropna().unique())\n",
    "        print(f\"   ğŸ¯ Target plates: {len(target_plates):,}\")\n",
    "\n",
    "        # æ˜¾ç¤ºä¸€äº›ç¤ºä¾‹æ¿å·\n",
    "        if target_plates:\n",
    "            print(f\"   ğŸ“‹ Sample target plates: {list(target_plates)[:5]}\")\n",
    "                \n",
    "        # 2. åŠ è½½ Level 3 Control æ•°æ®\n",
    "        l3_path = self.data_dir / \"level3_beta_ctl_n188708x12328.gctx\"\n",
    "        if not l3_path.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"Missing Level 3 control file: {l3_path}\\n\"\n",
    "                f\"This file is required for reconstruction. Please download from CMap.\"\n",
    "            )\n",
    "        \n",
    "        print(f\"   ğŸ“– Reading Level 3 Control data (landmarks only)...\")\n",
    "        \n",
    "        # åŠ è½½landmarkåŸºå› ç´¢å¼•\n",
    "        if self.gene_info is None:\n",
    "            self.load_gene_info()\n",
    "        \n",
    "        # è¯»å–GCTXæ–‡ä»¶ (ä»…landmark)\n",
    "        ctl_matrix, ctl_row_meta = self._read_gctx_landmarks(l3_path)\n",
    "        \n",
    "        print(f\"   âœ“ Loaded control matrix: {ctl_matrix.shape}\")\n",
    "        \n",
    "        # 3. å…³è”æ¿ä¿¡æ¯\n",
    "        print(\"   ğŸ”— Mapping control samples to plates...\")\n",
    "        if self.inst_info is None:\n",
    "            self.load_instance_info()\n",
    "\n",
    "        # è·å–controlæ ·æœ¬çš„sample_id\n",
    "        if 'id' in ctl_row_meta.columns:\n",
    "            ctl_sample_ids = ctl_row_meta['id']\n",
    "        elif 'sample_id' in ctl_row_meta.columns:\n",
    "            ctl_sample_ids = ctl_row_meta['sample_id']\n",
    "        else:\n",
    "            # å¦‚æœéƒ½æ²¡æœ‰ï¼Œä½¿ç”¨ç´¢å¼•\n",
    "            print(f\"   âš ï¸  No 'id' or 'sample_id' column found in control metadata\")\n",
    "            print(f\"   Available columns: {ctl_row_meta.columns.tolist()}\")\n",
    "            ctl_sample_ids = ctl_row_meta.iloc[:, 0]  # ä½¿ç”¨ç¬¬ä¸€åˆ—\n",
    "\n",
    "        # å»ºç«‹å¿«é€ŸæŸ¥æ‰¾è¡¨\n",
    "        plate_lookup = pd.Series(\n",
    "            self.inst_info['rna_plate'].values,\n",
    "            index=self.inst_info['sample_id']\n",
    "        ).to_dict()\n",
    "\n",
    "        # æ˜ å°„æ¿å·\n",
    "        ctl_plates = ctl_sample_ids.map(plate_lookup)\n",
    "\n",
    "        # ===== æ·»åŠ è°ƒè¯•ä¿¡æ¯ =====\n",
    "        print(f\"   ğŸ“Š Plate mapping diagnostics:\")\n",
    "        print(f\"      Control samples: {len(ctl_sample_ids):,}\")\n",
    "        print(f\"      Mapped to plates: {ctl_plates.notna().sum():,}\")\n",
    "        print(f\"      Unmapped (NaN): {ctl_plates.isna().sum():,}\")\n",
    "        print(f\"      Unique control plates: {ctl_plates.nunique()}\")\n",
    "        print(f\"      Target plates needed: {len(target_plates)}\")\n",
    "\n",
    "        # æ˜¾ç¤ºå‰å‡ ä¸ªæ˜ å°„ç¤ºä¾‹\n",
    "        print(f\"\\n   ğŸ” Sample mappings (first 5):\")\n",
    "        for idx in range(min(5, len(ctl_sample_ids))):\n",
    "            sid = ctl_sample_ids.iloc[idx]\n",
    "            plate = ctl_plates.iloc[idx]\n",
    "            print(f\"      {sid} â†’ {plate}\")\n",
    "\n",
    "        # æ£€æŸ¥é‡å \n",
    "        ctl_unique_plates = set(ctl_plates.dropna().unique())\n",
    "        overlap = ctl_unique_plates & target_plates\n",
    "        print(f\"\\n   ğŸ¯ Plate overlap:\")\n",
    "        print(f\"      Control plates: {len(ctl_unique_plates)}\")\n",
    "        print(f\"      Target plates: {len(target_plates)}\")\n",
    "        print(f\"      Overlap: {len(overlap)}\")\n",
    "\n",
    "        if len(overlap) > 0:\n",
    "            print(f\"      Example overlap plates: {list(overlap)[:5]}\")\n",
    "\n",
    "        # 4. è¿‡æ»¤åˆ°ç›®æ ‡æ¿å¹¶è®¡ç®—ç»Ÿè®¡é‡\n",
    "        valid_mask = ctl_plates.isin(target_plates)\n",
    "\n",
    "        if not valid_mask.any():\n",
    "            # æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯\n",
    "            print(f\"\\n   âŒ ERROR: No plate overlap found!\")\n",
    "            print(f\"\\n   Sample control plates: {list(ctl_unique_plates)[:10]}\")\n",
    "            print(f\"   Sample target plates: {list(target_plates)[:10]}\")\n",
    "            \n",
    "            # å°è¯•æ‰¾åˆ°ç›¸ä¼¼çš„æ¿å·\n",
    "            if ctl_unique_plates and target_plates:\n",
    "                ctl_example = list(ctl_unique_plates)[0]\n",
    "                target_example = list(target_plates)[0]\n",
    "                print(f\"\\n   Example control plate: '{ctl_example}' (type: {type(ctl_example).__name__})\")\n",
    "                print(f\"   Example target plate: '{target_example}' (type: {type(target_example).__name__})\")\n",
    "            \n",
    "            raise ValueError(\n",
    "                \"No overlap between control plates and training data plates!\\n\"\n",
    "                \"Possible causes:\\n\"\n",
    "                \"1. Different plate naming formats (check types above)\\n\"\n",
    "                \"2. Control data is from different batches\\n\"\n",
    "                \"3. instinfo_beta.txt doesn't match your datasets\\n\"\n",
    "                \"4. sample_id mapping failed\\n\\n\"\n",
    "                \"Please check the diagnostics above.\"\n",
    "            )\n",
    "        \n",
    "        print(f\"   ğŸ“Š Computing plate-level statistics (Median & MAD)...\")\n",
    "        print(f\"   Valid control samples: {valid_mask.sum():,} / {len(valid_mask):,}\")\n",
    "        \n",
    "        # è½¬ä¸ºDataFrameè¿›è¡Œåˆ†ç»„è®¡ç®—\n",
    "        df_ctl = pd.DataFrame(\n",
    "            ctl_matrix[valid_mask],\n",
    "            index=ctl_plates[valid_mask]\n",
    "        )\n",
    "        \n",
    "        grouped = df_ctl.groupby(df_ctl.index)\n",
    "        \n",
    "        # CMapç»Ÿè®¡é‡: Median å’Œ MAD*1.4826 (robust std)\n",
    "        self.medians = grouped.median()\n",
    "        self.mads = grouped.apply(\n",
    "            lambda x: median_abs_deviation(x, scale='normal', axis=0)\n",
    "        )\n",
    "        \n",
    "        print(f\"   âœ“ Cached statistics for {len(self.medians)} plates\")\n",
    "        \n",
    "        # æ¸…ç†å†…å­˜\n",
    "        del ctl_matrix, df_ctl, ctl_row_meta\n",
    "        gc.collect()\n",
    "        \n",
    "        return sample_meta  # è¿”å›å¸¦æ¿å·çš„å…ƒæ•°æ®\n",
    "    \n",
    "    def _read_gctx_landmarks(self, gctx_path):\n",
    "        \"\"\"è¯»å–GCTXæ–‡ä»¶çš„landmarkåŸºå› \"\"\"\n",
    "        import h5py\n",
    "        \n",
    "        with h5py.File(gctx_path, 'r') as f:\n",
    "            # âœ… ä¿®æ­£1: è¯»å– COL (æ ·æœ¬) å…ƒæ•°æ®\n",
    "            # GCTXæ ‡å‡†: æ ·æœ¬ä¿¡æ¯åœ¨ COLï¼ŒåŸºå› ä¿¡æ¯åœ¨ ROW\n",
    "            sample_meta_dict = {}\n",
    "            if '/0/META/COL' in f:\n",
    "                for key in f['/0/META/COL'].keys():\n",
    "                    sample_meta_dict[key] = f[f'/0/META/COL/{key}'][()].astype(str)\n",
    "            row_meta = pd.DataFrame(sample_meta_dict)\n",
    "\n",
    "            # è¯»å–æ•°æ®\n",
    "            data_key = '/0/DATA/0/matrix'\n",
    "            full_matrix = f[data_key]\n",
    "            matrix_shape = full_matrix.shape\n",
    "            \n",
    "            # Landmark æ˜¯åŸºå› ç´¢å¼•ï¼Œéœ€è¦æ’åº\n",
    "            landmark_indices = sorted(self.landmark_indices)\n",
    "            chunk_size = 10000 \n",
    "            chunks = []\n",
    "\n",
    "            print(f\"    ğŸ” Detected matrix shape: {matrix_shape}\")\n",
    "\n",
    "            # === åˆ¤æ–­çŸ©é˜µæ–¹å‘ ===\n",
    "            # æƒ…å†µ A: (Genes, Samples) - æ ‡å‡†æ ¼å¼ï¼ŒåŸºå› åœ¨ dim 0 (12328)\n",
    "            if matrix_shape[0] == 12328:\n",
    "                print(\"    âœ“ Format: Genes x Samples (Standard)\")\n",
    "                n_samples = matrix_shape[1]\n",
    "                \n",
    "                for i in tqdm(range(0, n_samples, chunk_size), desc=\"    Loading chunks\"):\n",
    "                    end_idx = min(i + chunk_size, n_samples)\n",
    "                    # è¯»å–: [Landmarks, Current Samples]\n",
    "                    chunk = full_matrix[landmark_indices, i:end_idx]\n",
    "                    chunks.append(chunk)\n",
    "                \n",
    "                # æ‹¼æ¥åæ˜¯ (978, N_Samples)ï¼Œéœ€è¦è½¬ç½®ä¸º (N_Samples, 978)\n",
    "                matrix = np.hstack(chunks).T.astype(np.float32)\n",
    "\n",
    "            # æƒ…å†µ B: (Samples, Genes) - è½¬ç½®æ ¼å¼ï¼ŒåŸºå› åœ¨ dim 1 (12328)\n",
    "            # è¿™å°±æ˜¯å¯¼è‡´ä½ æŠ¥é”™çš„å®é™…æƒ…å†µ\n",
    "            elif matrix_shape[1] == 12328:\n",
    "                print(\"    âœ“ Format: Samples x Genes (Transposed)\")\n",
    "                n_samples = matrix_shape[0]\n",
    "                \n",
    "                for i in tqdm(range(0, n_samples, chunk_size), desc=\"    Loading chunks\"):\n",
    "                    end_idx = min(i + chunk_size, n_samples)\n",
    "                    # è¯»å–: [Current Samples, Landmarks]\n",
    "                    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬åˆ‡ç‰‡ dim 0 (Samples)ï¼Œç­›é€‰ dim 1 (Genes)\n",
    "                    chunk = full_matrix[i:end_idx, landmark_indices]\n",
    "                    chunks.append(chunk)\n",
    "                \n",
    "                # æ‹¼æ¥åç›´æ¥æ˜¯ (N_Samples, 978)ï¼Œä¸éœ€è¦è½¬ç½®\n",
    "                matrix = np.vstack(chunks).astype(np.float32)\n",
    "                \n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected matrix dimensions: {matrix_shape}. Expected one dimension to be 12328 (genes).\")\n",
    "\n",
    "        return matrix, row_meta\n",
    "        \n",
    "    def reconstruct_and_quantize(self, training_data, n_bins=51, noise_threshold=4.0):\n",
    "        \"\"\"\n",
    "        æ­¥éª¤ 2: å°† Level 4 é‡æ„ä¸º Level 3ï¼Œç„¶åè½¬æ¢ä¸º scGPT Bins\n",
    "        \n",
    "        Args:\n",
    "            training_data: è®­ç»ƒæ•°æ®å­—å…¸ (åŒ…å« 'X' å’Œ 'sample_meta')\n",
    "            n_bins: scGPT binæ•°é‡ (é»˜è®¤51, èŒƒå›´0-50)\n",
    "            noise_threshold: Level 3ä¸­ä½äºæ­¤å€¼çš„è§†ä¸ºå™ªéŸ³ (è®¾ä¸ºbin 0)\n",
    "        \n",
    "        Returns:\n",
    "            æ›´æ–°åçš„ training_data\n",
    "        \"\"\"\n",
    "        if self.medians is None:\n",
    "            raise ValueError(\"Please run load_control_stats() first!\")\n",
    "        \n",
    "        X_zscore = training_data['X']\n",
    "        sample_meta = training_data['sample_meta']\n",
    "        \n",
    "        # ç¡®ä¿æœ‰æ¿å·\n",
    "        if 'rna_plate' not in sample_meta.columns:\n",
    "            sample_meta = self.load_control_stats(sample_meta)\n",
    "            training_data['sample_meta'] = sample_meta\n",
    "        \n",
    "        plates = sample_meta['rna_plate'].values\n",
    "        \n",
    "        # ç»“æœçŸ©é˜µ\n",
    "        X_bins = np.zeros(X_zscore.shape, dtype=np.int32)\n",
    "        # ğŸ”¥ æ–°å¢: åˆå§‹åŒ–åŸºçº¿(Ctx)çŸ©é˜µ\n",
    "        X_ctx_bins = np.zeros(X_zscore.shape, dtype=np.int32)\n",
    "        \n",
    "        unique_plates = np.unique(plates)\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"âš¡ scGPT Reconstruction: Level 4 â†’ Level 3 â†’ Bins\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"   Strategy: Z-score â†’ Expression â†’ Rank-based Binning (0-{n_bins-1})\")\n",
    "        print(f\"   Noise threshold: < {noise_threshold} â†’ Bin 0\")\n",
    "        \n",
    "        missing_plates = 0\n",
    "        total_noise_masked = 0\n",
    "        \n",
    "        for plate in tqdm(unique_plates, desc=\"   Processing Plates\"):\n",
    "            # 1. è·å–å½“å‰æ¿çš„æ ·æœ¬\n",
    "            idx = np.where(plates == plate)[0]\n",
    "            if len(idx) == 0:\n",
    "                continue\n",
    "            \n",
    "            # 2. è·å–ç»Ÿè®¡é‡\n",
    "            if plate in self.medians.index:\n",
    "                mu = self.medians.loc[plate].values\n",
    "                # âœ… ä¿®æ­£ï¼šç›´æ¥è·å–å¯¹è±¡...\n",
    "                sigma = self.mads.loc[plate]\n",
    "                if hasattr(sigma, 'values'):\n",
    "                    sigma = sigma.values\n",
    "                \n",
    "                # 3. é‡æ„ Level 3\n",
    "                # Broad Instituteå…¬å¼: Level3 = (Z * Sigma) + Mu\n",
    "                X_l3_batch = (X_zscore[idx] * sigma) + mu\n",
    "                \n",
    "                # 4. scGPT ç¦»æ•£åŒ–: Sample-wise Rank-based Binning\n",
    "                ranks = np.argsort(np.argsort(X_l3_batch, axis=1), axis=1)\n",
    "                \n",
    "                # æ˜ å°„åˆ° 1 ~ (n_bins-1)\n",
    "                n_genes = X_l3_batch.shape[1]\n",
    "                batch_bins = np.floor(\n",
    "                    (ranks / n_genes) * (n_bins - 1)\n",
    "                ).astype(np.int32) + 1\n",
    "                \n",
    "                batch_bins = np.clip(batch_bins, 1, n_bins - 1)\n",
    "                \n",
    "                # 5. å™ªéŸ³æ©ç  (å¯é€‰ä½†æ¨è)\n",
    "                noise_mask = X_l3_batch < noise_threshold\n",
    "                batch_bins[noise_mask] = 0\n",
    "                \n",
    "                total_noise_masked += noise_mask.sum()\n",
    "                \n",
    "                X_bins[idx] = batch_bins\n",
    "                \n",
    "                # æ–°å¢: å¤„ç† Control (Ctx) æ•°æ®\n",
    "                # mu æ˜¯è¯¥æ¿çš„åŸºçº¿è¡¨è¾¾ (1D array)ï¼Œæˆ‘ä»¬å°†å…¶ä¹Ÿè¿›è¡Œ Binning å¹¶å¹¿æ’­\n",
    "                mu_2d = mu.reshape(1, -1)\n",
    "                \n",
    "                # å¯¹åŸºçº¿æ•°æ®è¿›è¡ŒåŒæ ·çš„ Rank-based Binning\n",
    "                ctx_ranks = np.argsort(np.argsort(mu_2d, axis=1), axis=1)\n",
    "                ctx_bins_row = np.floor(\n",
    "                    (ctx_ranks / n_genes) * (n_bins - 1)\n",
    "                ).astype(np.int32) + 1\n",
    "                ctx_bins_row = np.clip(ctx_bins_row, 1, n_bins - 1)\n",
    "                \n",
    "                # åŸºçº¿å™ªéŸ³æ©ç \n",
    "                ctx_noise_mask = mu_2d < noise_threshold\n",
    "                ctx_bins_row[ctx_noise_mask] = 0\n",
    "                \n",
    "                # å¹¿æ’­åˆ°å½“å‰æ¿çš„æ‰€æœ‰æ ·æœ¬ (å¤åˆ¶ N æ¬¡)\n",
    "                X_ctx_bins[idx] = np.tile(ctx_bins_row, (len(idx), 1))\n",
    "                \n",
    "            else:\n",
    "                missing_plates += 1\n",
    "                # é™çº§æ–¹æ¡ˆ: ç›´æ¥å¯¹Z-scoreåˆ†ç®±\n",
    "                z_batch = X_zscore[idx]\n",
    "                # Zåœ¨ -2~2 èŒƒå›´å†…æ˜ å°„åˆ° 1~50\n",
    "                temp_bins = (\n",
    "                    (np.clip(z_batch, -2, 2) + 2) / 4 * (n_bins - 1)\n",
    "                ).astype(np.int32) + 1\n",
    "                temp_bins = np.clip(temp_bins, 1, n_bins - 1)\n",
    "                X_bins[idx] = temp_bins\n",
    "                \n",
    "                # æ–°å¢: ç¼ºå¤±æ¿çš„ Ctx è®¾ä¸ºé»˜è®¤å€¼ (Z=0 å¯¹åº”çš„ binï¼Œçº¦ 25)\n",
    "                mid_bin = int((n_bins - 1) / 2)\n",
    "                X_ctx_bins[idx] = mid_bin\n",
    "        \n",
    "        if missing_plates > 0:\n",
    "            print(f\"   âš ï¸  Warning: {missing_plates} plates missing from control data\")\n",
    "            print(f\"       Used fallback Z-score binning for those plates\")\n",
    "        \n",
    "        print(f\"   âœ“ Noise masking: {total_noise_masked:,} values set to bin 0\")\n",
    "        print(f\"   âœ“ Bin distribution:\")\n",
    "        \n",
    "        # ç»Ÿè®¡binåˆ†å¸ƒ\n",
    "        unique_bins, counts = np.unique(X_bins, return_counts=True)\n",
    "        bin_dist = dict(zip(unique_bins, counts))\n",
    "        \n",
    "        print(f\"       Bin 0 (noise/padding): {bin_dist.get(0, 0):,}\")\n",
    "        print(f\"       Bins 1-{n_bins-1} (expression): {X_bins[X_bins > 0].size:,}\")\n",
    "        \n",
    "        # æ›´æ–°æ•°æ®\n",
    "        print(f\"   âœ“ Updating training_data...\")\n",
    "        \n",
    "        # å¤‡ä»½åŸå§‹Z-score (å¯é€‰)\n",
    "        training_data['X_level4_zscore'] = training_data['X']\n",
    "        training_data['X'] = X_bins\n",
    "        training_data['X_ctx'] = X_ctx_bins\n",
    "        \n",
    "        # æ·»åŠ é‡æ„å…ƒæ•°æ®\n",
    "        training_data['reconstruction_info'] = {\n",
    "            'n_bins': n_bins,\n",
    "            'noise_threshold': noise_threshold,\n",
    "            'missing_plates': missing_plates,\n",
    "            'noise_masked_count': int(total_noise_masked)\n",
    "        }\n",
    "        \n",
    "        return training_data\n",
    "\n",
    "\n",
    "def apply_scgpt_reconstruction(\n",
    "    training_data_path,\n",
    "    data_dir,\n",
    "    output_path=None,\n",
    "    n_bins=51,\n",
    "    noise_threshold=4.0\n",
    "):\n",
    "    \"\"\"\n",
    "    å®Œæ•´çš„scGPTé‡æ„æµç¨‹\n",
    "    \n",
    "    Args:\n",
    "        training_data_path: å·²å¤„ç†çš„è®­ç»ƒæ•°æ®pklæ–‡ä»¶è·¯å¾„\n",
    "        data_dir: LINCS2020æ•°æ®ç›®å½•\n",
    "        output_path: è¾“å‡ºè·¯å¾„ (Noneåˆ™è‡ªåŠ¨ç”Ÿæˆ)\n",
    "        n_bins: binæ•°é‡\n",
    "        noise_threshold: å™ªéŸ³é˜ˆå€¼\n",
    "    \n",
    "    Returns:\n",
    "        é‡æ„åçš„è®­ç»ƒæ•°æ®\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ğŸ§¬ scGPT Data Reconstruction - Post-processing\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # 1. åŠ è½½è®­ç»ƒæ•°æ®\n",
    "    print(f\"\\nğŸ“– Loading training data...\")\n",
    "    print(f\"   File: {training_data_path}\")\n",
    "    \n",
    "    with open(training_data_path, 'rb') as f:\n",
    "        training_data = pickle.load(f)\n",
    "    \n",
    "    print(f\"   âœ“ Loaded successfully\")\n",
    "    print(f\"   Current data shape: {training_data['X'].shape}\")\n",
    "    print(f\"   Current data type: {training_data['X'].dtype}\")\n",
    "    print(f\"   Samples: {len(training_data['X']):,}\")\n",
    "    print(f\"   Genes: {training_data['X'].shape[1]}\")\n",
    "    \n",
    "    # æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯binsæ ¼å¼\n",
    "    if training_data['X'].dtype in [np.int32, np.int64]:\n",
    "        print(f\"   âš ï¸  Warning: Data appears to be already binned (dtype={training_data['X'].dtype})\")\n",
    "        user_input = input(\"   Continue anyway? (y/n): \")\n",
    "        if user_input.lower() != 'y':\n",
    "            print(\"   Aborted.\")\n",
    "            return None\n",
    "    \n",
    "    # 2. åˆå§‹åŒ–é‡æ„å™¨\n",
    "    reconstructor = Level3Reconstructor(data_dir)\n",
    "    \n",
    "    # 3. åŠ è½½controlç»Ÿè®¡é‡\n",
    "    sample_meta_with_plates = reconstructor.load_control_stats(\n",
    "        training_data['sample_meta']\n",
    "    )\n",
    "    training_data['sample_meta'] = sample_meta_with_plates\n",
    "    \n",
    "    # 4. æ‰§è¡Œé‡æ„å’Œç¦»æ•£åŒ–\n",
    "    training_data = reconstructor.reconstruct_and_quantize(\n",
    "        training_data,\n",
    "        n_bins=n_bins,\n",
    "        noise_threshold=noise_threshold\n",
    "    )\n",
    "    \n",
    "    # 5. ä¿å­˜\n",
    "    if output_path is None:\n",
    "        input_path = Path(training_data_path)\n",
    "        output_path = input_path.parent / f\"{input_path.stem}_scgpt.pkl\"\n",
    "    else:\n",
    "        output_path = Path(output_path)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ğŸ’¾ Saving reconstructed data...\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"   Output: {output_path}\")\n",
    "    \n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(output_path, 'wb') as f:\n",
    "        pickle.dump(training_data, f, protocol=4)\n",
    "    \n",
    "    file_size_mb = output_path.stat().st_size / (1024**2)\n",
    "    print(f\"   âœ“ Saved successfully! ({file_size_mb:.1f} MB)\")\n",
    "    \n",
    "    # 6. æ€»ç»“\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"âœ… scGPT RECONSTRUCTION COMPLETE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"ğŸ“Š Summary:\")\n",
    "    print(f\"   â€¢ Input:  Level 4 (Z-score)\")\n",
    "    print(f\"   â€¢ Output: scGPT Bins (0-{n_bins-1})\")\n",
    "    print(f\"   â€¢ Samples: {len(training_data['X']):,}\")\n",
    "    print(f\"   â€¢ Genes: {training_data['X'].shape[1]}\")\n",
    "    print(f\"   â€¢ File: {output_path}\")\n",
    "    \n",
    "    if 'reconstruction_info' in training_data:\n",
    "        info = training_data['reconstruction_info']\n",
    "        print(f\"\\nğŸ” Reconstruction Details:\")\n",
    "        print(f\"   â€¢ Bins: {info['n_bins']}\")\n",
    "        print(f\"   â€¢ Noise threshold: {info['noise_threshold']}\")\n",
    "        print(f\"   â€¢ Noise-masked values: {info['noise_masked_count']:,}\")\n",
    "        if info['missing_plates'] > 0:\n",
    "            print(f\"   â€¢ Missing plates (fallback): {info['missing_plates']}\")\n",
    "    \n",
    "    print(f\"\\nğŸ¯ Ready for scGPT training!\")\n",
    "    \n",
    "    return training_data\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"ä¸»ç¨‹åº\"\"\"\n",
    "    \n",
    "    # ============ é…ç½®è·¯å¾„ ============\n",
    "    # è¯·æ ¹æ®æ‚¨çš„å®é™…è·¯å¾„ä¿®æ”¹ä»¥ä¸‹é…ç½®\n",
    "    \n",
    "    # LINCS2020åŸå§‹æ•°æ®ç›®å½•\n",
    "    data_dir = r\"D:/ç§‘ç ”/Models/drugreflector/datasets/LINCS2020\"\n",
    "    \n",
    "    # å·²å¤„ç†çš„è®­ç»ƒæ•°æ®è·¯å¾„ (ä»æ‚¨çš„é¢„å¤„ç†æµç¨‹ç”Ÿæˆçš„pkl)\n",
    "    training_data_path = r\"D:/ç§‘ç ”/Models/drugreflector/processed_data/training_data_lincs2020_chemfiltered_1201_l.pkl\"\n",
    "    \n",
    "    # è¾“å‡ºè·¯å¾„ (Noneåˆ™è‡ªåŠ¨ç”Ÿæˆ)\n",
    "    output_path = None  # å°†è‡ªåŠ¨å‘½åä¸º *_scgpt.pkl\n",
    "    \n",
    "    # scGPTå‚æ•°\n",
    "    n_bins = 51  # scGPTä½¿ç”¨çš„binæ•°é‡ (0-50)\n",
    "    noise_threshold = 4.0  # Level 3ä¸­çš„å™ªéŸ³é˜ˆå€¼\n",
    "    \n",
    "    # ============ æ£€æŸ¥æ–‡ä»¶ ============\n",
    "    if not Path(training_data_path).exists():\n",
    "        print(f\"âŒ Training data not found: {training_data_path}\")\n",
    "        print(f\"\\nğŸ’¡ Hint:\")\n",
    "        print(f\"   1. Make sure you've run the preprocessing notebook first\")\n",
    "        print(f\"   2. Check if the path is correct\")\n",
    "        print(f\"   3. The file should end with .pkl\")\n",
    "        return None\n",
    "    \n",
    "    if not Path(data_dir).exists():\n",
    "        print(f\"âŒ Data directory not found: {data_dir}\")\n",
    "        print(f\"\\nğŸ’¡ Hint:\")\n",
    "        print(f\"   This should be your LINCS2020 download directory\")\n",
    "        print(f\"   It should contain files like:\")\n",
    "        print(f\"   - level3_beta_ctl_n188708x12328.gctx\")\n",
    "        print(f\"   - geneinfo_beta.txt\")\n",
    "        print(f\"   - instinfo_beta.txt\")\n",
    "        return None\n",
    "    \n",
    "    # ============ æ‰§è¡Œé‡æ„ ============\n",
    "    try:\n",
    "        reconstructed_data = apply_scgpt_reconstruction(\n",
    "            training_data_path=training_data_path,\n",
    "            data_dir=data_dir,\n",
    "            output_path=output_path,\n",
    "            n_bins=n_bins,\n",
    "            noise_threshold=noise_threshold\n",
    "        )\n",
    "        \n",
    "        return reconstructed_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"âŒ ERROR\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"   Type: {type(e).__name__}\")\n",
    "        print(f\"   Message: {e}\")\n",
    "        \n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        print(f\"\\nğŸ’¡ Common Issues:\")\n",
    "        print(f\"   1. Missing level3_beta_ctl file - download from CMap\")\n",
    "        print(f\"   2. Memory error - try processing smaller batches\")\n",
    "        print(f\"   3. Plate mismatch - check instinfo consistency\")\n",
    "        \n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    reconstructed_data = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b5851c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ§¬ Generating Baseline Context (X_ctx) for Dual-Head Model\n",
      "================================================================================\n",
      "\n",
      "ğŸ“– Loading input data: training_data_lincs2020_chemfiltered_1201_l.pkl...\n",
      "   âœ“ Samples: 509,006\n",
      "   âœ“ Genes: 978\n",
      "   âœ“ Target Plates: 4,583\n",
      "\n",
      "ğŸ“– Reading Level 3 Control Data from: level3_beta_ctl_n188708x12328.gctx\n",
      "   âœ“ Format: Samples x Genes\n",
      "   âœ“ Loaded Control Matrix: (188708, 978)\n",
      "\n",
      "ğŸ“Š Computing Plate Baselines (Median)...\n",
      "   âœ“ Computed baselines for 4,576 plates\n",
      "\n",
      "âš¡ Generating Binned Context Embeddings...\n",
      "   ğŸ‘‰ Performing Rank-based Binning on Baselines...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Mapping to samples: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509006/509006 [00:00<00:00, 627004.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âš ï¸ Warning: 515 samples had missing plate baselines (filled with default).\n",
      "\n",
      "ğŸ’¾ Saving to D:\\ç§‘ç ”\\Models\\drugreflector\\processed_data\\training_data_lincs2020_chemfiltered_1201_l_dual_head.pkl...\n",
      "âœ… Done! Output Dataset Keys: ['X', 'y', 'folds', 'sample_meta', 'metadata', 'gene_names', 'compound_names', 'pert_to_idx', 'X_ctx']\n",
      "   X shape (Z-score): (509006, 978), dtype=float32\n",
      "   X_ctx shape (Bins): (509006, 978), dtype=int32\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import h5py\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import median_abs_deviation\n",
    "\n",
    "# ================= é…ç½®è·¯å¾„ (å‚è€ƒä½ çš„Notebook) =================\n",
    "DATA_DIR = Path(r\"D:/ç§‘ç ”/Models/drugreflector/datasets/LINCS2020\")\n",
    "PROCESSED_DIR = Path(r\"D:/ç§‘ç ”/Models/drugreflector/processed_data\")\n",
    "\n",
    "# è¾“å…¥æ–‡ä»¶ï¼šç»è¿‡åŒ–å­¦è¿‡æ»¤çš„ Level 4 Z-score æ•°æ® (Float)\n",
    "INPUT_FILE = PROCESSED_DIR / \"training_data_lincs2020_chemfiltered_1201_l.pkl\"\n",
    "# è¾“å‡ºæ–‡ä»¶ï¼šåŒ…å« scGPT åŸºçº¿ç¼–ç çš„æ–°æ•°æ®\n",
    "OUTPUT_FILE = PROCESSED_DIR / \"training_data_lincs2020_chemfiltered_1201_l_dual_head.pkl\"\n",
    "\n",
    "# scGPT å‚æ•°\n",
    "N_BINS = 51  # 0-50\n",
    "NOISE_THRESHOLD = 4.0  # Level 3 (log2) çš„å™ªéŸ³é˜ˆå€¼ï¼Œä½äºæ­¤å€¼è®¾ä¸º Bin 0\n",
    "\n",
    "def load_and_process_ctx():\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"ğŸ§¬ Generating Baseline Context (X_ctx) for Dual-Head Model\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    # 1. åŠ è½½è®­ç»ƒæ•°æ® (Level 4 Z-scores)\n",
    "    print(f\"\\nğŸ“– Loading input data: {INPUT_FILE.name}...\")\n",
    "    with open(INPUT_FILE, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    # ç¡®ä¿ X æ˜¯ float32 (Level 4 Z-scores)\n",
    "    if not np.issubdtype(data['X'].dtype, np.floating):\n",
    "        print(\"âš ï¸ Warning: Input 'X' is not float. Please ensure you are using the raw Z-score dataset.\")\n",
    "    \n",
    "    sample_meta = data['sample_meta']\n",
    "    print(f\"   âœ“ Samples: {len(sample_meta):,}\")\n",
    "    print(f\"   âœ“ Genes: {data['X'].shape[1]}\")\n",
    "\n",
    "    # 2. å‡†å¤‡æ¿å· (Plate IDs)\n",
    "    if 'rna_plate' not in sample_meta.columns:\n",
    "        print(\"   â³ Mapping sample_id to rna_plate...\")\n",
    "        inst_path = DATA_DIR / \"instinfo_beta.txt\"\n",
    "        inst_info = pd.read_csv(inst_path, sep='\\t', usecols=['sample_id', 'rna_plate'])\n",
    "        plate_map = dict(zip(inst_info['sample_id'], inst_info['rna_plate']))\n",
    "        sample_meta['rna_plate'] = sample_meta['sample_id'].map(plate_map)\n",
    "    \n",
    "    target_plates = set(sample_meta['rna_plate'].dropna().unique())\n",
    "    print(f\"   âœ“ Target Plates: {len(target_plates):,}\")\n",
    "\n",
    "    # 3. è¯»å– Level 3 Control æ•°æ® (GCTX)\n",
    "    l3_path = DATA_DIR / \"level3_beta_ctl_n188708x12328.gctx\"\n",
    "    print(f\"\\nğŸ“– Reading Level 3 Control Data from: {l3_path.name}\")\n",
    "    \n",
    "    # è·å– Landmark åŸºå› ç´¢å¼•\n",
    "    gene_path = DATA_DIR / \"geneinfo_beta.txt\"\n",
    "    gene_info = pd.read_csv(gene_path, sep='\\t')\n",
    "    landmark_indices = sorted(np.where(gene_info['feature_space'] == 'landmark')[0])\n",
    "    \n",
    "    # è¯»å– GCTX (è‡ªåŠ¨å¤„ç†è½¬ç½®)\n",
    "    with h5py.File(l3_path, 'r') as f:\n",
    "        # è¯»å–æ ·æœ¬å…ƒæ•°æ®ä»¥è·å–æ¿å·\n",
    "        sample_ids_ctl = f['/0/META/COL/id'][()].astype(str)\n",
    "        \n",
    "        # è¯»å–çŸ©é˜µæ•°æ®\n",
    "        ds = f['/0/DATA/0/matrix']\n",
    "        if ds.shape[0] == 12328: # Genes x Samples\n",
    "            print(\"   âœ“ Format: Genes x Samples\")\n",
    "            # è¿™ç§æ ¼å¼å¾ˆéš¾æŒ‰æ ·æœ¬åˆ‡ç‰‡ï¼Œå¿…é¡»è¯»æ•´å—æˆ–è½¬ç½®è¯»å–\n",
    "            # ä¸ºäº†å†…å­˜å®‰å…¨ï¼Œæˆ‘ä»¬å‡è®¾å¤§éƒ¨åˆ†æƒ…å†µæ˜¯ Samples x Genes (LINCS 2020 L3)\n",
    "            # å¦‚æœç¡®å®æ˜¯ Genes x Samplesï¼Œå»ºè®®ä½¿ç”¨åˆ†å—è¯»å–å¹¶è½¬ç½®\n",
    "            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼šè¯»å–æ‰€æœ‰ landmark è¡Œ\n",
    "            ctl_matrix = ds[landmark_indices, :].T # -> (Samples, Landmarks)\n",
    "        else: # Samples x Genes (188708 x 12328) - æœ€å¸¸è§æƒ…å†µ\n",
    "            print(\"   âœ“ Format: Samples x Genes\")\n",
    "            # ç›´æ¥åˆ‡ç‰‡è¯»å–æ‰€æœ‰è¡Œï¼Œåˆ—å–landmark\n",
    "            ctl_matrix = ds[:, landmark_indices].astype(np.float32)\n",
    "            \n",
    "    print(f\"   âœ“ Loaded Control Matrix: {ctl_matrix.shape}\")\n",
    "\n",
    "    # 4. è®¡ç®—æ¯ä¸ªæ¿çš„åŸºçº¿ (Median)\n",
    "    print(\"\\nğŸ“Š Computing Plate Baselines (Median)...\")\n",
    "    \n",
    "    # æ˜ å°„ Control æ ·æœ¬åˆ°æ¿å·\n",
    "    inst_path = DATA_DIR / \"instinfo_beta.txt\"\n",
    "    # é‡æ–°è¯»å–ä»¥ç¡®ä¿å®Œæ•´ï¼ˆå¦‚æœå†…å­˜å¤Ÿï¼‰\n",
    "    if 'inst_info' not in locals():\n",
    "        inst_info = pd.read_csv(inst_path, sep='\\t', usecols=['sample_id', 'rna_plate'])\n",
    "    \n",
    "    plate_lookup = dict(zip(inst_info['sample_id'], inst_info['rna_plate']))\n",
    "    ctl_plates = pd.Series(sample_ids_ctl).map(plate_lookup)\n",
    "    \n",
    "    # è¿‡æ»¤å‡ºéœ€è¦çš„æ¿\n",
    "    valid_mask = ctl_plates.isin(target_plates)\n",
    "    df_ctl = pd.DataFrame(ctl_matrix[valid_mask], index=ctl_plates[valid_mask])\n",
    "    \n",
    "    # GroupBy è®¡ç®—ä¸­ä½æ•°\n",
    "    plate_medians = df_ctl.groupby(df_ctl.index).median()\n",
    "    print(f\"   âœ“ Computed baselines for {len(plate_medians):,} plates\")\n",
    "\n",
    "    # 5. ç”Ÿæˆ X_ctx (Binning)\n",
    "    print(\"\\nâš¡ Generating Binned Context Embeddings...\")\n",
    "    X_ctx = np.zeros(data['X'].shape, dtype=np.int32)\n",
    "    \n",
    "    # é¢„è®¡ç®—ï¼šæ‰€æœ‰æ¿çš„ä¸­ä½æ•°çŸ©é˜µ\n",
    "    # è¿™æ˜¯ä¸€ä¸ª (N_samples, N_genes) çš„çŸ©é˜µï¼Œæ¯è¡Œæ˜¯å¯¹åº”æ¿çš„åŸºçº¿\n",
    "    # ä¸ºäº†é€Ÿåº¦ï¼Œæˆ‘ä»¬å…ˆå¯¹ plate_medians è¿›è¡Œ Binning\n",
    "    \n",
    "    print(\"   ğŸ‘‰ Performing Rank-based Binning on Baselines...\")\n",
    "    # Rank-based binning: 0-50\n",
    "    # å¯¹æ¯ä¸ªæ¿çš„åŸºçº¿è¿›è¡Œæ’åºå’Œåˆ†ç®±\n",
    "    pm_values = plate_medians.values # (N_plates, N_genes)\n",
    "    n_genes = pm_values.shape[1]\n",
    "    \n",
    "    # argsort twice to get ranks (0 to n_genes-1)\n",
    "    ranks = np.argsort(np.argsort(pm_values, axis=1), axis=1)\n",
    "    \n",
    "    # Binning formula: floor(rank / n_genes * (n_bins - 1)) + 1\n",
    "    # Bins: 1 to 50\n",
    "    binned_medians = np.floor((ranks / n_genes) * (N_BINS - 1)).astype(np.int32) + 1\n",
    "    binned_medians = np.clip(binned_medians, 1, N_BINS - 1)\n",
    "    \n",
    "    # Noise masking: values < threshold -> Bin 0\n",
    "    noise_mask = pm_values < NOISE_THRESHOLD\n",
    "    binned_medians[noise_mask] = 0\n",
    "    \n",
    "    # åˆ›å»ºæ¿å·åˆ° binned vector çš„æ˜ å°„\n",
    "    plate_to_bin = dict(zip(plate_medians.index, binned_medians))\n",
    "    \n",
    "    # å¡«å……ä¸»çŸ©é˜µ\n",
    "    # ä½¿ç”¨ pandas map å¿«é€Ÿå¡«å……\n",
    "    # å¦‚æœæœ‰æ¿å·ç¼ºå¤±ï¼Œé»˜è®¤å¡«å……ä¸ºä¸­é—´å€¼(25)æˆ–0\n",
    "    default_vec = np.full(n_genes, int((N_BINS-1)/2), dtype=np.int32)\n",
    "    \n",
    "    # è·å–æ¯ä¸ªæ ·æœ¬å¯¹åº”çš„æ¿å·\n",
    "    sample_plates = sample_meta['rna_plate'].values\n",
    "    \n",
    "    # å¡«å……\n",
    "    missing_count = 0\n",
    "    for i, plate in enumerate(tqdm(sample_plates, desc=\"   Mapping to samples\")):\n",
    "        if plate in plate_to_bin:\n",
    "            X_ctx[i] = plate_to_bin[plate]\n",
    "        else:\n",
    "            X_ctx[i] = default_vec\n",
    "            missing_count += 1\n",
    "            \n",
    "    if missing_count > 0:\n",
    "        print(f\"   âš ï¸ Warning: {missing_count} samples had missing plate baselines (filled with default).\")\n",
    "\n",
    "    # 6. æ›´æ–°å¹¶ä¿å­˜\n",
    "    data['X_ctx'] = X_ctx\n",
    "    data['metadata']['rna_plate'] = sample_plates # ç¡®ä¿metadataé‡Œæœ‰æ¿å·\n",
    "    \n",
    "    print(f\"\\nğŸ’¾ Saving to {OUTPUT_FILE}...\")\n",
    "    with open(OUTPUT_FILE, 'wb') as f:\n",
    "        pickle.dump(data, f, protocol=4)\n",
    "        \n",
    "    print(f\"âœ… Done! Output Dataset Keys: {list(data.keys())}\")\n",
    "    print(f\"   X shape (Z-score): {data['X'].shape}, dtype={data['X'].dtype}\")\n",
    "    print(f\"   X_ctx shape (Bins): {data['X_ctx'].shape}, dtype={data['X_ctx'].dtype}\")\n",
    "\n",
    "# æ‰§è¡Œå¤„ç†\n",
    "if __name__ == \"__main__\":\n",
    "    load_and_process_ctx()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
