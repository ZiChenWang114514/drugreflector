{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a656975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ§¬ DRUGREFLECTOR DATA PREPROCESSING - Optimized Version\n",
      "   Fixed DOS filtering and timepoint matching\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "ğŸ”¬ LINCS 2020 Data Loader - Optimized Version\n",
      "================================================================================\n",
      "Data directory: E:\\ç§‘ç ”\\Models\\drugreflector\\datasets\\LINCS2020\n",
      "Dataset: Expanded CMap LINCS Resource 2020\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "STEP 1: Loading metadata\n",
      "================================================================================\n",
      "ğŸ“– Loading gene information...\n",
      "   File: geneinfo_beta.txt\n",
      "   âœ“ Loaded 12,328 genes\n",
      "   âœ“ Columns: ['gene_id', 'gene_symbol', 'ensembl_id', 'gene_title', 'gene_type', 'src', 'feature_space']\n",
      "   âœ“ Using 'feature_space' column to identify landmarks\n",
      "   âœ“ Landmark genes: 978\n",
      "   âœ“ Expected: 978\n",
      "\n",
      "   Landmark column indices (first 10): [2154 2155 2156 2157 2158 2159 2160 2161 2162 2163]\n",
      "   Sample IDs: ['10320', '2523', '10523', '11182', '596']\n",
      "   Sample symbols: ['ALDH7A1', 'PRKACA', 'PROS1', 'TRAPPC6A', 'RFX5']\n",
      "\n",
      "ğŸ“– Loading cell information...\n",
      "   File: cellinfo_beta.txt\n",
      "   âœ“ Loaded 240 cell lines\n",
      "   âœ“ Columns: ['cell_iname', 'cellosaurus_id', 'donor_age', 'donor_age_death', 'donor_disease_age_onset', 'doubling_time', 'growth_medium', 'provider_catalog_id', 'feature_id', 'cell_type']...\n",
      "   âœ“ Unique cell lines (cell_iname): 240\n",
      "\n",
      "   Sample cell lines:\n",
      "     - 1HAE\n",
      "     - AALE\n",
      "     - AG06263_2\n",
      "     - AG06840_A\n",
      "     - AG078N1_1\n",
      "\n",
      "ğŸ“– Loading compound information...\n",
      "   File: compoundinfo_beta.txt\n",
      "   âœ“ Loaded 39,321 compounds\n",
      "   âœ“ Columns: ['pert_id', 'cmap_name', 'target', 'moa', 'canonical_smiles', 'inchi_key', 'compound_aliases']\n",
      "   âœ“ Unique perturbagens: 34419\n",
      "\n",
      "   Sample compounds:\n",
      "     - BRD-A08715367: L-theanine\n",
      "     - BRD-A12237696: L-citrulline\n",
      "     - BRD-A18795974: BRD-A18795974\n",
      "\n",
      "================================================================================\n",
      "STEP 2: Loading Level 4 signatures\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "ğŸ“– Loading Level 4 Signatures\n",
      "================================================================================\n",
      "File: level4_beta_trt_cp_n1805898x12328.gctx\n",
      "\n",
      "ğŸ“– Reading GCTX file: level4_beta_trt_cp_n1805898x12328.gctx\n",
      "   File size: 82.94 GB\n",
      "   âš ï¸  Large file detected. Using memory-optimized loading...\n",
      "ğŸ“Š Inspecting HDF5 structure...\n",
      "   Available keys: ['0']\n",
      "   âœ“ Matrix shape: (1805898, 12328) (samples Ã— genes)\n",
      "\n",
      "ğŸ“‹ Loading metadata...\n",
      "   Reading sample metadata from: /0/META/ROW\n",
      "   Available row fields: ['id']\n",
      "   âœ“ Loaded 1 sample metadata fields\n",
      "   Reading gene metadata from: /0/META/COL\n",
      "   Available col fields: ['id']\n",
      "   âœ“ Loaded 1 gene metadata fields\n",
      "\n",
      "   âš ï¸ Detected ROW/COL swap. Correcting...\n",
      "\n",
      "   Sample metadata columns: ['id']...\n",
      "   Gene metadata columns: ['id']\n",
      "\n",
      "ğŸ”¬ Filtering to landmark genes...\n",
      "   âœ“ Using 978 landmark features out of 12328 total\n",
      "\n",
      "ğŸ¯ Loading data (memory-optimized)...\n",
      "   Reading 978 columns out of 12328...\n",
      "   Loading rows 1,220,000 to 1,805,898... (100.0%)\n",
      "   Finalizing matrix...\n",
      "   âœ“ Final matrix shape: (1805898, 978)\n",
      "   âœ“ Memory usage: 6.58 GB\n",
      "   âœ“ Data type: float32\n",
      "\n",
      "ğŸ“– Loading instance information...\n",
      "   File: instinfo_beta.txt\n",
      "   âœ“ Loaded 3,026,460 instances\n",
      "   âœ“ Columns: ['bead_batch', 'nearest_dose', 'pert_dose', 'pert_dose_unit', 'pert_idose', 'pert_time', 'pert_itime', 'pert_time_unit', 'cell_mfc_name', 'pert_mfc_id']...\n",
      "   âœ“ Using 'sample_id' as join key\n",
      "   ğŸ”— Merging GCTX metadata with instinfo on 'sample_id'...\n",
      "\n",
      "================================================================================\n",
      "STEP 3: Preparing training data\n",
      "================================================================================\n",
      "\n",
      "[DEBUG] row_meta shape: (1805898, 30)\n",
      "[DEBUG] columns (first 15): ['sample_id', 'bead_batch', 'nearest_dose', 'pert_dose', 'pert_dose_unit', 'pert_idose', 'pert_time', 'pert_itime', 'pert_time_unit', 'cell_mfc_name', 'pert_mfc_id', 'det_plate', 'det_well', 'rna_plate', 'rna_well']\n",
      "[DEBUG] Example row:\n",
      "sample_id     ABY001_A375_XH_X1_B15:A13\n",
      "pert_id                   BRD-K66175015\n",
      "pert_type                        trt_cp\n",
      "cell_iname                         A375\n",
      "pert_time                          24.0\n",
      "pert_dose                          10.0\n",
      "Name: 0, dtype: object\n",
      "\n",
      "================================================================================\n",
      "ğŸ” DRUGREFLECTOR QUALITY CONTROL PIPELINE - Optimized\n",
      "================================================================================\n",
      "Initial samples: 1,805,898\n",
      "Initial memory: 6.58 GB\n",
      "\n",
      "ğŸ“‹ Available metadata columns:\n",
      "   - sample_id: ABY001_A375_XH_X1_B15:A13\n",
      "   - bead_batch: b15\n",
      "   - nearest_dose: 10.0\n",
      "   - pert_dose: 10.0\n",
      "   - pert_dose_unit: uM\n",
      "   - pert_idose: 10 uM\n",
      "   - pert_time: 24.0\n",
      "   - pert_itime: 24 h\n",
      "   - pert_time_unit: h\n",
      "   - cell_mfc_name: A375\n",
      "   - pert_mfc_id: BRD-K66175015\n",
      "   - det_plate: ABY001_A375_XH_X1_B15\n",
      "   - det_well: A13\n",
      "   - rna_plate: ABY001_A375_XH_X1\n",
      "   - rna_well: A13\n",
      "   ... and 15 more\n",
      "\n",
      "âœ“ Using 'cell_iname' as cell line identifier\n",
      "\n",
      "Initial compounds: 34,419\n",
      "\n",
      "================================================================================\n",
      "FILTER 1: Remove DOS compounds (keep trt_cp only)\n",
      "================================================================================\n",
      "   pert_type value counts:\n",
      "     - trt_cp: 1,805,898 samples\n",
      "\n",
      "  âœ“ Keeping only 'trt_cp' perturbations\n",
      "  Removed 0 non-trt_cp observations\n",
      "  Remaining samples: 1,805,898\n",
      "  Remaining compounds: 34,419\n",
      "\n",
      "================================================================================\n",
      "FILTER 2: Remove compounds with <5 observations\n",
      "================================================================================\n",
      "  Compounds with â‰¥5 observations: 22,731/874\n",
      "  Remaining samples: 1,777,129\n",
      "  Remaining compounds: 22,731\n",
      "\n",
      "================================================================================\n",
      "FILTER 3: Remove observations with cosine similarity <0.12\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Calculating similarities (Numba-accelerated)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Computing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22731/22731 [17:17<00:00, 21.91it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ“ Mean similarity: 0.3843\n",
      "  Removed 37,549 low-similarity observations\n",
      "  Remaining samples: 1,739,580\n",
      "\n",
      "================================================================================\n",
      "FILTER 4: Remove compounds in <5 or >40 cell lines\n",
      "================================================================================\n",
      "  Compounds in 5-40 cell lines: 10,733/22,528\n",
      "  Removed 544,409 observations\n",
      "  Remaining samples: 1,195,171\n",
      "  Remaining compounds: 10,733\n",
      "\n",
      "================================================================================\n",
      "FILTER 5: Select most frequent dose in range 1.0-20.0 ÂµM\n",
      "================================================================================\n",
      "  Step 1/4: Parsing dose values...\n",
      "  âœ“ Parsed 1,805,898 dose entries\n",
      "  Step 2/4: Converting to ÂµM...\n",
      "  âœ“ Converted doses to ÂµM\n",
      "  Step 3/4: Filtering dose range 1.0-20.0 ÂµM...\n",
      "  âœ“ Samples in valid dose range (after previous filters): 780,076\n",
      "  âœ“ Removed 415,095 samples outside dose range\n",
      "  Step 4/4: Finding most frequent dose per compound...\n",
      "  Processing 10,721 compounds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Finding modal doses: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10721/10721 [00:02<00:00, 5339.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ“ Identified modal doses for 10,721 compounds\n",
      "  âœ“ Removed 298,932 samples with non-modal dose\n",
      "  âœ“ Remaining samples: 481,144\n",
      "  âœ“ Remaining compounds: 10,721\n",
      "\n",
      "================================================================================\n",
      "FILTER 6: Keep only measurements at [6, 24] hours\n",
      "================================================================================\n",
      "   Available timepoints (numeric):\n",
      "     - 24.0 hours: 1,338,831 samples\n",
      "     - 6.0 hours: 398,943 samples\n",
      "     - 3.0 hours: 31,925 samples\n",
      "     - 48.0 hours: 18,614 samples\n",
      "     - 4.0 hours: 11,372 samples\n",
      "     - 72.0 hours: 2,308 samples\n",
      "     - 2.0 hours: 1,135 samples\n",
      "     - 12.0 hours: 1,083 samples\n",
      "     - -666.0 hours: 581 samples\n",
      "     - 120.0 hours: 367 samples\n",
      "  âœ“ Kept samples at [6, 24] hours\n",
      "  Removed 7,046 observations (invalid timepoint)\n",
      "  Remaining samples: 474,098\n",
      "\n",
      "================================================================================\n",
      "âœ… FINAL DATASET\n",
      "================================================================================\n",
      "  Extracted 474,098 samples\n",
      "  Memory usage: 1.73 GB\n",
      "\n",
      "  Total samples: 474,098\n",
      "  Total compounds: 10,713\n",
      "  Cell lines: 141\n",
      "  Gene features: 978\n",
      "  Samples per compound (mean): 44.3\n",
      "  Samples per compound (median): 36\n",
      "  Compounds with >100 observations: 662\n",
      "\n",
      "ğŸ“Š Comparison with paper (SI page 2):\n",
      "  Paper: 425,242 obs, 9,597 compounds, 52 cell lines\n",
      "  Ours:  474,098 obs, 10,713 compounds, 141 cell lines\n",
      "  Compound retention rate: 31.1%\n",
      "  âœ“ Using geneinfo_beta.txt for gene names\n",
      "\n",
      "================================================================================\n",
      "STEP 4: Creating 3-fold splits\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "ğŸ² Creating 3-fold cross-validation splits\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Fold statistics:\n",
      "   Fold 0: 161,463 samples, 10,713 compounds\n",
      "   Fold 1: 158,301 samples, 10,712 compounds\n",
      "   Fold 2: 154,334 samples, 10,712 compounds\n",
      "\n",
      "================================================================================\n",
      "STEP 5: Saving processed data\n",
      "================================================================================\n",
      "ğŸ’¾ Saving to: E:\\ç§‘ç ”\\Models\\drugreflector\\processed_data\\training_data_lincs2020_25111801.pkl\n",
      "âœ“ Saved successfully!\n",
      "   File size: 1887.6 MB\n",
      "\n",
      "================================================================================\n",
      "âœ… DATA PREPARATION COMPLETE!\n",
      "================================================================================\n",
      "ğŸ“ Output: E:\\ç§‘ç ”\\Models\\drugreflector\\processed_data\\training_data_lincs2020_25111801.pkl\n",
      "\n",
      "ğŸ“Š Final dataset:\n",
      "   â€¢ Samples: 474,098\n",
      "   â€¢ Compounds: 10,713\n",
      "   â€¢ Genes: 978\n",
      "   â€¢ Memory: 1.73 GB\n",
      "\n",
      "ğŸ“ˆ Comparison with paper:\n",
      "   Samples: 474,098 / 425,242 (111.5%)\n",
      "   Compounds: 10,713 / 9,597 (111.6%)\n",
      "\n",
      "ğŸ¯ Ready for training!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "LINCS 2020æ•°æ®åŠ è½½å’Œé¢„å¤„ç†è„šæœ¬ - ä¼˜åŒ–ç‰ˆ\n",
    "ä¿®æ­£DOSè¿‡æ»¤å’Œæ—¶é—´ç‚¹åŒ¹é…é—®é¢˜\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import gzip\n",
    "import shutil\n",
    "import glob\n",
    "from scipy.spatial.distance import cosine, pdist, squareform\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from numba import jit, prange\n",
    "from sklearn.preprocessing import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, Tuple, Optional\n",
    "import warnings\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class LINCS2020DataLoader:\n",
    "    \"\"\"\n",
    "    åŠ è½½å’Œé¢„å¤„ç†LINCS 2020æ•°æ® - ä¼˜åŒ–ç‰ˆ\n",
    "    ä¿®æ­£DOSè¿‡æ»¤å’Œæ—¶é—´ç‚¹åŒ¹é…\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.gene_info = None\n",
    "        self.cell_info = None\n",
    "        self.compound_info = None\n",
    "        self.inst_info = None  \n",
    "        self.signatures = None\n",
    "        self.decompressed_files = []\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ğŸ”¬ LINCS 2020 Data Loader - Optimized Version\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Data directory: {self.data_dir}\")\n",
    "        print(f\"Dataset: Expanded CMap LINCS Resource 2020\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    def load_gene_info(self):\n",
    "        \"\"\"åŠ è½½åŸºå› ä¿¡æ¯ï¼Œè·å–978ä¸ªlandmark genes\"\"\"\n",
    "        gene_file = self.data_dir / \"geneinfo_beta.txt\"\n",
    "        \n",
    "        if not gene_file.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"âŒ Gene info file not found: {gene_file}\\n\"\n",
    "                f\"   Please download 'geneinfo_beta.txt' from:\\n\"\n",
    "                f\"   https://clue.io/data/CMap2020#LINCS2020\"\n",
    "            )\n",
    "        \n",
    "        print(f\"ğŸ“– Loading gene information...\")\n",
    "        print(f\"   File: {gene_file.name}\")\n",
    "        \n",
    "        gene_info = pd.read_csv(gene_file, sep='\\t')\n",
    "        \n",
    "        print(f\"   âœ“ Loaded {len(gene_info):,} genes\")\n",
    "        print(f\"   âœ“ Columns: {list(gene_info.columns)}\")\n",
    "        \n",
    "        if 'feature_space' in gene_info.columns:\n",
    "            landmark_mask = gene_info['feature_space'] == 'landmark'\n",
    "            landmark_genes = gene_info[landmark_mask].copy()\n",
    "            print(f\"   âœ“ Using 'feature_space' column to identify landmarks\")\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Cannot identify landmark genes. 'feature_space' column not found.\\n\"\n",
    "                f\"Available columns: {list(gene_info.columns)}\"\n",
    "            )\n",
    "\n",
    "        print(f\"   âœ“ Landmark genes: {len(landmark_genes):,}\")\n",
    "        print(f\"   âœ“ Expected: 978\")\n",
    "\n",
    "        self.landmark_col_indices = np.where(landmark_mask.values)[0]\n",
    "        print(f\"\\n   Landmark column indices (first 10): {self.landmark_col_indices[:10]}\")\n",
    "\n",
    "        self.landmark_gene_ids = set(landmark_genes['gene_id'].astype(str).values)\n",
    "        self.landmark_gene_symbols = set(landmark_genes['gene_symbol'].astype(str).values)\n",
    "\n",
    "        print(f\"   Sample IDs: {list(self.landmark_gene_ids)[:5]}\")\n",
    "        print(f\"   Sample symbols: {list(self.landmark_gene_symbols)[:5]}\")\n",
    "\n",
    "        self.gene_info = gene_info\n",
    "        return gene_info\n",
    "    \n",
    "    def load_cell_info(self):\n",
    "        \"\"\"åŠ è½½ç»†èƒç³»ä¿¡æ¯\"\"\"\n",
    "        cell_file = self.data_dir / \"cellinfo_beta.txt\"\n",
    "        \n",
    "        if not cell_file.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"âŒ Cell info file not found: {cell_file}\\n\"\n",
    "                f\"   Please download 'cellinfo_beta.txt' from:\\n\"\n",
    "                f\"   https://clue.io/data/CMap2020#LINCS2020\"\n",
    "            )\n",
    "        \n",
    "        print(f\"\\nğŸ“– Loading cell information...\")\n",
    "        print(f\"   File: {cell_file.name}\")\n",
    "        \n",
    "        cell_info = pd.read_csv(cell_file, sep='\\t')\n",
    "        \n",
    "        print(f\"   âœ“ Loaded {len(cell_info):,} cell lines\")\n",
    "        print(f\"   âœ“ Columns: {list(cell_info.columns[:10])}...\")\n",
    "        \n",
    "        if 'cell_iname' in cell_info.columns:\n",
    "            print(f\"   âœ“ Unique cell lines (cell_iname): {cell_info['cell_iname'].nunique()}\")\n",
    "        \n",
    "        print(f\"\\n   Sample cell lines:\")\n",
    "        for cell in cell_info['cell_iname'].head(5).values:\n",
    "            print(f\"     - {cell}\")\n",
    "        \n",
    "        self.cell_info = cell_info\n",
    "        return cell_info\n",
    "    \n",
    "    def load_compound_info(self):\n",
    "        \"\"\"åŠ è½½åŒ–åˆç‰©ä¿¡æ¯\"\"\"\n",
    "        compound_file = self.data_dir / \"compoundinfo_beta.txt\"\n",
    "        \n",
    "        if not compound_file.exists():\n",
    "            print(f\"âš ï¸  Compound info file not found: {compound_file}\")\n",
    "            print(f\"   This file is optional but recommended.\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"\\nğŸ“– Loading compound information...\")\n",
    "        print(f\"   File: {compound_file.name}\")\n",
    "        \n",
    "        compound_info = pd.read_csv(compound_file, sep='\\t')\n",
    "        \n",
    "        print(f\"   âœ“ Loaded {len(compound_info):,} compounds\")\n",
    "        print(f\"   âœ“ Columns: {list(compound_info.columns)}\")\n",
    "        \n",
    "        if 'pert_id' in compound_info.columns:\n",
    "            print(f\"   âœ“ Unique perturbagens: {compound_info['pert_id'].nunique()}\")\n",
    "        \n",
    "        print(f\"\\n   Sample compounds:\")\n",
    "        for _, row in compound_info.head(3).iterrows():\n",
    "            name = row.get('cmap_name', 'Unknown')\n",
    "            print(f\"     - {row['pert_id']}: {name}\")\n",
    "        \n",
    "        self.compound_info = compound_info\n",
    "        return compound_info\n",
    "    \n",
    "    def load_instance_info(self):\n",
    "        \"\"\"åŠ è½½å®ä¾‹ä¿¡æ¯\"\"\"\n",
    "        inst_file = self.data_dir / \"instinfo_beta.txt\"\n",
    "\n",
    "        if not inst_file.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"âŒ Instance info file not found: {inst_file}\\n\"\n",
    "                f\"   Please download 'instinfo_beta.txt' from:\\n\"\n",
    "                f\"   https://clue.io/data/CMap2020#LINCS2020\"\n",
    "            )\n",
    "\n",
    "        print(f\"\\nğŸ“– Loading instance information...\")\n",
    "        print(f\"   File: {inst_file.name}\")\n",
    "\n",
    "        inst_info = pd.read_csv(inst_file, sep='\\t')\n",
    "\n",
    "        print(f\"   âœ“ Loaded {len(inst_info):,} instances\")\n",
    "        print(f\"   âœ“ Columns: {list(inst_info.columns[:10])}...\")\n",
    "\n",
    "        if 'inst_id' in inst_info.columns:\n",
    "            self.instance_join_col = 'inst_id'\n",
    "        elif 'sample_id' in inst_info.columns:\n",
    "            self.instance_join_col = 'sample_id'\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Neither 'inst_id' nor 'sample_id' found in instinfo_beta.txt. \"\n",
    "                f\"Available columns: {list(inst_info.columns)}\"\n",
    "            )\n",
    "\n",
    "        print(f\"   âœ“ Using '{self.instance_join_col}' as join key\")\n",
    "\n",
    "        self.inst_info = inst_info\n",
    "        return inst_info\n",
    "\n",
    "    def decompress_gctx_file(self, gctx_file):\n",
    "        \"\"\"å¦‚æœGCTXæ–‡ä»¶è¢«å‹ç¼©ï¼Œåˆ™è§£å‹\"\"\"\n",
    "        gctx_file = Path(gctx_file)\n",
    "        \n",
    "        if not gctx_file.exists():\n",
    "            raise FileNotFoundError(f\"GCTX file not found: {gctx_file}\")\n",
    "        \n",
    "        if str(gctx_file).endswith('.gz'):\n",
    "            print(f\"âš ï¸  Detected compressed GCTX file: {gctx_file.name}\")\n",
    "            \n",
    "            decompressed_dir = self.data_dir / \"_decompressed\"\n",
    "            decompressed_dir.mkdir(exist_ok=True)\n",
    "            \n",
    "            output_path = decompressed_dir / gctx_file.stem\n",
    "            \n",
    "            if output_path.exists():\n",
    "                print(f\"âœ“ Found existing decompressed file: {output_path.name}\")\n",
    "                return str(output_path)\n",
    "            \n",
    "            print(f\"ğŸ“¦ Decompressing...\")\n",
    "            with gzip.open(gctx_file, 'rb') as f_in:\n",
    "                with open(output_path, 'wb') as f_out:\n",
    "                    shutil.copyfileobj(f_in, f_out, length=8*1024*1024)\n",
    "            \n",
    "            print(f\"âœ“ Decompressed to: {output_path}\")\n",
    "            return str(output_path)\n",
    "        \n",
    "        return str(gctx_file)\n",
    "    \n",
    "    def read_gctx(self, gctx_file, use_landmark_only=True):\n",
    "        \"\"\"è¯»å–GCTXæ–‡ä»¶ - å†…å­˜ä¼˜åŒ–ç‰ˆ\"\"\"\n",
    "        gctx_file = str(gctx_file)\n",
    "        print(f\"\\nğŸ“– Reading GCTX file: {Path(gctx_file).name}\")\n",
    "        \n",
    "        file_size_gb = Path(gctx_file).stat().st_size / (1024**3)\n",
    "        print(f\"   File size: {file_size_gb:.2f} GB\")\n",
    "        \n",
    "        if file_size_gb > 50:\n",
    "            print(f\"   âš ï¸  Large file detected. Using memory-optimized loading...\")\n",
    "        \n",
    "        gctx_file = self.decompress_gctx_file(gctx_file)\n",
    "        \n",
    "        with h5py.File(gctx_file, 'r') as f:\n",
    "            print(f\"ğŸ“Š Inspecting HDF5 structure...\")\n",
    "            print(f\"   Available keys: {list(f.keys())}\")\n",
    "            \n",
    "            if '/0/DATA/0/matrix' in f:\n",
    "                matrix_dataset = f['/0/DATA/0/matrix']\n",
    "                row_path = '/0/META/ROW'\n",
    "                col_path = '/0/META/COL'\n",
    "            elif '/matrix' in f:\n",
    "                matrix_dataset = f['/matrix']\n",
    "                row_path = '/row'\n",
    "                col_path = '/col'\n",
    "            else:\n",
    "                raise ValueError(f\"Cannot find matrix in GCTX file. Available keys: {list(f.keys())}\")\n",
    "            \n",
    "            matrix_shape = matrix_dataset.shape\n",
    "            print(f\"   âœ“ Matrix shape: {matrix_shape} (samples Ã— genes)\")\n",
    "            \n",
    "            print(f\"\\nğŸ“‹ Loading metadata...\")\n",
    "            \n",
    "            # è¯»å–æ ·æœ¬å…ƒæ•°æ®\n",
    "            sample_meta = {}\n",
    "            if row_path in f:\n",
    "                print(f\"   Reading sample metadata from: {row_path}\")\n",
    "                print(f\"   Available row fields: {list(f[row_path].keys())}\")\n",
    "                \n",
    "                for key in f[row_path].keys():\n",
    "                    data = f[f'{row_path}/{key}'][:]\n",
    "                    if data.dtype.kind in ['S', 'O', 'U']:\n",
    "                        try:\n",
    "                            sample_meta[key] = np.char.decode(data.astype('S'), 'utf-8')\n",
    "                        except:\n",
    "                            sample_meta[key] = data.astype(str)\n",
    "                    else:\n",
    "                        sample_meta[key] = data\n",
    "                \n",
    "                print(f\"   âœ“ Loaded {len(sample_meta)} sample metadata fields\")\n",
    "            else:\n",
    "                raise ValueError(f\"Cannot find row metadata at: {row_path}\")\n",
    "            \n",
    "            # è¯»å–åŸºå› å…ƒæ•°æ®\n",
    "            gene_meta = {}\n",
    "            if col_path in f:\n",
    "                print(f\"   Reading gene metadata from: {col_path}\")\n",
    "                print(f\"   Available col fields: {list(f[col_path].keys())}\")\n",
    "                \n",
    "                for key in f[col_path].keys():\n",
    "                    data = f[f'{col_path}/{key}'][:]\n",
    "                    if data.dtype.kind in ['S', 'O', 'U']:\n",
    "                        try:\n",
    "                            gene_meta[key] = np.char.decode(data.astype('S'), 'utf-8')\n",
    "                        except:\n",
    "                            gene_meta[key] = data.astype(str)\n",
    "                    else:\n",
    "                        gene_meta[key] = data\n",
    "                \n",
    "                print(f\"   âœ“ Loaded {len(gene_meta)} gene metadata fields\")\n",
    "            else:\n",
    "                raise ValueError(f\"Cannot find col metadata at: {col_path}\")\n",
    "            \n",
    "            sample_df = pd.DataFrame(sample_meta)\n",
    "            gene_df = pd.DataFrame(gene_meta)\n",
    "\n",
    "            # è‡ªåŠ¨æ£€æµ‹å¹¶äº¤æ¢ROW/COL\n",
    "            if self.gene_info is not None:\n",
    "                n_features_expected = len(self.gene_info)\n",
    "                if len(sample_df) == n_features_expected and len(gene_df) != n_features_expected:\n",
    "                    print(f\"\\n   âš ï¸ Detected ROW/COL swap. Correcting...\")\n",
    "                    sample_df, gene_df = gene_df, sample_df\n",
    "\n",
    "            print(f\"\\n   Sample metadata columns: {list(sample_df.columns[:10])}...\")\n",
    "            print(f\"   Gene metadata columns: {list(gene_df.columns)}\")\n",
    "            \n",
    "            # ç¡®å®šlandmarkåŸºå› çš„åˆ—ç´¢å¼•\n",
    "            landmark_col_indices = None\n",
    "            if use_landmark_only:\n",
    "                print(f\"\\nğŸ”¬ Filtering to landmark genes...\")\n",
    "\n",
    "                if (self.gene_info is None) or (not hasattr(self, \"landmark_col_indices\")):\n",
    "                    print(f\"   Loading gene info to get landmark indices...\")\n",
    "                    self.load_gene_info()\n",
    "\n",
    "                if len(self.gene_info) != matrix_shape[1]:\n",
    "                    print(f\"   âš ï¸  Warning: geneinfo rows ({len(self.gene_info)}) \"\n",
    "                        f\"!= GCTX feature count ({matrix_shape[1]})\")\n",
    "                \n",
    "                landmark_col_indices = np.array(self.landmark_col_indices, dtype=int)\n",
    "                print(f\"   âœ“ Using {len(landmark_col_indices)} landmark features \"\n",
    "                    f\"out of {matrix_shape[1]} total\")\n",
    "            \n",
    "            # å†…å­˜ä¼˜åŒ–ï¼šåˆ†å—è¯»å–\n",
    "            print(f\"\\nğŸ¯ Loading data (memory-optimized)...\")\n",
    "            if landmark_col_indices is not None:\n",
    "                print(f\"   Reading {len(landmark_col_indices)} columns out of {matrix_shape[1]}...\")\n",
    "                chunk_size = 610000\n",
    "                chunks = []\n",
    "                \n",
    "                for start_idx in range(0, matrix_shape[0], chunk_size):\n",
    "                    end_idx = min(start_idx + chunk_size, matrix_shape[0])\n",
    "                    print(f\"   Loading rows {start_idx:,} to {end_idx:,}... ({end_idx/matrix_shape[0]*100:.1f}%)\", end='\\r')\n",
    "                    \n",
    "                    chunk = matrix_dataset[start_idx:end_idx, landmark_col_indices].astype(np.float32)\n",
    "                    chunks.append(chunk)\n",
    "                    \n",
    "                    if len(chunks) >= 10:\n",
    "                        print(f\"\\n   Consolidating chunks...\")\n",
    "                        merged = np.vstack(chunks)\n",
    "                        chunks = [merged]\n",
    "                        gc.collect()\n",
    "                \n",
    "                print(f\"\\n   Finalizing matrix...\")\n",
    "                matrix = np.vstack(chunks) if len(chunks) > 1 else chunks[0]\n",
    "                del chunks\n",
    "                gc.collect()\n",
    "            else:\n",
    "                print(f\"   Reading full matrix...\")\n",
    "                matrix = matrix_dataset[:].astype(np.float32)\n",
    "            \n",
    "            print(f\"   âœ“ Final matrix shape: {matrix.shape}\")\n",
    "            print(f\"   âœ“ Memory usage: {matrix.nbytes / (1024**3):.2f} GB\")\n",
    "            print(f\"   âœ“ Data type: {matrix.dtype}\")\n",
    "        \n",
    "        return matrix, sample_df, gene_df\n",
    "    \n",
    "    def load_level4_signatures(self):\n",
    "        \"\"\"åŠ è½½Level 4æ•°æ®\"\"\"\n",
    "        level4_file = self.data_dir / \"level4_beta_trt_cp_n1805898x12328.gctx\"\n",
    "        \n",
    "        if not level4_file.exists():\n",
    "            pattern = self.data_dir / \"level4_beta_trt_cp*.gctx\"\n",
    "            files = glob.glob(str(pattern))\n",
    "            \n",
    "            if not files:\n",
    "                raise FileNotFoundError(\n",
    "                    f\"âŒ Level 4 file not found: {level4_file}\\n\"\n",
    "                    f\"   Please download from: https://clue.io/data/CMap2020#LINCS2020\"\n",
    "                )\n",
    "            \n",
    "            level4_file = files[0]\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ğŸ“– Loading Level 4 Signatures\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"File: {level4_file.name}\")\n",
    "        \n",
    "        matrix, sample_meta, gene_meta = self.read_gctx(\n",
    "            level4_file, \n",
    "            use_landmark_only=True\n",
    "        )\n",
    "\n",
    "        # ç”¨instinfoè¡¥é½metadata\n",
    "        if 'id' in sample_meta.columns:\n",
    "            sample_meta = sample_meta.rename(columns={'id': 'sample_id'})\n",
    "        elif 'sample_id' not in sample_meta.columns:\n",
    "            raise ValueError(\"Cannot find 'id' or 'sample_id' in GCTX ROW metadata\")\n",
    "\n",
    "        if self.inst_info is None:\n",
    "            self.load_instance_info()\n",
    "\n",
    "        inst_info = self.inst_info\n",
    "        join_col = getattr(self, \"instance_join_col\", None)\n",
    "        \n",
    "        if join_col is None:\n",
    "            join_col = 'sample_id' if 'sample_id' in inst_info.columns else 'inst_id'\n",
    "\n",
    "        if join_col not in sample_meta.columns:\n",
    "            if join_col == 'inst_id' and 'sample_id' in sample_meta.columns:\n",
    "                print(\"   âš ï¸ Using 'sample_id' instead of 'inst_id'\")\n",
    "                join_col = 'sample_id'\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Join column '{join_col}' not found in GCTX metadata. \"\n",
    "                    f\"Available: {list(sample_meta.columns)}\"\n",
    "                )\n",
    "\n",
    "        print(f\"   ğŸ”— Merging GCTX metadata with instinfo on '{join_col}'...\")\n",
    "        merged_meta = sample_meta.merge(inst_info, on=join_col, how='left')\n",
    "\n",
    "        if 'pert_id' not in merged_meta.columns:\n",
    "            raise ValueError(\"After merging, 'pert_id' is still missing\")\n",
    "\n",
    "        n_missing = merged_meta['pert_id'].isna().sum()\n",
    "        if n_missing > 0:\n",
    "            print(f\"   âš ï¸ {n_missing:,} rows have missing pert_id\")\n",
    "\n",
    "        self.signatures = {\n",
    "            'matrix': matrix,\n",
    "            'row_meta': merged_meta,\n",
    "            'col_meta': gene_meta\n",
    "        }\n",
    "\n",
    "        return matrix, merged_meta, gene_meta\n",
    "    \n",
    "    def calculate_cosine_similarity_to_nearest_replicate(\n",
    "        self, \n",
    "        matrix: np.ndarray, \n",
    "        pert_ids: pd.Series\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        ä½¿ç”¨Numba JITç¼–è¯‘çš„è¶…é«˜æ•ˆç‰ˆæœ¬\n",
    "        \n",
    "        è®¡ç®—æ¯ä¸ªæ ·æœ¬ä¸å…¶æœ€è¿‘åŒåŒ–åˆç‰©å¤åˆ¶å“çš„ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "        \"\"\"\n",
    "       \n",
    "        print(f\"\\nğŸ“Š Calculating similarities (Numba-accelerated)...\")\n",
    "        \n",
    "        # é¢„å½’ä¸€åŒ–\n",
    "        matrix_norm = normalize(matrix, norm='l2', axis=1).astype(np.float32)\n",
    "        \n",
    "        @jit(nopython=True, parallel=True, fastmath=True)\n",
    "        def compute_max_similarities(data, indices, n_total):\n",
    "            \"\"\"NumbaåŠ é€Ÿçš„æ ¸å¿ƒè®¡ç®—\"\"\"\n",
    "            n = len(indices)\n",
    "            result = np.full(n_total, -np.inf, dtype=np.float32)\n",
    "            \n",
    "            for i in prange(n):\n",
    "                idx_i = indices[i]\n",
    "                vec_i = data[i]\n",
    "                max_sim = -np.inf\n",
    "                \n",
    "                for j in range(n):\n",
    "                    if i != j:\n",
    "                        # ç‚¹ç§¯ï¼ˆå·²å½’ä¸€åŒ– = ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰\n",
    "                        sim = np.dot(vec_i, data[j])\n",
    "                        if sim > max_sim:\n",
    "                            max_sim = sim\n",
    "                \n",
    "                result[idx_i] = max_sim\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        nearest_similarities = np.zeros(len(pert_ids), dtype=np.float32)\n",
    "        \n",
    "        # æŒ‰åŒ–åˆç‰©åˆ†ç»„\n",
    "        pert_ids_array = pert_ids.values\n",
    "        unique_perts = np.unique(pert_ids_array)\n",
    "        \n",
    "        from tqdm import tqdm\n",
    "        for pert_id in tqdm(unique_perts, desc=\"   Computing\"):\n",
    "            mask = pert_ids_array == pert_id\n",
    "            indices = np.where(mask)[0]\n",
    "            \n",
    "            if len(indices) < 2:\n",
    "                nearest_similarities[indices] = 0.0\n",
    "                continue\n",
    "            \n",
    "            pert_data = matrix_norm[mask]\n",
    "            sims = compute_max_similarities(pert_data, indices, len(pert_ids))\n",
    "            nearest_similarities[indices] = sims[indices]\n",
    "        \n",
    "        print(f\"   âœ“ Mean similarity: {nearest_similarities.mean():.4f}\")\n",
    "        return nearest_similarities\n",
    "    \n",
    "    def prepare_training_data(\n",
    "        self, \n",
    "        min_observations_per_compound=5,\n",
    "        min_replicate_similarity=0.12,\n",
    "        dose_range=(1.0, 20.0),\n",
    "        valid_timepoints=[6, 24],  # ä¿®æ”¹ä¸ºæ•°å€¼åˆ—è¡¨\n",
    "        min_cell_lines=5,\n",
    "        max_cell_lines=40,\n",
    "        remove_dos=True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        å‡†å¤‡è®­ç»ƒæ•°æ®\n",
    "        ä¿®æ­£DOSè¿‡æ»¤å’Œæ—¶é—´ç‚¹åŒ¹é…\n",
    "        \"\"\"\n",
    "        if self.signatures is None:\n",
    "            raise ValueError(\"Please load signatures first\")\n",
    "        \n",
    "        matrix = self.signatures['matrix']\n",
    "        row_meta = self.signatures['row_meta'].copy()\n",
    "        col_meta = self.signatures['col_meta']\n",
    "        \n",
    "        print(f\"\\n[DEBUG] row_meta shape: {row_meta.shape}\")\n",
    "        print(f\"[DEBUG] columns (first 15): {list(row_meta.columns[:15])}\")\n",
    "        print(f\"[DEBUG] Example row:\")\n",
    "        print(row_meta.iloc[0][['sample_id', 'pert_id', 'pert_type', 'cell_iname', 'pert_time', 'pert_dose']])\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"ğŸ” DRUGREFLECTOR QUALITY CONTROL PIPELINE - Optimized\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Initial samples: {len(row_meta):,}\")\n",
    "        print(f\"Initial memory: {matrix.nbytes / (1024**3):.2f} GB\")\n",
    "        \n",
    "        print(f\"\\nğŸ“‹ Available metadata columns:\")\n",
    "        for col in row_meta.columns[:15]:\n",
    "            sample_val = row_meta[col].iloc[0] if len(row_meta) > 0 else 'N/A'\n",
    "            print(f\"   - {col}: {sample_val}\")\n",
    "        if len(row_meta.columns) > 15:\n",
    "            print(f\"   ... and {len(row_meta.columns) - 15} more\")\n",
    "        \n",
    "        # æ£€æŸ¥å¿…éœ€å­—æ®µ\n",
    "        required_fields = ['pert_id']\n",
    "        missing_fields = [f for f in required_fields if f not in row_meta.columns]\n",
    "        if missing_fields:\n",
    "            raise ValueError(f\"Missing required fields: {missing_fields}\")\n",
    "        \n",
    "        # ç¡®å®šç»†èƒç³»IDå­—æ®µ\n",
    "        cell_id_col = None\n",
    "        for possible_col in ['cell_id', 'cell_iname', 'cell_mfc_name']:\n",
    "            if possible_col in row_meta.columns:\n",
    "                cell_id_col = possible_col\n",
    "                print(f\"\\nâœ“ Using '{cell_id_col}' as cell line identifier\")\n",
    "                break\n",
    "        \n",
    "        if cell_id_col is None:\n",
    "            print(f\"\\nâš ï¸  Warning: Cannot find cell line identifier\")\n",
    "        \n",
    "        valid_mask = np.ones(len(row_meta), dtype=bool)\n",
    "        \n",
    "        initial_compounds = row_meta['pert_id'].nunique()\n",
    "        print(f\"\\nInitial compounds: {initial_compounds:,}\")\n",
    "        \n",
    "        # ========== Filter 1: Remove DOS compounds (ä¼˜åŒ–) ==========\n",
    "        if remove_dos:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"FILTER 1: Remove DOS compounds (keep trt_cp only)\")\n",
    "            print(f\"{'='*80}\")\n",
    "            \n",
    "            # ä½¿ç”¨pert_typeå­—æ®µï¼Œåªä¿ç•™trt_cp\n",
    "            if 'pert_type' in row_meta.columns:\n",
    "                # æ£€æŸ¥pert_typeçš„å€¼åˆ†å¸ƒ\n",
    "                print(f\"   pert_type value counts:\")\n",
    "                pert_type_counts = row_meta['pert_type'].value_counts()\n",
    "                for ptype, count in pert_type_counts.items():\n",
    "                    print(f\"     - {ptype}: {count:,} samples\")\n",
    "                \n",
    "                # åªä¿ç•™trt_cpç±»å‹\n",
    "                dos_mask = row_meta['pert_type'] == 'trt_cp'\n",
    "                n_removed = (~dos_mask).sum()\n",
    "                \n",
    "                print(f\"\\n  âœ“ Keeping only 'trt_cp' perturbations\")\n",
    "                print(f\"  Removed {n_removed:,} non-trt_cp observations\")\n",
    "            else:\n",
    "                # é™çº§æ–¹æ¡ˆï¼šå¦‚æœæ²¡æœ‰pert_typeï¼Œä½¿ç”¨æ›´ä¿å®ˆçš„BRD-Kè§„åˆ™\n",
    "                print(f\"   âš ï¸ 'pert_type' not found, using fallback method\")\n",
    "                # åªè¿‡æ»¤æ˜ç¡®æ ‡è®°ä¸ºDOSçš„\n",
    "                dos_mask = ~row_meta['pert_id'].str.contains('DOS', case=False, na=False)\n",
    "                n_removed = (~dos_mask).sum()\n",
    "                print(f\"  Removed {n_removed:,} DOS observations\")\n",
    "            \n",
    "            valid_mask &= dos_mask\n",
    "            print(f\"  Remaining samples: {valid_mask.sum():,}\")\n",
    "            print(f\"  Remaining compounds: {row_meta.loc[valid_mask, 'pert_id'].nunique():,}\")\n",
    "        \n",
    "        # ========== Filter 2: Minimum observations ==========\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FILTER 2: Remove compounds with <{min_observations_per_compound} observations\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        valid_meta = row_meta[valid_mask]\n",
    "        obs_counts = valid_meta.groupby('pert_id').size()\n",
    "        valid_perts = obs_counts[obs_counts >= min_observations_per_compound].index\n",
    "        \n",
    "        print(f\"  Compounds with â‰¥{min_observations_per_compound} observations: \"\n",
    "              f\"{len(valid_perts):,}/{obs_counts.nunique():,}\")\n",
    "        \n",
    "        obs_mask = row_meta['pert_id'].isin(valid_perts)\n",
    "        valid_mask &= obs_mask\n",
    "        \n",
    "        print(f\"  Remaining samples: {valid_mask.sum():,}\")\n",
    "        print(f\"  Remaining compounds: {row_meta.loc[valid_mask, 'pert_id'].nunique():,}\")\n",
    "        \n",
    "        del valid_meta, obs_counts, obs_mask\n",
    "        gc.collect()\n",
    "            \n",
    "        # ========== Filter 3: Cosine similarity ==========\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FILTER 3: Remove observations with cosine similarity <{min_replicate_similarity}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        valid_indices = np.where(valid_mask)[0]\n",
    "        valid_matrix = matrix[valid_mask]\n",
    "        valid_pert_ids = row_meta.loc[valid_mask, 'pert_id'].reset_index(drop=True)\n",
    "        \n",
    "        nearest_similarities = self.calculate_cosine_similarity_to_nearest_replicate(\n",
    "            valid_matrix, \n",
    "            valid_pert_ids\n",
    "        )\n",
    "        \n",
    "        full_similarities = np.zeros(len(row_meta), dtype=np.float32)\n",
    "        full_similarities[valid_indices] = nearest_similarities\n",
    "        \n",
    "        sim_mask = (full_similarities >= min_replicate_similarity) | (~valid_mask)\n",
    "        n_removed_sim = (~sim_mask & valid_mask).sum()\n",
    "        valid_mask &= sim_mask\n",
    "        \n",
    "        print(f\"  Removed {n_removed_sim:,} low-similarity observations\")\n",
    "        print(f\"  Remaining samples: {valid_mask.sum():,}\")\n",
    "        \n",
    "        del valid_matrix, valid_pert_ids, nearest_similarities, full_similarities, sim_mask\n",
    "        gc.collect()\n",
    "        \n",
    "        # ========== Filter 4: Dose selection (ä¿®æ­£ç‰ˆ + è¯¦ç»†ç»Ÿè®¡) ==========\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FILTER 4: Select most frequent dose in range {dose_range[0]}-{dose_range[1]} ÂµM\")\n",
    "        print(f\"{'='*80}\")\n",
    "\n",
    "        if 'pert_dose' in row_meta.columns:\n",
    "            \n",
    "            # Step 1: è§£æå‰‚é‡\n",
    "            print(f\"  Step 1/5: Parsing dose values...\")\n",
    "            dose_str = row_meta['pert_dose'].astype(str)\n",
    "            row_meta['dose_value'] = pd.to_numeric(\n",
    "                dose_str.str.extract(r'([\\d.]+)', expand=False), \n",
    "                errors='coerce'\n",
    "            )\n",
    "            row_meta['dose_unit'] = dose_str.str.extract(r'([a-zA-Z]+)', expand=False).str.lower().fillna('um')\n",
    "            \n",
    "            n_parsed = (~pd.isna(row_meta['dose_value'])).sum()\n",
    "            n_missing = pd.isna(row_meta['dose_value']).sum()\n",
    "            print(f\"  âœ“ Successfully parsed: {n_parsed:,} entries\")\n",
    "            print(f\"  âš ï¸  Missing/invalid doses: {n_missing:,} entries\")\n",
    "            \n",
    "            # Step 2: è½¬æ¢å•ä½\n",
    "            print(f\"\\n  Step 2/5: Converting to ÂµM...\")\n",
    "            dose_value = row_meta['dose_value'].values\n",
    "            dose_unit = row_meta['dose_unit'].values\n",
    "            \n",
    "            conditions = [\n",
    "                dose_unit == 'nm',\n",
    "                dose_unit == 'mm'\n",
    "            ]\n",
    "            choices = [\n",
    "                dose_value / 1000,\n",
    "                dose_value * 1000\n",
    "            ]\n",
    "            row_meta['dose_uM'] = np.select(conditions, choices, default=dose_value)\n",
    "            \n",
    "            # ç»Ÿè®¡å•ä½åˆ†å¸ƒ\n",
    "            print(f\"  Dose unit distribution:\")\n",
    "            unit_counts = row_meta['dose_unit'].value_counts()\n",
    "            for unit, count in unit_counts.head(5).items():\n",
    "                print(f\"    - {unit}: {count:,} samples\")\n",
    "            \n",
    "            # Step 3: å‰‚é‡åˆ†å¸ƒç»Ÿè®¡ (åœ¨è¿‡æ»¤å‰)\n",
    "            print(f\"\\n  Step 3/5: Dose distribution analysis...\")\n",
    "            \n",
    "            # åªåˆ†ææœ‰æ•ˆå‰‚é‡å€¼\n",
    "            valid_doses = row_meta.loc[valid_mask & (~pd.isna(row_meta['dose_uM'])), 'dose_uM']\n",
    "            \n",
    "            if len(valid_doses) > 0:\n",
    "                print(f\"\\n  ğŸ“Š Dose statistics (before filtering):\")\n",
    "                print(f\"    Total samples with valid dose: {len(valid_doses):,}\")\n",
    "                print(f\"    Dose range: {valid_doses.min():.4f} - {valid_doses.max():.2f} ÂµM\")\n",
    "                print(f\"    Mean dose: {valid_doses.mean():.4f} ÂµM\")\n",
    "                print(f\"    Median dose: {valid_doses.median():.4f} ÂµM\")\n",
    "                \n",
    "                # å‰‚é‡åˆ†å¸ƒï¼ˆæŒ‰åŒºé—´ï¼‰\n",
    "                dose_bins = [0, 0.1, 0.5, 1, 2, 5, 10, 20, 50, 100, 500, np.inf]\n",
    "                dose_labels = ['<0.1', '0.1-0.5', '0.5-1', '1-2', '2-5', '5-10', \n",
    "                            '10-20', '20-50', '50-100', '100-500', '>500']\n",
    "                dose_binned = pd.cut(valid_doses, bins=dose_bins, labels=dose_labels, include_lowest=True)\n",
    "                dose_dist = dose_binned.value_counts().sort_index()\n",
    "                \n",
    "                print(f\"\\n  Dose distribution by range:\")\n",
    "                for dose_range_label, count in dose_dist.items():\n",
    "                    pct = count / len(valid_doses) * 100\n",
    "                    print(f\"    {dose_range_label:>10} ÂµM: {count:>8,} ({pct:>5.1f}%)\")\n",
    "                \n",
    "                # ç»Ÿè®¡æœ€å¸¸è§çš„ç²¾ç¡®å‰‚é‡å€¼\n",
    "                print(f\"\\n  Top 20 most frequent exact doses:\")\n",
    "                top_doses = valid_doses.value_counts().head(20)\n",
    "                for dose_val, count in top_doses.items():\n",
    "                    pct = count / len(valid_doses) * 100\n",
    "                    print(f\"    {dose_val:>8.4f} ÂµM: {count:>8,} samples ({pct:>5.2f}%)\")\n",
    "            \n",
    "            # Step 4: åº”ç”¨å‰‚é‡èŒƒå›´è¿‡æ»¤\n",
    "            print(f\"\\n  Step 4/5: Filtering dose range {dose_range[0]}-{dose_range[1]} ÂµM...\")\n",
    "            dose_range_mask = (\n",
    "                (row_meta['dose_uM'] >= dose_range[0]) & \n",
    "                (row_meta['dose_uM'] <= dose_range[1]) &\n",
    "                (~pd.isna(row_meta['dose_uM']))\n",
    "            )\n",
    "            \n",
    "            n_in_range = (valid_mask & dose_range_mask).sum()\n",
    "            n_out_range = (valid_mask & ~dose_range_mask).sum()\n",
    "            print(f\"  âœ“ Samples in target range: {n_in_range:,}\")\n",
    "            print(f\"  âœ“ Samples outside range: {n_out_range:,}\")\n",
    "            \n",
    "            # å…ˆåº”ç”¨dose_range_mask\n",
    "            n_before_dose = valid_mask.sum()\n",
    "            valid_mask &= dose_range_mask\n",
    "            n_removed_range = n_before_dose - valid_mask.sum()\n",
    "            print(f\"  âœ“ Removed {n_removed_range:,} samples outside dose range\")\n",
    "            \n",
    "            # Step 5: åœ¨å·²è¿‡æ»¤çš„æ•°æ®ä¸­æ‰¾æœ€å¸¸è§å‰‚é‡\n",
    "            print(f\"\\n  Step 5/5: Finding most frequent dose per compound...\")\n",
    "            valid_meta = row_meta[valid_mask].copy()\n",
    "            unique_compounds = valid_meta['pert_id'].nunique()\n",
    "            print(f\"  Processing {unique_compounds:,} compounds...\")\n",
    "            \n",
    "            tqdm.pandas(desc=\"    Finding modal doses\")\n",
    "            most_common_doses = (\n",
    "                valid_meta.groupby('pert_id')['dose_uM']\n",
    "                .progress_apply(lambda x: x.mode()[0] if len(x.mode()) > 0 else np.nan)\n",
    "            )\n",
    "            \n",
    "            print(f\"  âœ“ Identified modal doses for {len(most_common_doses):,} compounds\")\n",
    "            \n",
    "            # ç»Ÿè®¡modal doseåˆ†å¸ƒ\n",
    "            print(f\"\\n  ğŸ“Š Modal dose distribution:\")\n",
    "            modal_dose_counts = most_common_doses.value_counts().head(15)\n",
    "            for dose_val, count in modal_dose_counts.items():\n",
    "                pct = count / len(most_common_doses) * 100\n",
    "                print(f\"    {dose_val:>8.4f} ÂµM: {count:>5,} compounds ({pct:>5.2f}%)\")\n",
    "            \n",
    "            # æ˜ å°„åˆ°æ‰€æœ‰è¡Œ\n",
    "            row_meta['most_common_dose'] = row_meta['pert_id'].map(most_common_doses)\n",
    "            \n",
    "            # åªåœ¨å·²ç»é€šè¿‡valid_maskçš„æ ·æœ¬ä¸­ç­›é€‰modal dose\n",
    "            dose_modal_mask = (\n",
    "                (row_meta['dose_uM'] == row_meta['most_common_dose']) &\n",
    "                (~pd.isna(row_meta['most_common_dose']))\n",
    "            )\n",
    "            \n",
    "            n_before_modal = valid_mask.sum()\n",
    "            valid_mask &= dose_modal_mask\n",
    "            n_removed_modal = n_before_modal - valid_mask.sum()\n",
    "            \n",
    "            print(f\"\\n  âœ“ Removed {n_removed_modal:,} samples with non-modal dose\")\n",
    "            print(f\"  âœ“ Remaining samples: {valid_mask.sum():,}\")\n",
    "            print(f\"  âœ“ Remaining compounds: {row_meta.loc[valid_mask, 'pert_id'].nunique():,}\")\n",
    "            \n",
    "            # ========== å¯è§†åŒ–éƒ¨åˆ† ==========\n",
    "            print(f\"\\n  ğŸ“Š Generating dose distribution visualizations...\")\n",
    "            \n",
    "            # åˆ›å»ºè¾“å‡ºç›®å½•\n",
    "            viz_dir = Path(\"E:/ç§‘ç ”/Models/drugreflector/visualizations\")\n",
    "            viz_dir.mkdir(exist_ok=True, parents=True)\n",
    "            \n",
    "            # å›¾1: å‰‚é‡åˆ†å¸ƒç›´æ–¹å›¾ï¼ˆå¯¹æ•°åˆ»åº¦ï¼‰\n",
    "            plt.figure(figsize=(14, 10))\n",
    "            \n",
    "            # å­å›¾1: è¿‡æ»¤å‰çš„å‰‚é‡åˆ†å¸ƒ\n",
    "            plt.subplot(2, 2, 1)\n",
    "            valid_doses_before = row_meta.loc[\n",
    "                row_meta.index[valid_indices] if 'valid_indices' in locals() else slice(None), \n",
    "                'dose_uM'\n",
    "            ].dropna()\n",
    "            \n",
    "            if len(valid_doses_before) > 0:\n",
    "                # ä½¿ç”¨å¯¹æ•°bins\n",
    "                log_bins = np.logspace(np.log10(0.001), np.log10(1000), 50)\n",
    "                plt.hist(valid_doses_before, bins=log_bins, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "                plt.axvline(dose_range[0], color='red', linestyle='--', linewidth=2, label=f'Range: {dose_range[0]}-{dose_range[1]} ÂµM')\n",
    "                plt.axvline(dose_range[1], color='red', linestyle='--', linewidth=2)\n",
    "                plt.xscale('log')\n",
    "                plt.xlabel('Dose (ÂµM, log scale)', fontsize=11)\n",
    "                plt.ylabel('Number of samples', fontsize=11)\n",
    "                plt.title('Dose Distribution (Before Filtering)', fontsize=13, fontweight='bold')\n",
    "                plt.legend()\n",
    "                plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            # å­å›¾2: è¿‡æ»¤åçš„å‰‚é‡åˆ†å¸ƒ\n",
    "            plt.subplot(2, 2, 2)\n",
    "            final_doses = row_meta.loc[valid_mask, 'dose_uM'].dropna()\n",
    "            \n",
    "            if len(final_doses) > 0:\n",
    "                log_bins = np.logspace(np.log10(dose_range[0]), np.log10(dose_range[1]), 30)\n",
    "                plt.hist(final_doses, bins=log_bins, edgecolor='black', alpha=0.7, color='forestgreen')\n",
    "                plt.xscale('log')\n",
    "                plt.xlabel('Dose (ÂµM, log scale)', fontsize=11)\n",
    "                plt.ylabel('Number of samples', fontsize=11)\n",
    "                plt.title('Dose Distribution (After Filtering)', fontsize=13, fontweight='bold')\n",
    "                plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            # å­å›¾3: Top 30 æœ€å¸¸è§çš„ç²¾ç¡®å‰‚é‡å€¼ï¼ˆæŸ±çŠ¶å›¾ï¼‰\n",
    "            plt.subplot(2, 2, 3)\n",
    "            top_doses_final = final_doses.value_counts().head(30)\n",
    "            \n",
    "            if len(top_doses_final) > 0:\n",
    "                bars = plt.barh(range(len(top_doses_final)), top_doses_final.values, color='coral', edgecolor='black')\n",
    "                plt.yticks(range(len(top_doses_final)), \n",
    "                        [f'{d:.4f} ÂµM' for d in top_doses_final.index])\n",
    "                plt.xlabel('Number of samples', fontsize=11)\n",
    "                plt.ylabel('Dose (ÂµM)', fontsize=11)\n",
    "                plt.title('Top 30 Most Frequent Exact Doses', fontsize=13, fontweight='bold')\n",
    "                plt.grid(axis='x', alpha=0.3)\n",
    "                \n",
    "                # æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "                for i, (bar, val) in enumerate(zip(bars, top_doses_final.values)):\n",
    "                    plt.text(val, i, f' {val:,}', va='center', fontsize=8)\n",
    "            \n",
    "            # å­å›¾4: Modal doseåˆ†å¸ƒï¼ˆåŒ–åˆç‰©å±‚é¢ï¼‰\n",
    "            plt.subplot(2, 2, 4)\n",
    "            modal_doses_final = most_common_doses.value_counts().head(20)\n",
    "            \n",
    "            if len(modal_doses_final) > 0:\n",
    "                bars = plt.barh(range(len(modal_doses_final)), modal_doses_final.values, \n",
    "                            color='mediumpurple', edgecolor='black')\n",
    "                plt.yticks(range(len(modal_doses_final)), \n",
    "                        [f'{d:.4f} ÂµM' for d in modal_doses_final.index])\n",
    "                plt.xlabel('Number of compounds', fontsize=11)\n",
    "                plt.ylabel('Modal Dose (ÂµM)', fontsize=11)\n",
    "                plt.title('Top 20 Modal Doses (Per Compound)', fontsize=13, fontweight='bold')\n",
    "                plt.grid(axis='x', alpha=0.3)\n",
    "                \n",
    "                # æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "                for i, (bar, val) in enumerate(zip(bars, modal_doses_final.values)):\n",
    "                    plt.text(val, i, f' {val:,}', va='center', fontsize=8)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            output_path = viz_dir / \"dose_distribution_analysis.png\"\n",
    "            plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"  âœ“ Saved figure: {output_path}\")\n",
    "            plt.close()\n",
    "            \n",
    "            # å›¾2: å‰‚é‡èŒƒå›´åˆ†å¸ƒï¼ˆåˆ†åŒºé—´ç»Ÿè®¡ï¼‰\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            \n",
    "            dose_bins = [0, 0.1, 0.5, 1, 2, 5, 10, 20, 50, 100, 500, np.inf]\n",
    "            dose_labels = ['<0.1', '0.1-0.5', '0.5-1', '1-2', '2-5', '5-10', \n",
    "                        '10-20', '20-50', '50-100', '100-500', '>500']\n",
    "            \n",
    "            # è¿‡æ»¤å‰\n",
    "            doses_before = row_meta.loc[\n",
    "                row_meta.index[valid_indices] if 'valid_indices' in locals() else slice(None), \n",
    "                'dose_uM'\n",
    "            ].dropna()\n",
    "            binned_before = pd.cut(doses_before, bins=dose_bins, labels=dose_labels, include_lowest=True)\n",
    "            dist_before = binned_before.value_counts().sort_index()\n",
    "            \n",
    "            # è¿‡æ»¤å\n",
    "            doses_after = row_meta.loc[valid_mask, 'dose_uM'].dropna()\n",
    "            binned_after = pd.cut(doses_after, bins=dose_bins, labels=dose_labels, include_lowest=True)\n",
    "            dist_after = binned_after.value_counts().sort_index()\n",
    "            \n",
    "            x = np.arange(len(dose_labels))\n",
    "            width = 0.35\n",
    "            \n",
    "            plt.bar(x - width/2, dist_before.values, width, label='Before filtering', \n",
    "                    alpha=0.8, color='steelblue', edgecolor='black')\n",
    "            plt.bar(x + width/2, dist_after.values, width, label='After filtering', \n",
    "                    alpha=0.8, color='forestgreen', edgecolor='black')\n",
    "            \n",
    "            plt.xlabel('Dose Range (ÂµM)', fontsize=12, fontweight='bold')\n",
    "            plt.ylabel('Number of Samples', fontsize=12, fontweight='bold')\n",
    "            plt.title('Sample Distribution by Dose Range (Before vs After Filtering)', \n",
    "                    fontsize=14, fontweight='bold')\n",
    "            plt.xticks(x, dose_labels, rotation=45, ha='right')\n",
    "            plt.legend(fontsize=11)\n",
    "            plt.grid(axis='y', alpha=0.3)\n",
    "            \n",
    "            # æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "            for i, (v_before, v_after) in enumerate(zip(dist_before.values, dist_after.values)):\n",
    "                plt.text(i - width/2, v_before, f'{v_before:,}', ha='center', va='bottom', fontsize=8)\n",
    "                plt.text(i + width/2, v_after, f'{v_after:,}', ha='center', va='bottom', fontsize=8)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            output_path2 = viz_dir / \"dose_range_comparison.png\"\n",
    "            plt.savefig(output_path2, dpi=300, bbox_inches='tight')\n",
    "            print(f\"  âœ“ Saved figure: {output_path2}\")\n",
    "            plt.close()\n",
    "            \n",
    "            print(f\"\\n  âœ… Visualization complete!\")\n",
    "            print(f\"     Figures saved to: {viz_dir}\")\n",
    "            \n",
    "            # æ¸…ç†ä¸´æ—¶åˆ—\n",
    "            row_meta.drop(['dose_value', 'dose_unit', 'most_common_dose'], \n",
    "                        axis=1, inplace=True, errors='ignore')\n",
    "            \n",
    "            del valid_meta, dose_range_mask, dose_modal_mask, most_common_doses\n",
    "            gc.collect()\n",
    "        else:\n",
    "            print(f\"  âš ï¸  'pert_dose' not found, skipping dose filter\")\n",
    "        \n",
    "        # ========== Filter 5: Timepoint selection (ä¼˜åŒ–) ==========\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FILTER 5: Keep only measurements at {valid_timepoints} hours\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        if 'pert_time' in row_meta.columns:\n",
    "            # è½¬æ¢ä¸ºæ•°å€¼ç±»å‹\n",
    "            row_meta['time_numeric'] = pd.to_numeric(row_meta['pert_time'], errors='coerce')\n",
    "            \n",
    "            # æ˜¾ç¤ºæ—¶é—´ç‚¹åˆ†å¸ƒ\n",
    "            print(f\"   Available timepoints (numeric):\")\n",
    "            time_counts = row_meta['time_numeric'].value_counts().head(10)\n",
    "            for time_val, count in time_counts.items():\n",
    "                print(f\"     - {time_val} hours: {count:,} samples\")\n",
    "            \n",
    "            # åŒ¹é…valid_timepointsä¸­çš„æ•°å€¼\n",
    "            time_mask = row_meta['time_numeric'].isin(valid_timepoints)\n",
    "            \n",
    "            if time_mask.sum() == 0:\n",
    "                print(f\"  âš ï¸  No samples match timepoints {valid_timepoints}\")\n",
    "                print(f\"  Skipping timepoint filter...\")\n",
    "            else:\n",
    "                n_before = valid_mask.sum()\n",
    "                valid_mask &= time_mask\n",
    "                n_removed = n_before - valid_mask.sum()\n",
    "                \n",
    "                print(f\"  âœ“ Kept samples at {valid_timepoints} hours\")\n",
    "                print(f\"  Removed {n_removed:,} observations (invalid timepoint)\")\n",
    "                print(f\"  Remaining samples: {valid_mask.sum():,}\")\n",
    "            \n",
    "            del time_mask\n",
    "            gc.collect()\n",
    "        else:\n",
    "            print(f\"  âš ï¸  'pert_time' not found, skipping timepoint filter\")\n",
    "        \n",
    "        # ========== Filter 6: Cell line count ==========\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FILTER 6: Remove compounds in <{min_cell_lines} or >{max_cell_lines} cell lines\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        if cell_id_col is not None:\n",
    "            valid_meta = row_meta[valid_mask]\n",
    "            cell_line_counts = valid_meta.groupby('pert_id')[cell_id_col].nunique()\n",
    "            valid_perts_cell = cell_line_counts[\n",
    "                (cell_line_counts >= min_cell_lines) & \n",
    "                (cell_line_counts <= max_cell_lines)\n",
    "            ].index\n",
    "            \n",
    "            print(f\"  Compounds in {min_cell_lines}-{max_cell_lines} cell lines: \"\n",
    "                  f\"{len(valid_perts_cell):,}/{len(cell_line_counts):,}\")\n",
    "            \n",
    "            cell_mask = row_meta['pert_id'].isin(valid_perts_cell)\n",
    "            n_before = valid_mask.sum()\n",
    "            valid_mask &= cell_mask\n",
    "            n_removed = n_before - valid_mask.sum()\n",
    "            \n",
    "            print(f\"  Removed {n_removed:,} observations\")\n",
    "            print(f\"  Remaining samples: {valid_mask.sum():,}\")\n",
    "            print(f\"  Remaining compounds: {row_meta.loc[valid_mask, 'pert_id'].nunique():,}\")\n",
    "            \n",
    "            del valid_meta, cell_line_counts, cell_mask\n",
    "            gc.collect()\n",
    "        else:\n",
    "            print(f\"  âš ï¸  Cell line column not found, skipping\")\n",
    "        \n",
    "        # ========== æœ€ç»ˆæå–æ•°æ® ==========\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"âœ… FINAL DATASET\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        final_matrix = matrix[valid_mask].copy()\n",
    "        final_meta = row_meta[valid_mask].reset_index(drop=True)\n",
    "        \n",
    "        del matrix\n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"  Extracted {len(final_matrix):,} samples\")\n",
    "        print(f\"  Memory usage: {final_matrix.nbytes / (1024**3):.2f} GB\")\n",
    "        \n",
    "        # åˆ›å»ºæ ‡ç­¾\n",
    "        unique_perts = sorted(final_meta['pert_id'].unique())\n",
    "        pert_to_idx = {pert: idx for idx, pert in enumerate(unique_perts)}\n",
    "        labels = np.array([pert_to_idx[p] for p in final_meta['pert_id']], dtype=np.int32)\n",
    "        \n",
    "        final_compounds = len(unique_perts)\n",
    "        final_samples = len(final_matrix)\n",
    "        \n",
    "        if cell_id_col:\n",
    "            final_cells = final_meta[cell_id_col].nunique()\n",
    "        else:\n",
    "            final_cells = 'Unknown'\n",
    "        \n",
    "        print(f\"\\n  Total samples: {final_samples:,}\")\n",
    "        print(f\"  Total compounds: {final_compounds:,}\")\n",
    "        print(f\"  Cell lines: {final_cells}\")\n",
    "        print(f\"  Gene features: {final_matrix.shape[1]}\")\n",
    "        \n",
    "        compound_obs = final_meta.groupby('pert_id').size()\n",
    "        print(f\"  Samples per compound (mean): {final_samples / final_compounds:.1f}\")\n",
    "        print(f\"  Samples per compound (median): {compound_obs.median():.0f}\")\n",
    "        \n",
    "        n_compounds_100plus = (compound_obs > 100).sum()\n",
    "        print(f\"  Compounds with >100 observations: {n_compounds_100plus:,}\")\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Comparison with paper (SI page 2):\")\n",
    "        print(f\"  Paper: 425,242 obs, 9,597 compounds, 52 cell lines\")\n",
    "        print(f\"  Ours:  {final_samples:,} obs, {final_compounds:,} compounds, {final_cells} cell lines\")\n",
    "        \n",
    "        if initial_compounds > 0:\n",
    "            retention_rate = (final_compounds / initial_compounds) * 100\n",
    "            print(f\"  Compound retention rate: {retention_rate:.1f}%\")\n",
    "        \n",
    "        # åŸºå› å\n",
    "        if (self.gene_info is not None) and hasattr(self, \"landmark_col_indices\"):\n",
    "            gi = self.gene_info\n",
    "            landmark_mask = gi['feature_space'] == 'landmark'\n",
    "            landmark_geneinfo = gi[landmark_mask]\n",
    "            gene_names = list(landmark_geneinfo['gene_symbol'].values)\n",
    "            print(f\"  âœ“ Using geneinfo_beta.txt for gene names\")\n",
    "        else:\n",
    "            gene_name_col = None\n",
    "            for possible_col in ['gene_symbol', 'pr_gene_symbol', 'symbol', 'id']:\n",
    "                if possible_col in col_meta.columns:\n",
    "                    gene_name_col = possible_col\n",
    "                    break\n",
    "            if gene_name_col is None:\n",
    "                gene_name_col = col_meta.columns[0]\n",
    "            gene_names = list(col_meta[gene_name_col].values)\n",
    "            print(f\"  âš ï¸ Using '{gene_name_col}' from col_meta for gene names\")\n",
    "\n",
    "        training_data = {\n",
    "            'X': final_matrix,\n",
    "            'y': labels,\n",
    "            'folds': np.zeros(len(final_matrix), dtype=np.int32),\n",
    "            'sample_meta': final_meta,\n",
    "            'metadata': final_meta,\n",
    "            'gene_names': gene_names,\n",
    "            'compound_names': list(unique_perts),\n",
    "            'pert_to_idx': pert_to_idx\n",
    "        }\n",
    "\n",
    "        gc.collect()\n",
    "        \n",
    "        return training_data\n",
    "    \n",
    "    def create_3fold_splits(self, training_data, random_state=42):\n",
    "        \"\"\"åˆ›å»º3æŠ˜äº¤å‰éªŒè¯åˆ’åˆ†\"\"\"\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        sample_meta = training_data['sample_meta']\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ğŸ² Creating 3-fold cross-validation splits\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        folds = np.zeros(len(sample_meta), dtype=np.int32)\n",
    "        \n",
    "        for pert_id in sample_meta['pert_id'].unique():\n",
    "            pert_mask = sample_meta['pert_id'] == pert_id\n",
    "            pert_indices = np.where(pert_mask)[0]\n",
    "            \n",
    "            np.random.shuffle(pert_indices)\n",
    "            n_samples = len(pert_indices)\n",
    "            \n",
    "            fold_sizes = [n_samples // 3] * 3\n",
    "            for i in range(n_samples % 3):\n",
    "                fold_sizes[i] += 1\n",
    "            \n",
    "            start_idx = 0\n",
    "            for fold_id, size in enumerate(fold_sizes):\n",
    "                end_idx = start_idx + size\n",
    "                folds[pert_indices[start_idx:end_idx]] = fold_id\n",
    "                start_idx = end_idx\n",
    "        \n",
    "        training_data['folds'] = folds\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Fold statistics:\")\n",
    "        for fold_id in range(3):\n",
    "            fold_mask = folds == fold_id\n",
    "            n_samples = fold_mask.sum()\n",
    "            n_compounds = sample_meta[fold_mask]['pert_id'].nunique()\n",
    "            print(f\"   Fold {fold_id}: {n_samples:,} samples, {n_compounds:,} compounds\")\n",
    "        \n",
    "        return training_data\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"ä¸»ç¨‹åº - ä¼˜åŒ–ç‰ˆ\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ğŸ§¬ DRUGREFLECTOR DATA PREPROCESSING - Optimized Version\")\n",
    "    print(\"   Fixed DOS filtering and timepoint matching\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    data_dir = \"E:/ç§‘ç ”/Models/drugreflector/datasets/LINCS2020\"\n",
    "    loader = LINCS2020DataLoader(data_dir)\n",
    "    \n",
    "    try:\n",
    "        # Step 1: åŠ è½½å…ƒæ•°æ®\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"STEP 1: Loading metadata\")\n",
    "        print(\"=\" * 80)\n",
    "        gene_info = loader.load_gene_info()\n",
    "        cell_info = loader.load_cell_info()\n",
    "        compound_info = loader.load_compound_info()\n",
    "        \n",
    "        # Step 2: åŠ è½½Level 4 signatures\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"STEP 2: Loading Level 4 signatures\")\n",
    "        print(\"=\" * 80)\n",
    "        matrix, row_meta, col_meta = loader.load_level4_signatures()\n",
    "        \n",
    "        # Step 3: å‡†å¤‡è®­ç»ƒæ•°æ®\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"STEP 3: Preparing training data\")\n",
    "        print(\"=\" * 80)\n",
    "        training_data = loader.prepare_training_data(\n",
    "            min_observations_per_compound=5,\n",
    "            min_replicate_similarity=0.12,\n",
    "            dose_range=(1.0, 20.0),\n",
    "            valid_timepoints=[6, 24],  # ä¿®æ”¹ä¸ºæ•°å€¼åˆ—è¡¨\n",
    "            min_cell_lines=5,\n",
    "            max_cell_lines=40,\n",
    "            remove_dos=True\n",
    "        )\n",
    "        \n",
    "        # Step 4: åˆ›å»º3æŠ˜åˆ’åˆ†\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"STEP 4: Creating 3-fold splits\")\n",
    "        print(\"=\" * 80)\n",
    "        training_data = loader.create_3fold_splits(training_data)\n",
    "        \n",
    "        # Step 5: ä¿å­˜\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"STEP 5: Saving processed data\")\n",
    "        print(\"=\" * 80)\n",
    "        output_dir = Path(\"E:/ç§‘ç ”/Models/drugreflector/processed_data\")\n",
    "        output_dir.mkdir(exist_ok=True, parents=True)\n",
    "        \n",
    "        output_file = output_dir / \"training_data_lincs2020_optimized_1121.pkl\"\n",
    "        print(f\"ğŸ’¾ Saving to: {output_file}\")\n",
    "        \n",
    "        with open(output_file, 'wb') as f:\n",
    "            pickle.dump(training_data, f, protocol=4)\n",
    "        \n",
    "        print(f\"âœ“ Saved successfully!\")\n",
    "        print(f\"   File size: {output_file.stat().st_size / (1024**2):.1f} MB\")\n",
    "        \n",
    "        # æœ€ç»ˆæ‘˜è¦\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"âœ… DATA PREPARATION COMPLETE!\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"ğŸ“ Output: {output_file}\")\n",
    "        print(f\"\\nğŸ“Š Final dataset:\")\n",
    "        print(f\"   â€¢ Samples: {len(training_data['X']):,}\")\n",
    "        print(f\"   â€¢ Compounds: {len(training_data['compound_names']):,}\")\n",
    "        print(f\"   â€¢ Genes: {training_data['X'].shape[1]}\")\n",
    "        print(f\"   â€¢ Memory: {training_data['X'].nbytes / (1024**3):.2f} GB\")\n",
    "        \n",
    "        # ä¸è®ºæ–‡å¯¹æ¯”\n",
    "        paper_samples = 425242\n",
    "        paper_compounds = 9597\n",
    "        our_samples = len(training_data['X'])\n",
    "        our_compounds = len(training_data['compound_names'])\n",
    "        \n",
    "        print(f\"\\nğŸ“ˆ Comparison with paper:\")\n",
    "        print(f\"   Samples: {our_samples:,} / {paper_samples:,} ({our_samples/paper_samples*100:.1f}%)\")\n",
    "        print(f\"   Compounds: {our_compounds:,} / {paper_compounds:,} ({our_compounds/paper_compounds*100:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nğŸ¯ Ready for training!\")\n",
    "        \n",
    "        return training_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n\" + \"=\" * 80)\n",
    "        print(\"âŒ ERROR\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"   Type: {type(e).__name__}\")\n",
    "        print(f\"   Message: {e}\")\n",
    "        \n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    training_data = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d41b504",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemprop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
