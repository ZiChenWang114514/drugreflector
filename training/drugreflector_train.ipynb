{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faff2c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DrugReflector åŸºåº§æ¨¡å‹è®­ç»ƒ\n",
    "ä¸¥æ ¼éµå¾ª Science 2025 Supplementary Materials (Pages 2-4)\n",
    "\n",
    "æ¨¡å‹æ¶æ„ï¼š\n",
    "- Input: 978 (landmark genes)\n",
    "- Hidden 1: 1,024 nodes + ReLU + Dropout(0.64) + BatchNorm\n",
    "- Hidden 2: 2,048 nodes + ReLU + Dropout(0.64) + BatchNorm\n",
    "- Output: 9,597 (compounds)\n",
    "\n",
    "è®­ç»ƒé…ç½®ï¼š\n",
    "- Focal Loss (Î³=2)\n",
    "- Cosine Annealing with Warm Restarts\n",
    "- 3-Fold Ensemble\n",
    "- 50 epochs per model\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from typing import Dict, Tuple, List\n",
    "from sklearn.metrics import accuracy_score, top_k_accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# æ•°æ®é¢„å¤„ç†\n",
    "# ============================================================================\n",
    "\n",
    "def clip_and_normalize_signature(X: np.ndarray, clip_range=(-2, 2)) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    è£å‰ªå¹¶æ ‡å‡†åŒ–ç­¾åï¼Œä½¿å…¶æ ‡å‡†å·®ä¸º1\n",
    "    \n",
    "    åŸæ–‡ï¼ˆSIç¬¬3é¡µï¼‰ï¼š\n",
    "    \"every transcriptional vector v is clipped to range [-2,2] such that \n",
    "    its standard deviation after clipping equals 1\"\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X: np.ndarray\n",
    "        åŸå§‹è¡¨è¾¾çŸ©é˜µ (n_samples, n_genes)\n",
    "    clip_range: tuple\n",
    "        è£å‰ªèŒƒå›´ï¼Œé»˜è®¤[-2, 2]\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    np.ndarray: å¤„ç†åçš„çŸ©é˜µ\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ“Š Clipping and normalizing signatures...\")\n",
    "    print(f\"   Clip range: {clip_range}\")\n",
    "    \n",
    "    X_processed = np.zeros_like(X)\n",
    "    \n",
    "    for i in range(X.shape[0]):\n",
    "        # è£å‰ªåˆ°æŒ‡å®šèŒƒå›´\n",
    "        vec = np.clip(X[i], clip_range[0], clip_range[1])\n",
    "        \n",
    "        # æ ‡å‡†åŒ–ä½¿æ ‡å‡†å·®ä¸º1\n",
    "        std = np.std(vec)\n",
    "        if std > 0:\n",
    "            vec = vec / std\n",
    "        \n",
    "        X_processed[i] = vec\n",
    "    \n",
    "    print(f\"   âœ“ Mean std after normalization: {np.std(X_processed, axis=1).mean():.4f}\")\n",
    "    print(f\"   âœ“ Data range after clipping: [{X_processed.min():.2f}, {X_processed.max():.2f}]\")\n",
    "    \n",
    "    return X_processed\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PyTorch Dataset\n",
    "# ============================================================================\n",
    "\n",
    "class LINCSDataset(Dataset):\n",
    "    \"\"\"LINCSæ•°æ®é›†ç±»\"\"\"\n",
    "    \n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray, fold_mask: np.ndarray = None):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: np.ndarray\n",
    "            è¡¨è¾¾çŸ©é˜µ (n_samples, 978)\n",
    "        y: np.ndarray\n",
    "            åŒ–åˆç‰©æ ‡ç­¾ (n_samples,)\n",
    "        fold_mask: np.ndarray\n",
    "            Foldæ©ç ï¼ŒTrueè¡¨ç¤ºåŒ…å«åœ¨æ­¤æ•°æ®é›†ä¸­\n",
    "        \"\"\"\n",
    "        if fold_mask is not None:\n",
    "            self.X = torch.FloatTensor(X[fold_mask])\n",
    "            self.y = torch.LongTensor(y[fold_mask])\n",
    "        else:\n",
    "            self.X = torch.FloatTensor(X)\n",
    "            self.y = torch.LongTensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Focal Loss\n",
    "# ============================================================================\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Focal Losså®ç°\n",
    "    \n",
    "    åŸæ–‡ï¼ˆSIç¬¬3é¡µï¼‰ï¼š\n",
    "    \"The models were trained using a focal loss function with a \n",
    "    focusing parameter of Î³ = 2\"\n",
    "    \n",
    "    Reference: Lin et al. \"Focal Loss for Dense Object Detection\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, gamma=2.0, alpha=None, reduction='mean'):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        gamma: float\n",
    "            èšç„¦å‚æ•°ï¼ŒåŸæ–‡ä½¿ç”¨2.0\n",
    "        alpha: Tensor or None\n",
    "            ç±»åˆ«æƒé‡\n",
    "        reduction: str\n",
    "            'mean', 'sum', or 'none'\n",
    "        \"\"\"\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        inputs: Tensor\n",
    "            æ¨¡å‹è¾“å‡º logits (N, C)\n",
    "        targets: Tensor\n",
    "            çœŸå®æ ‡ç­¾ (N,)\n",
    "        \"\"\"\n",
    "        # è®¡ç®—äº¤å‰ç†µ\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        \n",
    "        # è®¡ç®—pt\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        \n",
    "        # Focal loss\n",
    "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "        \n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha[targets]\n",
    "            focal_loss = alpha_t * focal_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# æ¨¡å‹æ¶æ„\n",
    "# ============================================================================\n",
    "\n",
    "class DrugReflectorModel(nn.Module):\n",
    "    \"\"\"\n",
    "    DrugReflectorç¥ç»ç½‘ç»œæ¶æ„\n",
    "    \n",
    "    åŸæ–‡ï¼ˆSIç¬¬2é¡µï¼‰ï¼š\n",
    "    \"The input layer has 978 nodes (one for each landmark gene), \n",
    "    and the output layer has 9,597 nodes (one for each target LINCS perturbation). \n",
    "    The first hidden layer has 1,024 nodes, and the second has 2,048 nodes \n",
    "    using rectified linear units (ReLU) to compute node activations.\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        input_size=978, \n",
    "        hidden1_size=1024, \n",
    "        hidden2_size=2048, \n",
    "        output_size=9597,\n",
    "        dropout_rate=0.64,\n",
    "        batch_norm_momentum=0.1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_size: int\n",
    "            è¾“å…¥ç‰¹å¾æ•°ï¼ˆlandmark genesï¼‰\n",
    "        hidden1_size: int\n",
    "            ç¬¬ä¸€éšè—å±‚å¤§å°\n",
    "        hidden2_size: int\n",
    "            ç¬¬äºŒéšè—å±‚å¤§å°\n",
    "        output_size: int\n",
    "            è¾“å‡ºç±»åˆ«æ•°ï¼ˆåŒ–åˆç‰©æ•°ï¼‰\n",
    "        dropout_rate: float\n",
    "            Dropoutæ¯”ç‡ï¼ŒåŸæ–‡0.64\n",
    "        batch_norm_momentum: float\n",
    "            BatchNormåŠ¨é‡ï¼ŒåŸæ–‡0.1\n",
    "        \"\"\"\n",
    "        super(DrugReflectorModel, self).__init__()\n",
    "        \n",
    "        # ç¬¬ä¸€éšè—å±‚\n",
    "        self.fc1 = nn.Linear(input_size, hidden1_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden1_size, momentum=batch_norm_momentum)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # ç¬¬äºŒéšè—å±‚\n",
    "        self.fc2 = nn.Linear(hidden1_size, hidden2_size)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden2_size, momentum=batch_norm_momentum)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # è¾“å‡ºå±‚\n",
    "        self.fc3 = nn.Linear(hidden2_size, output_size)\n",
    "        \n",
    "        # åˆå§‹åŒ–æƒé‡\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"æƒé‡åˆå§‹åŒ–\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        å‰å‘ä¼ æ’­\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x: Tensor\n",
    "            è¾“å…¥ (batch_size, 978)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        Tensor: è¾“å‡ºlogits (batch_size, 9597)\n",
    "        \"\"\"\n",
    "        # ç¬¬ä¸€éšè—å±‚\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # ç¬¬äºŒéšè—å±‚\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # è¾“å‡ºå±‚\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# è®­ç»ƒå™¨\n",
    "# ============================================================================\n",
    "\n",
    "class DrugReflectorTrainer:\n",
    "    \"\"\"\n",
    "    DrugReflectorè®­ç»ƒå™¨\n",
    "    \n",
    "    å®ç°3-fold ensembleè®­ç»ƒç­–ç•¥\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        initial_lr=0.0139,\n",
    "        min_lr=0.00001,\n",
    "        weight_decay=1e-5,\n",
    "        t_0=20,  # Time to first restart\n",
    "        t_mult=1.9,  # åŸæ–‡0.5ï¼Œä½†è¿™æ ·ä¼šå¯¼è‡´å‘¨æœŸå‡åŠï¼Œå®é™…åº”è¯¥>1\n",
    "        focal_gamma=2.0,\n",
    "        batch_size=256,\n",
    "        num_epochs=50,\n",
    "        early_stop_epoch=20\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        device: str\n",
    "            è®¡ç®—è®¾å¤‡\n",
    "        initial_lr: float\n",
    "            åˆå§‹å­¦ä¹ ç‡ï¼ˆåŸæ–‡ï¼š0.0139ï¼‰\n",
    "        min_lr: float\n",
    "            æœ€å°å­¦ä¹ ç‡ï¼ˆåŸæ–‡ï¼š0.00001ï¼‰\n",
    "        weight_decay: float\n",
    "            æƒé‡è¡°å‡ï¼ˆåŸæ–‡ï¼š1e-5ï¼‰\n",
    "        t_0: int\n",
    "            ç¬¬ä¸€æ¬¡warm restartå‰çš„epochæ•°ï¼ˆåŸæ–‡ï¼š20ï¼‰\n",
    "        t_mult: float\n",
    "            warm restartå‘¨æœŸå€å¢å› å­ï¼ˆåŸæ–‡ï¼š0.5ï¼‰\n",
    "        focal_gamma: float\n",
    "            Focal lossèšç„¦å‚æ•°ï¼ˆåŸæ–‡ï¼š2.0ï¼‰\n",
    "        batch_size: int\n",
    "            æ‰¹æ¬¡å¤§å°\n",
    "        num_epochs: int\n",
    "            æ€»è®­ç»ƒè½®æ•°ï¼ˆåŸæ–‡ï¼š50ï¼‰\n",
    "        early_stop_epoch: int\n",
    "            æ—©åœæ£€æŸ¥ç‚¹ï¼ˆåŸæ–‡ï¼š20ï¼‰\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.initial_lr = initial_lr\n",
    "        self.min_lr = min_lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.t_0 = t_0\n",
    "        self.t_mult = t_mult\n",
    "        self.focal_gamma = focal_gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.early_stop_epoch = early_stop_epoch\n",
    "        \n",
    "        print(f\"\\nğŸš€ DrugReflector Trainer initialized\")\n",
    "        print(f\"   Device: {self.device}\")\n",
    "        print(f\"   Initial LR: {self.initial_lr}\")\n",
    "        print(f\"   Min LR: {self.min_lr}\")\n",
    "        print(f\"   Weight Decay: {self.weight_decay}\")\n",
    "        print(f\"   T_0 (first restart): {self.t_0}\")\n",
    "        print(f\"   Focal Î³: {self.focal_gamma}\")\n",
    "        print(f\"   Batch size: {self.batch_size}\")\n",
    "        print(f\"   Epochs: {self.num_epochs}\")\n",
    "    \n",
    "    def train_single_model(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: DataLoader,\n",
    "        fold_id: int\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        è®­ç»ƒå•ä¸ªæ¨¡å‹\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        model: nn.Module\n",
    "            DrugReflectoræ¨¡å‹\n",
    "        train_loader: DataLoader\n",
    "            è®­ç»ƒæ•°æ®åŠ è½½å™¨\n",
    "        val_loader: DataLoader\n",
    "            éªŒè¯æ•°æ®åŠ è½½å™¨\n",
    "        fold_id: int\n",
    "            Fold ID (0, 1, 2)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        Dict: è®­ç»ƒå†å²\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Training Model {fold_id}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨\n",
    "        criterion = FocalLoss(gamma=self.focal_gamma)\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=self.initial_lr,\n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "        \n",
    "        # å­¦ä¹ ç‡è°ƒåº¦å™¨\n",
    "        scheduler = CosineAnnealingWarmRestarts(\n",
    "            optimizer,\n",
    "            T_0=self.t_0,\n",
    "            T_mult=int(self.t_mult) if self.t_mult >= 1 else 1,\n",
    "            eta_min=self.min_lr\n",
    "        )\n",
    "        \n",
    "        # è®­ç»ƒå†å²\n",
    "        history = {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'val_recall': [],\n",
    "            'val_top1_acc': [],\n",
    "            'val_top10_acc': [],\n",
    "            'learning_rates': []\n",
    "        }\n",
    "        \n",
    "        best_recall = 0.0\n",
    "        best_epoch = 0\n",
    "        best_model_state = None\n",
    "        \n",
    "        # è®­ç»ƒå¾ªç¯\n",
    "        for epoch in range(self.num_epochs):\n",
    "            # ========== è®­ç»ƒé˜¶æ®µ ==========\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            \n",
    "            pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{self.num_epochs}\")\n",
    "            for batch_X, batch_y in pbar:\n",
    "                batch_X = batch_X.to(self.device)\n",
    "                batch_y = batch_y.to(self.device)\n",
    "                \n",
    "                # å‰å‘ä¼ æ’­\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                \n",
    "                # åå‘ä¼ æ’­\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                pbar.set_postfix({'loss': loss.item()})\n",
    "            \n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            \n",
    "            # ========== éªŒè¯é˜¶æ®µ ==========\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "            all_probs = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch_X, batch_y in val_loader:\n",
    "                    batch_X = batch_X.to(self.device)\n",
    "                    batch_y = batch_y.to(self.device)\n",
    "                    \n",
    "                    outputs = model(batch_X)\n",
    "                    loss = criterion(outputs, batch_y)\n",
    "                    val_loss += loss.item()\n",
    "                    \n",
    "                    # è®¡ç®—é¢„æµ‹\n",
    "                    probs = F.softmax(outputs, dim=1)\n",
    "                    preds = torch.argmax(probs, dim=1)\n",
    "                    \n",
    "                    all_preds.append(preds.cpu().numpy())\n",
    "                    all_labels.append(batch_y.cpu().numpy())\n",
    "                    all_probs.append(probs.cpu().numpy())\n",
    "            \n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            \n",
    "            # è®¡ç®—æŒ‡æ ‡\n",
    "            all_preds = np.concatenate(all_preds)\n",
    "            all_labels = np.concatenate(all_labels)\n",
    "            all_probs = np.concatenate(all_probs)\n",
    "            \n",
    "            top1_acc = accuracy_score(all_labels, all_preds)\n",
    "            top10_acc = top_k_accuracy_score(all_labels, all_probs, k=10)\n",
    "            \n",
    "            # è®¡ç®—Top 1% recall (åŸæ–‡ä¸»è¦æŒ‡æ ‡)\n",
    "            top1_percent_k = max(1, int(0.01 * all_probs.shape[1]))\n",
    "            recall = top_k_accuracy_score(all_labels, all_probs, k=top1_percent_k)\n",
    "            \n",
    "            # è®°å½•å†å²\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            history['train_loss'].append(avg_train_loss)\n",
    "            history['val_loss'].append(avg_val_loss)\n",
    "            history['val_recall'].append(recall)\n",
    "            history['val_top1_acc'].append(top1_acc)\n",
    "            history['val_top10_acc'].append(top10_acc)\n",
    "            history['learning_rates'].append(current_lr)\n",
    "            \n",
    "            # æ‰“å°è¿›åº¦\n",
    "            print(f\"\\nEpoch {epoch+1}/{self.num_epochs}\")\n",
    "            print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "            print(f\"  Val Loss: {avg_val_loss:.4f}\")\n",
    "            print(f\"  Val Recall (top 1%): {recall:.4f}\")\n",
    "            print(f\"  Val Top-1 Acc: {top1_acc:.4f}\")\n",
    "            print(f\"  Val Top-10 Acc: {top10_acc:.4f}\")\n",
    "            print(f\"  Learning Rate: {current_lr:.6f}\")\n",
    "            \n",
    "            # ä¿å­˜æœ€ä½³æ¨¡å‹\n",
    "            if recall > best_recall:\n",
    "                best_recall = recall\n",
    "                best_epoch = epoch\n",
    "                best_model_state = model.state_dict().copy()\n",
    "                print(f\"  âœ“ New best model! (Recall: {best_recall:.4f})\")\n",
    "            \n",
    "            # å­¦ä¹ ç‡è°ƒåº¦\n",
    "            scheduler.step()\n",
    "        \n",
    "        # åŠ è½½æœ€ä½³æ¨¡å‹\n",
    "        if best_model_state is not None:\n",
    "            model.load_state_dict(best_model_state)\n",
    "            print(f\"\\nâœ“ Loaded best model from epoch {best_epoch+1} \"\n",
    "                  f\"(Recall: {best_recall:.4f})\")\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def train_ensemble(\n",
    "        self,\n",
    "        training_data: Dict,\n",
    "        output_dir: Path\n",
    "    ) -> List[nn.Module]:\n",
    "        \"\"\"\n",
    "        è®­ç»ƒ3-fold ensemble\n",
    "        \n",
    "        åŸæ–‡ï¼ˆSIç¬¬2-3é¡µï¼‰ï¼š\n",
    "        \"The training data was divided randomly into three folds, with perturbation \n",
    "        replicates balanced across the folds. Models were independently trained on \n",
    "        two of three folds.\"\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        training_data: Dict\n",
    "            åŒ…å«X, y, foldsçš„è®­ç»ƒæ•°æ®\n",
    "        output_dir: Path\n",
    "            æ¨¡å‹ä¿å­˜ç›®å½•\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        List[nn.Module]: è®­ç»ƒå¥½çš„3ä¸ªæ¨¡å‹\n",
    "        \"\"\"\n",
    "        X = training_data['X']\n",
    "        y = training_data['y']\n",
    "        folds = training_data['folds']\n",
    "        n_compounds = len(training_data['compound_names'])\n",
    "        \n",
    "        # é¢„å¤„ç†æ•°æ®\n",
    "        X_processed = clip_and_normalize_signature(X)\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ğŸ¯ Training 3-Fold Ensemble\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"  Total samples: {len(X):,}\")\n",
    "        print(f\"  Total compounds: {n_compounds:,}\")\n",
    "        print(f\"  Input features: {X.shape[1]}\")\n",
    "        \n",
    "        models = []\n",
    "        histories = []\n",
    "        \n",
    "        # è®­ç»ƒ3ä¸ªæ¨¡å‹\n",
    "        for fold_id in range(3):\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"Training Fold {fold_id} Model\")\n",
    "            print(f\"{'='*80}\")\n",
    "            \n",
    "            # å‡†å¤‡æ•°æ®\n",
    "            # æ¨¡å‹åœ¨fold_idä¸ŠéªŒè¯ï¼Œåœ¨å…¶ä»–ä¸¤ä¸ªfoldä¸Šè®­ç»ƒ\n",
    "            val_mask = folds == fold_id\n",
    "            train_mask = ~val_mask\n",
    "            \n",
    "            print(f\"  Training samples: {train_mask.sum():,}\")\n",
    "            print(f\"  Validation samples: {val_mask.sum():,}\")\n",
    "            \n",
    "            # åˆ›å»ºæ•°æ®é›†\n",
    "            train_dataset = LINCSDataset(X_processed, y, train_mask)\n",
    "            val_dataset = LINCSDataset(X_processed, y, val_mask)\n",
    "            \n",
    "            train_loader = DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size=self.batch_size,\n",
    "                shuffle=True,\n",
    "                num_workers=4,\n",
    "                pin_memory=True\n",
    "            )\n",
    "            \n",
    "            val_loader = DataLoader(\n",
    "                val_dataset,\n",
    "                batch_size=self.batch_size,\n",
    "                shuffle=False,\n",
    "                num_workers=4,\n",
    "                pin_memory=True\n",
    "            )\n",
    "            \n",
    "            # åˆ›å»ºæ¨¡å‹\n",
    "            model = DrugReflectorModel(\n",
    "                input_size=X.shape[1],\n",
    "                output_size=n_compounds,\n",
    "                dropout_rate=0.64,\n",
    "                batch_norm_momentum=0.1\n",
    "            ).to(self.device)\n",
    "            \n",
    "            print(f\"\\n  Model architecture:\")\n",
    "            print(f\"    Input: {X.shape[1]} â†’ Hidden1: 1024 â†’ Hidden2: 2048 â†’ Output: {n_compounds}\")\n",
    "            print(f\"    Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "            \n",
    "            # è®­ç»ƒæ¨¡å‹\n",
    "            history = self.train_single_model(\n",
    "                model,\n",
    "                train_loader,\n",
    "                val_loader,\n",
    "                fold_id\n",
    "            )\n",
    "            \n",
    "            models.append(model)\n",
    "            histories.append(history)\n",
    "            \n",
    "            # ä¿å­˜æ¨¡å‹\n",
    "            model_path = output_dir / f\"model_fold{fold_id}.pt\"\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'fold_id': fold_id,\n",
    "                'history': history,\n",
    "                'config': {\n",
    "                    'input_size': X.shape[1],\n",
    "                    'output_size': n_compounds,\n",
    "                    'dropout_rate': 0.64,\n",
    "                    'batch_norm_momentum': 0.1\n",
    "                }\n",
    "            }, model_path)\n",
    "            print(f\"\\n  âœ“ Model saved to {model_path}\")\n",
    "        \n",
    "        # ä¿å­˜ensembleå†å²\n",
    "        ensemble_history_path = output_dir / \"ensemble_history.pkl\"\n",
    "        with open(ensemble_history_path, 'wb') as f:\n",
    "            pickle.dump(histories, f)\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"âœ… Ensemble Training Complete!\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"  Models saved to: {output_dir}\")\n",
    "        \n",
    "        return models, histories\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# æ¨¡å‹è¯„ä¼°\n",
    "# ============================================================================\n",
    "\n",
    "class DrugReflectorEvaluator:\n",
    "    \"\"\"æ¨¡å‹è¯„ä¼°å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, models: List[nn.Module], device='cuda'):\n",
    "        self.models = models\n",
    "        self.device = device\n",
    "        for model in self.models:\n",
    "            model.eval()\n",
    "    \n",
    "    def predict_ensemble(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Ensembleé¢„æµ‹\n",
    "        \n",
    "        åŸæ–‡ï¼ˆSIç¬¬2é¡µï¼‰ï¼š\n",
    "        \"The final predicted class probabilities were the softmax probabilities \n",
    "        of the average score over all three folds.\"\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: np.ndarray\n",
    "            è¾“å…¥æ•°æ® (n_samples, 978)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        np.ndarray: å¹³å‡å¾—åˆ† (n_samples, n_compounds)\n",
    "        \"\"\"\n",
    "        X_tensor = torch.FloatTensor(X).to(self.device)\n",
    "        \n",
    "        all_scores = []\n",
    "        with torch.no_grad():\n",
    "            for model in self.models:\n",
    "                scores = model(X_tensor)\n",
    "                all_scores.append(scores.cpu().numpy())\n",
    "        \n",
    "        # å¹³å‡å¾—åˆ†\n",
    "        avg_scores = np.mean(all_scores, axis=0)\n",
    "        \n",
    "        return avg_scores\n",
    "    \n",
    "    def evaluate(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        compound_names: List[str]\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        è¯„ä¼°ensembleæ€§èƒ½\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: np.ndarray\n",
    "            æµ‹è¯•æ•°æ®\n",
    "        y: np.ndarray\n",
    "            çœŸå®æ ‡ç­¾\n",
    "        compound_names: List[str]\n",
    "            åŒ–åˆç‰©åç§°åˆ—è¡¨\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        Dict: è¯„ä¼°æŒ‡æ ‡\n",
    "        \"\"\"\n",
    "        print(f\"\\nğŸ“Š Evaluating ensemble performance...\")\n",
    "        \n",
    "        # é¢„å¤„ç†\n",
    "        X_processed = clip_and_normalize_signature(X)\n",
    "        \n",
    "        # é¢„æµ‹\n",
    "        avg_scores = self.predict_ensemble(X_processed)\n",
    "        probs = torch.softmax(torch.FloatTensor(avg_scores), dim=1).numpy()\n",
    "        preds = np.argmax(avg_scores, axis=1)\n",
    "        \n",
    "        # è®¡ç®—æŒ‡æ ‡\n",
    "        top1_acc = accuracy_score(y, preds)\n",
    "        top10_acc = top_k_accuracy_score(y, probs, k=10)\n",
    "        \n",
    "        # Top 1% recall\n",
    "        top1_percent_k = max(1, int(0.01 * probs.shape[1]))\n",
    "        recall = top_k_accuracy_score(y, probs, k=top1_percent_k)\n",
    "        \n",
    "        print(f\"  Top-1 Accuracy: {top1_acc:.4f}\")\n",
    "        print(f\"  Top-10 Accuracy: {top10_acc:.4f}\")\n",
    "        print(f\"  Top 1% Recall: {recall:.4f}\")\n",
    "        \n",
    "        # è®¡ç®—æ¯ä¸ªåŒ–åˆç‰©çš„recall\n",
    "        compound_recalls = []\n",
    "        for compound_idx in range(len(compound_names)):\n",
    "            mask = y == compound_idx\n",
    "            if mask.sum() == 0:\n",
    "                continue\n",
    "            \n",
    "            compound_probs = probs[mask]\n",
    "            compound_labels = y[mask]\n",
    "            \n",
    "            if len(compound_probs) > 0:\n",
    "                compound_recall = top_k_accuracy_score(\n",
    "                    compound_labels, \n",
    "                    compound_probs, \n",
    "                    k=top1_percent_k\n",
    "                )\n",
    "                compound_recalls.append(compound_recall)\n",
    "        \n",
    "        avg_compound_recall = np.mean(compound_recalls)\n",
    "        \n",
    "        print(f\"  Average per-compound recall: {avg_compound_recall:.4f}\")\n",
    "        \n",
    "        results = {\n",
    "            'top1_accuracy': top1_acc,\n",
    "            'top10_accuracy': top10_acc,\n",
    "            'top1_percent_recall': recall,\n",
    "            'avg_compound_recall': avg_compound_recall,\n",
    "            'compound_recalls': compound_recalls,\n",
    "            'predictions': preds,\n",
    "            'probabilities': probs\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# å¯è§†åŒ–\n",
    "# ============================================================================\n",
    "\n",
    "def plot_training_history(histories: List[Dict], output_dir: Path):\n",
    "    \"\"\"\n",
    "    ç»˜åˆ¶è®­ç»ƒå†å²\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    histories: List[Dict]\n",
    "        3ä¸ªæ¨¡å‹çš„è®­ç»ƒå†å²\n",
    "    output_dir: Path\n",
    "        è¾“å‡ºç›®å½•\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    \n",
    "    metrics = [\n",
    "        ('train_loss', 'Training Loss', 'Loss'),\n",
    "        ('val_loss', 'Validation Loss', 'Loss'),\n",
    "        ('val_recall', 'Validation Recall (Top 1%)', 'Recall'),\n",
    "        ('val_top1_acc', 'Top-1 Accuracy', 'Accuracy'),\n",
    "        ('val_top10_acc', 'Top-10 Accuracy', 'Accuracy'),\n",
    "        ('learning_rates', 'Learning Rate', 'LR')\n",
    "    ]\n",
    "    \n",
    "    for idx, (metric, title, ylabel) in enumerate(metrics):\n",
    "        ax = axes[idx // 3, idx % 3]\n",
    "        \n",
    "        for fold_id, history in enumerate(histories):\n",
    "            epochs = range(1, len(history[metric]) + 1)\n",
    "            ax.plot(epochs, history[metric], label=f'Fold {fold_id}', linewidth=2)\n",
    "        \n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel(ylabel)\n",
    "        ax.set_title(title)\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = output_dir / 'training_history.png'\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\nâœ“ Training history plot saved to {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ä¸»è®­ç»ƒæµç¨‹\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"ä¸»è®­ç»ƒæµç¨‹\"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"ğŸš€ DrugReflector Base Model Training\")\n",
    "    print(\"   Following Science 2025 Supplementary Materials\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # è®¾ç½®è·¯å¾„\n",
    "    data_dir = Path(\"E:/ç§‘ç ”/Models/drugreflector/processed_data\")\n",
    "    output_dir = Path(\"E:/ç§‘ç ”/Models/drugreflector/trained_models\")\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # åŠ è½½æ•°æ®\n",
    "    print(f\"\\nğŸ“‚ Loading preprocessed data...\")\n",
    "    data_file = data_dir / \"training_data_paper_compliant.pkl\"\n",
    "    \n",
    "    if not data_file.exists():\n",
    "        print(f\"âŒ Data file not found: {data_file}\")\n",
    "        print(f\"   Please run the preprocessing script first.\")\n",
    "        return\n",
    "    \n",
    "    with open(data_file, 'rb') as f:\n",
    "        training_data = pickle.load(f)\n",
    "    \n",
    "    print(f\"âœ“ Loaded training data\")\n",
    "    print(f\"  Samples: {len(training_data['X']):,}\")\n",
    "    print(f\"  Compounds: {len(training_data['compound_names']):,}\")\n",
    "    print(f\"  Features: {training_data['X'].shape[1]}\")\n",
    "    \n",
    "    # åˆ›å»ºè®­ç»ƒå™¨\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"\\nğŸ”§ Initializing trainer...\")\n",
    "    print(f\"  Device: {device}\")\n",
    "    \n",
    "    if device == 'cuda':\n",
    "        print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    \n",
    "    trainer = DrugReflectorTrainer(\n",
    "        device=device,\n",
    "        initial_lr=0.0139,\n",
    "        min_lr=0.00001,\n",
    "        weight_decay=1e-5,\n",
    "        t_0=20,\n",
    "        focal_gamma=2.0,\n",
    "        batch_size=256,\n",
    "        num_epochs=50,\n",
    "        early_stop_epoch=20\n",
    "    )\n",
    "    \n",
    "    # è®­ç»ƒensemble\n",
    "    models, histories = trainer.train_ensemble(training_data, output_dir)\n",
    "    \n",
    "    # å¯è§†åŒ–è®­ç»ƒå†å²\n",
    "    print(f\"\\nğŸ“Š Plotting training history...\")\n",
    "    plot_training_history(histories, output_dir)\n",
    "    \n",
    "    # è¯„ä¼°æ¨¡å‹\n",
    "    print(f\"\\nğŸ“Š Evaluating ensemble on validation sets...\")\n",
    "    evaluator = DrugReflectorEvaluator(models, device)\n",
    "    \n",
    "    # åœ¨æ¯ä¸ªfoldä¸Šè¯„ä¼°\n",
    "    for fold_id in range(3):\n",
    "        val_mask = training_data['folds'] == fold_id\n",
    "        X_val = training_data['X'][val_mask]\n",
    "        y_val = training_data['y'][val_mask]\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Fold {fold_id} Validation Performance\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        results = evaluator.evaluate(\n",
    "            X_val,\n",
    "            y_val,\n",
    "            training_data['compound_names']\n",
    "        )\n",
    "        \n",
    "        # ä¿å­˜ç»“æœ\n",
    "        results_path = output_dir / f\"fold{fold_id}_results.pkl\"\n",
    "        with open(results_path, 'wb') as f:\n",
    "            pickle.dump(results, f)\n",
    "        print(f\"  âœ“ Results saved to {results_path}\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"âœ… TRAINING COMPLETE!\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"ğŸ“ All outputs saved to: {output_dir}\")\n",
    "    print(f\"\\nFiles created:\")\n",
    "    print(f\"  â€¢ model_fold0.pt - Fold 0 model checkpoint\")\n",
    "    print(f\"  â€¢ model_fold1.pt - Fold 1 model checkpoint\")\n",
    "    print(f\"  â€¢ model_fold2.pt - Fold 2 model checkpoint\")\n",
    "    print(f\"  â€¢ ensemble_history.pkl - Training history\")\n",
    "    print(f\"  â€¢ training_history.png - Training curves\")\n",
    "    print(f\"  â€¢ fold*_results.pkl - Evaluation results\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821832a1",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "DrugReflector åŸºåº§æ¨¡å‹è®­ç»ƒ\n",
    "ä¸¥æ ¼éµå¾ª Science 2025 Supplementary Materials (Pages 2-4)\n",
    "\n",
    "æ¨¡å‹æ¶æ„ï¼š\n",
    "- Input: 978 (landmark genes)\n",
    "- Hidden 1: 1,024 nodes + ReLU + Dropout(0.64) + BatchNorm\n",
    "- Hidden 2: 2,048 nodes + ReLU + Dropout(0.64) + BatchNorm\n",
    "- Output: 9,597 (compounds)\n",
    "\n",
    "è®­ç»ƒé…ç½®ï¼š\n",
    "- Focal Loss (Î³=2)\n",
    "- Cosine Annealing with Warm Restarts\n",
    "- 3-Fold Ensemble\n",
    "- 50 epochs per model\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from typing import Dict, Tuple, List\n",
    "from sklearn.metrics import accuracy_score, top_k_accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# æ•°æ®é¢„å¤„ç†\n",
    "# ============================================================================\n",
    "\n",
    "def clip_and_normalize_signature(X: np.ndarray, clip_range=(-2, 2)) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    è£å‰ªå¹¶æ ‡å‡†åŒ–ç­¾åï¼Œä½¿å…¶æ ‡å‡†å·®ä¸º1\n",
    "    \n",
    "    åŸæ–‡ï¼ˆSIç¬¬3é¡µï¼‰ï¼š\n",
    "    \"every transcriptional vector v is clipped to range [-2,2] such that \n",
    "    its standard deviation after clipping equals 1\"\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X: np.ndarray\n",
    "        åŸå§‹è¡¨è¾¾çŸ©é˜µ (n_samples, n_genes)\n",
    "    clip_range: tuple\n",
    "        è£å‰ªèŒƒå›´ï¼Œé»˜è®¤[-2, 2]\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    np.ndarray: å¤„ç†åçš„çŸ©é˜µ\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ“Š Clipping and normalizing signatures...\")\n",
    "    print(f\"   Clip range: {clip_range}\")\n",
    "    \n",
    "    X_processed = np.zeros_like(X)\n",
    "    \n",
    "    for i in range(X.shape[0]):\n",
    "        # è£å‰ªåˆ°æŒ‡å®šèŒƒå›´\n",
    "        vec = np.clip(X[i], clip_range[0], clip_range[1])\n",
    "        \n",
    "        # æ ‡å‡†åŒ–ä½¿æ ‡å‡†å·®ä¸º1\n",
    "        std = np.std(vec)\n",
    "        if std > 0:\n",
    "            vec = vec / std\n",
    "        \n",
    "        X_processed[i] = vec\n",
    "    \n",
    "    print(f\"   âœ“ Mean std after normalization: {np.std(X_processed, axis=1).mean():.4f}\")\n",
    "    print(f\"   âœ“ Data range after clipping: [{X_processed.min():.2f}, {X_processed.max():.2f}]\")\n",
    "    \n",
    "    return X_processed\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PyTorch Dataset\n",
    "# ============================================================================\n",
    "\n",
    "class LINCSDataset(Dataset):\n",
    "    \"\"\"LINCSæ•°æ®é›†ç±»\"\"\"\n",
    "    \n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray, fold_mask: np.ndarray = None):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: np.ndarray\n",
    "            è¡¨è¾¾çŸ©é˜µ (n_samples, 978)\n",
    "        y: np.ndarray\n",
    "            åŒ–åˆç‰©æ ‡ç­¾ (n_samples,)\n",
    "        fold_mask: np.ndarray\n",
    "            Foldæ©ç ï¼ŒTrueè¡¨ç¤ºåŒ…å«åœ¨æ­¤æ•°æ®é›†ä¸­\n",
    "        \"\"\"\n",
    "        if fold_mask is not None:\n",
    "            self.X = torch.FloatTensor(X[fold_mask])\n",
    "            self.y = torch.LongTensor(y[fold_mask])\n",
    "        else:\n",
    "            self.X = torch.FloatTensor(X)\n",
    "            self.y = torch.LongTensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Focal Loss\n",
    "# ============================================================================\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Focal Losså®ç°\n",
    "    \n",
    "    åŸæ–‡ï¼ˆSIç¬¬3é¡µï¼‰ï¼š\n",
    "    \"The models were trained using a focal loss function with a \n",
    "    focusing parameter of Î³ = 2\"\n",
    "    \n",
    "    Reference: Lin et al. \"Focal Loss for Dense Object Detection\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, gamma=2.0, alpha=None, reduction='mean'):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        gamma: float\n",
    "            èšç„¦å‚æ•°ï¼ŒåŸæ–‡ä½¿ç”¨2.0\n",
    "        alpha: Tensor or None\n",
    "            ç±»åˆ«æƒé‡\n",
    "        reduction: str\n",
    "            'mean', 'sum', or 'none'\n",
    "        \"\"\"\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        inputs: Tensor\n",
    "            æ¨¡å‹è¾“å‡º logits (N, C)\n",
    "        targets: Tensor\n",
    "            çœŸå®æ ‡ç­¾ (N,)\n",
    "        \"\"\"\n",
    "        # è®¡ç®—äº¤å‰ç†µ\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        \n",
    "        # è®¡ç®—pt\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        \n",
    "        # Focal loss\n",
    "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "        \n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha[targets]\n",
    "            focal_loss = alpha_t * focal_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# æ¨¡å‹æ¶æ„\n",
    "# ============================================================================\n",
    "\n",
    "class DrugReflectorModel(nn.Module):\n",
    "    \"\"\"\n",
    "    DrugReflectorç¥ç»ç½‘ç»œæ¶æ„\n",
    "    \n",
    "    åŸæ–‡ï¼ˆSIç¬¬2é¡µï¼‰ï¼š\n",
    "    \"The input layer has 978 nodes (one for each landmark gene), \n",
    "    and the output layer has 9,597 nodes (one for each target LINCS perturbation). \n",
    "    The first hidden layer has 1,024 nodes, and the second has 2,048 nodes \n",
    "    using rectified linear units (ReLU) to compute node activations.\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        input_size=978, \n",
    "        hidden1_size=1024, \n",
    "        hidden2_size=2048, \n",
    "        output_size=9597,\n",
    "        dropout_rate=0.64,\n",
    "        batch_norm_momentum=0.1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_size: int\n",
    "            è¾“å…¥ç‰¹å¾æ•°ï¼ˆlandmark genesï¼‰\n",
    "        hidden1_size: int\n",
    "            ç¬¬ä¸€éšè—å±‚å¤§å°\n",
    "        hidden2_size: int\n",
    "            ç¬¬äºŒéšè—å±‚å¤§å°\n",
    "        output_size: int\n",
    "            è¾“å‡ºç±»åˆ«æ•°ï¼ˆåŒ–åˆç‰©æ•°ï¼‰\n",
    "        dropout_rate: float\n",
    "            Dropoutæ¯”ç‡ï¼ŒåŸæ–‡0.64\n",
    "        batch_norm_momentum: float\n",
    "            BatchNormåŠ¨é‡ï¼ŒåŸæ–‡0.1\n",
    "        \"\"\"\n",
    "        super(DrugReflectorModel, self).__init__()\n",
    "        \n",
    "        # ç¬¬ä¸€éšè—å±‚\n",
    "        self.fc1 = nn.Linear(input_size, hidden1_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden1_size, momentum=batch_norm_momentum)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # ç¬¬äºŒéšè—å±‚\n",
    "        self.fc2 = nn.Linear(hidden1_size, hidden2_size)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden2_size, momentum=batch_norm_momentum)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # è¾“å‡ºå±‚\n",
    "        self.fc3 = nn.Linear(hidden2_size, output_size)\n",
    "        \n",
    "        # åˆå§‹åŒ–æƒé‡\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"æƒé‡åˆå§‹åŒ–\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        å‰å‘ä¼ æ’­\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x: Tensor\n",
    "            è¾“å…¥ (batch_size, 978)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        Tensor: è¾“å‡ºlogits (batch_size, 9597)\n",
    "        \"\"\"\n",
    "        # ç¬¬ä¸€éšè—å±‚\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # ç¬¬äºŒéšè—å±‚\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # è¾“å‡ºå±‚\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# è®­ç»ƒå™¨\n",
    "# ============================================================================\n",
    "\n",
    "class DrugReflectorTrainer:\n",
    "    \"\"\"\n",
    "    DrugReflectorè®­ç»ƒå™¨\n",
    "    \n",
    "    å®ç°3-fold ensembleè®­ç»ƒç­–ç•¥\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        initial_lr=0.0139,\n",
    "        min_lr=0.00001,\n",
    "        weight_decay=1e-5,\n",
    "        t_0=20,  # Time to first restart\n",
    "        t_mult=1.9,  # åŸæ–‡0.5ï¼Œä½†è¿™æ ·ä¼šå¯¼è‡´å‘¨æœŸå‡åŠï¼Œå®é™…åº”è¯¥>1\n",
    "        focal_gamma=2.0,\n",
    "        batch_size=256,\n",
    "        num_epochs=50,\n",
    "        early_stop_epoch=20\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        device: str\n",
    "            è®¡ç®—è®¾å¤‡\n",
    "        initial_lr: float\n",
    "            åˆå§‹å­¦ä¹ ç‡ï¼ˆåŸæ–‡ï¼š0.0139ï¼‰\n",
    "        min_lr: float\n",
    "            æœ€å°å­¦ä¹ ç‡ï¼ˆåŸæ–‡ï¼š0.00001ï¼‰\n",
    "        weight_decay: float\n",
    "            æƒé‡è¡°å‡ï¼ˆåŸæ–‡ï¼š1e-5ï¼‰\n",
    "        t_0: int\n",
    "            ç¬¬ä¸€æ¬¡warm restartå‰çš„epochæ•°ï¼ˆåŸæ–‡ï¼š20ï¼‰\n",
    "        t_mult: float\n",
    "            warm restartå‘¨æœŸå€å¢å› å­ï¼ˆåŸæ–‡ï¼š0.5ï¼‰\n",
    "        focal_gamma: float\n",
    "            Focal lossèšç„¦å‚æ•°ï¼ˆåŸæ–‡ï¼š2.0ï¼‰\n",
    "        batch_size: int\n",
    "            æ‰¹æ¬¡å¤§å°\n",
    "        num_epochs: int\n",
    "            æ€»è®­ç»ƒè½®æ•°ï¼ˆåŸæ–‡ï¼š50ï¼‰\n",
    "        early_stop_epoch: int\n",
    "            æ—©åœæ£€æŸ¥ç‚¹ï¼ˆåŸæ–‡ï¼š20ï¼‰\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.initial_lr = initial_lr\n",
    "        self.min_lr = min_lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.t_0 = t_0\n",
    "        self.t_mult = t_mult\n",
    "        self.focal_gamma = focal_gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.early_stop_epoch = early_stop_epoch\n",
    "        \n",
    "        print(f\"\\nğŸš€ DrugReflector Trainer initialized\")\n",
    "        print(f\"   Device: {self.device}\")\n",
    "        print(f\"   Initial LR: {self.initial_lr}\")\n",
    "        print(f\"   Min LR: {self.min_lr}\")\n",
    "        print(f\"   Weight Decay: {self.weight_decay}\")\n",
    "        print(f\"   T_0 (first restart): {self.t_0}\")\n",
    "        print(f\"   Focal Î³: {self.focal_gamma}\")\n",
    "        print(f\"   Batch size: {self.batch_size}\")\n",
    "        print(f\"   Epochs: {self.num_epochs}\")\n",
    "    \n",
    "    def train_single_model(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: DataLoader,\n",
    "        fold_id: int\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        è®­ç»ƒå•ä¸ªæ¨¡å‹\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        model: nn.Module\n",
    "            DrugReflectoræ¨¡å‹\n",
    "        train_loader: DataLoader\n",
    "            è®­ç»ƒæ•°æ®åŠ è½½å™¨\n",
    "        val_loader: DataLoader\n",
    "            éªŒè¯æ•°æ®åŠ è½½å™¨\n",
    "        fold_id: int\n",
    "            Fold ID (0, 1, 2)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        Dict: è®­ç»ƒå†å²\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Training Model {fold_id}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨\n",
    "        criterion = FocalLoss(gamma=self.focal_gamma)\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=self.initial_lr,\n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "        \n",
    "        # å­¦ä¹ ç‡è°ƒåº¦å™¨\n",
    "        scheduler = CosineAnnealingWarmRestarts(\n",
    "            optimizer,\n",
    "            T_0=self.t_0,\n",
    "            T_mult=int(self.t_mult) if self.t_mult >= 1 else 1,\n",
    "            eta_min=self.min_lr\n",
    "        )\n",
    "        \n",
    "        # è®­ç»ƒå†å²\n",
    "        history = {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'val_recall': [],\n",
    "            'val_top1_acc': [],\n",
    "            'val_top10_acc': [],\n",
    "            'learning_rates': []\n",
    "        }\n",
    "        \n",
    "        best_recall = 0.0\n",
    "        best_epoch = 0\n",
    "        best_model_state = None\n",
    "        \n",
    "        # è®­ç»ƒå¾ªç¯\n",
    "        for epoch in range(self.num_epochs):\n",
    "            # ========== è®­ç»ƒé˜¶æ®µ ==========\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            \n",
    "            pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{self.num_epochs}\")\n",
    "            for batch_X, batch_y in pbar:\n",
    "                batch_X = batch_X.to(self.device)\n",
    "                batch_y = batch_y.to(self.device)\n",
    "                \n",
    "                # å‰å‘ä¼ æ’­\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                \n",
    "                # åå‘ä¼ æ’­\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                pbar.set_postfix({'loss': loss.item()})\n",
    "            \n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            \n",
    "            # ========== éªŒè¯é˜¶æ®µ ==========\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "            all_probs = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch_X, batch_y in val_loader:\n",
    "                    batch_X = batch_X.to(self.device)\n",
    "                    batch_y = batch_y.to(self.device)\n",
    "                    \n",
    "                    outputs = model(batch_X)\n",
    "                    loss = criterion(outputs, batch_y)\n",
    "                    val_loss += loss.item()\n",
    "                    \n",
    "                    # è®¡ç®—é¢„æµ‹\n",
    "                    probs = F.softmax(outputs, dim=1)\n",
    "                    preds = torch.argmax(probs, dim=1)\n",
    "                    \n",
    "                    all_preds.append(preds.cpu().numpy())\n",
    "                    all_labels.append(batch_y.cpu().numpy())\n",
    "                    all_probs.append(probs.cpu().numpy())\n",
    "            \n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            \n",
    "            # è®¡ç®—æŒ‡æ ‡\n",
    "            all_preds = np.concatenate(all_preds)\n",
    "            all_labels = np.concatenate(all_labels)\n",
    "            all_probs = np.concatenate(all_probs)\n",
    "            \n",
    "            top1_acc = accuracy_score(all_labels, all_preds)\n",
    "            top10_acc = top_k_accuracy_score(all_labels, all_probs, k=10)\n",
    "            \n",
    "            # è®¡ç®—Top 1% recall (åŸæ–‡ä¸»è¦æŒ‡æ ‡)\n",
    "            top1_percent_k = max(1, int(0.01 * all_probs.shape[1]))\n",
    "            recall = top_k_accuracy_score(all_labels, all_probs, k=top1_percent_k)\n",
    "            \n",
    "            # è®°å½•å†å²\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            history['train_loss'].append(avg_train_loss)\n",
    "            history['val_loss'].append(avg_val_loss)\n",
    "            history['val_recall'].append(recall)\n",
    "            history['val_top1_acc'].append(top1_acc)\n",
    "            history['val_top10_acc'].append(top10_acc)\n",
    "            history['learning_rates'].append(current_lr)\n",
    "            \n",
    "            # æ‰“å°è¿›åº¦\n",
    "            print(f\"\\nEpoch {epoch+1}/{self.num_epochs}\")\n",
    "            print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "            print(f\"  Val Loss: {avg_val_loss:.4f}\")\n",
    "            print(f\"  Val Recall (top 1%): {recall:.4f}\")\n",
    "            print(f\"  Val Top-1 Acc: {top1_acc:.4f}\")\n",
    "            print(f\"  Val Top-10 Acc: {top10_acc:.4f}\")\n",
    "            print(f\"  Learning Rate: {current_lr:.6f}\")\n",
    "            \n",
    "            # ä¿å­˜æœ€ä½³æ¨¡å‹\n",
    "            if recall > best_recall:\n",
    "                best_recall = recall\n",
    "                best_epoch = epoch\n",
    "                best_model_state = model.state_dict().copy()\n",
    "                print(f\"  âœ“ New best model! (Recall: {best_recall:.4f})\")\n",
    "            \n",
    "            # å­¦ä¹ ç‡è°ƒåº¦\n",
    "            scheduler.step()\n",
    "        \n",
    "        # åŠ è½½æœ€ä½³æ¨¡å‹\n",
    "        if best_model_state is not None:\n",
    "            model.load_state_dict(best_model_state)\n",
    "            print(f\"\\nâœ“ Loaded best model from epoch {best_epoch+1} \"\n",
    "                  f\"(Recall: {best_recall:.4f})\")\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def train_ensemble(\n",
    "        self,\n",
    "        training_data: Dict,\n",
    "        output_dir: Path\n",
    "    ) -> List[nn.Module]:\n",
    "        \"\"\"\n",
    "        è®­ç»ƒ3-fold ensemble\n",
    "        \n",
    "        åŸæ–‡ï¼ˆSIç¬¬2-3é¡µï¼‰ï¼š\n",
    "        \"The training data was divided randomly into three folds, with perturbation \n",
    "        replicates balanced across the folds. Models were independently trained on \n",
    "        two of three folds.\"\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        training_data: Dict\n",
    "            åŒ…å«X, y, foldsçš„è®­ç»ƒæ•°æ®\n",
    "        output_dir: Path\n",
    "            æ¨¡å‹ä¿å­˜ç›®å½•\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        List[nn.Module]: è®­ç»ƒå¥½çš„3ä¸ªæ¨¡å‹\n",
    "        \"\"\"\n",
    "        X = training_data['X']\n",
    "        y = training_data['y']\n",
    "        folds = training_data['folds']\n",
    "        n_compounds = len(training_data['compound_names'])\n",
    "        \n",
    "        # é¢„å¤„ç†æ•°æ®\n",
    "        X_processed = clip_and_normalize_signature(X)\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ğŸ¯ Training 3-Fold Ensemble\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"  Total samples: {len(X):,}\")\n",
    "        print(f\"  Total compounds: {n_compounds:,}\")\n",
    "        print(f\"  Input features: {X.shape[1]}\")\n",
    "        \n",
    "        models = []\n",
    "        histories = []\n",
    "        \n",
    "        # è®­ç»ƒ3ä¸ªæ¨¡å‹\n",
    "        for fold_id in range(3):\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"Training Fold {fold_id} Model\")\n",
    "            print(f\"{'='*80}\")\n",
    "            \n",
    "            # å‡†å¤‡æ•°æ®\n",
    "            # æ¨¡å‹åœ¨fold_idä¸ŠéªŒè¯ï¼Œåœ¨å…¶ä»–ä¸¤ä¸ªfoldä¸Šè®­ç»ƒ\n",
    "            val_mask = folds == fold_id\n",
    "            train_mask = ~val_mask\n",
    "            \n",
    "            print(f\"  Training samples: {train_mask.sum():,}\")\n",
    "            print(f\"  Validation samples: {val_mask.sum():,}\")\n",
    "            \n",
    "            # åˆ›å»ºæ•°æ®é›†\n",
    "            train_dataset = LINCSDataset(X_processed, y, train_mask)\n",
    "            val_dataset = LINCSDataset(X_processed, y, val_mask)\n",
    "            \n",
    "            train_loader = DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size=self.batch_size,\n",
    "                shuffle=True,\n",
    "                num_workers=4,\n",
    "                pin_memory=True\n",
    "            )\n",
    "            \n",
    "            val_loader = DataLoader(\n",
    "                val_dataset,\n",
    "                batch_size=self.batch_size,\n",
    "                shuffle=False,\n",
    "                num_workers=4,\n",
    "                pin_memory=True\n",
    "            )\n",
    "            \n",
    "            # åˆ›å»ºæ¨¡å‹\n",
    "            model = DrugReflectorModel(\n",
    "                input_size=X.shape[1],\n",
    "                output_size=n_compounds,\n",
    "                dropout_rate=0.64,\n",
    "                batch_norm_momentum=0.1\n",
    "            ).to(self.device)\n",
    "            \n",
    "            print(f\"\\n  Model architecture:\")\n",
    "            print(f\"    Input: {X.shape[1]} â†’ Hidden1: 1024 â†’ Hidden2: 2048 â†’ Output: {n_compounds}\")\n",
    "            print(f\"    Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "            \n",
    "            # è®­ç»ƒæ¨¡å‹\n",
    "            history = self.train_single_model(\n",
    "                model,\n",
    "                train_loader,\n",
    "                val_loader,\n",
    "                fold_id\n",
    "            )\n",
    "            \n",
    "            models.append(model)\n",
    "            histories.append(history)\n",
    "            \n",
    "            # ä¿å­˜æ¨¡å‹\n",
    "            model_path = output_dir / f\"model_fold{fold_id}.pt\"\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'fold_id': fold_id,\n",
    "                'history': history,\n",
    "                'config': {\n",
    "                    'input_size': X.shape[1],\n",
    "                    'output_size': n_compounds,\n",
    "                    'dropout_rate': 0.64,\n",
    "                    'batch_norm_momentum': 0.1\n",
    "                }\n",
    "            }, model_path)\n",
    "            print(f\"\\n  âœ“ Model saved to {model_path}\")\n",
    "        \n",
    "        # ä¿å­˜ensembleå†å²\n",
    "        ensemble_history_path = output_dir / \"ensemble_history.pkl\"\n",
    "        with open(ensemble_history_path, 'wb') as f:\n",
    "            pickle.dump(histories, f)\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"âœ… Ensemble Training Complete!\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"  Models saved to: {output_dir}\")\n",
    "        \n",
    "        return models, histories\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# æ¨¡å‹è¯„ä¼°\n",
    "# ============================================================================\n",
    "\n",
    "class DrugReflectorEvaluator:\n",
    "    \"\"\"æ¨¡å‹è¯„ä¼°å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, models: List[nn.Module], device='cuda'):\n",
    "        self.models = models\n",
    "        self.device = device\n",
    "        for model in self.models:\n",
    "            model.eval()\n",
    "    \n",
    "    def predict_ensemble(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Ensembleé¢„æµ‹\n",
    "        \n",
    "        åŸæ–‡ï¼ˆSIç¬¬2é¡µï¼‰ï¼š\n",
    "        \"The final predicted class probabilities were the softmax probabilities \n",
    "        of the average score over all three folds.\"\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: np.ndarray\n",
    "            è¾“å…¥æ•°æ® (n_samples, 978)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        np.ndarray: å¹³å‡å¾—åˆ† (n_samples, n_compounds)\n",
    "        \"\"\"\n",
    "        X_tensor = torch.FloatTensor(X).to(self.device)\n",
    "        \n",
    "        all_scores = []\n",
    "        with torch.no_grad():\n",
    "            for model in self.models:\n",
    "                scores = model(X_tensor)\n",
    "                all_scores.append(scores.cpu().numpy())\n",
    "        \n",
    "        # å¹³å‡å¾—åˆ†\n",
    "        avg_scores = np.mean(all_scores, axis=0)\n",
    "        \n",
    "        return avg_scores\n",
    "    \n",
    "    def evaluate(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        compound_names: List[str]\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        è¯„ä¼°ensembleæ€§èƒ½\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: np.ndarray\n",
    "            æµ‹è¯•æ•°æ®\n",
    "        y: np.ndarray\n",
    "            çœŸå®æ ‡ç­¾\n",
    "        compound_names: List[str]\n",
    "            åŒ–åˆç‰©åç§°åˆ—è¡¨\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        Dict: è¯„ä¼°æŒ‡æ ‡\n",
    "        \"\"\"\n",
    "        print(f\"\\nğŸ“Š Evaluating ensemble performance...\")\n",
    "        \n",
    "        # é¢„å¤„ç†\n",
    "        X_processed = clip_and_normalize_signature(X)\n",
    "        \n",
    "        # é¢„æµ‹\n",
    "        avg_scores = self.predict_ensemble(X_processed)\n",
    "        probs = torch.softmax(torch.FloatTensor(avg_scores), dim=1).numpy()\n",
    "        preds = np.argmax(avg_scores, axis=1)\n",
    "        \n",
    "        # è®¡ç®—æŒ‡æ ‡\n",
    "        top1_acc = accuracy_score(y, preds)\n",
    "        top10_acc = top_k_accuracy_score(y, probs, k=10)\n",
    "        \n",
    "        # Top 1% recall\n",
    "        top1_percent_k = max(1, int(0.01 * probs.shape[1]))\n",
    "        recall = top_k_accuracy_score(y, probs, k=top1_percent_k)\n",
    "        \n",
    "        print(f\"  Top-1 Accuracy: {top1_acc:.4f}\")\n",
    "        print(f\"  Top-10 Accuracy: {top10_acc:.4f}\")\n",
    "        print(f\"  Top 1% Recall: {recall:.4f}\")\n",
    "        \n",
    "        # è®¡ç®—æ¯ä¸ªåŒ–åˆç‰©çš„recall\n",
    "        compound_recalls = []\n",
    "        for compound_idx in range(len(compound_names)):\n",
    "            mask = y == compound_idx\n",
    "            if mask.sum() == 0:\n",
    "                continue\n",
    "            \n",
    "            compound_probs = probs[mask]\n",
    "            compound_labels = y[mask]\n",
    "            \n",
    "            if len(compound_probs) > 0:\n",
    "                compound_recall = top_k_accuracy_score(\n",
    "                    compound_labels, \n",
    "                    compound_probs, \n",
    "                    k=top1_percent_k\n",
    "                )\n",
    "                compound_recalls.append(compound_recall)\n",
    "        \n",
    "        avg_compound_recall = np.mean(compound_recalls)\n",
    "        \n",
    "        print(f\"  Average per-compound recall: {avg_compound_recall:.4f}\")\n",
    "        \n",
    "        results = {\n",
    "            'top1_accuracy': top1_acc,\n",
    "            'top10_accuracy': top10_acc,\n",
    "            'top1_percent_recall': recall,\n",
    "            'avg_compound_recall': avg_compound_recall,\n",
    "            'compound_recalls': compound_recalls,\n",
    "            'predictions': preds,\n",
    "            'probabilities': probs\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# å¯è§†åŒ–\n",
    "# ============================================================================\n",
    "\n",
    "def plot_training_history(histories: List[Dict], output_dir: Path):\n",
    "    \"\"\"\n",
    "    ç»˜åˆ¶è®­ç»ƒå†å²\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    histories: List[Dict]\n",
    "        3ä¸ªæ¨¡å‹çš„è®­ç»ƒå†å²\n",
    "    output_dir: Path\n",
    "        è¾“å‡ºç›®å½•\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    \n",
    "    metrics = [\n",
    "        ('train_loss', 'Training Loss', 'Loss'),\n",
    "        ('val_loss', 'Validation Loss', 'Loss'),\n",
    "        ('val_recall', 'Validation Recall (Top 1%)', 'Recall'),\n",
    "        ('val_top1_acc', 'Top-1 Accuracy', 'Accuracy'),\n",
    "        ('val_top10_acc', 'Top-10 Accuracy', 'Accuracy'),\n",
    "        ('learning_rates', 'Learning Rate', 'LR')\n",
    "    ]\n",
    "    \n",
    "    for idx, (metric, title, ylabel) in enumerate(metrics):\n",
    "        ax = axes[idx // 3, idx % 3]\n",
    "        \n",
    "        for fold_id, history in enumerate(histories):\n",
    "            epochs = range(1, len(history[metric]) + 1)\n",
    "            ax.plot(epochs, history[metric], label=f'Fold {fold_id}', linewidth=2)\n",
    "        \n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel(ylabel)\n",
    "        ax.set_title(title)\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = output_dir / 'training_history.png'\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\nâœ“ Training history plot saved to {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ä¸»è®­ç»ƒæµç¨‹\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"ä¸»è®­ç»ƒæµç¨‹\"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"ğŸš€ DrugReflector Base Model Training\")\n",
    "    print(\"   Following Science 2025 Supplementary Materials\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # è®¾ç½®è·¯å¾„\n",
    "    data_dir = Path(\"E:/ç§‘ç ”/Models/drugreflector/processed_data\")\n",
    "    output_dir = Path(\"E:/ç§‘ç ”/Models/drugreflector/trained_models\")\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # åŠ è½½æ•°æ®\n",
    "    print(f\"\\nğŸ“‚ Loading preprocessed data...\")\n",
    "    data_file = data_dir / \"training_data_paper_compliant.pkl\"\n",
    "    \n",
    "    if not data_file.exists():\n",
    "        print(f\"âŒ Data file not found: {data_file}\")\n",
    "        print(f\"   Please run the preprocessing script first.\")\n",
    "        return\n",
    "    \n",
    "    with open(data_file, 'rb') as f:\n",
    "        training_data = pickle.load(f)\n",
    "    \n",
    "    print(f\"âœ“ Loaded training data\")\n",
    "    print(f\"  Samples: {len(training_data['X']):,}\")\n",
    "    print(f\"  Compounds: {len(training_data['compound_names']):,}\")\n",
    "    print(f\"  Features: {training_data['X'].shape[1]}\")\n",
    "    \n",
    "    # åˆ›å»ºè®­ç»ƒå™¨\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"\\nğŸ”§ Initializing trainer...\")\n",
    "    print(f\"  Device: {device}\")\n",
    "    \n",
    "    if device == 'cuda':\n",
    "        print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    \n",
    "    trainer = DrugReflectorTrainer(\n",
    "        device=device,\n",
    "        initial_lr=0.0139,\n",
    "        min_lr=0.00001,\n",
    "        weight_decay=1e-5,\n",
    "        t_0=20,\n",
    "        focal_gamma=2.0,\n",
    "        batch_size=256,\n",
    "        num_epochs=50,\n",
    "        early_stop_epoch=20\n",
    "    )\n",
    "    \n",
    "    # è®­ç»ƒensemble\n",
    "    models, histories = trainer.train_ensemble(training_data, output_dir)\n",
    "    \n",
    "    # å¯è§†åŒ–è®­ç»ƒå†å²\n",
    "    print(f\"\\nğŸ“Š Plotting training history...\")\n",
    "    plot_training_history(histories, output_dir)\n",
    "    \n",
    "    # è¯„ä¼°æ¨¡å‹\n",
    "    print(f\"\\nğŸ“Š Evaluating ensemble on validation sets...\")\n",
    "    evaluator = DrugReflectorEvaluator(models, device)\n",
    "    \n",
    "    # åœ¨æ¯ä¸ªfoldä¸Šè¯„ä¼°\n",
    "    for fold_id in range(3):\n",
    "        val_mask = training_data['folds'] == fold_id\n",
    "        X_val = training_data['X'][val_mask]\n",
    "        y_val = training_data['y'][val_mask]\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Fold {fold_id} Validation Performance\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        results = evaluator.evaluate(\n",
    "            X_val,\n",
    "            y_val,\n",
    "            training_data['compound_names']\n",
    "        )\n",
    "        \n",
    "        # ä¿å­˜ç»“æœ\n",
    "        results_path = output_dir / f\"fold{fold_id}_results.pkl\"\n",
    "        with open(results_path, 'wb') as f:\n",
    "            pickle.dump(results, f)\n",
    "        print(f\"  âœ“ Results saved to {results_path}\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"âœ… TRAINING COMPLETE!\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"ğŸ“ All outputs saved to: {output_dir}\")\n",
    "    print(f\"\\nFiles created:\")\n",
    "    print(f\"  â€¢ model_fold0.pt - Fold 0 model checkpoint\")\n",
    "    print(f\"  â€¢ model_fold1.pt - Fold 1 model checkpoint\")\n",
    "    print(f\"  â€¢ model_fold2.pt - Fold 2 model checkpoint\")\n",
    "    print(f\"  â€¢ ensemble_history.pkl - Training history\")\n",
    "    print(f\"  â€¢ training_history.png - Training curves\")\n",
    "    print(f\"  â€¢ fold*_results.pkl - Evaluation results\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
