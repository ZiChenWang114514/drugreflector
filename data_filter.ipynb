{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f889a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ”¬ LINCS 2020 Unified Filter Pipeline\n",
      "================================================================================\n",
      "Data directory: D:\\ç§‘ç ”\\Models\\drugreflector\\datasets\\LINCS2020\n",
      "Cache directory: D:\\ç§‘ç ”\\Models\\drugreflector\\datasets\\LINCS2020\\filter_cache\n",
      "Overwrite mode: ON\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ğŸš€ Starting Full Filter Pipeline\n",
      "================================================================================\n",
      "\n",
      "Step 1: Loading metadata...\n",
      "ğŸ“– Loading gene information...\n",
      "   âœ“ Loaded 12,328 genes\n",
      "   âœ“ Landmark genes: 978\n",
      "ğŸ“– Loading cell information...\n",
      "   âœ“ Loaded 240 cell lines\n",
      "ğŸ“– Loading compound information...\n",
      "   âœ“ Loaded 39,321 compounds\n",
      "\n",
      "Step 2: Loading Level 4 signatures...\n",
      "\n",
      "================================================================================\n",
      "ğŸ“– Loading Level 4 Signatures\n",
      "================================================================================\n",
      "ğŸ“– Reading GCTX file: level4_beta_trt_cp_n1805898x12328.gctx\n",
      "   File size: 82.94 GB\n",
      "   Matrix shape: (1805898, 12328)\n",
      "   âš ï¸  Detected ROW/COL swap, correcting...\n",
      "   Using 978 landmark genes\n",
      "   Loading data...\n",
      "   âœ“ Loaded matrix: (1805898, 978)\n",
      "   âœ“ Memory: 6.58 GB\n",
      "ğŸ“– Loading instance information...\n",
      "   âœ“ Loaded 3,026,460 instances\n",
      "\n",
      "Initial dataset:\n",
      "  Samples: 1,805,898\n",
      "  Compounds: 34,419\n",
      "\n",
      "Step 3: Applying filters...\n",
      "\n",
      "================================================================================\n",
      "FILTER 1: DOS Removal\n",
      "================================================================================\n",
      "   âš ï¸  Overwrite mode enabled, ignoring existing checkpoint\n",
      "   pert_type distribution:\n",
      "     - trt_cp: 1,805,898\n",
      "   âœ“ Removed 0 non-trt_cp samples\n",
      "   Remaining: 1,805,898 samples, 34,419 compounds\n",
      "   ğŸ—‘ï¸  Removing old checkpoint: filter_1_dos_removal__remove_dos=True__8bb50fb2.pkl\n",
      "   ğŸ’¾ Saving checkpoint: filter_1_dos_removal__remove_dos=True__8bb50fb2.pkl\n",
      "   âœ“ Saved successfully (7132.9 MB)\n",
      "\n",
      "================================================================================\n",
      "FILTER 2: Minimum Observations (â‰¥5)\n",
      "================================================================================\n",
      "   âš ï¸  Overwrite mode enabled, ignoring existing checkpoint\n",
      "   Compounds with â‰¥5 obs: 22,731/34,419\n",
      "   Remaining: 1,777,129 samples, 22,731 compounds\n",
      "   ğŸ—‘ï¸  Removing old checkpoint: filter_2_min_observations__min_observations=5__96926616.pkl\n",
      "   ğŸ’¾ Saving checkpoint: filter_2_min_observations__min_observations=5__96926616.pkl\n",
      "   âœ“ Saved successfully (7018.4 MB)\n",
      "\n",
      "================================================================================\n",
      "FILTER 3: Cosine Similarity (â‰¥0.12)\n",
      "================================================================================\n",
      "   âš ï¸  Overwrite mode enabled, ignoring existing checkpoint\n",
      "   Calculating cosine similarities...\n",
      "   Computing:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 20419/22731 [13:33<01:36, 24.01it/s]  "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "LINCS 2020 ç»Ÿä¸€æ•°æ®è¿‡æ»¤ç®¡é“\n",
    "æ”¯æŒcheckpointç¼“å­˜ï¼Œé¿å…é‡å¤è®¡ç®—\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import gzip\n",
    "import shutil\n",
    "import glob\n",
    "import json\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "from scipy.spatial.distance import cosine, pdist, squareform\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from numba import jit, prange\n",
    "from sklearn.preprocessing import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, Tuple, Optional, List, Any\n",
    "import warnings\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# RDKit imports\n",
    "try:\n",
    "    from rdkit import Chem\n",
    "    from rdkit.Chem import Descriptors, AllChem, FilterCatalog\n",
    "    from rdkit.Chem.FilterCatalog import FilterCatalogParams\n",
    "    RDKIT_AVAILABLE = True\n",
    "    # ç¦ç”¨RDKitçš„è­¦å‘Šè¾“å‡º\n",
    "    from rdkit import RDLogger\n",
    "    RDLogger.DisableLog('rdApp.*')\n",
    "except ImportError:\n",
    "    print(\"âš ï¸  RDKit not installed. Chemical filters will be unavailable.\")\n",
    "    RDKIT_AVAILABLE = False\n",
    "\n",
    "\n",
    "class FilterCheckpointManager:\n",
    "    \"\"\"\n",
    "    è¿‡æ»¤å™¨checkpointç®¡ç†å™¨\n",
    "    è´Ÿè´£ä¿å­˜å’ŒåŠ è½½æ¯ä¸ªè¿‡æ»¤æ­¥éª¤çš„ä¸­é—´ç»“æœ\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cache_dir: str, overwrite: bool = True):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–checkpointç®¡ç†å™¨\n",
    "        \n",
    "        å‚æ•°:\n",
    "            cache_dir: checkpointç¼“å­˜ç›®å½•\n",
    "            overwrite: æ˜¯å¦è¦†ç›–å·²æœ‰çš„checkpointï¼ˆé»˜è®¤Trueï¼‰\n",
    "        \"\"\"\n",
    "        self.cache_dir = Path(cache_dir)\n",
    "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.overwrite = overwrite\n",
    "        self.metadata_file = self.cache_dir / \"filter_metadata.json\"\n",
    "        self.metadata = self._load_metadata()\n",
    "        \n",
    "        # é…ç½®æ—¥å¿—\n",
    "        self.logger = self._setup_logger()\n",
    "        \n",
    "    def _setup_logger(self):\n",
    "        \"\"\"é…ç½®æ—¥å¿—ç³»ç»Ÿ\"\"\"\n",
    "        logger = logging.getLogger('FilterPipeline')\n",
    "        logger.setLevel(logging.INFO)\n",
    "        \n",
    "        # æ¸…é™¤å·²æœ‰çš„handlers\n",
    "        logger.handlers = []\n",
    "        \n",
    "        # æ§åˆ¶å°handler\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setLevel(logging.INFO)\n",
    "        formatter = logging.Formatter('%(message)s')\n",
    "        console_handler.setFormatter(formatter)\n",
    "        logger.addHandler(console_handler)\n",
    "        \n",
    "        # æ–‡ä»¶handler\n",
    "        log_file = self.cache_dir / \"filter_pipeline.log\"\n",
    "        file_handler = logging.FileHandler(log_file, mode='a', encoding='utf-8')\n",
    "        file_handler.setLevel(logging.DEBUG)\n",
    "        detailed_formatter = logging.Formatter(\n",
    "            '[%(asctime)s] %(levelname)s: %(message)s',\n",
    "            datefmt='%Y-%m-%d %H:%M:%S'\n",
    "        )\n",
    "        file_handler.setFormatter(detailed_formatter)\n",
    "        logger.addHandler(file_handler)\n",
    "        \n",
    "        return logger\n",
    "    \n",
    "    def _load_metadata(self) -> Dict:\n",
    "        \"\"\"åŠ è½½å…ƒæ•°æ®\"\"\"\n",
    "        if self.metadata_file.exists():\n",
    "            try:\n",
    "                with open(self.metadata_file, 'r', encoding='utf-8') as f:\n",
    "                    return json.load(f)\n",
    "            except (json.JSONDecodeError, ValueError) as e:\n",
    "                # JSONæ–‡ä»¶æŸåï¼Œå¤‡ä»½ååˆ›å»ºæ–°çš„\n",
    "                backup_file = self.metadata_file.with_suffix('.json.backup')\n",
    "                try:\n",
    "                    shutil.copy(self.metadata_file, backup_file)\n",
    "                    print(f\"âš ï¸  Metadata file corrupted, backed up to: {backup_file.name}\")\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                # åˆ é™¤æŸåçš„æ–‡ä»¶\n",
    "                try:\n",
    "                    self.metadata_file.unlink()\n",
    "                    print(f\"ğŸ—‘ï¸  Removed corrupted metadata file\")\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                print(f\"âœ“ Starting with fresh metadata\")\n",
    "                return {}\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸  Error loading metadata: {e}\")\n",
    "                return {}\n",
    "        return {}   \n",
    "    \n",
    "    def _convert_to_json_serializable(self, obj):\n",
    "        \"\"\"é€’å½’è½¬æ¢å¯¹è±¡ä¸º JSON å¯åºåˆ—åŒ–ç±»å‹\"\"\"\n",
    "        if isinstance(obj, dict):\n",
    "            return {k: self._convert_to_json_serializable(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, (list, tuple)):\n",
    "            return [self._convert_to_json_serializable(item) for item in obj]\n",
    "        elif isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif pd.isna(obj):\n",
    "            return None\n",
    "        else:\n",
    "            return obj\n",
    "\n",
    "    def _save_metadata(self):\n",
    "        \"\"\"ä¿å­˜å…ƒæ•°æ®\"\"\"\n",
    "        # è½¬æ¢åå†ä¿å­˜\n",
    "        serializable_metadata = self._convert_to_json_serializable(self.metadata)\n",
    "        with open(self.metadata_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(serializable_metadata, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    def _get_checkpoint_path(self, filter_name: str, filter_params: Dict) -> Path:\n",
    "        \"\"\"\n",
    "        ç”Ÿæˆcheckpointæ–‡ä»¶è·¯å¾„\n",
    "        \n",
    "        å‚æ•°:\n",
    "            filter_name: è¿‡æ»¤å™¨åç§°\n",
    "            filter_params: è¿‡æ»¤å™¨å‚æ•°ï¼ˆç”¨äºç”Ÿæˆå”¯ä¸€æ–‡ä»¶åï¼‰\n",
    "        \n",
    "        è¿”å›:\n",
    "            checkpointæ–‡ä»¶è·¯å¾„\n",
    "        \"\"\"\n",
    "        # ç”Ÿæˆå‚æ•°hash\n",
    "        params_str = json.dumps(filter_params, sort_keys=True)\n",
    "        params_hash = hashlib.md5(params_str.encode()).hexdigest()[:8]\n",
    "        \n",
    "        # ç”Ÿæˆå¯è¯»çš„å‚æ•°å­—ç¬¦ä¸²\n",
    "        param_parts = []\n",
    "        for k, v in sorted(filter_params.items()):\n",
    "            if isinstance(v, (list, tuple)):\n",
    "                v_str = '_'.join(map(str, v))\n",
    "            else:\n",
    "                v_str = str(v)\n",
    "            param_parts.append(f\"{k}={v_str}\")\n",
    "        \n",
    "        params_readable = '__'.join(param_parts)\n",
    "        # é™åˆ¶æ–‡ä»¶åé•¿åº¦\n",
    "        if len(params_readable) > 100:\n",
    "            params_readable = params_readable[:100]\n",
    "        \n",
    "        filename = f\"{filter_name}__{params_readable}__{params_hash}.pkl\"\n",
    "        return self.cache_dir / filename\n",
    "    \n",
    "    def has_checkpoint(self, filter_name: str, filter_params: Dict) -> bool:\n",
    "        \"\"\"æ£€æŸ¥æ˜¯å¦å­˜åœ¨checkpoint\"\"\"\n",
    "        ckpt_path = self._get_checkpoint_path(filter_name, filter_params)\n",
    "        return ckpt_path.exists() and not self.overwrite\n",
    "    \n",
    "    def load_checkpoint(self, filter_name: str, filter_params: Dict) -> Optional[Dict]:\n",
    "        \"\"\"\n",
    "        åŠ è½½checkpoint\n",
    "        \n",
    "        è¿”å›:\n",
    "            å¦‚æœæˆåŠŸåŠ è½½è¿”å›æ•°æ®å­—å…¸ï¼Œå¦åˆ™è¿”å›None\n",
    "        \"\"\"\n",
    "        ckpt_path = self._get_checkpoint_path(filter_name, filter_params)\n",
    "        \n",
    "        if not ckpt_path.exists():\n",
    "            return None\n",
    "        \n",
    "        if self.overwrite:\n",
    "            self.logger.info(f\"   âš ï¸  Overwrite mode enabled, ignoring existing checkpoint\")\n",
    "            return None\n",
    "        \n",
    "        self.logger.info(f\"   âœ“ Loading checkpoint: {ckpt_path.name}\")\n",
    "        \n",
    "        try:\n",
    "            with open(ckpt_path, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "            \n",
    "            # éªŒè¯æ•°æ®å®Œæ•´æ€§\n",
    "            if 'matrix' not in data or 'metadata' not in data:\n",
    "                self.logger.warning(f\"   âš ï¸  Checkpoint corrupted, will recompute\")\n",
    "                return None\n",
    "            \n",
    "            self.logger.info(f\"   âœ“ Loaded: {len(data['matrix']):,} samples\")\n",
    "            return data\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"   âŒ Error loading checkpoint: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def save_checkpoint(\n",
    "        self, \n",
    "        filter_name: str, \n",
    "        filter_params: Dict, \n",
    "        data: Dict,\n",
    "        filter_stats: Dict\n",
    "    ):\n",
    "        \"\"\"\n",
    "        ä¿å­˜checkpoint\n",
    "        \n",
    "        å‚æ•°:\n",
    "            filter_name: è¿‡æ»¤å™¨åç§°\n",
    "            filter_params: è¿‡æ»¤å™¨å‚æ•°\n",
    "            data: æ•°æ®å­—å…¸ï¼ˆåŒ…å«matrixå’Œmetadataï¼‰\n",
    "            filter_stats: è¿‡æ»¤ç»Ÿè®¡ä¿¡æ¯\n",
    "        \"\"\"\n",
    "        ckpt_path = self._get_checkpoint_path(filter_name, filter_params)\n",
    "        \n",
    "        # å¦‚æœoverwriteæ¨¡å¼å¼€å¯ï¼Œåˆ é™¤æ—§çš„checkpoint\n",
    "        if self.overwrite and ckpt_path.exists():\n",
    "            self.logger.info(f\"   ğŸ—‘ï¸  Removing old checkpoint: {ckpt_path.name}\")\n",
    "            ckpt_path.unlink()\n",
    "        \n",
    "        self.logger.info(f\"   ğŸ’¾ Saving checkpoint: {ckpt_path.name}\")\n",
    "        \n",
    "        # ä¿å­˜æ•°æ®\n",
    "        with open(ckpt_path, 'wb') as f:\n",
    "            pickle.dump(data, f, protocol=4)\n",
    "        \n",
    "        file_size_mb = ckpt_path.stat().st_size / (1024**2)\n",
    "        self.logger.info(f\"   âœ“ Saved successfully ({file_size_mb:.1f} MB)\")\n",
    "        \n",
    "        # æ›´æ–°å…ƒæ•°æ®\n",
    "        filter_id = f\"{filter_name}__{hashlib.md5(json.dumps(filter_params, sort_keys=True).encode()).hexdigest()[:8]}\"\n",
    "        \n",
    "        self.metadata[filter_id] = {\n",
    "            'filter_name': filter_name,\n",
    "            'filter_params': filter_params,\n",
    "            'checkpoint_path': str(ckpt_path),\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'stats': filter_stats,\n",
    "            'n_samples': len(data['matrix']),\n",
    "            'n_compounds': data['metadata']['pert_id'].nunique() if 'pert_id' in data['metadata'].columns else 0\n",
    "        }\n",
    "        \n",
    "        self._save_metadata()\n",
    "    \n",
    "    def clear_cache(self, filter_name: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        æ¸…é™¤ç¼“å­˜\n",
    "        \n",
    "        å‚æ•°:\n",
    "            filter_name: å¦‚æœæŒ‡å®šï¼Œåªæ¸…é™¤è¯¥è¿‡æ»¤å™¨çš„ç¼“å­˜ï¼›å¦åˆ™æ¸…é™¤æ‰€æœ‰\n",
    "        \"\"\"\n",
    "        if filter_name is None:\n",
    "            # æ¸…é™¤æ‰€æœ‰checkpoint\n",
    "            for ckpt_file in self.cache_dir.glob(\"*.pkl\"):\n",
    "                ckpt_file.unlink()\n",
    "            self.metadata = {}\n",
    "            self._save_metadata()\n",
    "            self.logger.info(f\"   âœ“ Cleared all checkpoints\")\n",
    "        else:\n",
    "            # æ¸…é™¤æŒ‡å®šè¿‡æ»¤å™¨çš„checkpoint\n",
    "            pattern = f\"{filter_name}__*.pkl\"\n",
    "            removed_count = 0\n",
    "            for ckpt_file in self.cache_dir.glob(pattern):\n",
    "                ckpt_file.unlink()\n",
    "                removed_count += 1\n",
    "            \n",
    "            # æ›´æ–°å…ƒæ•°æ®\n",
    "            keys_to_remove = [k for k in self.metadata if self.metadata[k]['filter_name'] == filter_name]\n",
    "            for key in keys_to_remove:\n",
    "                del self.metadata[key]\n",
    "            self._save_metadata()\n",
    "            \n",
    "            self.logger.info(f\"   âœ“ Cleared {removed_count} checkpoints for filter '{filter_name}'\")\n",
    "    \n",
    "    def get_filter_history(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        è·å–è¿‡æ»¤å†å²è®°å½•\n",
    "        \n",
    "        è¿”å›:\n",
    "            DataFrameæ ¼å¼çš„è¿‡æ»¤å†å²\n",
    "        \"\"\"\n",
    "        if not self.metadata:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        records = []\n",
    "        for filter_id, info in self.metadata.items():\n",
    "            record = {\n",
    "                'filter_name': info['filter_name'],\n",
    "                'timestamp': info['timestamp'],\n",
    "                'n_samples': info['n_samples'],\n",
    "                'n_compounds': info['n_compounds'],\n",
    "                'params': str(info['filter_params'])\n",
    "            }\n",
    "            records.append(record)\n",
    "        \n",
    "        df = pd.DataFrame(records)\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        return df.sort_values('timestamp', ascending=False)\n",
    "\n",
    "\n",
    "class LINCS2020FilterPipeline:\n",
    "    \"\"\"\n",
    "    LINCS 2020ç»Ÿä¸€è¿‡æ»¤ç®¡é“\n",
    "    æ•´åˆæ‰€æœ‰æ•°æ®è´¨é‡æ§åˆ¶å’ŒåŒ–å­¦è¿‡æ»¤æ­¥éª¤\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        data_dir: str,\n",
    "        cache_dir: Optional[str] = None,\n",
    "        overwrite: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–è¿‡æ»¤ç®¡é“\n",
    "        \n",
    "        å‚æ•°:\n",
    "            data_dir: LINCS 2020æ•°æ®ç›®å½•\n",
    "            cache_dir: checkpointç¼“å­˜ç›®å½•ï¼ˆé»˜è®¤ä¸ºdata_dir/filter_cacheï¼‰\n",
    "            overwrite: æ˜¯å¦è¦†ç›–å·²æœ‰checkpointï¼ˆé»˜è®¤Trueï¼‰\n",
    "        \"\"\"\n",
    "        self.data_dir = Path(data_dir)\n",
    "        \n",
    "        if cache_dir is None:\n",
    "            cache_dir = self.data_dir / \"filter_cache\"\n",
    "        \n",
    "        self.checkpoint_mgr = FilterCheckpointManager(cache_dir, overwrite=overwrite)\n",
    "        self.logger = self.checkpoint_mgr.logger\n",
    "        \n",
    "        # æ•°æ®å­˜å‚¨\n",
    "        self.gene_info = None\n",
    "        self.cell_info = None\n",
    "        self.compound_info = None\n",
    "        self.inst_info = None\n",
    "        self.signatures = None\n",
    "        self.landmark_col_indices = None\n",
    "        self.landmark_gene_ids = None\n",
    "        self.landmark_gene_symbols = None\n",
    "        \n",
    "        # åŒ–å­¦è¿‡æ»¤å™¨\n",
    "        self.covalent_smarts = None\n",
    "        self.brenk_catalog = None\n",
    "        self.nibr_catalog = None\n",
    "        \n",
    "        self._print_header()\n",
    "    \n",
    "    def _print_header(self):\n",
    "        \"\"\"æ‰“å°æ¬¢è¿ä¿¡æ¯\"\"\"\n",
    "        self.logger.info(f\"{'='*80}\")\n",
    "        self.logger.info(f\"ğŸ”¬ LINCS 2020 Unified Filter Pipeline\")\n",
    "        self.logger.info(f\"{'='*80}\")\n",
    "        self.logger.info(f\"Data directory: {self.data_dir}\")\n",
    "        self.logger.info(f\"Cache directory: {self.checkpoint_mgr.cache_dir}\")\n",
    "        self.logger.info(f\"Overwrite mode: {'ON' if self.checkpoint_mgr.overwrite else 'OFF'}\")\n",
    "        self.logger.info(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # ========== æ•°æ®åŠ è½½æ–¹æ³• ==========\n",
    "    \n",
    "    def load_gene_info(self):\n",
    "        \"\"\"åŠ è½½åŸºå› ä¿¡æ¯\"\"\"\n",
    "        gene_file = self.data_dir / \"geneinfo_beta.txt\"\n",
    "        \n",
    "        if not gene_file.exists():\n",
    "            raise FileNotFoundError(f\"Gene info file not found: {gene_file}\")\n",
    "        \n",
    "        self.logger.info(f\"ğŸ“– Loading gene information...\")\n",
    "        gene_info = pd.read_csv(gene_file, sep='\\t')\n",
    "        self.logger.info(f\"   âœ“ Loaded {len(gene_info):,} genes\")\n",
    "        \n",
    "        if 'feature_space' in gene_info.columns:\n",
    "            landmark_mask = gene_info['feature_space'] == 'landmark'\n",
    "            landmark_genes = gene_info[landmark_mask].copy()\n",
    "            self.landmark_col_indices = np.where(landmark_mask.values)[0]\n",
    "            self.landmark_gene_ids = set(landmark_genes['gene_id'].astype(str).values)\n",
    "            self.landmark_gene_symbols = set(landmark_genes['gene_symbol'].astype(str).values)\n",
    "            self.logger.info(f\"   âœ“ Landmark genes: {len(landmark_genes):,}\")\n",
    "        else:\n",
    "            raise ValueError(\"Cannot identify landmark genes. 'feature_space' column not found.\")\n",
    "        \n",
    "        self.gene_info = gene_info\n",
    "        return gene_info\n",
    "    \n",
    "    def load_cell_info(self):\n",
    "        \"\"\"åŠ è½½ç»†èƒç³»ä¿¡æ¯\"\"\"\n",
    "        cell_file = self.data_dir / \"cellinfo_beta.txt\"\n",
    "        \n",
    "        if not cell_file.exists():\n",
    "            raise FileNotFoundError(f\"Cell info file not found: {cell_file}\")\n",
    "        \n",
    "        self.logger.info(f\"ğŸ“– Loading cell information...\")\n",
    "        cell_info = pd.read_csv(cell_file, sep='\\t')\n",
    "        self.logger.info(f\"   âœ“ Loaded {len(cell_info):,} cell lines\")\n",
    "        \n",
    "        self.cell_info = cell_info\n",
    "        return cell_info\n",
    "    \n",
    "    def load_compound_info(self):\n",
    "        \"\"\"åŠ è½½åŒ–åˆç‰©ä¿¡æ¯\"\"\"\n",
    "        compound_file = self.data_dir / \"compoundinfo_beta.txt\"\n",
    "        \n",
    "        if not compound_file.exists():\n",
    "            self.logger.warning(f\"âš ï¸  Compound info file not found: {compound_file}\")\n",
    "            return None\n",
    "        \n",
    "        self.logger.info(f\"ğŸ“– Loading compound information...\")\n",
    "        compound_info = pd.read_csv(compound_file, sep='\\t')\n",
    "        self.logger.info(f\"   âœ“ Loaded {len(compound_info):,} compounds\")\n",
    "        \n",
    "        self.compound_info = compound_info\n",
    "        return compound_info\n",
    "    \n",
    "    def load_instance_info(self):\n",
    "        \"\"\"åŠ è½½å®ä¾‹ä¿¡æ¯\"\"\"\n",
    "        inst_file = self.data_dir / \"instinfo_beta.txt\"\n",
    "        \n",
    "        if not inst_file.exists():\n",
    "            raise FileNotFoundError(f\"Instance info file not found: {inst_file}\")\n",
    "        \n",
    "        self.logger.info(f\"ğŸ“– Loading instance information...\")\n",
    "        inst_info = pd.read_csv(inst_file, sep='\\t')\n",
    "        self.logger.info(f\"   âœ“ Loaded {len(inst_info):,} instances\")\n",
    "        \n",
    "        if 'inst_id' in inst_info.columns:\n",
    "            self.instance_join_col = 'inst_id'\n",
    "        elif 'sample_id' in inst_info.columns:\n",
    "            self.instance_join_col = 'sample_id'\n",
    "        else:\n",
    "            raise ValueError(\"Neither 'inst_id' nor 'sample_id' found in instinfo\")\n",
    "        \n",
    "        self.inst_info = inst_info\n",
    "        return inst_info\n",
    "    \n",
    "    def decompress_gctx_file(self, gctx_file):\n",
    "        \"\"\"è§£å‹GCTXæ–‡ä»¶\"\"\"\n",
    "        gctx_file = Path(gctx_file)\n",
    "        \n",
    "        if not gctx_file.exists():\n",
    "            raise FileNotFoundError(f\"GCTX file not found: {gctx_file}\")\n",
    "        \n",
    "        if str(gctx_file).endswith('.gz'):\n",
    "            self.logger.info(f\"âš ï¸  Detected compressed GCTX file, decompressing...\")\n",
    "            \n",
    "            decompressed_dir = self.data_dir / \"_decompressed\"\n",
    "            decompressed_dir.mkdir(exist_ok=True)\n",
    "            \n",
    "            output_path = decompressed_dir / gctx_file.stem\n",
    "            \n",
    "            if output_path.exists():\n",
    "                self.logger.info(f\"âœ“ Found existing decompressed file\")\n",
    "                return str(output_path)\n",
    "            \n",
    "            with gzip.open(gctx_file, 'rb') as f_in:\n",
    "                with open(output_path, 'wb') as f_out:\n",
    "                    shutil.copyfileobj(f_in, f_out, length=8*1024*1024)\n",
    "            \n",
    "            self.logger.info(f\"âœ“ Decompressed to: {output_path}\")\n",
    "            return str(output_path)\n",
    "        \n",
    "        return str(gctx_file)\n",
    "    \n",
    "    def read_gctx(self, gctx_file, use_landmark_only=True):\n",
    "        \"\"\"è¯»å–GCTXæ–‡ä»¶\"\"\"\n",
    "        gctx_file = str(gctx_file)\n",
    "        self.logger.info(f\"ğŸ“– Reading GCTX file: {Path(gctx_file).name}\")\n",
    "        \n",
    "        file_size_gb = Path(gctx_file).stat().st_size / (1024**3)\n",
    "        self.logger.info(f\"   File size: {file_size_gb:.2f} GB\")\n",
    "        \n",
    "        gctx_file = self.decompress_gctx_file(gctx_file)\n",
    "        \n",
    "        with h5py.File(gctx_file, 'r') as f:\n",
    "            # ç¡®å®šè·¯å¾„\n",
    "            if '/0/DATA/0/matrix' in f:\n",
    "                matrix_dataset = f['/0/DATA/0/matrix']\n",
    "                row_path = '/0/META/ROW'\n",
    "                col_path = '/0/META/COL'\n",
    "            elif '/matrix' in f:\n",
    "                matrix_dataset = f['/matrix']\n",
    "                row_path = '/row'\n",
    "                col_path = '/col'\n",
    "            else:\n",
    "                raise ValueError(\"Cannot find matrix in GCTX file\")\n",
    "            \n",
    "            matrix_shape = matrix_dataset.shape\n",
    "            self.logger.info(f\"   Matrix shape: {matrix_shape}\")\n",
    "            \n",
    "            # è¯»å–å…ƒæ•°æ®\n",
    "            sample_meta = {}\n",
    "            for key in f[row_path].keys():\n",
    "                data = f[f'{row_path}/{key}'][:]\n",
    "                if data.dtype.kind in ['S', 'O', 'U']:\n",
    "                    try:\n",
    "                        sample_meta[key] = np.char.decode(data.astype('S'), 'utf-8')\n",
    "                    except:\n",
    "                        sample_meta[key] = data.astype(str)\n",
    "                else:\n",
    "                    sample_meta[key] = data\n",
    "            \n",
    "            gene_meta = {}\n",
    "            for key in f[col_path].keys():\n",
    "                data = f[f'{col_path}/{key}'][:]\n",
    "                if data.dtype.kind in ['S', 'O', 'U']:\n",
    "                    try:\n",
    "                        gene_meta[key] = np.char.decode(data.astype('S'), 'utf-8')\n",
    "                    except:\n",
    "                        gene_meta[key] = data.astype(str)\n",
    "                else:\n",
    "                    gene_meta[key] = data\n",
    "            \n",
    "            sample_df = pd.DataFrame(sample_meta)\n",
    "            gene_df = pd.DataFrame(gene_meta)\n",
    "            \n",
    "            # è‡ªåŠ¨æ£€æµ‹å¹¶äº¤æ¢ROW/COL\n",
    "            if self.gene_info is not None:\n",
    "                n_features_expected = len(self.gene_info)\n",
    "                if len(sample_df) == n_features_expected and len(gene_df) != n_features_expected:\n",
    "                    self.logger.info(f\"   âš ï¸  Detected ROW/COL swap, correcting...\")\n",
    "                    sample_df, gene_df = gene_df, sample_df\n",
    "            \n",
    "            # ç¡®å®šlandmarkåŸºå› çš„åˆ—ç´¢å¼•\n",
    "            landmark_col_indices = None\n",
    "            if use_landmark_only:\n",
    "                if self.gene_info is None or not hasattr(self, \"landmark_col_indices\"):\n",
    "                    self.load_gene_info()\n",
    "                \n",
    "                landmark_col_indices = np.array(self.landmark_col_indices, dtype=int)\n",
    "                self.logger.info(f\"   Using {len(landmark_col_indices)} landmark genes\")\n",
    "            \n",
    "            # åˆ†å—è¯»å–\n",
    "            self.logger.info(f\"   Loading data...\")\n",
    "            if landmark_col_indices is not None:\n",
    "                chunk_size = 610000\n",
    "                chunks = []\n",
    "                \n",
    "                for start_idx in range(0, matrix_shape[0], chunk_size):\n",
    "                    end_idx = min(start_idx + chunk_size, matrix_shape[0])\n",
    "                    chunk = matrix_dataset[start_idx:end_idx, landmark_col_indices].astype(np.float32)\n",
    "                    chunks.append(chunk)\n",
    "                    \n",
    "                    if len(chunks) >= 10:\n",
    "                        merged = np.vstack(chunks)\n",
    "                        chunks = [merged]\n",
    "                        gc.collect()\n",
    "                \n",
    "                matrix = np.vstack(chunks) if len(chunks) > 1 else chunks[0]\n",
    "                del chunks\n",
    "                gc.collect()\n",
    "            else:\n",
    "                matrix = matrix_dataset[:].astype(np.float32)\n",
    "            \n",
    "            self.logger.info(f\"   âœ“ Loaded matrix: {matrix.shape}\")\n",
    "            self.logger.info(f\"   âœ“ Memory: {matrix.nbytes / (1024**3):.2f} GB\")\n",
    "        \n",
    "        return matrix, sample_df, gene_df\n",
    "    \n",
    "    def load_level4_signatures(self):\n",
    "        \"\"\"åŠ è½½Level 4æ•°æ®\"\"\"\n",
    "        level4_file = self.data_dir / \"level4_beta_trt_cp_n1805898x12328.gctx\"\n",
    "        \n",
    "        if not level4_file.exists():\n",
    "            pattern = self.data_dir / \"level4_beta_trt_cp*.gctx\"\n",
    "            files = glob.glob(str(pattern))\n",
    "            if not files:\n",
    "                raise FileNotFoundError(f\"Level 4 file not found: {level4_file}\")\n",
    "            level4_file = files[0]\n",
    "        \n",
    "        self.logger.info(f\"\\n{'='*80}\")\n",
    "        self.logger.info(f\"ğŸ“– Loading Level 4 Signatures\")\n",
    "        self.logger.info(f\"{'='*80}\")\n",
    "        \n",
    "        matrix, sample_meta, gene_meta = self.read_gctx(level4_file, use_landmark_only=True)\n",
    "        \n",
    "        # åˆå¹¶å…ƒæ•°æ®\n",
    "        if 'id' in sample_meta.columns:\n",
    "            sample_meta = sample_meta.rename(columns={'id': 'sample_id'})\n",
    "        elif 'sample_id' not in sample_meta.columns:\n",
    "            raise ValueError(\"Cannot find 'id' or 'sample_id' in GCTX metadata\")\n",
    "        \n",
    "        if self.inst_info is None:\n",
    "            self.load_instance_info()\n",
    "        \n",
    "        join_col = getattr(self, \"instance_join_col\", 'sample_id')\n",
    "        merged_meta = sample_meta.merge(self.inst_info, on=join_col, how='left')\n",
    "        \n",
    "        if 'pert_id' not in merged_meta.columns:\n",
    "            raise ValueError(\"'pert_id' missing after merge\")\n",
    "        \n",
    "        self.signatures = {\n",
    "            'matrix': matrix,\n",
    "            'row_meta': merged_meta,\n",
    "            'col_meta': gene_meta\n",
    "        }\n",
    "        \n",
    "        return matrix, merged_meta, gene_meta\n",
    "    \n",
    "    # ========== è¾…åŠ©æ–¹æ³• ==========\n",
    "    @staticmethod\n",
    "    @jit(nopython=True, parallel=True, fastmath=True)\n",
    "    def _compute_max_similarities_jit(data, indices, n_total):\n",
    "        \"\"\"NumbaåŠ é€Ÿçš„ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—\"\"\"\n",
    "        n = len(indices)\n",
    "        result = np.full(n_total, -np.inf, dtype=np.float32)\n",
    "        \n",
    "        for i in prange(n):\n",
    "            idx_i = indices[i]\n",
    "            vec_i = data[i]\n",
    "            max_sim = -np.inf\n",
    "            \n",
    "            for j in range(n):\n",
    "                if i != j:\n",
    "                    sim = np.dot(vec_i, data[j])\n",
    "                    if sim > max_sim:\n",
    "                        max_sim = sim\n",
    "            \n",
    "            result[idx_i] = max_sim\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def calculate_cosine_similarity(self, matrix: np.ndarray, pert_ids: pd.Series) -> np.ndarray:\n",
    "        \"\"\"è®¡ç®—æ¯ä¸ªæ ·æœ¬ä¸å…¶æœ€è¿‘åŒåŒ–åˆç‰©å¤åˆ¶å“çš„ä½™å¼¦ç›¸ä¼¼åº¦\"\"\"\n",
    "        self.logger.info(f\"   Calculating cosine similarities...\")\n",
    "        \n",
    "        matrix_norm = normalize(matrix, norm='l2', axis=1).astype(np.float32)\n",
    "        nearest_similarities = np.zeros(len(pert_ids), dtype=np.float32)\n",
    "        \n",
    "        pert_ids_array = pert_ids.values\n",
    "        unique_perts = np.unique(pert_ids_array)\n",
    "        \n",
    "        for pert_id in tqdm(unique_perts, desc=\"   Computing\", leave=False):\n",
    "            mask = pert_ids_array == pert_id\n",
    "            indices = np.where(mask)[0]\n",
    "            \n",
    "            if len(indices) < 2:\n",
    "                nearest_similarities[indices] = 0.0\n",
    "                continue\n",
    "            \n",
    "            pert_data = matrix_norm[mask]\n",
    "            sims = self._compute_max_similarities_jit(pert_data, indices, len(pert_ids))\n",
    "            nearest_similarities[indices] = sims[indices]\n",
    "        \n",
    "        self.logger.info(f\"   âœ“ Mean similarity: {nearest_similarities.mean():.4f}\")\n",
    "        return nearest_similarities\n",
    "    \n",
    "    # ========== è¿‡æ»¤å™¨æ–¹æ³• ==========\n",
    "    \n",
    "    def filter_1_dos_removal(\n",
    "        self,\n",
    "        matrix: np.ndarray,\n",
    "        metadata: pd.DataFrame,\n",
    "        remove_dos: bool = True\n",
    "    ) -> Tuple[np.ndarray, pd.DataFrame, Dict]:\n",
    "        \"\"\"\n",
    "        Filter 1: ç§»é™¤DOSåŒ–åˆç‰©ï¼ˆåªä¿ç•™trt_cpï¼‰\n",
    "        \n",
    "        å‚æ•°:\n",
    "            matrix: è¡¨è¾¾çŸ©é˜µ\n",
    "            metadata: å…ƒæ•°æ®\n",
    "            remove_dos: æ˜¯å¦ç§»é™¤DOS\n",
    "        \n",
    "        è¿”å›:\n",
    "            (è¿‡æ»¤åçš„matrix, è¿‡æ»¤åçš„metadata, ç»Ÿè®¡ä¿¡æ¯)\n",
    "        \"\"\"\n",
    "        filter_name = \"filter_1_dos_removal\"\n",
    "        filter_params = {'remove_dos': remove_dos}\n",
    "        \n",
    "        self.logger.info(f\"\\n{'='*80}\")\n",
    "        self.logger.info(f\"FILTER 1: DOS Removal\")\n",
    "        self.logger.info(f\"{'='*80}\")\n",
    "        \n",
    "        # æ£€æŸ¥checkpoint\n",
    "        ckpt_data = self.checkpoint_mgr.load_checkpoint(filter_name, filter_params)\n",
    "        if ckpt_data is not None:\n",
    "            return ckpt_data['matrix'], ckpt_data['metadata'], ckpt_data['stats']\n",
    "        \n",
    "        # æ‰§è¡Œè¿‡æ»¤\n",
    "        initial_samples = len(metadata)\n",
    "        initial_compounds = metadata['pert_id'].nunique()\n",
    "        \n",
    "        if remove_dos and 'pert_type' in metadata.columns:\n",
    "            pert_type_counts = metadata['pert_type'].value_counts()\n",
    "            self.logger.info(f\"   pert_type distribution:\")\n",
    "            for ptype, count in pert_type_counts.items():\n",
    "                self.logger.info(f\"     - {ptype}: {count:,}\")\n",
    "            \n",
    "            dos_mask = metadata['pert_type'] == 'trt_cp'\n",
    "            n_removed = (~dos_mask).sum()\n",
    "            \n",
    "            matrix = matrix[dos_mask]\n",
    "            metadata = metadata[dos_mask].reset_index(drop=True)\n",
    "            \n",
    "            self.logger.info(f\"   âœ“ Removed {n_removed:,} non-trt_cp samples\")\n",
    "        else:\n",
    "            dos_mask = ~metadata['pert_id'].str.contains('DOS', case=False, na=False)\n",
    "            n_removed = (~dos_mask).sum()\n",
    "            \n",
    "            matrix = matrix[dos_mask]\n",
    "            metadata = metadata[dos_mask].reset_index(drop=True)\n",
    "            \n",
    "            self.logger.info(f\"   âœ“ Removed {n_removed:,} DOS samples\")\n",
    "        \n",
    "        final_samples = len(metadata)\n",
    "        final_compounds = metadata['pert_id'].nunique()\n",
    "        \n",
    "        self.logger.info(f\"   Remaining: {final_samples:,} samples, {final_compounds:,} compounds\")\n",
    "        \n",
    "        stats = {\n",
    "            'initial_samples': initial_samples,\n",
    "            'final_samples': final_samples,\n",
    "            'removed_samples': n_removed,\n",
    "            'initial_compounds': initial_compounds,\n",
    "            'final_compounds': final_compounds\n",
    "        }\n",
    "        \n",
    "        # ä¿å­˜checkpoint\n",
    "        data = {'matrix': matrix, 'metadata': metadata, 'stats': stats}\n",
    "        self.checkpoint_mgr.save_checkpoint(filter_name, filter_params, data, stats)\n",
    "        \n",
    "        return matrix, metadata, stats\n",
    "    \n",
    "    def filter_2_min_observations(\n",
    "        self,\n",
    "        matrix: np.ndarray,\n",
    "        metadata: pd.DataFrame,\n",
    "        min_observations: int = 5\n",
    "    ) -> Tuple[np.ndarray, pd.DataFrame, Dict]:\n",
    "        \"\"\"\n",
    "        Filter 2: ç§»é™¤è§‚æµ‹æ•°å°‘äºé˜ˆå€¼çš„åŒ–åˆç‰©\n",
    "        \n",
    "        å‚æ•°:\n",
    "            matrix: è¡¨è¾¾çŸ©é˜µ\n",
    "            metadata: å…ƒæ•°æ®\n",
    "            min_observations: æœ€å°è§‚æµ‹æ•°\n",
    "        \n",
    "        è¿”å›:\n",
    "            (è¿‡æ»¤åçš„matrix, è¿‡æ»¤åçš„metadata, ç»Ÿè®¡ä¿¡æ¯)\n",
    "        \"\"\"\n",
    "        filter_name = \"filter_2_min_observations\"\n",
    "        filter_params = {'min_observations': min_observations}\n",
    "        \n",
    "        self.logger.info(f\"\\n{'='*80}\")\n",
    "        self.logger.info(f\"FILTER 2: Minimum Observations (â‰¥{min_observations})\")\n",
    "        self.logger.info(f\"{'='*80}\")\n",
    "        \n",
    "        # æ£€æŸ¥checkpoint\n",
    "        ckpt_data = self.checkpoint_mgr.load_checkpoint(filter_name, filter_params)\n",
    "        if ckpt_data is not None:\n",
    "            return ckpt_data['matrix'], ckpt_data['metadata'], ckpt_data['stats']\n",
    "        \n",
    "        # æ‰§è¡Œè¿‡æ»¤\n",
    "        initial_samples = len(metadata)\n",
    "        initial_compounds = metadata['pert_id'].nunique()\n",
    "        \n",
    "        obs_counts = metadata.groupby('pert_id').size()\n",
    "        valid_perts = obs_counts[obs_counts >= min_observations].index\n",
    "        \n",
    "        self.logger.info(f\"   Compounds with â‰¥{min_observations} obs: {len(valid_perts):,}/{initial_compounds:,}\")\n",
    "        \n",
    "        obs_mask = metadata['pert_id'].isin(valid_perts)\n",
    "        \n",
    "        matrix = matrix[obs_mask]\n",
    "        metadata = metadata[obs_mask].reset_index(drop=True)\n",
    "        \n",
    "        final_samples = len(metadata)\n",
    "        final_compounds = metadata['pert_id'].nunique()\n",
    "        \n",
    "        self.logger.info(f\"   Remaining: {final_samples:,} samples, {final_compounds:,} compounds\")\n",
    "        \n",
    "        stats = {\n",
    "            'initial_samples': initial_samples,\n",
    "            'final_samples': final_samples,\n",
    "            'removed_samples': initial_samples - final_samples,\n",
    "            'initial_compounds': initial_compounds,\n",
    "            'final_compounds': final_compounds\n",
    "        }\n",
    "        \n",
    "        # ä¿å­˜checkpoint\n",
    "        data = {'matrix': matrix, 'metadata': metadata, 'stats': stats}\n",
    "        self.checkpoint_mgr.save_checkpoint(filter_name, filter_params, data, stats)\n",
    "        \n",
    "        gc.collect()\n",
    "        return matrix, metadata, stats\n",
    "    \n",
    "    def filter_3_cosine_similarity(\n",
    "        self,\n",
    "        matrix: np.ndarray,\n",
    "        metadata: pd.DataFrame,\n",
    "        min_similarity: float = 0.12\n",
    "    ) -> Tuple[np.ndarray, pd.DataFrame, Dict]:\n",
    "        \"\"\"\n",
    "        Filter 3: ä½™å¼¦ç›¸ä¼¼åº¦è¿‡æ»¤\n",
    "        \n",
    "        å‚æ•°:\n",
    "            matrix: è¡¨è¾¾çŸ©é˜µ\n",
    "            metadata: å…ƒæ•°æ®\n",
    "            min_similarity: æœ€å°ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "        \n",
    "        è¿”å›:\n",
    "            (è¿‡æ»¤åçš„matrix, è¿‡æ»¤åçš„metadata, ç»Ÿè®¡ä¿¡æ¯)\n",
    "        \"\"\"\n",
    "        filter_name = \"filter_3_cosine_similarity\"\n",
    "        filter_params = {'min_similarity': min_similarity}\n",
    "        \n",
    "        self.logger.info(f\"\\n{'='*80}\")\n",
    "        self.logger.info(f\"FILTER 3: Cosine Similarity (â‰¥{min_similarity})\")\n",
    "        self.logger.info(f\"{'='*80}\")\n",
    "        \n",
    "        # æ£€æŸ¥checkpoint\n",
    "        ckpt_data = self.checkpoint_mgr.load_checkpoint(filter_name, filter_params)\n",
    "        if ckpt_data is not None:\n",
    "            return ckpt_data['matrix'], ckpt_data['metadata'], ckpt_data['stats']\n",
    "        \n",
    "        # æ‰§è¡Œè¿‡æ»¤\n",
    "        initial_samples = len(metadata)\n",
    "        \n",
    "        similarities = self.calculate_cosine_similarity(matrix, metadata['pert_id'])\n",
    "        sim_mask = similarities >= min_similarity\n",
    "        \n",
    "        matrix = matrix[sim_mask]\n",
    "        metadata = metadata[sim_mask].reset_index(drop=True)\n",
    "        \n",
    "        final_samples = len(metadata)\n",
    "        n_removed = initial_samples - final_samples\n",
    "        \n",
    "        self.logger.info(f\"   Removed: {n_removed:,} low-similarity samples\")\n",
    "        self.logger.info(f\"   Remaining: {final_samples:,} samples\")\n",
    "        \n",
    "        stats = {\n",
    "            'initial_samples': initial_samples,\n",
    "            'final_samples': final_samples,\n",
    "            'removed_samples': n_removed,\n",
    "            'mean_similarity': float(similarities[sim_mask].mean())\n",
    "        }\n",
    "        \n",
    "        # ä¿å­˜checkpoint\n",
    "        data = {'matrix': matrix, 'metadata': metadata, 'stats': stats}\n",
    "        self.checkpoint_mgr.save_checkpoint(filter_name, filter_params, data, stats)\n",
    "        \n",
    "        gc.collect()\n",
    "        return matrix, metadata, stats\n",
    "    \n",
    "    def filter_4_dose_selection(\n",
    "        self,\n",
    "        matrix: np.ndarray,\n",
    "        metadata: pd.DataFrame,\n",
    "        dose_range: Tuple[float, float] = (1.0, 20.0),\n",
    "        dose_bins: Optional[List[float]] = None,\n",
    "        n_bins: int = 4\n",
    "    ) -> Tuple[np.ndarray, pd.DataFrame, Dict]:\n",
    "        \"\"\"\n",
    "        Filter 4: å‰‚é‡é€‰æ‹©ï¼ˆåŸºäºç›´æ–¹å›¾åŒºé—´ï¼‰\n",
    "        \n",
    "        å¯¹æ¯ä¸ªåŒ–åˆç‰©ï¼š\n",
    "        1. å°†å‰‚é‡åˆ†ä¸ºå¤šä¸ªåŒºé—´ï¼ˆç›´æ–¹å›¾ï¼‰\n",
    "        2. é€‰æ‹©æ ·æœ¬æ•°æœ€å¤šçš„åŒºé—´\n",
    "        3. åªä¿ç•™è¯¥åŒºé—´å†…çš„æ ·æœ¬\n",
    "        \n",
    "        å‚æ•°:\n",
    "            matrix: è¡¨è¾¾çŸ©é˜µ\n",
    "            metadata: å…ƒæ•°æ®\n",
    "            dose_range: å‰‚é‡èŒƒå›´ï¼ˆÂµMï¼‰ï¼Œä¾‹å¦‚ (1.0, 20.0)\n",
    "            dose_bins: è‡ªå®šä¹‰åŒºé—´è¾¹ç•Œï¼Œä¾‹å¦‚ [1, 5, 10, 15, 20]\n",
    "                    å¦‚æœä¸º Noneï¼Œåˆ™åœ¨ dose_range å†…å‡åŒ€åˆ†ä¸º n_bins ä¸ªåŒºé—´\n",
    "            n_bins: åŒºé—´æ•°é‡ï¼ˆä»…åœ¨ dose_bins=None æ—¶ç”Ÿæ•ˆï¼‰ï¼Œé»˜è®¤ 4\n",
    "        \n",
    "        è¿”å›:\n",
    "            (è¿‡æ»¤åçš„matrix, è¿‡æ»¤åçš„metadata, ç»Ÿè®¡ä¿¡æ¯)\n",
    "        \n",
    "        ç¤ºä¾‹:\n",
    "            # ä½¿ç”¨é»˜è®¤ 4 ä¸ªå‡åŒ€åŒºé—´ï¼š[1-5.75), [5.75-10.5), [10.5-15.25), [15.25-20]\n",
    "            filter_4_dose_selection(matrix, metadata)\n",
    "            \n",
    "            # è‡ªå®šä¹‰åŒºé—´ï¼š[1-5), [5-10), [10-15), [15-20]\n",
    "            filter_4_dose_selection(matrix, metadata, dose_bins=[1, 5, 10, 15, 20])\n",
    "        \"\"\"\n",
    "        filter_name = \"filter_4_dose_selection\"\n",
    "        filter_params = {\n",
    "            'dose_range': dose_range,\n",
    "            'dose_bins': dose_bins if dose_bins is not None else 'auto',\n",
    "            'n_bins': n_bins\n",
    "        }\n",
    "        \n",
    "        self.logger.info(f\"\\n{'='*80}\")\n",
    "        self.logger.info(f\"FILTER 4: Dose Selection (Histogram-based)\")\n",
    "        self.logger.info(f\"{'='*80}\")\n",
    "        self.logger.info(f\"   Dose range: {dose_range[0]}-{dose_range[1]} ÂµM\")\n",
    "        \n",
    "        # æ£€æŸ¥checkpoint\n",
    "        ckpt_data = self.checkpoint_mgr.load_checkpoint(filter_name, filter_params)\n",
    "        if ckpt_data is not None:\n",
    "            return ckpt_data['matrix'], ckpt_data['metadata'], ckpt_data['stats']\n",
    "        \n",
    "        # æ‰§è¡Œè¿‡æ»¤\n",
    "        if 'pert_dose' not in metadata.columns:\n",
    "            self.logger.warning(f\"   âš ï¸  'pert_dose' not found, skipping\")\n",
    "            stats = {'skipped': True}\n",
    "            return matrix, metadata, stats\n",
    "        \n",
    "        initial_samples = len(metadata)\n",
    "        initial_compounds = metadata['pert_id'].nunique()\n",
    "        \n",
    "        # ========== Step 1: è§£æå‰‚é‡ ==========\n",
    "        self.logger.info(f\"\\n   Step 1/4: Parsing dose values...\")\n",
    "        dose_str = metadata['pert_dose'].astype(str)\n",
    "        metadata['dose_value'] = pd.to_numeric(\n",
    "            dose_str.str.replace(r'[^\\d.]', '', regex=True),\n",
    "            errors='coerce'\n",
    "        )\n",
    "        metadata['dose_unit'] = dose_str.str.extract(r'([a-zA-Z]+)', expand=False).str.lower()\n",
    "        \n",
    "        # è½¬æ¢å•ä½åˆ°ÂµM\n",
    "        dose_value = metadata['dose_value'].values\n",
    "        dose_unit = metadata['dose_unit'].values\n",
    "        \n",
    "        conditions = [\n",
    "            dose_unit == 'nm',\n",
    "            dose_unit == 'mm'\n",
    "        ]\n",
    "        choices = [\n",
    "            dose_value / 1000,\n",
    "            dose_value * 1000\n",
    "        ]\n",
    "        metadata['dose_uM'] = np.select(conditions, choices, default=dose_value)\n",
    "        \n",
    "        valid_dose_count = metadata['dose_uM'].notna().sum()\n",
    "        self.logger.info(f\"   âœ“ Parsed {valid_dose_count:,} valid doses\")\n",
    "        \n",
    "        # ========== Step 2: åº”ç”¨å‰‚é‡èŒƒå›´è¿‡æ»¤ ==========\n",
    "        self.logger.info(f\"\\n   Step 2/4: Filtering dose range...\")\n",
    "        dose_range_mask = (\n",
    "            (metadata['dose_uM'] >= dose_range[0]) &\n",
    "            (metadata['dose_uM'] <= dose_range[1]) &\n",
    "            (~pd.isna(metadata['dose_uM']))\n",
    "        )\n",
    "        \n",
    "        n_in_range = dose_range_mask.sum()\n",
    "        n_out_range = len(metadata) - n_in_range\n",
    "        self.logger.info(f\"   âœ“ Samples in range {dose_range[0]}-{dose_range[1]} ÂµM: {n_in_range:,}\")\n",
    "        self.logger.info(f\"   âœ“ Samples outside range: {n_out_range:,}\")\n",
    "        \n",
    "        matrix = matrix[dose_range_mask]\n",
    "        metadata = metadata[dose_range_mask].reset_index(drop=True)\n",
    "        \n",
    "        # ========== Step 3: å®šä¹‰å‰‚é‡åŒºé—´ï¼ˆbinsï¼‰==========\n",
    "        self.logger.info(f\"\\n   Step 3/4: Defining dose bins...\")\n",
    "        \n",
    "        if dose_bins is None:\n",
    "            # è‡ªåŠ¨ç”Ÿæˆå‡åŒ€åŒºé—´\n",
    "            dose_bins = np.linspace(dose_range[0], dose_range[1], n_bins + 1)\n",
    "            self.logger.info(f\"   Using {n_bins} uniform bins:\")\n",
    "        else:\n",
    "            dose_bins = np.array(dose_bins)\n",
    "            n_bins = len(dose_bins) - 1\n",
    "            self.logger.info(f\"   Using {n_bins} custom bins:\")\n",
    "        \n",
    "        # æ‰“å°åŒºé—´ä¿¡æ¯\n",
    "        for i in range(len(dose_bins) - 1):\n",
    "            left = dose_bins[i]\n",
    "            right = dose_bins[i + 1]\n",
    "            bracket_right = ']' if i == len(dose_bins) - 2 else ')'\n",
    "            self.logger.info(f\"     Bin {i+1}: [{left:.2f}, {right:.2f}{bracket_right} ÂµM\")\n",
    "        \n",
    "        # ä¸ºæ¯ä¸ªæ ·æœ¬åˆ†é…åŒºé—´\n",
    "        metadata['dose_bin'] = pd.cut(\n",
    "            metadata['dose_uM'],\n",
    "            bins=dose_bins,\n",
    "            labels=range(n_bins),\n",
    "            include_lowest=True\n",
    "        )\n",
    "        \n",
    "        # ========== Step 4: ä¸ºæ¯ä¸ªåŒ–åˆç‰©é€‰æ‹©æ ·æœ¬æœ€å¤šçš„åŒºé—´ ==========\n",
    "        self.logger.info(f\"\\n   Step 4/4: Selecting modal bin for each compound...\")\n",
    "        \n",
    "        def find_modal_bin(bin_series):\n",
    "            \"\"\"æ‰¾åˆ°æ ·æœ¬æ•°æœ€å¤šçš„åŒºé—´\"\"\"\n",
    "            bin_counts = bin_series.value_counts()\n",
    "            if len(bin_counts) == 0:\n",
    "                return np.nan\n",
    "            return bin_counts.idxmax()\n",
    "        \n",
    "        # ç»Ÿè®¡æ¯ä¸ªåŒ–åˆç‰©åœ¨å„åŒºé—´çš„æ ·æœ¬æ•°\n",
    "        modal_bins = metadata.groupby('pert_id')['dose_bin'].apply(find_modal_bin)\n",
    "        \n",
    "        # æ‰“å°åŒºé—´é€‰æ‹©ç»Ÿè®¡\n",
    "        self.logger.info(f\"   Modal bin distribution across compounds:\")\n",
    "        modal_bin_dist = modal_bins.value_counts().sort_index()\n",
    "        for bin_id, count in modal_bin_dist.items():\n",
    "            if pd.notna(bin_id):\n",
    "                left = dose_bins[int(bin_id)]\n",
    "                right = dose_bins[int(bin_id) + 1]\n",
    "                bracket_right = ']' if int(bin_id) == n_bins - 1 else ')'\n",
    "                self.logger.info(\n",
    "                    f\"     Bin {int(bin_id)+1} [{left:.2f}, {right:.2f}{bracket_right}: \"\n",
    "                    f\"{count:,} compounds ({count/len(modal_bins)*100:.1f}%)\"\n",
    "                )\n",
    "        \n",
    "        # ä¸ºæ¯ä¸ªæ ·æœ¬æ·»åŠ å…¶åŒ–åˆç‰©çš„modal bin\n",
    "        metadata['modal_bin'] = metadata['pert_id'].map(modal_bins)\n",
    "        \n",
    "        # åªä¿ç•™modal binå†…çš„æ ·æœ¬\n",
    "        modal_bin_mask = (\n",
    "            (metadata['dose_bin'] == metadata['modal_bin']) &\n",
    "            (~pd.isna(metadata['modal_bin']))\n",
    "        )\n",
    "        \n",
    "        n_removed = len(metadata) - modal_bin_mask.sum()\n",
    "        \n",
    "        matrix = matrix[modal_bin_mask]\n",
    "        metadata = metadata[modal_bin_mask].reset_index(drop=True)\n",
    "        \n",
    "        final_samples = len(metadata)\n",
    "        final_compounds = metadata['pert_id'].nunique()\n",
    "        \n",
    "        self.logger.info(f\"\\n   âœ… Filter 4 Results:\")\n",
    "        self.logger.info(f\"   âœ“ Removed {n_removed:,} samples (non-modal bin)\")\n",
    "        self.logger.info(f\"   âœ“ Remaining: {final_samples:,} samples, {final_compounds:,} compounds\")\n",
    "        \n",
    "        # ========== ç»Ÿè®¡ä¿¡æ¯ ==========\n",
    "        stats = {\n",
    "            'initial_samples': int(initial_samples),\n",
    "            'final_samples': int(final_samples),\n",
    "            'removed_samples': int(initial_samples - final_samples),\n",
    "            'initial_compounds': int(initial_compounds),\n",
    "            'final_compounds': int(final_compounds),\n",
    "            'dose_range': dose_range,\n",
    "            'dose_bins': dose_bins.tolist(),\n",
    "            'n_bins': int(n_bins),\n",
    "            'modal_bin_distribution': {\n",
    "                int(k): int(v) for k, v in modal_bin_dist.items() if pd.notna(k)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # ========== å¯è§†åŒ–ï¼ˆå¯é€‰ï¼‰==========\n",
    "        if False:  # è®¾ç½®ä¸º True å¯ç”¨å¯è§†åŒ–\n",
    "            self._plot_dose_histogram(metadata, dose_bins, modal_bins)\n",
    "        \n",
    "        # æ¸…ç†ä¸´æ—¶åˆ—\n",
    "        metadata.drop(\n",
    "            ['dose_value', 'dose_unit', 'dose_bin', 'modal_bin'],\n",
    "            axis=1,\n",
    "            inplace=True,\n",
    "            errors='ignore'\n",
    "        )\n",
    "        \n",
    "        # ä¿å­˜checkpoint\n",
    "        data = {'matrix': matrix, 'metadata': metadata, 'stats': stats}\n",
    "        self.checkpoint_mgr.save_checkpoint(filter_name, filter_params, data, stats)\n",
    "        \n",
    "        gc.collect()\n",
    "        return matrix, metadata, stats\n",
    "\n",
    "\n",
    "    def _plot_dose_histogram(\n",
    "        self,\n",
    "        metadata: pd.DataFrame,\n",
    "        dose_bins: np.ndarray,\n",
    "        modal_bins: pd.Series\n",
    "    ):\n",
    "        \"\"\"\n",
    "        å¯è§†åŒ–å‰‚é‡åˆ†å¸ƒç›´æ–¹å›¾\n",
    "        \n",
    "        å‚æ•°:\n",
    "            metadata: åŒ…å«å‰‚é‡ä¿¡æ¯çš„å…ƒæ•°æ®\n",
    "            dose_bins: åŒºé—´è¾¹ç•Œ\n",
    "            modal_bins: æ¯ä¸ªåŒ–åˆç‰©çš„modal bin\n",
    "        \"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        from pathlib import Path\n",
    "        \n",
    "        self.logger.info(f\"\\n   ğŸ“Š Generating dose histogram visualization...\")\n",
    "        \n",
    "        viz_dir = Path(self.data_dir) / \"visualizations\"\n",
    "        viz_dir.mkdir(exist_ok=True, parents=True)\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # ===== å­å›¾1: æ•´ä½“å‰‚é‡åˆ†å¸ƒ =====\n",
    "        ax1 = axes[0, 0]\n",
    "        ax1.hist(\n",
    "            metadata['dose_uM'],\n",
    "            bins=dose_bins,\n",
    "            edgecolor='black',\n",
    "            alpha=0.7,\n",
    "            color='steelblue'\n",
    "        )\n",
    "        ax1.set_xlabel('Dose (ÂµM)', fontsize=11)\n",
    "        ax1.set_ylabel('Number of Samples', fontsize=11)\n",
    "        ax1.set_title('Overall Dose Distribution', fontsize=12, fontweight='bold')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # æ·»åŠ åŒºé—´åˆ†éš”çº¿\n",
    "        for bin_edge in dose_bins[1:-1]:\n",
    "            ax1.axvline(bin_edge, color='red', linestyle='--', alpha=0.5, linewidth=1)\n",
    "        \n",
    "        # ===== å­å›¾2: Modal binåˆ†å¸ƒï¼ˆåŒ–åˆç‰©å±‚é¢ï¼‰=====\n",
    "        ax2 = axes[0, 1]\n",
    "        modal_bin_counts = modal_bins.value_counts().sort_index()\n",
    "        \n",
    "        x_pos = range(len(modal_bin_counts))\n",
    "        bars = ax2.bar(\n",
    "            x_pos,\n",
    "            modal_bin_counts.values,\n",
    "            color='darkorange',\n",
    "            edgecolor='black',\n",
    "            alpha=0.8\n",
    "        )\n",
    "        \n",
    "        # è®¾ç½®xè½´æ ‡ç­¾\n",
    "        bin_labels = []\n",
    "        for bin_id in modal_bin_counts.index:\n",
    "            if pd.notna(bin_id):\n",
    "                left = dose_bins[int(bin_id)]\n",
    "                right = dose_bins[int(bin_id) + 1]\n",
    "                bin_labels.append(f'[{left:.1f}, {right:.1f})')\n",
    "            else:\n",
    "                bin_labels.append('NaN')\n",
    "        \n",
    "        ax2.set_xticks(x_pos)\n",
    "        ax2.set_xticklabels(bin_labels, rotation=45, ha='right')\n",
    "        ax2.set_xlabel('Dose Bin (ÂµM)', fontsize=11)\n",
    "        ax2.set_ylabel('Number of Compounds', fontsize=11)\n",
    "        ax2.set_title('Modal Bin Distribution\\n(Compound Level)', fontsize=12, fontweight='bold')\n",
    "        ax2.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "        for bar, val in zip(bars, modal_bin_counts.values):\n",
    "            ax2.text(\n",
    "                bar.get_x() + bar.get_width()/2,\n",
    "                bar.get_height(),\n",
    "                f'{val:,}',\n",
    "                ha='center',\n",
    "                va='bottom',\n",
    "                fontsize=9\n",
    "            )\n",
    "        \n",
    "        # ===== å­å›¾3: éšæœºé€‰æ‹©10ä¸ªåŒ–åˆç‰©çš„å‰‚é‡åˆ†å¸ƒ =====\n",
    "        ax3 = axes[1, 0]\n",
    "        \n",
    "        sample_compounds = np.random.choice(\n",
    "            metadata['pert_id'].unique(),\n",
    "            size=min(10, metadata['pert_id'].nunique()),\n",
    "            replace=False\n",
    "        )\n",
    "        \n",
    "        for i, pert_id in enumerate(sample_compounds):\n",
    "            pert_data = metadata[metadata['pert_id'] == pert_id]['dose_uM']\n",
    "            ax3.scatter(\n",
    "                pert_data,\n",
    "                [i] * len(pert_data),\n",
    "                alpha=0.6,\n",
    "                s=50,\n",
    "                label=pert_id[:10]\n",
    "            )\n",
    "        \n",
    "        ax3.set_xlabel('Dose (ÂµM)', fontsize=11)\n",
    "        ax3.set_ylabel('Compound Index', fontsize=11)\n",
    "        ax3.set_title('Sample Dose Distribution\\n(10 Random Compounds)', fontsize=12, fontweight='bold')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # æ·»åŠ binè¾¹ç•Œçº¿\n",
    "        for bin_edge in dose_bins:\n",
    "            ax3.axvline(bin_edge, color='red', linestyle='--', alpha=0.3, linewidth=1)\n",
    "        \n",
    "        # ===== å­å›¾4: æ¯ä¸ªbinçš„æ ·æœ¬æ•°ç»Ÿè®¡ =====\n",
    "        ax4 = axes[1, 1]\n",
    "        \n",
    "        bin_sample_counts = metadata['dose_bin'].value_counts().sort_index()\n",
    "        \n",
    "        x_pos = range(len(bin_sample_counts))\n",
    "        bars = ax4.bar(\n",
    "            x_pos,\n",
    "            bin_sample_counts.values,\n",
    "            color='forestgreen',\n",
    "            edgecolor='black',\n",
    "            alpha=0.8\n",
    "        )\n",
    "        \n",
    "        # è®¾ç½®xè½´æ ‡ç­¾\n",
    "        bin_labels = []\n",
    "        for bin_id in bin_sample_counts.index:\n",
    "            left = dose_bins[int(bin_id)]\n",
    "            right = dose_bins[int(bin_id) + 1]\n",
    "            bin_labels.append(f'[{left:.1f}, {right:.1f})')\n",
    "        \n",
    "        ax4.set_xticks(x_pos)\n",
    "        ax4.set_xticklabels(bin_labels, rotation=45, ha='right')\n",
    "        ax4.set_xlabel('Dose Bin (ÂµM)', fontsize=11)\n",
    "        ax4.set_ylabel('Number of Samples', fontsize=11)\n",
    "        ax4.set_title('Sample Distribution by Bin\\n(Sample Level)', fontsize=12, fontweight='bold')\n",
    "        ax4.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "        for bar, val in zip(bars, bin_sample_counts.values):\n",
    "            ax4.text(\n",
    "                bar.get_x() + bar.get_width()/2,\n",
    "                bar.get_height(),\n",
    "                f'{val:,}',\n",
    "                ha='center',\n",
    "                va='bottom',\n",
    "                fontsize=9\n",
    "            )\n",
    "        \n",
    "        plt.suptitle(\n",
    "            'Dose Selection Filter - Histogram-based Approach',\n",
    "            fontsize=14,\n",
    "            fontweight='bold',\n",
    "            y=0.995\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        output_path = viz_dir / \"dose_histogram_filter.png\"\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        self.logger.info(f\"   âœ“ Saved visualization: {output_path}\")\n",
    "        plt.close()\n",
    "    \n",
    "    def filter_5_timepoint_selection(\n",
    "        self,\n",
    "        matrix: np.ndarray,\n",
    "        metadata: pd.DataFrame,\n",
    "        valid_timepoints: List[int] = [6, 24]\n",
    "    ) -> Tuple[np.ndarray, pd.DataFrame, Dict]:\n",
    "        \"\"\"\n",
    "        Filter 5: æ—¶é—´ç‚¹é€‰æ‹©\n",
    "        \n",
    "        å‚æ•°:\n",
    "            matrix: è¡¨è¾¾çŸ©é˜µ\n",
    "            metadata: å…ƒæ•°æ®\n",
    "            valid_timepoints: æœ‰æ•ˆæ—¶é—´ç‚¹ï¼ˆå°æ—¶ï¼‰\n",
    "        \n",
    "        è¿”å›:\n",
    "            (è¿‡æ»¤åçš„matrix, è¿‡æ»¤åçš„metadata, ç»Ÿè®¡ä¿¡æ¯)\n",
    "        \"\"\"\n",
    "        filter_name = \"filter_5_timepoint_selection\"\n",
    "        filter_params = {'valid_timepoints': valid_timepoints}\n",
    "        \n",
    "        self.logger.info(f\"\\n{'='*80}\")\n",
    "        self.logger.info(f\"FILTER 5: Timepoint Selection ({valid_timepoints} hours)\")\n",
    "        self.logger.info(f\"{'='*80}\")\n",
    "        \n",
    "        # æ£€æŸ¥checkpoint\n",
    "        ckpt_data = self.checkpoint_mgr.load_checkpoint(filter_name, filter_params)\n",
    "        if ckpt_data is not None:\n",
    "            return ckpt_data['matrix'], ckpt_data['metadata'], ckpt_data['stats']\n",
    "        \n",
    "        # æ‰§è¡Œè¿‡æ»¤\n",
    "        if 'pert_time' not in metadata.columns:\n",
    "            self.logger.warning(f\"   âš ï¸  'pert_time' not found, skipping\")\n",
    "            stats = {'skipped': True}\n",
    "            return matrix, metadata, stats\n",
    "        \n",
    "        initial_samples = len(metadata)\n",
    "        \n",
    "        # è½¬æ¢ä¸ºæ•°å€¼\n",
    "        metadata['time_numeric'] = pd.to_numeric(metadata['pert_time'], errors='coerce')\n",
    "        \n",
    "        time_counts = metadata['time_numeric'].value_counts().head(10)\n",
    "        self.logger.info(f\"   Available timepoints:\")\n",
    "        for time_val, count in time_counts.items():\n",
    "            self.logger.info(f\"     - {time_val} hours: {count:,}\")\n",
    "        \n",
    "        time_mask = metadata['time_numeric'].isin(valid_timepoints)\n",
    "        \n",
    "        matrix = matrix[time_mask]\n",
    "        metadata = metadata[time_mask].reset_index(drop=True)\n",
    "        \n",
    "        final_samples = len(metadata)\n",
    "        \n",
    "        self.logger.info(f\"   Removed: {initial_samples - final_samples:,} samples\")\n",
    "        self.logger.info(f\"   Remaining: {final_samples:,} samples\")\n",
    "        \n",
    "        stats = {\n",
    "            'initial_samples': initial_samples,\n",
    "            'final_samples': final_samples,\n",
    "            'removed_samples': initial_samples - final_samples\n",
    "        }\n",
    "        \n",
    "        # ä¿å­˜checkpoint\n",
    "        data = {'matrix': matrix, 'metadata': metadata, 'stats': stats}\n",
    "        self.checkpoint_mgr.save_checkpoint(filter_name, filter_params, data, stats)\n",
    "        \n",
    "        gc.collect()\n",
    "        return matrix, metadata, stats\n",
    "    \n",
    "    def filter_6_min_compounds_per_cell(\n",
    "        self,\n",
    "        matrix: np.ndarray,\n",
    "        metadata: pd.DataFrame,\n",
    "        min_compounds: int = 200,\n",
    "        cell_col: str = 'cell_iname'\n",
    "    ) -> Tuple[np.ndarray, pd.DataFrame, Dict]:\n",
    "        \"\"\"\n",
    "        Filter 6: ç§»é™¤åŒ–åˆç‰©æ•°å°‘äºé˜ˆå€¼çš„ç»†èƒç³»\n",
    "        \n",
    "        å‚æ•°:\n",
    "            matrix: è¡¨è¾¾çŸ©é˜µ\n",
    "            metadata: å…ƒæ•°æ®\n",
    "            min_compounds: æ¯ä¸ªç»†èƒç³»çš„æœ€å°åŒ–åˆç‰©æ•°\n",
    "            cell_col: ç»†èƒç³»IDåˆ—å\n",
    "        \n",
    "        è¿”å›:\n",
    "            (è¿‡æ»¤åçš„matrix, è¿‡æ»¤åçš„metadata, ç»Ÿè®¡ä¿¡æ¯)\n",
    "        \"\"\"\n",
    "        filter_name = \"filter_6_min_compounds_per_cell\"\n",
    "        filter_params = {\n",
    "            'min_compounds': min_compounds,\n",
    "            'cell_col': cell_col\n",
    "        }\n",
    "        \n",
    "        self.logger.info(f\"\\n{'='*80}\")\n",
    "        self.logger.info(f\"FILTER 6: Min Compounds Per Cell (â‰¥{min_compounds})\")\n",
    "        self.logger.info(f\"{'='*80}\")\n",
    "        \n",
    "        # æ£€æŸ¥checkpoint\n",
    "        ckpt_data = self.checkpoint_mgr.load_checkpoint(filter_name, filter_params)\n",
    "        if ckpt_data is not None:\n",
    "            return ckpt_data['matrix'], ckpt_data['metadata'], ckpt_data['stats']\n",
    "        \n",
    "        # æ‰§è¡Œè¿‡æ»¤\n",
    "        if cell_col not in metadata.columns:\n",
    "            self.logger.warning(f\"   âš ï¸  '{cell_col}' not found, skipping\")\n",
    "            stats = {'skipped': True}\n",
    "            return matrix, metadata, stats\n",
    "        \n",
    "        initial_samples = len(metadata)\n",
    "        initial_cells = metadata[cell_col].nunique()\n",
    "        \n",
    "        compounds_per_cell = metadata.groupby(cell_col)['pert_id'].nunique()\n",
    "        \n",
    "        self.logger.info(f\"   Cell lines before: {initial_cells:,}\")\n",
    "        self.logger.info(f\"   Compounds per cell (mean): {compounds_per_cell.mean():.1f}\")\n",
    "        \n",
    "        valid_cells = compounds_per_cell[compounds_per_cell >= min_compounds].index\n",
    "        \n",
    "        self.logger.info(f\"   Cell lines with â‰¥{min_compounds} compounds: {len(valid_cells):,}\")\n",
    "        \n",
    "        cell_mask = metadata[cell_col].isin(valid_cells)\n",
    "        \n",
    "        matrix = matrix[cell_mask]\n",
    "        metadata = metadata[cell_mask].reset_index(drop=True)\n",
    "        \n",
    "        final_samples = len(metadata)\n",
    "        final_cells = metadata[cell_col].nunique()\n",
    "        \n",
    "        self.logger.info(f\"   Removed: {initial_samples - final_samples:,} samples from {initial_cells - final_cells:,} cell lines\")\n",
    "        self.logger.info(f\"   Remaining: {final_samples:,} samples, {final_cells:,} cell lines\")\n",
    "        \n",
    "        stats = {\n",
    "            'initial_samples': initial_samples,\n",
    "            'final_samples': final_samples,\n",
    "            'removed_samples': initial_samples - final_samples,\n",
    "            'initial_cells': initial_cells,\n",
    "            'final_cells': final_cells\n",
    "        }\n",
    "        \n",
    "        # ä¿å­˜checkpoint\n",
    "        data = {'matrix': matrix, 'metadata': metadata, 'stats': stats}\n",
    "        self.checkpoint_mgr.save_checkpoint(filter_name, filter_params, data, stats)\n",
    "        \n",
    "        gc.collect()\n",
    "        return matrix, metadata, stats\n",
    "    \n",
    "    def filter_7_cell_line_count(\n",
    "        self,\n",
    "        matrix: np.ndarray,\n",
    "        metadata: pd.DataFrame,\n",
    "        min_cell_lines: int = 5,\n",
    "        max_cell_lines: int = 40,\n",
    "        cell_col: str = 'cell_iname'\n",
    "    ) -> Tuple[np.ndarray, pd.DataFrame, Dict]:\n",
    "        \"\"\"\n",
    "        Filter 7: ç»†èƒç³»æ•°é‡è¿‡æ»¤\n",
    "        \n",
    "        å‚æ•°:\n",
    "            matrix: è¡¨è¾¾çŸ©é˜µ\n",
    "            metadata: å…ƒæ•°æ®\n",
    "            min_cell_lines: æœ€å°ç»†èƒç³»æ•°\n",
    "            max_cell_lines: æœ€å¤§ç»†èƒç³»æ•°\n",
    "            cell_col: ç»†èƒç³»IDåˆ—å\n",
    "        \n",
    "        è¿”å›:\n",
    "            (è¿‡æ»¤åçš„matrix, è¿‡æ»¤åçš„metadata, ç»Ÿè®¡ä¿¡æ¯)\n",
    "        \"\"\"\n",
    "        filter_name = \"filter_7_cell_line_count\"\n",
    "        filter_params = {\n",
    "            'min_cell_lines': min_cell_lines,\n",
    "            'max_cell_lines': max_cell_lines,\n",
    "            'cell_col': cell_col\n",
    "        }\n",
    "        \n",
    "        self.logger.info(f\"\\n{'='*80}\")\n",
    "        self.logger.info(f\"FILTER 7: Cell Line Count ({min_cell_lines}-{max_cell_lines})\")\n",
    "        self.logger.info(f\"{'='*80}\")\n",
    "        \n",
    "        # æ£€æŸ¥checkpoint\n",
    "        ckpt_data = self.checkpoint_mgr.load_checkpoint(filter_name, filter_params)\n",
    "        if ckpt_data is not None:\n",
    "            return ckpt_data['matrix'], ckpt_data['metadata'], ckpt_data['stats']\n",
    "        \n",
    "        # æ‰§è¡Œè¿‡æ»¤\n",
    "        if cell_col not in metadata.columns:\n",
    "            self.logger.warning(f\"   âš ï¸  '{cell_col}' not found, skipping\")\n",
    "            stats = {'skipped': True}\n",
    "            return matrix, metadata, stats\n",
    "        \n",
    "        initial_samples = len(metadata)\n",
    "        initial_compounds = metadata['pert_id'].nunique()\n",
    "        \n",
    "        cell_line_counts = metadata.groupby('pert_id')[cell_col].nunique()\n",
    "        valid_perts = cell_line_counts[\n",
    "            (cell_line_counts >= min_cell_lines) &\n",
    "            (cell_line_counts <= max_cell_lines)\n",
    "        ].index\n",
    "        \n",
    "        self.logger.info(f\"   Compounds in {min_cell_lines}-{max_cell_lines} cell lines: {len(valid_perts):,}/{initial_compounds:,}\")\n",
    "        \n",
    "        cell_mask = metadata['pert_id'].isin(valid_perts)\n",
    "        \n",
    "        matrix = matrix[cell_mask]\n",
    "        metadata = metadata[cell_mask].reset_index(drop=True)\n",
    "        \n",
    "        final_samples = len(metadata)\n",
    "        final_compounds = metadata['pert_id'].nunique()\n",
    "        \n",
    "        self.logger.info(f\"   Removed: {initial_samples - final_samples:,} samples\")\n",
    "        self.logger.info(f\"   Remaining: {final_samples:,} samples, {final_compounds:,} compounds\")\n",
    "        \n",
    "        stats = {\n",
    "            'initial_samples': initial_samples,\n",
    "            'final_samples': final_samples,\n",
    "            'removed_samples': initial_samples - final_samples,\n",
    "            'initial_compounds': initial_compounds,\n",
    "            'final_compounds': final_compounds\n",
    "        }\n",
    "        \n",
    "        # ä¿å­˜checkpoint\n",
    "        data = {'matrix': matrix, 'metadata': metadata, 'stats': stats}\n",
    "        self.checkpoint_mgr.save_checkpoint(filter_name, filter_params, data, stats)\n",
    "        \n",
    "        gc.collect()\n",
    "        return matrix, metadata, stats\n",
    "    \n",
    "    # ========== åŒ–å­¦è¿‡æ»¤å™¨æ–¹æ³• ==========\n",
    "    \n",
    "    def _init_chemical_filters(self, nibr_csv_path: Optional[str] = None):\n",
    "        \"\"\"åˆå§‹åŒ–åŒ–å­¦è¿‡æ»¤å™¨\"\"\"\n",
    "        if not RDKIT_AVAILABLE:\n",
    "            self.logger.error(\"âŒ RDKit not available, chemical filters disabled\")\n",
    "            return False\n",
    "        \n",
    "        # åˆå§‹åŒ–BRENKè¿‡æ»¤å™¨\n",
    "        params = FilterCatalogParams()\n",
    "        params.AddCatalog(FilterCatalogParams.FilterCatalogs.BRENK)\n",
    "        self.brenk_catalog = FilterCatalog.FilterCatalog(params)\n",
    "        \n",
    "        # åˆå§‹åŒ–å…±ä»·åŸºå›¢SMARTS\n",
    "        self.covalent_smarts = [\n",
    "            'C=CC(=O)', 'C=CC(=O)N', 'C=CC#N',\n",
    "            '[C;!R](=O)Cl', '[C;!R](=O)O[C;!R](=O)', 'C(=O)N=[N+]=[N-]',\n",
    "            'C1OC1', 'C1NC1',\n",
    "            'ClCC(=O)N', 'BrCC(=O)N',\n",
    "            '[CH;!R]=O',\n",
    "            'N=C=O', 'N=C=S',\n",
    "            'S(=O)(=O)Cl', 'S(=O)(=O)F',\n",
    "            '[C;!R]#N',\n",
    "            'OO',\n",
    "            'C1(=O)NCC1',\n",
    "            'C(=O)OC(=O)',\n",
    "            'C1=CC(=O)C=CC1=O'\n",
    "        ]\n",
    "        \n",
    "        # åˆå§‹åŒ–NIBRè¿‡æ»¤å™¨\n",
    "        self.nibr_catalog = self._init_nibr_catalog(nibr_csv_path)\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def _init_nibr_catalog(self, csv_path: Optional[str] = None):\n",
    "        \"\"\"åˆå§‹åŒ–NIBRè¿‡æ»¤å™¨ç›®å½•\"\"\"\n",
    "        if csv_path is None:\n",
    "            csv_path = self.data_dir / \"chem_filter\" / \"SubstructureFilter_HitTriaging_wPubChemExamples.csv\"\n",
    "        else:\n",
    "            csv_path = Path(csv_path)\n",
    "        \n",
    "        if not csv_path.exists():\n",
    "            self.logger.warning(f\"âš ï¸  NIBR filter CSV not found: {csv_path}\")\n",
    "            return FilterCatalog.FilterCatalog()\n",
    "        \n",
    "        self.logger.info(f\"   Loading NIBR filters from: {csv_path.name}\")\n",
    "        \n",
    "        try:\n",
    "            nibr_df = pd.read_csv(csv_path)\n",
    "            self.logger.info(f\"   âœ“ Loaded {len(nibr_df)} NIBR patterns\")\n",
    "            \n",
    "            nibr_catalog = FilterCatalog.FilterCatalog()\n",
    "            n_added = 0\n",
    "            \n",
    "            for idx, row in nibr_df.iterrows():\n",
    "                try:\n",
    "                    pattern_name = row['PATTERN_NAME']\n",
    "                    smarts = row['SMARTS']\n",
    "                    min_count = 1 if row['MIN_COUNT'] == 0 else int(row['MIN_COUNT'])\n",
    "                    severity = int(row['SEVERITY_SCORE'])\n",
    "                    covalent = int(row['COVALENT'])\n",
    "                    special_mol = int(row['SPECIAL_MOL'])\n",
    "                    set_name = row['SET_NAME']\n",
    "                    \n",
    "                    filter_name = f\"{pattern_name}_min({min_count})__{severity}__{covalent}__{special_mol}\"\n",
    "                    matcher = FilterCatalog.SmartsMatcher(filter_name, smarts, min_count)\n",
    "                    \n",
    "                    entry = FilterCatalog.FilterCatalogEntry(filter_name, matcher)\n",
    "                    entry.SetProp('Scope', set_name)\n",
    "                    entry.SetProp('Severity', str(severity))\n",
    "                    nibr_catalog.AddEntry(entry)\n",
    "                    \n",
    "                    n_added += 1\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            self.logger.info(f\"   âœ“ Added {n_added} NIBR filters\")\n",
    "            return nibr_catalog\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"   âŒ Error loading NIBR filters: {e}\")\n",
    "            return FilterCatalog.FilterCatalog()\n",
    "    \n",
    "    def filter_8_chemical_filters(\n",
    "        self,\n",
    "        matrix: np.ndarray,\n",
    "        metadata: pd.DataFrame,\n",
    "        min_mw: float = 60,\n",
    "        max_mw: float = 1000,\n",
    "        max_covalent_motifs: int = 1,\n",
    "        max_nibr_flags: int = 9,\n",
    "        nibr_csv_path: Optional[str] = None\n",
    "    ) -> Tuple[np.ndarray, pd.DataFrame, Dict]:\n",
    "        \"\"\"\n",
    "        Filter 8: åŒ–å­¦è¿‡æ»¤å™¨ï¼ˆæ•´åˆæ‰€æœ‰åŒ–å­¦è¿‡æ»¤ï¼‰\n",
    "        \n",
    "        å‚æ•°:\n",
    "            matrix: è¡¨è¾¾çŸ©é˜µ\n",
    "            metadata: å…ƒæ•°æ®\n",
    "            min_mw: æœ€å°åˆ†å­é‡\n",
    "            max_mw: æœ€å¤§åˆ†å­é‡\n",
    "            max_covalent_motifs: æœ€å¤§å…±ä»·åŸºå›¢æ•°\n",
    "            max_nibr_flags: æœ€å¤§NIBRæ ‡è®°æ•°\n",
    "            nibr_csv_path: NIBRè¿‡æ»¤å™¨CSVè·¯å¾„\n",
    "        \n",
    "        è¿”å›:\n",
    "            (è¿‡æ»¤åçš„matrix, è¿‡æ»¤åçš„metadata, ç»Ÿè®¡ä¿¡æ¯)\n",
    "        \"\"\"\n",
    "        filter_name = \"filter_8_chemical\"\n",
    "        filter_params = {\n",
    "            'min_mw': min_mw,\n",
    "            'max_mw': max_mw,\n",
    "            'max_covalent_motifs': max_covalent_motifs,\n",
    "            'max_nibr_flags': max_nibr_flags\n",
    "        }\n",
    "        \n",
    "        self.logger.info(f\"\\n{'='*80}\")\n",
    "        self.logger.info(f\"FILTER 8: Chemical Filters\")\n",
    "        self.logger.info(f\"{'='*80}\")\n",
    "        \n",
    "        # æ£€æŸ¥checkpoint\n",
    "        ckpt_data = self.checkpoint_mgr.load_checkpoint(filter_name, filter_params)\n",
    "        if ckpt_data is not None:\n",
    "            return ckpt_data['matrix'], ckpt_data['metadata'], ckpt_data['stats']\n",
    "        \n",
    "        # æ£€æŸ¥RDKit\n",
    "        if not RDKIT_AVAILABLE:\n",
    "            self.logger.error(\"   âŒ RDKit not available, skipping chemical filters\")\n",
    "            stats = {'skipped': True, 'reason': 'RDKit not available'}\n",
    "            return matrix, metadata, stats\n",
    "        \n",
    "        # åˆå§‹åŒ–åŒ–å­¦è¿‡æ»¤å™¨\n",
    "        if self.covalent_smarts is None:\n",
    "            self._init_chemical_filters(nibr_csv_path)\n",
    "        \n",
    "        # åŠ è½½åŒ–åˆç‰©ä¿¡æ¯\n",
    "        if self.compound_info is None:\n",
    "            self.load_compound_info()\n",
    "        \n",
    "        if self.compound_info is None:\n",
    "            self.logger.error(\"   âŒ Compound info not available\")\n",
    "            stats = {'skipped': True, 'reason': 'Compound info not available'}\n",
    "            return matrix, metadata, stats\n",
    "        \n",
    "        initial_samples = len(metadata)\n",
    "        initial_compounds = metadata['pert_id'].nunique()\n",
    "        \n",
    "        # ç¡®å®šSMILESåˆ—\n",
    "        smiles_col = None\n",
    "        for col in ['canonical_smiles', 'smiles', 'SMILES']:\n",
    "            if col in self.compound_info.columns:\n",
    "                smiles_col = col\n",
    "                break\n",
    "        \n",
    "        if smiles_col is None:\n",
    "            self.logger.error(\"   âŒ SMILES column not found\")\n",
    "            stats = {'skipped': True, 'reason': 'SMILES column not found'}\n",
    "            return matrix, metadata, stats\n",
    "        \n",
    "        self.logger.info(f\"   Using '{smiles_col}' for chemical structures\")\n",
    "        \n",
    "        # åªå¤„ç†è®­ç»ƒæ•°æ®ä¸­çš„åŒ–åˆç‰©\n",
    "        training_compounds = set(metadata['pert_id'].unique())\n",
    "        compound_subset = self.compound_info[\n",
    "            self.compound_info['pert_id'].isin(training_compounds)\n",
    "        ].copy()\n",
    "        \n",
    "        self.logger.info(f\"   Processing {len(compound_subset):,} compounds\")\n",
    "        \n",
    "        # åº”ç”¨å„ä¸ªåŒ–å­¦è¿‡æ»¤å™¨\n",
    "        passed_compounds = set()\n",
    "        filter_results = {\n",
    "            'mw': [],\n",
    "            'covalent': [],\n",
    "            'nibr': [],\n",
    "            'brenk': []\n",
    "        }\n",
    "        \n",
    "        # é¢„ç¼–è¯‘SMARTS\n",
    "        smarts_mols = []\n",
    "        for smarts in self.covalent_smarts:\n",
    "            try:\n",
    "                smarts_mol = Chem.MolFromSmarts(smarts)\n",
    "                if smarts_mol:\n",
    "                    smarts_mols.append(smarts_mol)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        self.logger.info(f\"   Running chemical filters...\")\n",
    "        \n",
    "        for idx, row in tqdm(compound_subset.iterrows(), total=len(compound_subset), desc=\"   Processing\", leave=False):\n",
    "            pert_id = row['pert_id']\n",
    "            smiles = row[smiles_col]\n",
    "            \n",
    "            # è·³è¿‡æ— æ•ˆSMILES\n",
    "            if pd.isna(smiles) or smiles == '' or smiles == 'restricted':\n",
    "                continue\n",
    "            \n",
    "            # è§£æåˆ†å­ï¼ˆç¦ç”¨RDKitè­¦å‘Šï¼‰\n",
    "            mol = Chem.MolFromSmiles(str(smiles))\n",
    "            if mol is None:\n",
    "                continue\n",
    "            \n",
    "            # Filter 8.1: åˆ†å­é‡\n",
    "            mw = Descriptors.MolWt(mol)\n",
    "            if not (min_mw <= mw <= max_mw):\n",
    "                filter_results['mw'].append(pert_id)\n",
    "                continue\n",
    "            \n",
    "            # Filter 8.2: å…±ä»·åŸºå›¢\n",
    "            motif_count = 0\n",
    "            for smarts_mol in smarts_mols:\n",
    "                if mol.HasSubstructMatch(smarts_mol):\n",
    "                    motif_count += 1\n",
    "            \n",
    "            if motif_count > max_covalent_motifs:\n",
    "                filter_results['covalent'].append(pert_id)\n",
    "                continue\n",
    "            \n",
    "            # Filter 8.3: NIBRæ ‡è®°\n",
    "            if self.nibr_catalog and self.nibr_catalog.GetNumEntries() > 0:\n",
    "                matches = self.nibr_catalog.GetMatches(mol)\n",
    "                \n",
    "                if len(matches) > 0:\n",
    "                    severity_scores = []\n",
    "                    for entry in matches:\n",
    "                        desc = entry.GetDescription()\n",
    "                        parts = desc.split('__')\n",
    "                        if len(parts) >= 2:\n",
    "                            severity = int(parts[1])\n",
    "                            severity_scores.append(severity)\n",
    "                    \n",
    "                    if 2 in severity_scores:\n",
    "                        flag_count = 10\n",
    "                    else:\n",
    "                        flag_count = sum(1 for s in severity_scores if s == 1)\n",
    "                    \n",
    "                    if flag_count > max_nibr_flags:\n",
    "                        filter_results['nibr'].append(pert_id)\n",
    "                        continue\n",
    "            \n",
    "            # Filter 8.4: BRENK\n",
    "            brenk_matches = self.brenk_catalog.GetMatches(mol)\n",
    "            if len(brenk_matches) > 0:\n",
    "                filter_results['brenk'].append(pert_id)\n",
    "                continue\n",
    "            \n",
    "            # é€šè¿‡æ‰€æœ‰è¿‡æ»¤å™¨\n",
    "            passed_compounds.add(pert_id)\n",
    "        \n",
    "        # åº”ç”¨è¿‡æ»¤\n",
    "        chem_mask = metadata['pert_id'].isin(passed_compounds)\n",
    "        \n",
    "        matrix = matrix[chem_mask]\n",
    "        metadata = metadata[chem_mask].reset_index(drop=True)\n",
    "        \n",
    "        final_samples = len(metadata)\n",
    "        final_compounds = metadata['pert_id'].nunique()\n",
    "        \n",
    "        self.logger.info(f\"\\n   Chemical Filter Results:\")\n",
    "        self.logger.info(f\"     Passed all filters: {len(passed_compounds):,}/{len(training_compounds):,}\")\n",
    "        self.logger.info(f\"     Failed MW filter: {len(filter_results['mw']):,}\")\n",
    "        self.logger.info(f\"     Failed covalent filter: {len(filter_results['covalent']):,}\")\n",
    "        self.logger.info(f\"     Failed NIBR filter: {len(filter_results['nibr']):,}\")\n",
    "        self.logger.info(f\"     Failed BRENK filter: {len(filter_results['brenk']):,}\")\n",
    "        self.logger.info(f\"\\n   Removed: {initial_samples - final_samples:,} samples\")\n",
    "        self.logger.info(f\"   Remaining: {final_samples:,} samples, {final_compounds:,} compounds\")\n",
    "        \n",
    "        stats = {\n",
    "            'initial_samples': initial_samples,\n",
    "            'final_samples': final_samples,\n",
    "            'removed_samples': initial_samples - final_samples,\n",
    "            'initial_compounds': initial_compounds,\n",
    "            'final_compounds': final_compounds,\n",
    "            'passed_compounds': len(passed_compounds),\n",
    "            'failed_mw': len(filter_results['mw']),\n",
    "            'failed_covalent': len(filter_results['covalent']),\n",
    "            'failed_nibr': len(filter_results['nibr']),\n",
    "            'failed_brenk': len(filter_results['brenk'])\n",
    "        }\n",
    "        \n",
    "        # ä¿å­˜checkpoint\n",
    "        data = {'matrix': matrix, 'metadata': metadata, 'stats': stats}\n",
    "        self.checkpoint_mgr.save_checkpoint(filter_name, filter_params, data, stats)\n",
    "        \n",
    "        gc.collect()\n",
    "        return matrix, metadata, stats\n",
    "    \n",
    "    # ========== ä¸»æµç¨‹æ–¹æ³• ==========\n",
    "    \n",
    "    def run_full_pipeline(\n",
    "        self,\n",
    "        # æ•°æ®è´¨é‡æ§åˆ¶å‚æ•°\n",
    "        remove_dos: bool = True,\n",
    "        min_observations: int = 5,\n",
    "        min_similarity: float = 0.12,\n",
    "        dose_range: Tuple[float, float] = (1.0, 20.0),\n",
    "        dose_bins: Optional[List[float]] = None,\n",
    "        valid_timepoints: List[int] = [6, 24],\n",
    "        min_compounds_per_cell: int = 200,\n",
    "        min_cell_lines: int = 5,\n",
    "        max_cell_lines: int = 40,\n",
    "        # åŒ–å­¦è¿‡æ»¤å‚æ•°\n",
    "        apply_chemical_filters: bool = True,\n",
    "        min_mw: float = 60,\n",
    "        max_mw: float = 1000,\n",
    "        max_covalent_motifs: int = 1,\n",
    "        max_nibr_flags: int = 9,\n",
    "        nibr_csv_path: Optional[str] = None,\n",
    "        # è¾“å‡ºå‚æ•°\n",
    "        output_path: Optional[str] = None\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        è¿è¡Œå®Œæ•´çš„è¿‡æ»¤ç®¡é“\n",
    "        \n",
    "        å‚æ•°:\n",
    "            # æ•°æ®è´¨é‡æ§åˆ¶\n",
    "            remove_dos: æ˜¯å¦ç§»é™¤DOSåŒ–åˆç‰©\n",
    "            min_observations: æ¯ä¸ªåŒ–åˆç‰©çš„æœ€å°è§‚æµ‹æ•°\n",
    "            min_similarity: æœ€å°ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "            dose_range: å‰‚é‡èŒƒå›´ï¼ˆÂµMï¼‰\n",
    "            standard_dose: æ ‡å‡†å‰‚é‡ï¼ˆÂµMï¼‰\n",
    "            valid_timepoints: æœ‰æ•ˆæ—¶é—´ç‚¹ï¼ˆå°æ—¶ï¼‰\n",
    "            min_compounds_per_cell: æ¯ä¸ªç»†èƒç³»çš„æœ€å°åŒ–åˆç‰©æ•°\n",
    "            min_cell_lines: æ¯ä¸ªåŒ–åˆç‰©çš„æœ€å°ç»†èƒç³»æ•°\n",
    "            max_cell_lines: æ¯ä¸ªåŒ–åˆç‰©çš„æœ€å¤§ç»†èƒç³»æ•°\n",
    "            \n",
    "            # åŒ–å­¦è¿‡æ»¤\n",
    "            apply_chemical_filters: æ˜¯å¦åº”ç”¨åŒ–å­¦è¿‡æ»¤å™¨\n",
    "            min_mw: æœ€å°åˆ†å­é‡\n",
    "            max_mw: æœ€å¤§åˆ†å­é‡\n",
    "            max_covalent_motifs: æœ€å¤§å…±ä»·åŸºå›¢æ•°\n",
    "            max_nibr_flags: æœ€å¤§NIBRæ ‡è®°æ•°\n",
    "            nibr_csv_path: NIBRè¿‡æ»¤å™¨CSVè·¯å¾„\n",
    "            \n",
    "            # è¾“å‡º\n",
    "            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„\n",
    "        \n",
    "        è¿”å›:\n",
    "            è®­ç»ƒæ•°æ®å­—å…¸\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"\\n{'='*80}\")\n",
    "        self.logger.info(f\"ğŸš€ Starting Full Filter Pipeline\")\n",
    "        self.logger.info(f\"{'='*80}\\n\")\n",
    "        \n",
    "        # åŠ è½½å…ƒæ•°æ®\n",
    "        self.logger.info(\"Step 1: Loading metadata...\")\n",
    "        self.load_gene_info()\n",
    "        self.load_cell_info()\n",
    "        self.load_compound_info()\n",
    "        \n",
    "        # åŠ è½½Level 4æ•°æ®\n",
    "        self.logger.info(\"\\nStep 2: Loading Level 4 signatures...\")\n",
    "        matrix, metadata, gene_meta = self.load_level4_signatures()\n",
    "        \n",
    "        initial_samples = len(matrix)\n",
    "        initial_compounds = metadata['pert_id'].nunique()\n",
    "        \n",
    "        self.logger.info(f\"\\nInitial dataset:\")\n",
    "        self.logger.info(f\"  Samples: {initial_samples:,}\")\n",
    "        self.logger.info(f\"  Compounds: {initial_compounds:,}\")\n",
    "        \n",
    "        # åº”ç”¨è¿‡æ»¤å™¨\n",
    "        self.logger.info(\"\\nStep 3: Applying filters...\")\n",
    "        \n",
    "        matrix, metadata, _ = self.filter_1_dos_removal(matrix, metadata, remove_dos)\n",
    "        matrix, metadata, _ = self.filter_2_min_observations(matrix, metadata, min_observations)\n",
    "        matrix, metadata, _ = self.filter_3_cosine_similarity(matrix, metadata, min_similarity)\n",
    "        matrix, metadata, _ = self.filter_4_dose_selection(matrix, metadata, dose_range, dose_bins)\n",
    "        matrix, metadata, _ = self.filter_5_timepoint_selection(matrix, metadata, valid_timepoints)\n",
    "        matrix, metadata, _ = self.filter_6_min_compounds_per_cell(matrix, metadata, min_compounds_per_cell)\n",
    "        matrix, metadata, _ = self.filter_7_cell_line_count(matrix, metadata, min_cell_lines, max_cell_lines)\n",
    "        \n",
    "        if apply_chemical_filters:\n",
    "            matrix, metadata, _ = self.filter_8_chemical_filters(\n",
    "                matrix, metadata, min_mw, max_mw, max_covalent_motifs, max_nibr_flags, nibr_csv_path\n",
    "            )\n",
    "        \n",
    "        # åˆ›å»ºè®­ç»ƒæ•°æ®\n",
    "        self.logger.info(f\"\\n{'='*80}\")\n",
    "        self.logger.info(f\"âœ… Filtering Complete\")\n",
    "        self.logger.info(f\"{'='*80}\")\n",
    "        \n",
    "        final_samples = len(matrix)\n",
    "        final_compounds = metadata['pert_id'].nunique()\n",
    "        final_cells = metadata['cell_iname'].nunique() if 'cell_iname' in metadata.columns else 'Unknown'\n",
    "        \n",
    "        self.logger.info(f\"Final dataset:\")\n",
    "        self.logger.info(f\"  Samples: {final_samples:,} ({final_samples/initial_samples*100:.1f}%)\")\n",
    "        self.logger.info(f\"  Compounds: {final_compounds:,} ({final_compounds/initial_compounds*100:.1f}%)\")\n",
    "        self.logger.info(f\"  Cell lines: {final_cells}\")\n",
    "        self.logger.info(f\"  Gene features: {matrix.shape[1]}\")\n",
    "        \n",
    "        # åˆ›å»ºæ ‡ç­¾å’Œcompoundæ˜ å°„\n",
    "        unique_perts = sorted(metadata['pert_id'].unique())\n",
    "        pert_to_idx = {pert: idx for idx, pert in enumerate(unique_perts)}\n",
    "        labels = np.array([pert_to_idx[p] for p in metadata['pert_id']], dtype=np.int32)\n",
    "        \n",
    "        # è·å–åŸºå› å\n",
    "        if self.gene_info is not None and hasattr(self, \"landmark_col_indices\"):\n",
    "            landmark_mask = self.gene_info['feature_space'] == 'landmark'\n",
    "            landmark_geneinfo = self.gene_info[landmark_mask]\n",
    "            gene_names = list(landmark_geneinfo['gene_symbol'].values)\n",
    "        else:\n",
    "            gene_name_col = gene_meta.columns[0]\n",
    "            gene_names = list(gene_meta[gene_name_col].values)\n",
    "        \n",
    "        # åˆ›å»º3æŠ˜åˆ’åˆ†\n",
    "        self.logger.info(f\"\\nCreating 3-fold splits...\")\n",
    "        np.random.seed(42)\n",
    "        folds = np.zeros(len(metadata), dtype=np.int32)\n",
    "        \n",
    "        for pert_id in metadata['pert_id'].unique():\n",
    "            pert_mask = metadata['pert_id'] == pert_id\n",
    "            pert_indices = np.where(pert_mask)[0]\n",
    "            \n",
    "            np.random.shuffle(pert_indices)\n",
    "            n_samples = len(pert_indices)\n",
    "            \n",
    "            fold_sizes = [n_samples // 3] * 3\n",
    "            for i in range(n_samples % 3):\n",
    "                fold_sizes[i] += 1\n",
    "            \n",
    "            start_idx = 0\n",
    "            for fold_id, size in enumerate(fold_sizes):\n",
    "                end_idx = start_idx + size\n",
    "                folds[pert_indices[start_idx:end_idx]] = fold_id\n",
    "                start_idx = end_idx\n",
    "        \n",
    "        self.logger.info(f\"Fold statistics:\")\n",
    "        for fold_id in range(3):\n",
    "            fold_mask = folds == fold_id\n",
    "            n_samples = fold_mask.sum()\n",
    "            n_compounds = metadata[fold_mask]['pert_id'].nunique()\n",
    "            self.logger.info(f\"  Fold {fold_id}: {n_samples:,} samples, {n_compounds:,} compounds\")\n",
    "        \n",
    "        # æ„å»ºè®­ç»ƒæ•°æ®å­—å…¸\n",
    "        training_data = {\n",
    "            'X': matrix,\n",
    "            'y': labels,\n",
    "            'folds': folds,\n",
    "            'sample_meta': metadata,\n",
    "            'metadata': metadata,\n",
    "            'gene_names': gene_names,\n",
    "            'compound_names': list(unique_perts),\n",
    "            'pert_to_idx': pert_to_idx\n",
    "        }\n",
    "        \n",
    "        # ä¿å­˜\n",
    "        if output_path is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            output_path = self.data_dir / \"processed_data\" / f\"training_data_{timestamp}.pkl\"\n",
    "        \n",
    "        output_path = Path(output_path)\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        self.logger.info(f\"\\nğŸ’¾ Saving to: {output_path}\")\n",
    "        with open(output_path, 'wb') as f:\n",
    "            pickle.dump(training_data, f, protocol=4)\n",
    "        \n",
    "        file_size_mb = output_path.stat().st_size / (1024**2)\n",
    "        self.logger.info(f\"âœ“ Saved successfully ({file_size_mb:.1f} MB)\")\n",
    "        \n",
    "        # ä¸è®ºæ–‡å¯¹æ¯”\n",
    "        self.logger.info(f\"\\nğŸ“Š Comparison with DrugReflector paper:\")\n",
    "        self.logger.info(f\"  Paper: 425,242 obs, 9,597 compounds, 52 cells\")\n",
    "        self.logger.info(f\"  Ours:  {final_samples:,} obs, {final_compounds:,} compounds, {final_cells} cells\")\n",
    "        \n",
    "        self.logger.info(f\"\\n{'='*80}\")\n",
    "        self.logger.info(f\"âœ… Pipeline Complete!\")\n",
    "        self.logger.info(f\"{'='*80}\")\n",
    "        \n",
    "        return training_data\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"ä¸»ç¨‹åº\"\"\"\n",
    "    # é…ç½®è·¯å¾„\n",
    "    data_dir = \"D:/ç§‘ç ”/Models/drugreflector/datasets/LINCS2020\"\n",
    "    output_dir = \"D:/ç§‘ç ”/Models/drugreflector/processed_data\"\n",
    "    \n",
    "    # åˆ›å»ºè¿‡æ»¤ç®¡é“\n",
    "    pipeline = LINCS2020FilterPipeline(\n",
    "        data_dir=data_dir,\n",
    "        cache_dir=Path(data_dir) / \"filter_cache\",\n",
    "        overwrite=True  # è®¾ç½®ä¸ºFalseå¯ä»¥ä½¿ç”¨ç¼“å­˜\n",
    "    )\n",
    "    \n",
    "    # è¿è¡Œå®Œæ•´ç®¡é“\n",
    "    try:\n",
    "        training_data = pipeline.run_full_pipeline(\n",
    "            # æ•°æ®è´¨é‡æ§åˆ¶\n",
    "            remove_dos=True,\n",
    "            min_observations=5,\n",
    "            min_similarity=0.12,\n",
    "            dose_range=(1.0, 20.0),\n",
    "            dose_bins=[1.0, 20.0],\n",
    "            valid_timepoints=[6, 24],\n",
    "            min_compounds_per_cell=270,\n",
    "            min_cell_lines=5,\n",
    "            max_cell_lines=200,\n",
    "            # åŒ–å­¦è¿‡æ»¤\n",
    "            apply_chemical_filters=True,\n",
    "            min_mw=60,\n",
    "            max_mw=1000,\n",
    "            max_covalent_motifs=1,\n",
    "            max_nibr_flags=9,\n",
    "            nibr_csv_path=None,  # è‡ªåŠ¨æŸ¥æ‰¾\n",
    "            # è¾“å‡º\n",
    "            output_path=Path(output_dir) / \"training_data_unified_pipeline.pkl\"\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nğŸ¯ Ready for training!\")\n",
    "        return training_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"âŒ ERROR\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Type: {type(e).__name__}\")\n",
    "        print(f\"Message: {e}\")\n",
    "        \n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    training_data = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb3c9dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ§¬ Adding Level 3 Expression Data (X_post)\n",
      "================================================================================\n",
      "\n",
      "ğŸ“– Loading input data: training_data_lincs2020_chemfiltered_1201_l_dual_head.pkl...\n",
      "   âœ“ Current keys: ['X', 'y', 'folds', 'sample_meta', 'metadata', 'gene_names', 'compound_names', 'pert_to_idx', 'X_ctx', 'X_ctx_embeddings']\n",
      "   âœ“ Samples: 509,006\n",
      "   âœ“ Genes: 978\n",
      "   âœ“ X_ctx already exists: (509006, 978)\n",
      "\n",
      "   Using column 'sample_id' for sample matching\n",
      "   Target samples: 509,006\n",
      "   âœ“ Landmark genes: 978\n",
      "\n",
      "ğŸ“– Reading GCTX metadata from: level3_beta_trt_cp_n1805898x12328.gctx...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to synchronously open file (unable to open file: name = 'D:\\ç§‘ç ”\\Models\\drugreflector\\datasets\\LINCS2020\\level3_beta_trt_cp_n1805898x12328.gctx', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 262\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   â€¢ X_post: Level 3 treatment expression (for reconstruction)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 262\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 217\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    214\u001b[0m landmark_indices \u001b[38;5;241m=\u001b[39m load_landmark_indices()\n\u001b[0;32m    216\u001b[0m \u001b[38;5;66;03m# ========== 4. è¯»å– Level 3 å…ƒæ•°æ® ==========\u001b[39;00m\n\u001b[1;32m--> 217\u001b[0m _, is_transposed \u001b[38;5;241m=\u001b[39m \u001b[43mread_level3_gctx_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLEVEL3_TREATMENT_FILE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;66;03m# ========== 5. æå– Level 3 è¡¨è¾¾æ•°æ® ==========\u001b[39;00m\n\u001b[0;32m    220\u001b[0m X_post \u001b[38;5;241m=\u001b[39m read_level3_treatment_data(\n\u001b[0;32m    221\u001b[0m     LEVEL3_TREATMENT_FILE,\n\u001b[0;32m    222\u001b[0m     target_sample_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    225\u001b[0m     chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50000\u001b[39m\n\u001b[0;32m    226\u001b[0m )\n",
      "Cell \u001b[1;32mIn[2], line 43\u001b[0m, in \u001b[0;36mread_level3_gctx_metadata\u001b[1;34m(gctx_path)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"è¯»å– GCTX çš„æ ·æœ¬å…ƒæ•°æ®ï¼ˆsample_idï¼‰\"\"\"\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mğŸ“– Reading GCTX metadata from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgctx_path\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mh5py\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgctx_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m# æ£€æŸ¥æ•°æ®ç»“æ„\u001b[39;00m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/0/META/ROW/id\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;66;03m# æ ‡å‡† LINCS GCTX ç»“æ„ï¼šROW=samples, COL=genes\u001b[39;00m\n\u001b[0;32m     47\u001b[0m         sample_ids \u001b[38;5;241m=\u001b[39m f[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/0/META/ROW/id\u001b[39m\u001b[38;5;124m'\u001b[39m][()]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n",
      "File \u001b[1;32md:\\ProgramFiles\\Anaconda\\envs\\general\\Lib\\site-packages\\h5py\\_hl\\files.py:564\u001b[0m, in \u001b[0;36mFile.__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[0;32m    555\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[0;32m    556\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[0;32m    557\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[0;32m    558\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[0;32m    559\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[0;32m    560\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    561\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[0;32m    562\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[0;32m    563\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[1;32m--> 564\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    567\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[1;32md:\\ProgramFiles\\Anaconda\\envs\\general\\Lib\\site-packages\\h5py\\_hl\\files.py:238\u001b[0m, in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[0;32m    237\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[1;32m--> 238\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    240\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[1;32mh5py\\\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\h5f.pyx:102\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to synchronously open file (unable to open file: name = 'D:\\ç§‘ç ”\\Models\\drugreflector\\datasets\\LINCS2020\\level3_beta_trt_cp_n1805898x12328.gctx', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ä¸ºå·²å¤„ç†çš„æ•°æ®æ·»åŠ  Level 3 è¡¨è¾¾å€¼\n",
    "- X: Level 4 Z-score (å·²æœ‰)\n",
    "- X_ctx: Level 3 control baseline (å·²æœ‰)\n",
    "- X_post: Level 3 treatment åŸå§‹è¡¨è¾¾å€¼ (æ–°å¢)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import h5py\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ================= é…ç½®è·¯å¾„ =================\n",
    "DATA_DIR = Path(r\"D:/ç§‘ç ”/Models/drugreflector/datasets/LINCS2020\")\n",
    "PROCESSED_DIR = Path(r\"D:/ç§‘ç ”/Models/drugreflector/processed_data\")\n",
    "\n",
    "# è¾“å…¥ï¼šå·²åŒ…å« X_ctx çš„æ•°æ®\n",
    "INPUT_FILE = PROCESSED_DIR / \"training_data_lincs2020_chemfiltered_1201_l_dual_head.pkl\"\n",
    "\n",
    "# è¾“å‡ºï¼šæ·»åŠ  X_post åçš„æ•°æ®\n",
    "OUTPUT_FILE = PROCESSED_DIR / \"training_data_lincs2020_chemfiltered_1201_l_triple_head.pkl\"\n",
    "\n",
    "# Level 3 treatment GCTX æ–‡ä»¶\n",
    "LEVEL3_TREATMENT_FILE = DATA_DIR / \"level3_beta_trt_cp_n1805898x12328.gctx\"\n",
    "\n",
    "\n",
    "def load_landmark_indices():\n",
    "    \"\"\"åŠ è½½ landmark åŸºå› çš„åˆ—ç´¢å¼•\"\"\"\n",
    "    gene_path = DATA_DIR / \"geneinfo_beta.txt\"\n",
    "    gene_info = pd.read_csv(gene_path, sep='\\t')\n",
    "    landmark_indices = np.where(gene_info['feature_space'] == 'landmark')[0]\n",
    "    print(f\"   âœ“ Landmark genes: {len(landmark_indices)}\")\n",
    "    return sorted(landmark_indices)\n",
    "\n",
    "\n",
    "def read_level3_gctx_metadata(gctx_path):\n",
    "    \"\"\"è¯»å– GCTX çš„æ ·æœ¬å…ƒæ•°æ®ï¼ˆsample_idï¼‰\"\"\"\n",
    "    print(f\"\\nğŸ“– Reading GCTX metadata from: {gctx_path.name}...\")\n",
    "    \n",
    "    with h5py.File(gctx_path, 'r') as f:\n",
    "        # æ£€æŸ¥æ•°æ®ç»“æ„\n",
    "        if '/0/META/ROW/id' in f:\n",
    "            # æ ‡å‡† LINCS GCTX ç»“æ„ï¼šROW=samples, COL=genes\n",
    "            sample_ids = f['/0/META/ROW/id'][()].astype(str)\n",
    "            data_shape = f['/0/DATA/0/matrix'].shape\n",
    "            print(f\"   âœ“ Format: Samples x Genes\")\n",
    "            print(f\"   âœ“ Matrix shape: {data_shape}\")\n",
    "            is_transposed = False\n",
    "        elif '/0/META/COL/id' in f:\n",
    "            # è½¬ç½®æ ¼å¼ï¼šCOL=samples, ROW=genes\n",
    "            sample_ids = f['/0/META/COL/id'][()].astype(str)\n",
    "            data_shape = f['/0/DATA/0/matrix'].shape\n",
    "            print(f\"   âœ“ Format: Genes x Samples (transposed)\")\n",
    "            print(f\"   âœ“ Matrix shape: {data_shape}\")\n",
    "            is_transposed = True\n",
    "        else:\n",
    "            raise ValueError(\"Cannot find sample IDs in GCTX file\")\n",
    "    \n",
    "    return sample_ids, is_transposed\n",
    "\n",
    "\n",
    "def read_level3_treatment_data(\n",
    "    gctx_path,\n",
    "    target_sample_ids,\n",
    "    landmark_indices,\n",
    "    is_transposed=False,\n",
    "    chunk_size=50000\n",
    "):\n",
    "    \"\"\"\n",
    "    ä» Level 3 GCTX ä¸­è¯»å–æŒ‡å®šæ ·æœ¬çš„ landmark åŸºå› è¡¨è¾¾å€¼\n",
    "    \n",
    "    å‚æ•°:\n",
    "        gctx_path: GCTX æ–‡ä»¶è·¯å¾„\n",
    "        target_sample_ids: éœ€è¦æå–çš„æ ·æœ¬IDåˆ—è¡¨ï¼ˆæŒ‰é¡ºåºï¼‰\n",
    "        landmark_indices: landmark åŸºå› çš„åˆ—ç´¢å¼•\n",
    "        is_transposed: æ•°æ®æ˜¯å¦ä¸º Genes x Samples æ ¼å¼\n",
    "        chunk_size: åˆ†å—è¯»å–å¤§å°ï¼ˆæ ·æœ¬æ•°ï¼‰\n",
    "    \n",
    "    è¿”å›:\n",
    "        X_post: (N_samples, N_genes) çš„ Level 3 è¡¨è¾¾çŸ©é˜µ\n",
    "    \"\"\"\n",
    "    print(f\"\\nâš¡ Extracting Level 3 expression data...\")\n",
    "    print(f\"   Target samples: {len(target_sample_ids):,}\")\n",
    "    print(f\"   Landmark genes: {len(landmark_indices)}\")\n",
    "    \n",
    "    with h5py.File(gctx_path, 'r') as f:\n",
    "        # è¯»å–æ‰€æœ‰æ ·æœ¬ID\n",
    "        if is_transposed:\n",
    "            all_sample_ids = f['/0/META/COL/id'][()].astype(str)\n",
    "        else:\n",
    "            all_sample_ids = f['/0/META/ROW/id'][()].astype(str)\n",
    "        \n",
    "        # åˆ›å»ºæ ·æœ¬IDåˆ°ç´¢å¼•çš„æ˜ å°„\n",
    "        print(f\"   Creating sample ID index...\")\n",
    "        sample_id_to_idx = {sid: idx for idx, sid in enumerate(all_sample_ids)}\n",
    "        \n",
    "        # æ‰¾åˆ°ç›®æ ‡æ ·æœ¬åœ¨ GCTX ä¸­çš„è¡Œç´¢å¼•\n",
    "        target_indices = []\n",
    "        missing_samples = []\n",
    "        \n",
    "        for sid in tqdm(target_sample_ids, desc=\"   Mapping sample IDs\"):\n",
    "            if sid in sample_id_to_idx:\n",
    "                target_indices.append(sample_id_to_idx[sid])\n",
    "            else:\n",
    "                target_indices.append(-1)  # æ ‡è®°ä¸ºç¼ºå¤±\n",
    "                missing_samples.append(sid)\n",
    "        \n",
    "        target_indices = np.array(target_indices)\n",
    "        \n",
    "        if missing_samples:\n",
    "            print(f\"   âš ï¸  Warning: {len(missing_samples):,} samples not found in Level 3 data\")\n",
    "            print(f\"      (Will fill with zeros)\")\n",
    "        \n",
    "        # å‡†å¤‡è¾“å‡ºçŸ©é˜µ\n",
    "        n_samples = len(target_sample_ids)\n",
    "        n_genes = len(landmark_indices)\n",
    "        X_post = np.zeros((n_samples, n_genes), dtype=np.float32)\n",
    "        \n",
    "        # è®¿é—®æ•°æ®é›†\n",
    "        ds = f['/0/DATA/0/matrix']\n",
    "        \n",
    "        if is_transposed:\n",
    "            # æ ¼å¼: Genes x Samples\n",
    "            print(f\"   Reading transposed data (Genes x Samples)...\")\n",
    "            \n",
    "            # å…ˆè¯»å–æ‰€æœ‰ landmark åŸºå› çš„è¡Œ\n",
    "            print(f\"   Loading landmark genes...\")\n",
    "            landmark_data = ds[landmark_indices, :].astype(np.float32)  # (N_genes, All_samples)\n",
    "            \n",
    "            # ç„¶åæŒ‰åˆ—ç´¢å¼•æå–ç›®æ ‡æ ·æœ¬\n",
    "            print(f\"   Extracting target samples...\")\n",
    "            valid_mask = target_indices >= 0\n",
    "            valid_indices = target_indices[valid_mask]\n",
    "            \n",
    "            X_post[valid_mask] = landmark_data[:, valid_indices].T  # è½¬ç½®ä¸º (N_samples, N_genes)\n",
    "            \n",
    "        else:\n",
    "            # æ ¼å¼: Samples x Genes\n",
    "            print(f\"   Reading data (Samples x Genes)...\")\n",
    "            \n",
    "            # åˆ†æ‰¹è¯»å–æ ·æœ¬\n",
    "            valid_mask = target_indices >= 0\n",
    "            valid_indices = target_indices[valid_mask]\n",
    "            valid_positions = np.where(valid_mask)[0]\n",
    "            \n",
    "            # æŒ‰ chunk è¯»å–\n",
    "            n_valid = len(valid_indices)\n",
    "            n_chunks = (n_valid + chunk_size - 1) // chunk_size\n",
    "            \n",
    "            for chunk_idx in tqdm(range(n_chunks), desc=\"   Reading chunks\"):\n",
    "                start_idx = chunk_idx * chunk_size\n",
    "                end_idx = min(start_idx + chunk_size, n_valid)\n",
    "                \n",
    "                chunk_gctx_indices = valid_indices[start_idx:end_idx]\n",
    "                chunk_output_positions = valid_positions[start_idx:end_idx]\n",
    "                \n",
    "                # è¯»å–è¿™æ‰¹æ ·æœ¬çš„ landmark åŸºå› \n",
    "                # æ³¨æ„ï¼šå¯¹äºä¸è¿ç»­çš„è¡Œç´¢å¼•ï¼Œéœ€è¦é€è¡Œè¯»å–æˆ–ä½¿ç”¨ fancy indexing\n",
    "                chunk_data = ds[chunk_gctx_indices][:, landmark_indices].astype(np.float32)\n",
    "                \n",
    "                X_post[chunk_output_positions] = chunk_data\n",
    "        \n",
    "        print(f\"   âœ“ Extracted Level 3 data: {X_post.shape}\")\n",
    "        print(f\"   âœ“ Data range: [{X_post.min():.3f}, {X_post.max():.3f}]\")\n",
    "        print(f\"   âœ“ Mean: {X_post.mean():.3f}, Std: {X_post.std():.3f}\")\n",
    "        \n",
    "        # ç»Ÿè®¡éé›¶æ ·æœ¬\n",
    "        non_zero_samples = (X_post.sum(axis=1) != 0).sum()\n",
    "        print(f\"   âœ“ Non-zero samples: {non_zero_samples:,} / {n_samples:,}\")\n",
    "        \n",
    "    return X_post\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"ğŸ§¬ Adding Level 3 Expression Data (X_post)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # ========== 1. åŠ è½½å·²å¤„ç†çš„æ•°æ® ==========\n",
    "    print(f\"\\nğŸ“– Loading input data: {INPUT_FILE.name}...\")\n",
    "    with open(INPUT_FILE, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    print(f\"   âœ“ Current keys: {list(data.keys())}\")\n",
    "    print(f\"   âœ“ Samples: {len(data['X']):,}\")\n",
    "    print(f\"   âœ“ Genes: {data['X'].shape[1]}\")\n",
    "    \n",
    "    # æ£€æŸ¥æ˜¯å¦å·²æœ‰ X_ctx\n",
    "    if 'X_ctx' in data:\n",
    "        print(f\"   âœ“ X_ctx already exists: {data['X_ctx'].shape}\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸  X_ctx not found. Please run the baseline generation script first.\")\n",
    "        return\n",
    "    \n",
    "    # ========== 2. è·å–æ ·æœ¬IDåˆ—è¡¨ ==========\n",
    "    sample_meta = data['sample_meta']\n",
    "    \n",
    "    # ç¡®å®šæ ·æœ¬IDåˆ—å\n",
    "    if 'sample_id' in sample_meta.columns:\n",
    "        sample_id_col = 'sample_id'\n",
    "    elif 'id' in sample_meta.columns:\n",
    "        sample_id_col = 'id'\n",
    "    else:\n",
    "        raise ValueError(\"Cannot find sample_id column in metadata\")\n",
    "    \n",
    "    target_sample_ids = sample_meta[sample_id_col].values\n",
    "    print(f\"\\n   Using column '{sample_id_col}' for sample matching\")\n",
    "    print(f\"   Target samples: {len(target_sample_ids):,}\")\n",
    "    \n",
    "    # ========== 3. åŠ è½½ landmark åŸºå› ç´¢å¼• ==========\n",
    "    landmark_indices = load_landmark_indices()\n",
    "    \n",
    "    # ========== 4. è¯»å– Level 3 å…ƒæ•°æ® ==========\n",
    "    _, is_transposed = read_level3_gctx_metadata(LEVEL3_TREATMENT_FILE)\n",
    "    \n",
    "    # ========== 5. æå– Level 3 è¡¨è¾¾æ•°æ® ==========\n",
    "    X_post = read_level3_treatment_data(\n",
    "        LEVEL3_TREATMENT_FILE,\n",
    "        target_sample_ids,\n",
    "        landmark_indices,\n",
    "        is_transposed=is_transposed,\n",
    "        chunk_size=50000\n",
    "    )\n",
    "    \n",
    "    # ========== 6. æ·»åŠ åˆ°æ•°æ®å­—å…¸å¹¶ä¿å­˜ ==========\n",
    "    data['X_post'] = X_post\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ğŸ“Š Final Dataset Summary\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"  X (Level 4 Z-score):       {data['X'].shape}, dtype={data['X'].dtype}\")\n",
    "    if 'X_ctx' in data:\n",
    "        print(f\"  X_ctx (Level 3 baseline):  {data['X_ctx'].shape}, dtype={data['X_ctx'].dtype}\")\n",
    "    print(f\"  X_post (Level 3 treatment): {data['X_post'].shape}, dtype={data['X_post'].dtype}\")\n",
    "    print(f\"  Samples: {len(sample_meta):,}\")\n",
    "    print(f\"  Compounds: {len(data['compound_names']):,}\")\n",
    "    \n",
    "    # ä¿å­˜\n",
    "    print(f\"\\nğŸ’¾ Saving to: {OUTPUT_FILE}...\")\n",
    "    OUTPUT_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(OUTPUT_FILE, 'wb') as f:\n",
    "        pickle.dump(data, f, protocol=4)\n",
    "    \n",
    "    file_size_mb = OUTPUT_FILE.stat().st_size / (1024**2)\n",
    "    print(f\"   âœ“ Saved successfully ({file_size_mb:.1f} MB)\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"âœ… Complete!\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"ğŸ“ Output file: {OUTPUT_FILE}\")\n",
    "    print(f\"\\nğŸ¯ Dataset now contains:\")\n",
    "    print(f\"   â€¢ X: Level 4 Z-scores (for main prediction)\")\n",
    "    print(f\"   â€¢ X_ctx: Level 3 control baseline (for context)\")\n",
    "    print(f\"   â€¢ X_post: Level 3 treatment expression (for reconstruction)\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
