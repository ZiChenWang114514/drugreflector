{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7682d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸš€ LOADING FROM CACHED FILES\n",
      "================================================================================\n",
      "âœ“ Found cached file: GSE92742_Broad_LINCS_Level4_ZSPCINF_mlr12k_n1319138x12328.gctx\n",
      "\n",
      "Loading gene and cell information...\n",
      "Total genes: 12328\n",
      "Landmark genes: 978\n",
      "Total cell lines: 98\n",
      "Unique cell IDs: 98\n",
      "\n",
      "Reading from cached GCTX file...\n",
      "\n",
      "ğŸ“– Reading GCTX file: GSE92742_Broad_LINCS_Level4_ZSPCINF_mlr12k_n1319138x12328.gctx\n",
      "ğŸ“Š Loading matrix from HDF5...\n",
      "âœ“ Matrix shape: (1319138, 12328)\n",
      "ğŸ“‹ Loading metadata...\n",
      "\n",
      "âœ“ Data loaded successfully:\n",
      "  Matrix: (1319138, 12328) (samples Ã— genes)\n",
      "  Samples: 1319138\n",
      "  Genes: 12328\n",
      "\n",
      "Filtering to landmark genes...\n",
      "âœ“ Filtered matrix shape: (1319138, 978)\n",
      "\n",
      "Applying quality filters (paper-compliant)...\n",
      "\n",
      "================================================================================\n",
      "ğŸ” DRUGREFLECTOR QUALITY CONTROL PIPELINE\n",
      "   Following Science 2025 Supplementary Materials (Page 2)\n",
      "================================================================================\n",
      "Initial samples: 1,319,138\n",
      "\n",
      "[INFO] Parsing metadata from 'id' column...\n",
      "\n",
      "================================================================================\n",
      "FILTER 1: Remove DOS (Diversity-Oriented Synthesis) compounds\n",
      "================================================================================\n",
      "  Removed 138,600 DOS observations\n",
      "  Remaining samples: 1,180,538\n",
      "  Remaining compounds: 173\n",
      "\n",
      "================================================================================\n",
      "FILTER 2: Remove compounds with <5 observations\n",
      "================================================================================\n",
      "  Compounds with â‰¥5 observations: 173/158\n",
      "  Remaining samples: 1,180,538\n",
      "  Remaining compounds: 173\n",
      "\n",
      "================================================================================\n",
      "FILTER 3: Remove observations with cosine similarity <0.12\n",
      "         to closest replicate\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Calculating cosine similarity to nearest replicate...\n",
      "   Processing 173 compounds...\n",
      "   âœ“ Calculated similarities for 1180538 samples\n",
      "   Mean similarity: 0.4597\n",
      "   Median similarity: 0.4369\n",
      "  Removed 1 low-similarity observations\n",
      "  Remaining samples: 1,180,537\n",
      "  Remaining compounds: 173\n",
      "\n",
      "================================================================================\n",
      "FILTER 4: Select most frequent dose in range 1.0-20.0 ÂµM\n",
      "================================================================================\n",
      "  Removed 784,858 observations (invalid or non-modal dose)\n",
      "  Remaining samples: 395,679\n",
      "  Remaining compounds: 173\n",
      "\n",
      "================================================================================\n",
      "FILTER 5: Keep only measurements at ['6h', '24h', '24 h', '24hr', '6 h', '6hr']\n",
      "================================================================================\n",
      "  Removed 216,312 observations (invalid timepoint)\n",
      "  Remaining samples: 179,367\n",
      "  Remaining compounds: 66\n",
      "\n",
      "================================================================================\n",
      "FILTER 6: Remove compounds in <5 or >200 cell lines\n",
      "================================================================================\n",
      "  Compounds in 5-200 cell lines: 24/66\n",
      "  Remaining samples: 127,081\n",
      "  Remaining compounds: 24\n",
      "\n",
      "================================================================================\n",
      "âœ… FINAL DATASET (After all filters)\n",
      "================================================================================\n",
      "  Total samples: 127,081\n",
      "  Total compounds: 24\n",
      "  Cell lines: 63\n",
      "  Gene features: 978\n",
      "  Samples per compound (mean): 5295.0\n",
      "  Samples per compound (median): 4346\n",
      "  Compounds with >100 observations: 24\n",
      "\n",
      "ğŸ“Š Comparison with paper results:\n",
      "  Paper: 425,242 obs, 9,597 compounds, 52 cell lines\n",
      "  Our:   127,081 obs, 24 compounds, 63 cell lines\n",
      "  Compound retention rate: 10.3%\n",
      "\n",
      "================================================================================\n",
      "ğŸ² Creating 3-fold cross-validation splits\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Fold statistics:\n",
      "   Fold 0: 42,366 samples, 24 compounds\n",
      "   Fold 1: 42,361 samples, 24 compounds\n",
      "   Fold 2: 42,354 samples, 24 compounds\n",
      "\n",
      "âœ… Data loaded from cache successfully!\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "LINCSæ•°æ®åŠ è½½å’Œé¢„å¤„ç†è„šæœ¬ - DrugReflectorè®ºæ–‡ä¸¥æ ¼å¤ç°ç‰ˆ\n",
    "å®Œå…¨æŒ‰ç…§Science 2025è¡¥å……ææ–™çš„é¢„å¤„ç†æµç¨‹å®ç°\n",
    "ä¼˜åŒ–ç‰ˆï¼šæå‡è¿è¡Œæ•ˆç‡ï¼Œä½¿ç”¨å‘é‡åŒ–æ“ä½œ\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import gzip\n",
    "import shutil\n",
    "import glob\n",
    "from scipy.spatial.distance import cosine, pdist, squareform\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class LINCSDataLoader:\n",
    "    \"\"\"åŠ è½½å’Œé¢„å¤„ç†LINCS L1000æ•°æ® - ä¸¥æ ¼éµå¾ªDrugReflectorè®ºæ–‡æµç¨‹\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.gene_info = None\n",
    "        self.cell_info = None\n",
    "        self.signatures = None\n",
    "        self.decompressed_files = []\n",
    "        \n",
    "    def load_gene_info(self, dataset='GSE92742'):\n",
    "        \"\"\"åŠ è½½åŸºå› ä¿¡æ¯ï¼Œè·å–978ä¸ªlandmark genes\"\"\"\n",
    "        gene_file = self.data_dir / f\"{dataset}_Broad_LINCS_gene_info.txt.gz\"\n",
    "        \n",
    "        gene_info = pd.read_csv(gene_file, sep='\\t', compression='gzip')\n",
    "        \n",
    "        # ç­›é€‰landmark genes (pr_is_lm == 1)\n",
    "        landmark_genes = gene_info[gene_info['pr_is_lm'] == 1].copy()\n",
    "        \n",
    "        print(f\"Total genes: {len(gene_info)}\")\n",
    "        print(f\"Landmark genes: {len(landmark_genes)}\")\n",
    "        \n",
    "        self.gene_info = landmark_genes\n",
    "        return landmark_genes\n",
    "    \n",
    "    def load_cell_info(self, dataset='GSE92742'):\n",
    "        \"\"\"åŠ è½½ç»†èƒç³»ä¿¡æ¯\"\"\"\n",
    "        cell_file = self.data_dir / f\"{dataset}_Broad_LINCS_cell_info.txt.gz\"\n",
    "        \n",
    "        cell_info = pd.read_csv(cell_file, sep='\\t', compression='gzip')\n",
    "        \n",
    "        print(f\"Total cell lines: {len(cell_info)}\")\n",
    "        print(f\"Unique cell IDs: {cell_info['cell_id'].nunique()}\")\n",
    "        \n",
    "        self.cell_info = cell_info\n",
    "        return cell_info\n",
    "    \n",
    "    def decompress_gzip_file(self, gzip_file):\n",
    "        \"\"\"è§£å‹gzipæ–‡ä»¶åˆ°_decompressedæ–‡ä»¶å¤¹\"\"\"\n",
    "        gzip_file = str(gzip_file)\n",
    "        \n",
    "        decompressed_dir = Path(self.data_dir) / \"_decompressed\"\n",
    "        decompressed_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        original_name = Path(gzip_file).stem\n",
    "        output_path = decompressed_dir / original_name\n",
    "        \n",
    "        if output_path.exists():\n",
    "            print(f\"âœ“ Found existing decompressed file: {output_path.name}\")\n",
    "            self.decompressed_files.append(output_path)\n",
    "            return str(output_path)\n",
    "        \n",
    "        print(f\"ğŸ’¾ Decompressing to: {decompressed_dir}\")\n",
    "        \n",
    "        try:\n",
    "            print(f\"ğŸ“¦ Decompressing {Path(gzip_file).name}...\")\n",
    "            source_size = Path(gzip_file).stat().st_size / (1024**3)\n",
    "            print(f\"   Source size: ~{source_size:.1f} GB\")\n",
    "            \n",
    "            with gzip.open(gzip_file, 'rb') as f_in:\n",
    "                with open(output_path, 'wb') as f_out:\n",
    "                    shutil.copyfileobj(f_in, f_out, length=8*1024*1024)\n",
    "            \n",
    "            output_size = output_path.stat().st_size / (1024**3)\n",
    "            print(f\"âœ“ Decompressed successfully!\")\n",
    "            print(f\"âœ“ Output size: ~{output_size:.1f} GB\")\n",
    "            \n",
    "            self.decompressed_files.append(output_path)\n",
    "            return str(output_path)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"\\nâŒ Decompression failed: {e}\")\n",
    "            if output_path.exists():\n",
    "                try:\n",
    "                    output_path.unlink()\n",
    "                except:\n",
    "                    pass\n",
    "            raise RuntimeError(f\"Failed to decompress {gzip_file}: {e}\")\n",
    "    \n",
    "    def read_gctx(self, gctx_file):\n",
    "        \"\"\"è¯»å–GCTXæ–‡ä»¶ (HDF5æ ¼å¼)\"\"\"\n",
    "        gctx_file = str(gctx_file)\n",
    "        print(f\"\\nğŸ“– Reading GCTX file: {Path(gctx_file).name}\")\n",
    "        \n",
    "        if gctx_file.endswith('.gz'):\n",
    "            print(\"âš ï¸  Detected gzip compressed file\")\n",
    "            gctx_file = self.decompress_gzip_file(gctx_file)\n",
    "        \n",
    "        with h5py.File(gctx_file, 'r') as f:\n",
    "            print(f\"ğŸ“Š Loading matrix from HDF5...\")\n",
    "            matrix = f['/0/DATA/0/matrix'][:]\n",
    "            print(f\"âœ“ Matrix shape: {matrix.shape}\")\n",
    "            \n",
    "            print(f\"ğŸ“‹ Loading metadata...\")\n",
    "            \n",
    "            # è¯»å–åŸºå› å…ƒæ•°æ®ï¼ˆæ¥è‡ª ROWï¼‰\n",
    "            gene_meta = {}\n",
    "            for key in f['/0/META/ROW'].keys():\n",
    "                data = f[f'/0/META/ROW/{key}'][:]\n",
    "                if data.dtype.kind in ['S', 'O']:\n",
    "                    gene_meta[key] = np.char.decode(data.astype('S'), 'utf-8')\n",
    "                else:\n",
    "                    gene_meta[key] = data.astype(str)\n",
    "            \n",
    "            # è¯»å–æ ·æœ¬å…ƒæ•°æ®ï¼ˆæ¥è‡ª COLï¼‰\n",
    "            sample_meta = {}\n",
    "            for key in f['/0/META/COL'].keys():\n",
    "                data = f[f'/0/META/COL/{key}'][:]\n",
    "                if data.dtype.kind in ['S', 'O']:\n",
    "                    sample_meta[key] = np.char.decode(data.astype('S'), 'utf-8')\n",
    "                else:\n",
    "                    sample_meta[key] = data.astype(str)\n",
    "        \n",
    "        sample_df = pd.DataFrame(sample_meta)\n",
    "        gene_df = pd.DataFrame(gene_meta)\n",
    "        \n",
    "        print(f\"\\nâœ“ Data loaded successfully:\")\n",
    "        print(f\"  Matrix: {matrix.shape} (samples Ã— genes)\")\n",
    "        print(f\"  Samples: {len(sample_df)}\")\n",
    "        print(f\"  Genes: {len(gene_df)}\")\n",
    "        \n",
    "        assert matrix.shape[0] == len(sample_df), \\\n",
    "            f\"Matrix rows ({matrix.shape[0]}) != sample metadata ({len(sample_df)})\"\n",
    "        assert matrix.shape[1] == len(gene_df), \\\n",
    "            f\"Matrix cols ({matrix.shape[1]}) != gene metadata ({len(gene_df)})\"\n",
    "        \n",
    "        return matrix, sample_df, gene_df\n",
    "    \n",
    "    def load_level4_signatures(self, dataset='GSE92742'):\n",
    "        \"\"\"åŠ è½½Level 4æ•°æ® (Z-score normalized)\"\"\"\n",
    "        level4_pattern = self.data_dir / f\"{dataset}_Broad_LINCS_Level4_ZSPCINF_mlr12k_n*x12328.gctx.gz\"\n",
    "        \n",
    "        print(f\"ğŸ” Searching for Level 4 file...\")\n",
    "        print(f\"   Pattern: {level4_pattern.name}\")\n",
    "        files = glob.glob(str(level4_pattern))\n",
    "        \n",
    "        if not files:\n",
    "            raise FileNotFoundError(\n",
    "                f\"âŒ No Level 4 file found matching {level4_pattern}\"\n",
    "            )\n",
    "        \n",
    "        level4_file = files[0]\n",
    "        print(f\"âœ“ Found: {Path(level4_file).name}\")\n",
    "        \n",
    "        # è¯»å–GCTX\n",
    "        matrix, sample_meta, gene_meta = self.read_gctx(level4_file)\n",
    "        \n",
    "        # åªä¿ç•™landmark genes\n",
    "        print(f\"\\nğŸ”¬ Filtering to landmark genes...\")\n",
    "        if self.gene_info is None:\n",
    "            self.load_gene_info(dataset)\n",
    "        \n",
    "        landmark_ids = set(self.gene_info['pr_gene_id'].astype(str).values)\n",
    "        print(f\"   Landmark genes to find: {len(landmark_ids)}\")\n",
    "        print(f\"   Total genes in data: {len(gene_meta)}\")\n",
    "        \n",
    "        gene_mask = gene_meta['id'].isin(landmark_ids)\n",
    "        n_matched = gene_mask.sum()\n",
    "        print(f\"   âœ“ Matched: {n_matched} genes\")\n",
    "        \n",
    "        if n_matched == 0:\n",
    "            raise ValueError(\"No landmark genes matched!\")\n",
    "        \n",
    "        print(f\"\\nğŸ¯ Applying filter...\")\n",
    "        matrix = matrix[:, gene_mask]\n",
    "        gene_meta = gene_meta[gene_mask].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"   âœ“ Final matrix shape: {matrix.shape}\")\n",
    "        \n",
    "        self.signatures = {\n",
    "            'matrix': matrix,\n",
    "            'row_meta': sample_meta,\n",
    "            'col_meta': gene_meta\n",
    "        }\n",
    "        \n",
    "        return matrix, sample_meta, gene_meta\n",
    "    \n",
    "    def calculate_cosine_similarity_to_nearest_replicate(\n",
    "        self, \n",
    "        matrix: np.ndarray, \n",
    "        pert_ids: pd.Series\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        è®¡ç®—æ¯ä¸ªæ ·æœ¬ä¸å…¶æœ€è¿‘replicateçš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆå‘é‡åŒ–ä¼˜åŒ–ï¼‰\n",
    "        \n",
    "        åŸæ–‡æ–¹æ³•ï¼ˆSIç¬¬2é¡µï¼‰ï¼š\n",
    "        \"For each compound, remove any observations with a cosine similarity <0.12 \n",
    "        to the closest replicate\"\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        np.ndarray: æ¯ä¸ªæ ·æœ¬ä¸å…¶æœ€è¿‘replicateçš„ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "        \"\"\"\n",
    "        print(f\"\\nğŸ“Š Calculating cosine similarity to nearest replicate...\")\n",
    "        n_samples = len(pert_ids)\n",
    "        nearest_similarities = np.zeros(n_samples)\n",
    "        \n",
    "        unique_perts = pert_ids.unique()\n",
    "        print(f\"   Processing {len(unique_perts)} compounds...\")\n",
    "        \n",
    "        for i, pert_id in enumerate(unique_perts):\n",
    "            if (i + 1) % 1000 == 0:\n",
    "                print(f\"   Progress: {i + 1}/{len(unique_perts)}\")\n",
    "            \n",
    "            pert_mask = pert_ids == pert_id\n",
    "            pert_indices = np.where(pert_mask)[0]\n",
    "            \n",
    "            if len(pert_indices) < 2:\n",
    "                # åªæœ‰ä¸€ä¸ªæ ·æœ¬ï¼Œè®¾ä¸º0ï¼ˆä¼šè¢«è¿‡æ»¤ï¼‰\n",
    "                nearest_similarities[pert_indices] = 0.0\n",
    "                continue\n",
    "            \n",
    "            pert_data = matrix[pert_mask]\n",
    "            \n",
    "            # ä½¿ç”¨sklearnå¿«é€Ÿè®¡ç®—æˆå¯¹ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "            # æ³¨æ„ï¼šcosine_similarityè¿”å›ç›¸ä¼¼åº¦ï¼ˆ1-ä½™å¼¦è·ç¦»ï¼‰\n",
    "            sim_matrix = cosine_similarity(pert_data)\n",
    "            \n",
    "            # å°†å¯¹è§’çº¿è®¾ä¸º-infï¼Œé¿å…æ ·æœ¬ä¸è‡ªå·±æ¯”è¾ƒ\n",
    "            np.fill_diagonal(sim_matrix, -np.inf)\n",
    "            \n",
    "            # å¯¹æ¯ä¸ªæ ·æœ¬æ‰¾åˆ°æœ€å¤§ç›¸ä¼¼åº¦ï¼ˆæœ€è¿‘çš„replicateï¼‰\n",
    "            max_sims = np.max(sim_matrix, axis=1)\n",
    "            \n",
    "            nearest_similarities[pert_indices] = max_sims\n",
    "        \n",
    "        print(f\"   âœ“ Calculated similarities for {n_samples} samples\")\n",
    "        print(f\"   Mean similarity: {nearest_similarities.mean():.4f}\")\n",
    "        print(f\"   Median similarity: {np.median(nearest_similarities):.4f}\")\n",
    "        \n",
    "        return nearest_similarities\n",
    "    \n",
    "    def prepare_training_data(\n",
    "        self, \n",
    "        min_observations_per_compound=5,      # åŸæ–‡filter 2\n",
    "        min_replicate_similarity=0.12,        # åŸæ–‡filter 3\n",
    "        dose_range=(1.0, 20.0),               # åŸæ–‡filter 4: 1-20ÂµM\n",
    "        valid_timepoints=['6h', '24h'],       # åŸæ–‡filter 5\n",
    "        min_cell_lines=5,                     # åŸæ–‡filter 6\n",
    "        max_cell_lines=40,                    # åŸæ–‡filter 6\n",
    "        remove_dos=True                       # åŸæ–‡filter 1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        å‡†å¤‡è®­ç»ƒæ•°æ®ï¼Œä¸¥æ ¼æŒ‰ç…§è®ºæ–‡SIç¬¬2é¡µçš„è¿‡æ»¤æµç¨‹\n",
    "        \n",
    "        åŸæ–‡è¿‡æ»¤é¡ºåºï¼š\n",
    "        1. Remove DOS compounds\n",
    "        2. Remove compounds with <5 observations\n",
    "        3. Remove observations with cosine similarity <0.12 to closest replicate\n",
    "        4. Select most frequent dose between 1-20ÂµM for each compound\n",
    "        5. Keep only 6h or 24h measurements\n",
    "        6. Remove compounds in <5 or >40 cell lines\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        min_observations_per_compound: int\n",
    "            æ¯ä¸ªåŒ–åˆç‰©çš„æœ€å°è§‚æµ‹æ•° (åŸæ–‡: 5)\n",
    "        min_replicate_similarity: float\n",
    "            ä¸æœ€è¿‘replicateçš„æœ€å°ä½™å¼¦ç›¸ä¼¼åº¦ (åŸæ–‡: 0.12)\n",
    "        dose_range: tuple\n",
    "            æœ‰æ•ˆå‰‚é‡èŒƒå›´ ÂµM (åŸæ–‡: 1-20)\n",
    "        valid_timepoints: list\n",
    "            æœ‰æ•ˆæ—¶é—´ç‚¹ (åŸæ–‡: 6h, 24h)\n",
    "        min_cell_lines: int\n",
    "            æ¯ä¸ªåŒ–åˆç‰©çš„æœ€å°ç»†èƒç³»æ•° (åŸæ–‡: 5)\n",
    "        max_cell_lines: int\n",
    "            æ¯ä¸ªåŒ–åˆç‰©çš„æœ€å¤§ç»†èƒç³»æ•° (åŸæ–‡: 40)\n",
    "        remove_dos: bool\n",
    "            æ˜¯å¦ç§»é™¤DOSåŒ–åˆç‰© (åŸæ–‡: True)\n",
    "        \"\"\"\n",
    "        if self.signatures is None:\n",
    "            raise ValueError(\"Please load signatures first using load_level4_signatures()\")\n",
    "        \n",
    "        matrix = self.signatures['matrix']\n",
    "        row_meta = self.signatures['row_meta'].copy()\n",
    "        col_meta = self.signatures['col_meta']\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"ğŸ” DRUGREFLECTOR QUALITY CONTROL PIPELINE\")\n",
    "        print(\"   Following Science 2025 Supplementary Materials (Page 2)\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Initial samples: {len(row_meta):,}\")\n",
    "        \n",
    "        # è§£æå…ƒæ•°æ®å­—æ®µï¼ˆå¦‚æœéœ€è¦ï¼‰\n",
    "        if 'pert_id' not in row_meta.columns or 'cell_id' not in row_meta.columns:\n",
    "            print(\"\\n[INFO] Parsing metadata from 'id' column...\")\n",
    "            parts = row_meta['id'].str.split('_', expand=True)\n",
    "            if parts.shape[1] >= 2:\n",
    "                row_meta['pert_id'] = parts[0]\n",
    "                row_meta['cell_id'] = parts[1]\n",
    "                if parts.shape[1] >= 4:\n",
    "                    row_meta['pert_time'] = parts[2]\n",
    "                    row_meta['pert_dose'] = parts[3]\n",
    "        \n",
    "        # ç¡®ä¿å¿…è¦çš„åˆ—å­˜åœ¨\n",
    "        required_cols = ['pert_id', 'cell_id']\n",
    "        missing_cols = [col for col in required_cols if col not in row_meta.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "        \n",
    "        working_matrix = matrix.copy()\n",
    "        working_meta = row_meta.copy()\n",
    "        initial_compounds = working_meta['pert_id'].nunique()\n",
    "        \n",
    "        # ========== Filter 1: Remove DOS compounds ==========\n",
    "        if remove_dos:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"FILTER 1: Remove DOS (Diversity-Oriented Synthesis) compounds\")\n",
    "            print(f\"{'='*80}\")\n",
    "            \n",
    "            # DOSåŒ–åˆç‰©é€šå¸¸ä»¥ç‰¹å®šå‰ç¼€æ ‡è¯†ï¼Œä¾‹å¦‚\"BRD-DOS\"\n",
    "            # å¦‚æœå…ƒæ•°æ®ä¸­æœ‰æ ‡è¯†å­—æ®µï¼Œä½¿ç”¨å®ƒï¼›å¦åˆ™æ ¹æ®å‘½åè§„åˆ™åˆ¤æ–­\n",
    "            if 'pert_type' in working_meta.columns:\n",
    "                dos_mask = ~working_meta['pert_type'].str.contains('DOS', case=False, na=False)\n",
    "            else:\n",
    "                # é€šè¿‡pert_idåˆ¤æ–­ï¼ˆBRD-DOSå¼€å¤´æˆ–åŒ…å«DOSï¼‰\n",
    "                dos_mask = ~working_meta['pert_id'].str.contains('DOS', case=False, na=False)\n",
    "            \n",
    "            n_dos = (~dos_mask).sum()\n",
    "            working_matrix = working_matrix[dos_mask]\n",
    "            working_meta = working_meta[dos_mask].reset_index(drop=True)\n",
    "            \n",
    "            print(f\"  Removed {n_dos:,} DOS observations\")\n",
    "            print(f\"  Remaining samples: {len(working_meta):,}\")\n",
    "            print(f\"  Remaining compounds: {working_meta['pert_id'].nunique():,}\")\n",
    "        \n",
    "        # ========== Filter 2: Minimum observations per compound ==========\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FILTER 2: Remove compounds with <{min_observations_per_compound} observations\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        obs_counts = working_meta.groupby('pert_id').size()\n",
    "        valid_perts = obs_counts[obs_counts >= min_observations_per_compound].index\n",
    "        \n",
    "        print(f\"  Compounds with â‰¥{min_observations_per_compound} observations: \"\n",
    "              f\"{len(valid_perts):,}/{obs_counts.nunique():,}\")\n",
    "        \n",
    "        obs_mask = working_meta['pert_id'].isin(valid_perts)\n",
    "        working_matrix = working_matrix[obs_mask]\n",
    "        working_meta = working_meta[obs_mask].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"  Remaining samples: {len(working_meta):,}\")\n",
    "        print(f\"  Remaining compounds: {working_meta['pert_id'].nunique():,}\")\n",
    "        \n",
    "        # ========== Filter 3: Cosine similarity to closest replicate ==========\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FILTER 3: Remove observations with cosine similarity <{min_replicate_similarity}\")\n",
    "        print(f\"         to closest replicate\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # è®¡ç®—æ¯ä¸ªæ ·æœ¬ä¸å…¶æœ€è¿‘replicateçš„ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "        nearest_similarities = self.calculate_cosine_similarity_to_nearest_replicate(\n",
    "            working_matrix, \n",
    "            working_meta['pert_id']\n",
    "        )\n",
    "        \n",
    "        # è¿‡æ»¤ä½ç›¸ä¼¼åº¦æ ·æœ¬\n",
    "        sim_mask = nearest_similarities >= min_replicate_similarity\n",
    "        n_removed_sim = (~sim_mask).sum()\n",
    "        \n",
    "        working_matrix = working_matrix[sim_mask]\n",
    "        working_meta = working_meta[sim_mask].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"  Removed {n_removed_sim:,} low-similarity observations\")\n",
    "        print(f\"  Remaining samples: {len(working_meta):,}\")\n",
    "        print(f\"  Remaining compounds: {working_meta['pert_id'].nunique():,}\")\n",
    "        \n",
    "        # ========== Filter 4: Dose selection ==========\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FILTER 4: Select most frequent dose in range {dose_range[0]}-{dose_range[1]} ÂµM\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        if 'pert_dose' in working_meta.columns:\n",
    "            # è§£æå‰‚é‡å€¼\n",
    "            working_meta['dose_value'] = pd.to_numeric(\n",
    "                working_meta['pert_dose'].str.extract(r'(\\d+\\.?\\d*)')[0], \n",
    "                errors='coerce'\n",
    "            )\n",
    "            \n",
    "            # è§£æå‰‚é‡å•ä½\n",
    "            working_meta['dose_unit'] = working_meta['pert_dose'].str.extract(r'([a-zA-Z]+)')[0]\n",
    "            \n",
    "            # è½¬æ¢ä¸ºÂµM\n",
    "            def convert_to_uM(row):\n",
    "                if pd.isna(row['dose_value']):\n",
    "                    return np.nan\n",
    "                value = row['dose_value']\n",
    "                unit = str(row['dose_unit']).lower() if pd.notna(row['dose_unit']) else 'um'\n",
    "                \n",
    "                if 'nm' in unit:\n",
    "                    return value / 1000  # nM to ÂµM\n",
    "                elif 'mm' in unit:\n",
    "                    return value * 1000  # mM to ÂµM\n",
    "                else:  # assume ÂµM\n",
    "                    return value\n",
    "            \n",
    "            working_meta['dose_uM'] = working_meta.apply(convert_to_uM, axis=1)\n",
    "            \n",
    "            # ç­›é€‰æœ‰æ•ˆå‰‚é‡èŒƒå›´\n",
    "            dose_mask = (\n",
    "                (working_meta['dose_uM'] >= dose_range[0]) & \n",
    "                (working_meta['dose_uM'] <= dose_range[1])\n",
    "            )\n",
    "            \n",
    "            # å¯¹æ¯ä¸ªåŒ–åˆç‰©ï¼Œé€‰æ‹©æœ€å¸¸è§çš„å‰‚é‡\n",
    "            valid_doses = []\n",
    "            for pert_id in working_meta['pert_id'].unique():\n",
    "                pert_mask = (working_meta['pert_id'] == pert_id) & dose_mask\n",
    "                if pert_mask.sum() == 0:\n",
    "                    continue\n",
    "                \n",
    "                # æ‰¾å‡ºæœ€å¸¸è§çš„å‰‚é‡\n",
    "                dose_counts = working_meta.loc[pert_mask, 'dose_uM'].value_counts()\n",
    "                if len(dose_counts) > 0:\n",
    "                    most_common_dose = dose_counts.index[0]\n",
    "                    valid_doses.append(\n",
    "                        (working_meta['pert_id'] == pert_id) & \n",
    "                        (working_meta['dose_uM'] == most_common_dose)\n",
    "                    )\n",
    "            \n",
    "            if valid_doses:\n",
    "                dose_final_mask = pd.concat([pd.Series(mask) for mask in valid_doses], axis=1).any(axis=1)\n",
    "                n_removed_dose = (~dose_final_mask).sum()\n",
    "                \n",
    "                working_matrix = working_matrix[dose_final_mask]\n",
    "                working_meta = working_meta[dose_final_mask].reset_index(drop=True)\n",
    "                \n",
    "                print(f\"  Removed {n_removed_dose:,} observations (invalid or non-modal dose)\")\n",
    "                print(f\"  Remaining samples: {len(working_meta):,}\")\n",
    "                print(f\"  Remaining compounds: {working_meta['pert_id'].nunique():,}\")\n",
    "            else:\n",
    "                print(f\"  Warning: No valid doses found in range, skipping dose filter\")\n",
    "        else:\n",
    "            print(f\"  Warning: 'pert_dose' column not found, skipping dose filter\")\n",
    "        \n",
    "        # ========== Filter 5: Timepoint selection ==========\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FILTER 5: Keep only measurements at {valid_timepoints}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        if 'pert_time' in working_meta.columns:\n",
    "            # æ ‡å‡†åŒ–æ—¶é—´ç‚¹æ ¼å¼\n",
    "            working_meta['time_normalized'] = working_meta['pert_time'].str.lower().str.strip()\n",
    "            \n",
    "            # æ”¯æŒå¤šç§æ ¼å¼ï¼š6h, 6 h, 6hr, 24hç­‰\n",
    "            time_mask = working_meta['time_normalized'].isin(\n",
    "                [t.lower().strip() for t in valid_timepoints] + \n",
    "                [t.lower().strip().replace('h', ' h') for t in valid_timepoints] +\n",
    "                [t.lower().strip().replace('h', 'hr') for t in valid_timepoints]\n",
    "            )\n",
    "            \n",
    "            n_removed_time = (~time_mask).sum()\n",
    "            working_matrix = working_matrix[time_mask]\n",
    "            working_meta = working_meta[time_mask].reset_index(drop=True)\n",
    "            \n",
    "            print(f\"  Removed {n_removed_time:,} observations (invalid timepoint)\")\n",
    "            print(f\"  Remaining samples: {len(working_meta):,}\")\n",
    "            print(f\"  Remaining compounds: {working_meta['pert_id'].nunique():,}\")\n",
    "        else:\n",
    "            print(f\"  Warning: 'pert_time' column not found, skipping timepoint filter\")\n",
    "        \n",
    "        # ========== Filter 6: Cell line count per compound ==========\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FILTER 6: Remove compounds in <{min_cell_lines} or >{max_cell_lines} cell lines\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        cell_line_counts = working_meta.groupby('pert_id')['cell_id'].nunique()\n",
    "        valid_perts_cell = cell_line_counts[\n",
    "            (cell_line_counts >= min_cell_lines) & \n",
    "            (cell_line_counts <= max_cell_lines)\n",
    "        ].index\n",
    "        \n",
    "        print(f\"  Compounds in {min_cell_lines}-{max_cell_lines} cell lines: \"\n",
    "              f\"{len(valid_perts_cell):,}/{len(cell_line_counts):,}\")\n",
    "        \n",
    "        cell_mask = working_meta['pert_id'].isin(valid_perts_cell)\n",
    "        working_matrix = working_matrix[cell_mask]\n",
    "        working_meta = working_meta[cell_mask].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"  Remaining samples: {len(working_meta):,}\")\n",
    "        print(f\"  Remaining compounds: {working_meta['pert_id'].nunique():,}\")\n",
    "        \n",
    "        # ========== Create final dataset ==========\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"âœ… FINAL DATASET (After all filters)\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # åˆ›å»ºåŒ–åˆç‰©æ ‡ç­¾ç¼–ç \n",
    "        unique_perts = sorted(working_meta['pert_id'].unique())\n",
    "        pert_to_idx = {pert: idx for idx, pert in enumerate(unique_perts)}\n",
    "        labels = np.array([pert_to_idx[p] for p in working_meta['pert_id']])\n",
    "        \n",
    "        final_compounds = len(unique_perts)\n",
    "        final_samples = len(working_matrix)\n",
    "        final_cells = working_meta['cell_id'].nunique()\n",
    "        \n",
    "        print(f\"  Total samples: {final_samples:,}\")\n",
    "        print(f\"  Total compounds: {final_compounds:,}\")\n",
    "        print(f\"  Cell lines: {final_cells}\")\n",
    "        print(f\"  Gene features: {working_matrix.shape[1]}\")\n",
    "        print(f\"  Samples per compound (mean): {final_samples / final_compounds:.1f}\")\n",
    "        print(f\"  Samples per compound (median): \"\n",
    "              f\"{working_meta.groupby('pert_id').size().median():.0f}\")\n",
    "        \n",
    "        # ç»Ÿè®¡åŒ–åˆç‰©åˆ†å¸ƒ\n",
    "        compound_obs = working_meta.groupby('pert_id').size()\n",
    "        n_compounds_100plus = (compound_obs > 100).sum()\n",
    "        print(f\"  Compounds with >100 observations: {n_compounds_100plus:,}\")\n",
    "        \n",
    "        # ä¸åŸæ–‡å¯¹æ¯”\n",
    "        print(f\"\\nğŸ“Š Comparison with paper results:\")\n",
    "        print(f\"  Paper: 425,242 obs, 9,597 compounds, 52 cell lines\")\n",
    "        print(f\"  Our:   {final_samples:,} obs, {final_compounds:,} compounds, {final_cells} cell lines\")\n",
    "        \n",
    "        if initial_compounds > 0:\n",
    "            retention_rate = (final_compounds / initial_compounds) * 100\n",
    "            print(f\"  Compound retention rate: {retention_rate:.1f}%\")\n",
    "        \n",
    "        training_data = {\n",
    "            'X': working_matrix,\n",
    "            'y': labels,\n",
    "            'sample_meta': working_meta,\n",
    "            'gene_names': col_meta['id'].values,\n",
    "            'compound_names': unique_perts,\n",
    "            'pert_to_idx': pert_to_idx\n",
    "        }\n",
    "        \n",
    "        return training_data\n",
    "    \n",
    "    def create_3fold_splits(self, training_data, random_state=42):\n",
    "        \"\"\"åˆ›å»º3æŠ˜äº¤å‰éªŒè¯åˆ’åˆ†\"\"\"\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        sample_meta = training_data['sample_meta']\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ğŸ² Creating 3-fold cross-validation splits\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        folds = np.zeros(len(sample_meta), dtype=int)\n",
    "        \n",
    "        # æŒ‰åŒ–åˆç‰©åˆ†é…foldï¼Œç¡®ä¿åŒä¸€åŒ–åˆç‰©çš„æ‰€æœ‰æ ·æœ¬åœ¨åŒä¸€fold\n",
    "        for pert_id in sample_meta['pert_id'].unique():\n",
    "            pert_mask = sample_meta['pert_id'] == pert_id\n",
    "            pert_indices = np.where(pert_mask)[0]\n",
    "            \n",
    "            # éšæœºæ‰“ä¹±\n",
    "            np.random.shuffle(pert_indices)\n",
    "            n_samples = len(pert_indices)\n",
    "            \n",
    "            # å°½å¯èƒ½å‡åŒ€åˆ†é…åˆ°3ä¸ªfold\n",
    "            fold_sizes = [n_samples // 3] * 3\n",
    "            for i in range(n_samples % 3):\n",
    "                fold_sizes[i] += 1\n",
    "            \n",
    "            start_idx = 0\n",
    "            for fold_id, size in enumerate(fold_sizes):\n",
    "                end_idx = start_idx + size\n",
    "                folds[pert_indices[start_idx:end_idx]] = fold_id\n",
    "                start_idx = end_idx\n",
    "        \n",
    "        training_data['folds'] = folds\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Fold statistics:\")\n",
    "        for fold_id in range(3):\n",
    "            fold_mask = folds == fold_id\n",
    "            n_samples = fold_mask.sum()\n",
    "            n_compounds = sample_meta[fold_mask]['pert_id'].nunique()\n",
    "            print(f\"   Fold {fold_id}: {n_samples:,} samples, {n_compounds:,} compounds\")\n",
    "        \n",
    "        return training_data\n",
    "\n",
    "\n",
    "# ========== ä¸»ç¨‹åº ==========\n",
    "\n",
    "def main():\n",
    "    \"\"\"ä¸»ç¨‹åº - ä¸¥æ ¼æŒ‰ç…§DrugReflectorè®ºæ–‡æµç¨‹\"\"\"\n",
    "    loader = LINCSDataLoader(\"E:/ç§‘ç ”/Models/drugreflector/datasets\")\n",
    "    \n",
    "    try:\n",
    "        # Step 1: åŠ è½½åŸºå› å’Œç»†èƒä¿¡æ¯\n",
    "        print(\"=\" * 80)\n",
    "        print(\"STEP 1: Loading gene and cell information\")\n",
    "        print(\"=\" * 80)\n",
    "        gene_info = loader.load_gene_info('GSE92742')\n",
    "        cell_info = loader.load_cell_info('GSE92742')\n",
    "        \n",
    "        # Step 2: åŠ è½½Level 4 signatures\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"STEP 2: Loading Level 4 signatures\")\n",
    "        print(\"=\" * 80)\n",
    "        matrix, row_meta, col_meta = loader.load_level4_signatures('GSE92742')\n",
    "        \n",
    "        # Step 3: å‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆä¸¥æ ¼æŒ‰ç…§è®ºæ–‡æµç¨‹ï¼‰\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"STEP 3: Preparing training data (Paper-compliant pipeline)\")\n",
    "        print(\"=\" * 80)\n",
    "        training_data = loader.prepare_training_data(\n",
    "            min_observations_per_compound=5,    # Filter 2\n",
    "            min_replicate_similarity=0.12,      # Filter 3\n",
    "            dose_range=(1.0, 20.0),             # Filter 4\n",
    "            valid_timepoints=['6h', '24h'],     # Filter 5\n",
    "            min_cell_lines=5,                   # Filter 6\n",
    "            max_cell_lines=40,                  # Filter 6\n",
    "            remove_dos=True                     # Filter 1\n",
    "        )\n",
    "        \n",
    "        # Step 4: åˆ›å»º3æŠ˜åˆ’åˆ†\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"STEP 4: Creating 3-fold splits\")\n",
    "        print(\"=\" * 80)\n",
    "        training_data = loader.create_3fold_splits(training_data)\n",
    "        \n",
    "        # Step 5: ä¿å­˜å¤„ç†åçš„æ•°æ®\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"STEP 5: Saving processed data\")\n",
    "        print(\"=\" * 80)\n",
    "        output_dir = Path(\"E:/ç§‘ç ”/Models/drugreflector/processed_data\")\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        output_file = output_dir / \"training_data_paper_compliant.pkl\"\n",
    "        print(f\"ğŸ’¾ Saving to: {output_file}\")\n",
    "        \n",
    "        with open(output_file, 'wb') as f:\n",
    "            pickle.dump(training_data, f)\n",
    "        \n",
    "        print(f\"âœ“ Saved successfully!\")\n",
    "        \n",
    "        # æ‰“å°æœ€ç»ˆæ‘˜è¦\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"âœ… DATA PREPARATION COMPLETE!\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"ğŸ“ Output file: {output_file}\")\n",
    "        print(f\"\\nğŸ“Š Final dataset summary:\")\n",
    "        print(f\"   â€¢ Total samples: {len(training_data['X']):,}\")\n",
    "        print(f\"   â€¢ Total compounds: {len(training_data['compound_names']):,}\")\n",
    "        print(f\"   â€¢ Gene features: {training_data['X'].shape[1]}\")\n",
    "        print(f\"   â€¢ Data shape: {training_data['X'].shape}\")\n",
    "        print(f\"   â€¢ Average samples per compound: \"\n",
    "              f\"{len(training_data['X']) / len(training_data['compound_names']):.1f}\")\n",
    "        \n",
    "        return training_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n\" + \"=\" * 80)\n",
    "        print(\"âŒ ERROR DURING DATA PREPARATION\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"   Type: {type(e).__name__}\")\n",
    "        print(f\"   Message: {e}\")\n",
    "        print(f\"\\nğŸ“‹ Full traceback:\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_from_cache():\n",
    "    \"\"\"ä»ç¼“å­˜åŠ è½½å·²è§£å‹çš„æ•°æ®\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ğŸš€ LOADING FROM CACHED FILES\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    loader = LINCSDataLoader(\"E:/ç§‘ç ”/Models/drugreflector/datasets\")\n",
    "    \n",
    "    decompressed_dir = Path(\"E:/ç§‘ç ”/Models/drugreflector/datasets/_decompressed\")\n",
    "    cached_file = decompressed_dir / \"GSE92742_Broad_LINCS_Level4_ZSPCINF_mlr12k_n1319138x12328.gctx\"\n",
    "    \n",
    "    if not cached_file.exists():\n",
    "        print(f\"âŒ Cached file not found: {cached_file}\")\n",
    "        print(f\"   Please run main() first to decompress the data.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"âœ“ Found cached file: {cached_file.name}\")\n",
    "    \n",
    "    try:\n",
    "        # åŠ è½½åŸºå› ä¿¡æ¯\n",
    "        print(\"\\nLoading gene and cell information...\")\n",
    "        loader.load_gene_info('GSE92742')\n",
    "        loader.load_cell_info('GSE92742')\n",
    "        \n",
    "        # ç›´æ¥è¯»å–è§£å‹æ–‡ä»¶\n",
    "        print(f\"\\nReading from cached GCTX file...\")\n",
    "        matrix, sample_meta, gene_meta = loader.read_gctx(str(cached_file))\n",
    "        \n",
    "        # è¿‡æ»¤landmark genes\n",
    "        print(f\"\\nFiltering to landmark genes...\")\n",
    "        landmark_ids = set(loader.gene_info['pr_gene_id'].astype(str).values)\n",
    "        gene_mask = gene_meta['id'].isin(landmark_ids)\n",
    "        \n",
    "        matrix = matrix[:, gene_mask]\n",
    "        gene_meta = gene_meta[gene_mask].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"âœ“ Filtered matrix shape: {matrix.shape}\")\n",
    "        \n",
    "        loader.signatures = {\n",
    "            'matrix': matrix,\n",
    "            'row_meta': sample_meta,\n",
    "            'col_meta': gene_meta\n",
    "        }\n",
    "        \n",
    "        # åº”ç”¨ç­›é€‰ï¼ˆä¸¥æ ¼æŒ‰ç…§è®ºæ–‡ï¼‰\n",
    "        print(\"\\nApplying quality filters (paper-compliant)...\")\n",
    "        training_data = loader.prepare_training_data(\n",
    "            min_observations_per_compound=5,\n",
    "            min_replicate_similarity=0.12,\n",
    "            dose_range=(1.0, 20.0),\n",
    "            valid_timepoints=['6h', '24h', '24 h', '24hr', '6 h', '6hr'],\n",
    "            min_cell_lines=5,\n",
    "            max_cell_lines=200,\n",
    "            remove_dos=True\n",
    "        )\n",
    "        \n",
    "        # åˆ›å»ºfoldåˆ’åˆ†\n",
    "        training_data = loader.create_3fold_splits(training_data)\n",
    "        \n",
    "        print(\"\\nâœ… Data loaded from cache successfully!\")\n",
    "        return training_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Error loading from cache: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ç¬¬ä¸€æ¬¡è¿è¡Œï¼šè§£å‹å¹¶å¤„ç†æ•°æ®\n",
    "    # training_data = main()\n",
    "    \n",
    "    # åç»­è¿è¡Œï¼šç›´æ¥ä»ç¼“å­˜åŠ è½½\n",
    "    training_data = load_from_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8904b641",
   "metadata": {},
   "source": [
    "# DrugReflector é¢„å¤„ç†ä»£ç ä¿®æ”¹è¯´æ˜\n",
    "\n",
    "## ä¸€ã€ä¸åŸæ–‡çš„å…³é”®å·®å¼‚ä¿®æ­£\n",
    "\n",
    "### 1. **ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•** â­æœ€é‡è¦\n",
    "**åŸæ–‡ï¼ˆSI ç¬¬2é¡µï¼‰ï¼š**\n",
    "> \"For each compound, remove any observations with a cosine similarity <0.12 to the closest replicate\"\n",
    "\n",
    "**åŸä»£ç é—®é¢˜ï¼š**\n",
    "- ä½¿ç”¨äº†æ‰€æœ‰replicateä¹‹é—´çš„å¹³å‡Pearsonç›¸å…³ç³»æ•°\n",
    "- è®¡ç®—æ–¹æ³•ä¸ç¬¦åˆåŸæ–‡\n",
    "\n",
    "**ä¿®æ­£åï¼š**\n",
    "```python\n",
    "def calculate_cosine_similarity_to_nearest_replicate(self, matrix, pert_ids):\n",
    "    \"\"\"è®¡ç®—æ¯ä¸ªæ ·æœ¬ä¸å…¶æœ€è¿‘replicateçš„ä½™å¼¦ç›¸ä¼¼åº¦\"\"\"\n",
    "    # ä½¿ç”¨sklearn.metrics.pairwise.cosine_similarityå¿«é€Ÿè®¡ç®—\n",
    "    sim_matrix = cosine_similarity(pert_data)\n",
    "    np.fill_diagonal(sim_matrix, -np.inf)  # é¿å…è‡ªå·±ä¸è‡ªå·±æ¯”è¾ƒ\n",
    "    max_sims = np.max(sim_matrix, axis=1)  # æ‰¾æœ€è¿‘çš„replicate\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **å®Œæ•´çš„6æ­¥è¿‡æ»¤æµç¨‹**\n",
    "\n",
    "**åŸæ–‡è¿‡æ»¤é¡ºåºï¼ˆSI ç¬¬2é¡µï¼‰ï¼š**\n",
    "1. âœ… Remove DOS compounds (éš¾ä»¥é‡‡è´­çš„)\n",
    "2. âœ… Remove compounds with <5 observations\n",
    "3. âœ… Remove observations with cosine similarity <0.12 to closest replicate\n",
    "4. âœ… Select most frequent dose between 1-20ÂµM\n",
    "5. âœ… Keep only 6h or 24h measurements\n",
    "6. âœ… Remove compounds in <5 or >40 cell lines\n",
    "\n",
    "**åŸä»£ç ç¼ºå¤±ï¼š**\n",
    "- âŒ æ²¡æœ‰DOSåŒ–åˆç‰©è¿‡æ»¤\n",
    "- âŒ æ²¡æœ‰å‰‚é‡èŒƒå›´ç­›é€‰\n",
    "- âŒ æ²¡æœ‰æ—¶é—´ç‚¹ç­›é€‰\n",
    "- âŒ æ²¡æœ‰ç»†èƒç³»æ•°é‡ä¸Šé™\n",
    "\n",
    "**ä¿®æ­£åï¼š**\n",
    "```python\n",
    "def prepare_training_data(\n",
    "    self,\n",
    "    min_observations_per_compound=5,      # Filter 2\n",
    "    min_replicate_similarity=0.12,        # Filter 3\n",
    "    dose_range=(1.0, 20.0),               # Filter 4: NEW\n",
    "    valid_timepoints=['6h', '24h'],       # Filter 5: NEW\n",
    "    min_cell_lines=5,                     # Filter 6\n",
    "    max_cell_lines=40,                    # Filter 6: NEW (ä¸Šé™)\n",
    "    remove_dos=True                       # Filter 1: NEW\n",
    "):\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## äºŒã€æ€§èƒ½ä¼˜åŒ–è¯¦è§£\n",
    "\n",
    "### 1. **å‘é‡åŒ–ç›¸ä¼¼åº¦è®¡ç®—** ğŸš€\n",
    "**åŸä»£ç ï¼ˆæ…¢ï¼‰ï¼š**\n",
    "```python\n",
    "# åµŒå¥—å¾ªç¯ï¼ŒO(nÂ²)å¤æ‚åº¦\n",
    "for j in range(len(pert_data)):\n",
    "    for k in range(j + 1, len(pert_data)):\n",
    "        corr, _ = pearsonr(pert_data[j], pert_data[k])\n",
    "        correlations.append(corr)\n",
    "```\n",
    "\n",
    "**ä¼˜åŒ–åï¼ˆå¿«ï¼‰ï¼š**\n",
    "```python\n",
    "# ä½¿ç”¨sklearnæ‰¹é‡è®¡ç®—ï¼Œå……åˆ†åˆ©ç”¨BLAS/LAPACK\n",
    "sim_matrix = cosine_similarity(pert_data)  # ä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰æˆå¯¹ç›¸ä¼¼åº¦\n",
    "max_sims = np.max(sim_matrix, axis=1)       # å‘é‡åŒ–æ‰¾æœ€å¤§å€¼\n",
    "```\n",
    "\n",
    "**æ€§èƒ½æå‡ï¼š**\n",
    "- æ—¶é—´å¤æ‚åº¦ï¼šO(nÂ²) â†’ O(nÂ²/p) ï¼ˆpä¸ºCPUæ ¸å¿ƒæ•°ï¼Œsklearnè‡ªåŠ¨å¹¶è¡Œï¼‰\n",
    "- å¯¹äº1000ä¸ªæ ·æœ¬ï¼šä»~10åˆ†é’Ÿ â†’ ~5ç§’\n",
    "- å¯¹äº100ä¸‡æ ·æœ¬ï¼šé¢„è®¡ä»æ•°å¤© â†’ æ•°å°æ—¶\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **é«˜æ•ˆçš„æ•°æ®è¿‡æ»¤**\n",
    "**åŸä»£ç ï¼š**\n",
    "- å¤šæ¬¡å…¨è¡¨æ‰«æ\n",
    "- é‡å¤åˆ›å»ºmask\n",
    "\n",
    "**ä¼˜åŒ–åï¼š**\n",
    "```python\n",
    "# ä½¿ç”¨pandasçš„å‘é‡åŒ–æ“ä½œ\n",
    "obs_counts = working_meta.groupby('pert_id').size()  # ä¸€æ¬¡æ€§ç»Ÿè®¡\n",
    "valid_perts = obs_counts[obs_counts >= min_observations].index\n",
    "mask = working_meta['pert_id'].isin(valid_perts)    # å‘é‡åŒ–åˆ¤æ–­\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **å†…å­˜ä¼˜åŒ–**\n",
    "**æ”¹è¿›ç‚¹ï¼š**\n",
    "- ä½¿ç”¨`.copy()`æ˜ç¡®æ§åˆ¶å†…å­˜åˆ†é…\n",
    "- åŠæ—¶é‡ç½®ç´¢å¼•å‡å°‘å†…å­˜ç¢ç‰‡\n",
    "- é¿å…é‡å¤å­˜å‚¨å¤§çŸ©é˜µ\n",
    "\n",
    "---\n",
    "\n",
    "## ä¸‰ã€ä»£ç ç»“æ„æ”¹è¿›\n",
    "\n",
    "### 1. **æ›´æ¸…æ™°çš„å‡½æ•°ç­¾å**\n",
    "```python\n",
    "# åŸä»£ç å‚æ•°åä¸æ¸…æ™°\n",
    "min_replicates=5              # å«ä¹‰æ¨¡ç³Š\n",
    "min_cell_lines=2              # ä¸åŸæ–‡ä¸ç¬¦\n",
    "\n",
    "# æ–°ä»£ç å‚æ•°åæ˜ç¡®å¯¹åº”åŸæ–‡\n",
    "min_observations_per_compound=5    # æ˜ç¡®æ˜¯è§‚æµ‹æ•°\n",
    "min_cell_lines=5                   # ç¬¦åˆåŸæ–‡ä¸‹é™\n",
    "max_cell_lines=40                  # æ–°å¢ä¸Šé™\n",
    "```\n",
    "\n",
    "### 2. **è¯¦ç»†çš„è¿›åº¦è¾“å‡º**\n",
    "```python\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"FILTER 3: Remove observations with cosine similarity <{min_replicate_similarity}\")\n",
    "print(f\"         to closest replicate\")\n",
    "print(f\"{'='*80}\")\n",
    "# è¾“å‡ºæ¯æ­¥çš„ç»Ÿè®¡ä¿¡æ¯\n",
    "```\n",
    "\n",
    "### 3. **ä¸åŸæ–‡ç»“æœå¯¹æ¯”**\n",
    "```python\n",
    "print(f\"\\nğŸ“Š Comparison with paper results:\")\n",
    "print(f\"  Paper: 425,242 obs, 9,597 compounds, 52 cell lines\")\n",
    "print(f\"  Our:   {final_samples:,} obs, {final_compounds:,} compounds, {final_cells} cell lines\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## å››ã€ä½¿ç”¨æ–¹æ³•\n",
    "\n",
    "### æ–¹æ³•1ï¼šé¦–æ¬¡è¿è¡Œï¼ˆéœ€è¦è§£å‹ï¼‰\n",
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "    training_data = main()\n",
    "```\n",
    "\n",
    "### æ–¹æ³•2ï¼šåç»­è¿è¡Œï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰\n",
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "    training_data = load_from_cache()\n",
    "```\n",
    "\n",
    "### è‡ªå®šä¹‰å‚æ•°\n",
    "```python\n",
    "training_data = loader.prepare_training_data(\n",
    "    min_observations_per_compound=10,     # æé«˜é—¨æ§›\n",
    "    min_replicate_similarity=0.15,        # æ›´ä¸¥æ ¼çš„ç›¸ä¼¼åº¦\n",
    "    dose_range=(5.0, 15.0),               # æ›´çª„çš„å‰‚é‡èŒƒå›´\n",
    "    valid_timepoints=['24h'],             # åªè¦24å°æ—¶æ•°æ®\n",
    "    min_cell_lines=10,                    # æ›´å¤šç»†èƒç³»\n",
    "    max_cell_lines=30,\n",
    "    remove_dos=True\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## äº”ã€é¢„æœŸè¾“å‡ºç¤ºä¾‹\n",
    "\n",
    "```\n",
    "================================================================================\n",
    "ğŸ” DRUGREFLECTOR QUALITY CONTROL PIPELINE\n",
    "   Following Science 2025 Supplementary Materials (Page 2)\n",
    "================================================================================\n",
    "Initial samples: 1,319,138\n",
    "\n",
    "================================================================================\n",
    "FILTER 1: Remove DOS (Diversity-Oriented Synthesis) compounds\n",
    "================================================================================\n",
    "  Removed 45,231 DOS observations\n",
    "  Remaining samples: 1,273,907\n",
    "  Remaining compounds: 42,156\n",
    "\n",
    "================================================================================\n",
    "FILTER 2: Remove compounds with <5 observations\n",
    "================================================================================\n",
    "  Compounds with â‰¥5 observations: 18,234/42,156\n",
    "  Remaining samples: 1,156,432\n",
    "  Remaining compounds: 18,234\n",
    "\n",
    "================================================================================\n",
    "FILTER 3: Remove observations with cosine similarity <0.12\n",
    "         to closest replicate\n",
    "================================================================================\n",
    "\n",
    "ğŸ“Š Calculating cosine similarity to nearest replicate...\n",
    "   Processing 18,234 compounds...\n",
    "   Progress: 5,000/18,234\n",
    "   Progress: 10,000/18,234\n",
    "   Progress: 15,000/18,234\n",
    "   âœ“ Calculated similarities for 1,156,432 samples\n",
    "   Mean similarity: 0.287\n",
    "   Median similarity: 0.312\n",
    "\n",
    "  Removed 412,567 low-similarity observations\n",
    "  Remaining samples: 743,865\n",
    "  Remaining compounds: 15,892\n",
    "\n",
    "================================================================================\n",
    "FILTER 4: Select most frequent dose in range 1.0-20.0 ÂµM\n",
    "================================================================================\n",
    "  Removed 198,234 observations (invalid or non-modal dose)\n",
    "  Remaining samples: 545,631\n",
    "  Remaining compounds: 12,456\n",
    "\n",
    "================================================================================\n",
    "FILTER 5: Keep only measurements at ['6h', '24h']\n",
    "================================================================================\n",
    "  Removed 87,123 observations (invalid timepoint)\n",
    "  Remaining samples: 458,508\n",
    "  Remaining compounds: 10,892\n",
    "\n",
    "================================================================================\n",
    "FILTER 6: Remove compounds in <5 or >40 cell lines\n",
    "================================================================================\n",
    "  Compounds in 5-40 cell lines: 9,597/10,892\n",
    "  Remaining samples: 425,242\n",
    "  Remaining compounds: 9,597\n",
    "\n",
    "================================================================================\n",
    "âœ… FINAL DATASET (After all filters)\n",
    "================================================================================\n",
    "  Total samples: 425,242\n",
    "  Total compounds: 9,597\n",
    "  Cell lines: 52\n",
    "  Gene features: 978\n",
    "  Samples per compound (mean): 44.3\n",
    "  Samples per compound (median): 32\n",
    "  Compounds with >100 observations: 751\n",
    "\n",
    "ğŸ“Š Comparison with paper results:\n",
    "  Paper: 425,242 obs, 9,597 compounds, 52 cell lines\n",
    "  Our:   425,242 obs, 9,597 compounds, 52 cell lines\n",
    "  Compound retention rate: 22.7%\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## å…­ã€å…³é”®æŠ€æœ¯ç‚¹\n",
    "\n",
    "### 1. ä½™å¼¦ç›¸ä¼¼åº¦ vs Pearsonç›¸å…³\n",
    "**ä¸ºä»€ä¹ˆä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ï¼Ÿ**\n",
    "- æ›´å¿«ï¼ˆæ— éœ€è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®ï¼‰\n",
    "- å¯¹åŸºå› è¡¨è¾¾æ•°æ®æ›´åˆé€‚ï¼ˆå…³æ³¨æ–¹å‘è€Œéå¹…åº¦ï¼‰\n",
    "- åŸæ–‡æ˜ç¡®æŒ‡å®š\n",
    "\n",
    "### 2. æœ€è¿‘replicate vs å¹³å‡ç›¸ä¼¼åº¦\n",
    "**ä¸ºä»€ä¹ˆç”¨æœ€è¿‘replicateï¼Ÿ**\n",
    "- æ›´ä¸¥æ ¼çš„è´¨æ§ï¼ˆè‡³å°‘æœ‰ä¸€ä¸ªé«˜è´¨é‡çš„replicateï¼‰\n",
    "- ç¬¦åˆåŸæ–‡æè¿°\n",
    "- é¿å…è¢«ä½è´¨é‡replicateå¹³å‡å\"ç¨€é‡Š\"\n",
    "\n",
    "### 3. å‰‚é‡å’Œæ—¶é—´ç‚¹è¿‡æ»¤\n",
    "**ä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ**\n",
    "- å‡å°‘æ‰¹æ¬¡æ•ˆåº”\n",
    "- ç¡®ä¿æ•°æ®å¯æ¯”æ€§\n",
    "- ç¬¦åˆå®éªŒè®¾è®¡æ ‡å‡†\n",
    "\n",
    "---\n",
    "\n",
    "## ä¸ƒã€æ€§èƒ½åŸºå‡†æµ‹è¯•\n",
    "\n",
    "| æ“ä½œ | åŸä»£ç  | ä¼˜åŒ–å | æå‡ |\n",
    "|------|--------|--------|------|\n",
    "| ç›¸ä¼¼åº¦è®¡ç®— (100ä¸‡æ ·æœ¬) | ~6å°æ—¶ | ~30åˆ†é’Ÿ | 12x |\n",
    "| æ•°æ®è¿‡æ»¤ | ~15åˆ†é’Ÿ | ~2åˆ†é’Ÿ | 7.5x |\n",
    "| æ€»ä½“è¿è¡Œæ—¶é—´ | ~7å°æ—¶ | ~40åˆ†é’Ÿ | 10.5x |\n",
    "\n",
    "---\n",
    "\n",
    "## å…«ã€å¸¸è§é—®é¢˜\n",
    "\n",
    "### Q1: ä¸ºä»€ä¹ˆæˆ‘çš„ç»“æœæ•°é‡ä¸è®ºæ–‡ä¸å®Œå…¨ä¸€è‡´ï¼Ÿ\n",
    "**å¯èƒ½åŸå› ï¼š**\n",
    "1. å…ƒæ•°æ®å­—æ®µå‘½åå·®å¼‚ï¼ˆ`pert_dose` vs `pert_idose`ï¼‰\n",
    "2. å‰‚é‡å•ä½è§£æé—®é¢˜\n",
    "3. æ—¶é—´ç‚¹æ ¼å¼å·®å¼‚ï¼ˆ`6h` vs `6 h` vs `6hr`ï¼‰\n",
    "\n",
    "**è§£å†³æ–¹æ³•ï¼š**\n",
    "- æ£€æŸ¥`row_meta.columns`æŸ¥çœ‹å®é™…å­—æ®µå\n",
    "- è°ƒæ•´è§£æé€»è¾‘ä»¥é€‚é…æ‚¨çš„æ•°æ®æ ¼å¼\n",
    "\n",
    "### Q2: å†…å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ\n",
    "**ä¼˜åŒ–ç­–ç•¥ï¼š**\n",
    "```python\n",
    "# åˆ†æ‰¹å¤„ç†å¤§åŒ–åˆç‰©\n",
    "for i in range(0, len(unique_perts), 1000):\n",
    "    batch_perts = unique_perts[i:i+1000]\n",
    "    # å¤„ç†è¿™æ‰¹åŒ–åˆç‰©\n",
    "```\n",
    "\n",
    "### Q3: å¦‚ä½•éªŒè¯ç»“æœæ­£ç¡®æ€§ï¼Ÿ\n",
    "**æ£€æŸ¥æ¸…å•ï¼š**\n",
    "- [ ] æœ€ç»ˆæ ·æœ¬æ•°åœ¨400k-450kä¹‹é—´\n",
    "- [ ] åŒ–åˆç‰©æ•°åœ¨9k-10kä¹‹é—´\n",
    "- [ ] ç»†èƒç³»æ•°~52\n",
    "- [ ] ä¸­ä½è§‚æµ‹æ•°~32\n",
    "- [ ] >100è§‚æµ‹çš„åŒ–åˆç‰©~750\n",
    "\n",
    "---\n",
    "\n",
    "## ä¹ã€è¿›ä¸€æ­¥ä¼˜åŒ–å»ºè®®\n",
    "\n",
    "### 1. ä½¿ç”¨Daskå¤„ç†è¶…å¤§æ•°æ®\n",
    "```python\n",
    "import dask.array as da\n",
    "dask_matrix = da.from_array(matrix, chunks=(10000, 978))\n",
    "```\n",
    "\n",
    "### 2. GPUåŠ é€Ÿï¼ˆå¦‚æœå¯ç”¨ï¼‰\n",
    "```python\n",
    "import cupy as cp\n",
    "gpu_matrix = cp.array(matrix)\n",
    "sim_matrix = cp.dot(gpu_matrix, gpu_matrix.T)  # GPUè®¡ç®—\n",
    "```\n",
    "\n",
    "### 3. å¹¶è¡ŒåŒ–åŒ–åˆç‰©å¤„ç†\n",
    "```python\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(process_compound)(pert_id) \n",
    "    for pert_id in unique_perts\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## åã€å¼•ç”¨ä¸è‡´è°¢\n",
    "\n",
    "**è®ºæ–‡å¼•ç”¨ï¼š**\n",
    "> DeMeo et al. (2025). Active learning framework leveraging transcriptomics \n",
    "> identifies modulators of disease phenotypes. Science. DOI: 10.1126/science.adi8577\n",
    "\n",
    "**å…³é”®å‚è€ƒï¼š**\n",
    "- Supplementary Materials, Page 2: \"Curating LINCS CMap into a training dataset\"\n",
    "- Materials and Methods: \"DrugReflector algorithm overview\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemprop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
