{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c56b21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DrugReflector åŸºåº§æ¨¡å‹è®­ç»ƒ\n",
    "ä¸¥æ ¼éµå¾ª Science 2025 Supplementary Materials (Pages 2-4)\n",
    "\n",
    "æ¨¡å‹æ¶æ„ï¼š\n",
    "- Input: 978 (landmark genes)\n",
    "- Hidden 1: 1,024 nodes + ReLU + Dropout(0.64) + BatchNorm\n",
    "- Hidden 2: 2,048 nodes + ReLU + Dropout(0.64) + BatchNorm\n",
    "- Output: 9,597 (compounds)\n",
    "\n",
    "è®­ç»ƒé…ç½®ï¼š\n",
    "- Focal Loss (Î³=2)\n",
    "- Cosine Annealing with Warm Restarts\n",
    "- 3-Fold Ensemble\n",
    "- 50 epochs per model\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from typing import Dict, Tuple, List\n",
    "from sklearn.metrics import accuracy_score, top_k_accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# æ•°æ®é¢„å¤„ç†\n",
    "# ============================================================================\n",
    "\n",
    "def clip_and_normalize_signature(X: np.ndarray, clip_range=(-2, 2)) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    è£å‰ªå¹¶æ ‡å‡†åŒ–ç­¾åï¼Œä½¿å…¶æ ‡å‡†å·®ä¸º1\n",
    "    \n",
    "    åŸæ–‡ï¼ˆSIç¬¬3é¡µï¼‰ï¼š\n",
    "    \"every transcriptional vector v is clipped to range [-2,2] such that \n",
    "    its standard deviation after clipping equals 1\"\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X: np.ndarray\n",
    "        åŸå§‹è¡¨è¾¾çŸ©é˜µ (n_samples, n_genes)\n",
    "    clip_range: tuple\n",
    "        è£å‰ªèŒƒå›´ï¼Œé»˜è®¤[-2, 2]\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    np.ndarray: å¤„ç†åçš„çŸ©é˜µ\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ“Š Clipping and normalizing signatures...\")\n",
    "    print(f\"   Clip range: {clip_range}\")\n",
    "    \n",
    "    X_processed = np.zeros_like(X)\n",
    "    \n",
    "    for i in range(X.shape[0]):\n",
    "        # è£å‰ªåˆ°æŒ‡å®šèŒƒå›´\n",
    "        vec = np.clip(X[i], clip_range[0], clip_range[1])\n",
    "        \n",
    "        # æ ‡å‡†åŒ–ä½¿æ ‡å‡†å·®ä¸º1\n",
    "        std = np.std(vec)\n",
    "        if std > 0:\n",
    "            vec = vec / std\n",
    "        \n",
    "        X_processed[i] = vec\n",
    "    \n",
    "    print(f\"   âœ“ Mean std after normalization: {np.std(X_processed, axis=1).mean():.4f}\")\n",
    "    print(f\"   âœ“ Data range after clipping: [{X_processed.min():.2f}, {X_processed.max():.2f}]\")\n",
    "    \n",
    "    return X_processed\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PyTorch Dataset\n",
    "# ============================================================================\n",
    "\n",
    "class LINCSDataset(Dataset):\n",
    "    \"\"\"LINCSæ•°æ®é›†ç±»\"\"\"\n",
    "    \n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray, fold_mask: np.ndarray = None):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: np.ndarray\n",
    "            è¡¨è¾¾çŸ©é˜µ (n_samples, 978)\n",
    "        y: np.ndarray\n",
    "            åŒ–åˆç‰©æ ‡ç­¾ (n_samples,)\n",
    "        fold_mask: np.ndarray\n",
    "            Foldæ©ç ï¼ŒTrueè¡¨ç¤ºåŒ…å«åœ¨æ­¤æ•°æ®é›†ä¸­\n",
    "        \"\"\"\n",
    "        if fold_mask is not None:\n",
    "            self.X = torch.FloatTensor(X[fold_mask])\n",
    "            self.y = torch.LongTensor(y[fold_mask])\n",
    "        else:\n",
    "            self.X = torch.FloatTensor(X)\n",
    "            self.y = torch.LongTensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Focal Loss\n",
    "# ============================================================================\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Focal Losså®ç°\n",
    "    \n",
    "    åŸæ–‡ï¼ˆSIç¬¬3é¡µï¼‰ï¼š\n",
    "    \"The models were trained using a focal loss function with a \n",
    "    focusing parameter of Î³ = 2\"\n",
    "    \n",
    "    Reference: Lin et al. \"Focal Loss for Dense Object Detection\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, gamma=2.0, alpha=None, reduction='mean'):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        gamma: float\n",
    "            èšç„¦å‚æ•°ï¼ŒåŸæ–‡ä½¿ç”¨2.0\n",
    "        alpha: Tensor or None\n",
    "            ç±»åˆ«æƒé‡\n",
    "        reduction: str\n",
    "            'mean', 'sum', or 'none'\n",
    "        \"\"\"\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        inputs: Tensor\n",
    "            æ¨¡å‹è¾“å‡º logits (N, C)\n",
    "        targets: Tensor\n",
    "            çœŸå®æ ‡ç­¾ (N,)\n",
    "        \"\"\"\n",
    "        # è®¡ç®—äº¤å‰ç†µ\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        \n",
    "        # è®¡ç®—pt\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        \n",
    "        # Focal loss\n",
    "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "        \n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha[targets]\n",
    "            focal_loss = alpha_t * focal_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# æ¨¡å‹æ¶æ„\n",
    "# ============================================================================\n",
    "\n",
    "class DrugReflectorModel(nn.Module):\n",
    "    \"\"\"\n",
    "    DrugReflectorç¥ç»ç½‘ç»œæ¶æ„\n",
    "    \n",
    "    åŸæ–‡ï¼ˆSIç¬¬2é¡µï¼‰ï¼š\n",
    "    \"The input layer has 978 nodes (one for each landmark gene), \n",
    "    and the output layer has 9,597 nodes (one for each target LINCS perturbation). \n",
    "    The first hidden layer has 1,024 nodes, and the second has 2,048 nodes \n",
    "    using rectified linear units (ReLU) to compute node activations.\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        input_size=978, \n",
    "        hidden1_size=1024, \n",
    "        hidden2_size=2048, \n",
    "        output_size=9597,\n",
    "        dropout_rate=0.64,\n",
    "        batch_norm_momentum=0.1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_size: int\n",
    "            è¾“å…¥ç‰¹å¾æ•°ï¼ˆlandmark genesï¼‰\n",
    "        hidden1_size: int\n",
    "            ç¬¬ä¸€éšè—å±‚å¤§å°\n",
    "        hidden2_size: int\n",
    "            ç¬¬äºŒéšè—å±‚å¤§å°\n",
    "        output_size: int\n",
    "            è¾“å‡ºç±»åˆ«æ•°ï¼ˆåŒ–åˆç‰©æ•°ï¼‰\n",
    "        dropout_rate: float\n",
    "            Dropoutæ¯”ç‡ï¼ŒåŸæ–‡0.64\n",
    "        batch_norm_momentum: float\n",
    "            BatchNormåŠ¨é‡ï¼ŒåŸæ–‡0.1\n",
    "        \"\"\"\n",
    "        super(DrugReflectorModel, self).__init__()\n",
    "        \n",
    "        # ç¬¬ä¸€éšè—å±‚\n",
    "        self.fc1 = nn.Linear(input_size, hidden1_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden1_size, momentum=batch_norm_momentum)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # ç¬¬äºŒéšè—å±‚\n",
    "        self.fc2 = nn.Linear(hidden1_size, hidden2_size)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden2_size, momentum=batch_norm_momentum)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # è¾“å‡ºå±‚\n",
    "        self.fc3 = nn.Linear(hidden2_size, output_size)\n",
    "        \n",
    "        # åˆå§‹åŒ–æƒé‡\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"æƒé‡åˆå§‹åŒ–\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        å‰å‘ä¼ æ’­\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x: Tensor\n",
    "            è¾“å…¥ (batch_size, 978)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        Tensor: è¾“å‡ºlogits (batch_size, 9597)\n",
    "        \"\"\"\n",
    "        # ç¬¬ä¸€éšè—å±‚\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # ç¬¬äºŒéšè—å±‚\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # è¾“å‡ºå±‚\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# è®­ç»ƒå™¨\n",
    "# ============================================================================\n",
    "\n",
    "class DrugReflectorTrainer:\n",
    "    \"\"\"\n",
    "    DrugReflectorè®­ç»ƒå™¨\n",
    "    \n",
    "    å®ç°3-fold ensembleè®­ç»ƒç­–ç•¥\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        initial_lr=0.0139,\n",
    "        min_lr=0.00001,\n",
    "        weight_decay=1e-5,\n",
    "        t_0=20,  # Time to first restart\n",
    "        t_mult=1.9,  # åŸæ–‡0.5ï¼Œä½†è¿™æ ·ä¼šå¯¼è‡´å‘¨æœŸå‡åŠï¼Œå®é™…åº”è¯¥>1\n",
    "        focal_gamma=2.0,\n",
    "        batch_size=256,\n",
    "        num_epochs=50,\n",
    "        early_stop_epoch=20\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        device: str\n",
    "            è®¡ç®—è®¾å¤‡\n",
    "        initial_lr: float\n",
    "            åˆå§‹å­¦ä¹ ç‡ï¼ˆåŸæ–‡ï¼š0.0139ï¼‰\n",
    "        min_lr: float\n",
    "            æœ€å°å­¦ä¹ ç‡ï¼ˆåŸæ–‡ï¼š0.00001ï¼‰\n",
    "        weight_decay: float\n",
    "            æƒé‡è¡°å‡ï¼ˆåŸæ–‡ï¼š1e-5ï¼‰\n",
    "        t_0: int\n",
    "            ç¬¬ä¸€æ¬¡warm restartå‰çš„epochæ•°ï¼ˆåŸæ–‡ï¼š20ï¼‰\n",
    "        t_mult: float\n",
    "            warm restartå‘¨æœŸå€å¢å› å­ï¼ˆåŸæ–‡ï¼š0.5ï¼‰\n",
    "        focal_gamma: float\n",
    "            Focal lossèšç„¦å‚æ•°ï¼ˆåŸæ–‡ï¼š2.0ï¼‰\n",
    "        batch_size: int\n",
    "            æ‰¹æ¬¡å¤§å°\n",
    "        num_epochs: int\n",
    "            æ€»è®­ç»ƒè½®æ•°ï¼ˆåŸæ–‡ï¼š50ï¼‰\n",
    "        early_stop_epoch: int\n",
    "            æ—©åœæ£€æŸ¥ç‚¹ï¼ˆåŸæ–‡ï¼š20ï¼‰\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.initial_lr = initial_lr\n",
    "        self.min_lr = min_lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.t_0 = t_0\n",
    "        self.t_mult = t_mult\n",
    "        self.focal_gamma = focal_gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.early_stop_epoch = early_stop_epoch\n",
    "        \n",
    "        print(f\"\\nğŸš€ DrugReflector Trainer initialized\")\n",
    "        print(f\"   Device: {self.device}\")\n",
    "        print(f\"   Initial LR: {self.initial_lr}\")\n",
    "        print(f\"   Min LR: {self.min_lr}\")\n",
    "        print(f\"   Weight Decay: {self.weight_decay}\")\n",
    "        print(f\"   T_0 (first restart): {self.t_0}\")\n",
    "        print(f\"   Focal Î³: {self.focal_gamma}\")\n",
    "        print(f\"   Batch size: {self.batch_size}\")\n",
    "        print(f\"   Epochs: {self.num_epochs}\")\n",
    "    \n",
    "    def train_single_model(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: DataLoader,\n",
    "        fold_id: int\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        è®­ç»ƒå•ä¸ªæ¨¡å‹\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        model: nn.Module\n",
    "            DrugReflectoræ¨¡å‹\n",
    "        train_loader: DataLoader\n",
    "            è®­ç»ƒæ•°æ®åŠ è½½å™¨\n",
    "        val_loader: DataLoader\n",
    "            éªŒè¯æ•°æ®åŠ è½½å™¨\n",
    "        fold_id: int\n",
    "            Fold ID (0, 1, 2)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        Dict: è®­ç»ƒå†å²\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Training Model {fold_id}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨\n",
    "        criterion = FocalLoss(gamma=self.focal_gamma)\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=self.initial_lr,\n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "        \n",
    "        # å­¦ä¹ ç‡è°ƒåº¦å™¨\n",
    "        scheduler = CosineAnnealingWarmRestarts(\n",
    "            optimizer,\n",
    "            T_0=self.t_0,\n",
    "            T_mult=int(self.t_mult) if self.t_mult >= 1 else 1,\n",
    "            eta_min=self.min_lr\n",
    "        )\n",
    "        \n",
    "        # è®­ç»ƒå†å²\n",
    "        history = {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'val_recall': [],\n",
    "            'val_top1_acc': [],\n",
    "            'val_top10_acc': [],\n",
    "            'learning_rates': []\n",
    "        }\n",
    "        \n",
    "        best_recall = 0.0\n",
    "        best_epoch = 0\n",
    "        best_model_state = None\n",
    "        \n",
    "        # è®­ç»ƒå¾ªç¯\n",
    "        for epoch in range(self.num_epochs):\n",
    "            # ========== è®­ç»ƒé˜¶æ®µ ==========\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            \n",
    "            pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{self.num_epochs}\")\n",
    "            for batch_X, batch_y in pbar:\n",
    "                batch_X = batch_X.to(self.device)\n",
    "                batch_y = batch_y.to(self.device)\n",
    "                \n",
    "                # å‰å‘ä¼ æ’­\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                \n",
    "                # åå‘ä¼ æ’­\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                pbar.set_postfix({'loss': loss.item()})\n",
    "            \n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            \n",
    "            # ========== éªŒè¯é˜¶æ®µ ==========\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "            all_probs = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch_X, batch_y in val_loader:\n",
    "                    batch_X = batch_X.to(self.device)\n",
    "                    batch_y = batch_y.to(self.device)\n",
    "                    \n",
    "                    outputs = model(batch_X)\n",
    "                    loss = criterion(outputs, batch_y)\n",
    "                    val_loss += loss.item()\n",
    "                    \n",
    "                    # è®¡ç®—é¢„æµ‹\n",
    "                    probs = F.softmax(outputs, dim=1)\n",
    "                    preds = torch.argmax(probs, dim=1)\n",
    "                    \n",
    "                    all_preds.append(preds.cpu().numpy())\n",
    "                    all_labels.append(batch_y.cpu().numpy())\n",
    "                    all_probs.append(probs.cpu().numpy())\n",
    "            \n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            \n",
    "            # è®¡ç®—æŒ‡æ ‡\n",
    "            all_preds = np.concatenate(all_preds)\n",
    "            all_labels = np.concatenate(all_labels)\n",
    "            all_probs = np.concatenate(all_probs)\n",
    "            \n",
    "            top1_acc = accuracy_score(all_labels, all_preds)\n",
    "            top10_acc = top_k_accuracy_score(all_labels, all_probs, k=10)\n",
    "            \n",
    "            # è®¡ç®—Top 1% recall (åŸæ–‡ä¸»è¦æŒ‡æ ‡)\n",
    "            top1_percent_k = max(1, int(0.01 * all_probs.shape[1]))\n",
    "            recall = top_k_accuracy_score(all_labels, all_probs, k=top1_percent_k)\n",
    "            \n",
    "            # è®°å½•å†å²\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            history['train_loss'].append(avg_train_loss)\n",
    "            history['val_loss'].append(avg_val_loss)\n",
    "            history['val_recall'].append(recall)\n",
    "            history['val_top1_acc'].append(top1_acc)\n",
    "            history['val_top10_acc'].append(top10_acc)\n",
    "            history['learning_rates'].append(current_lr)\n",
    "            \n",
    "            # æ‰“å°è¿›åº¦\n",
    "            print(f\"\\nEpoch {epoch+1}/{self.num_epochs}\")\n",
    "            print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "            print(f\"  Val Loss: {avg_val_loss:.4f}\")\n",
    "            print(f\"  Val Recall (top 1%): {recall:.4f}\")\n",
    "            print(f\"  Val Top-1 Acc: {top1_acc:.4f}\")\n",
    "            print(f\"  Val Top-10 Acc: {top10_acc:.4f}\")\n",
    "            print(f\"  Learning Rate: {current_lr:.6f}\")\n",
    "            \n",
    "            # ä¿å­˜æœ€ä½³æ¨¡å‹\n",
    "            if recall > best_recall:\n",
    "                best_recall = recall\n",
    "                best_epoch = epoch\n",
    "                best_model_state = model.state_dict().copy()\n",
    "                print(f\"  âœ“ New best model! (Recall: {best_recall:.4f})\")\n",
    "            \n",
    "            # å­¦ä¹ ç‡è°ƒåº¦\n",
    "            scheduler.step()\n",
    "        \n",
    "        # åŠ è½½æœ€ä½³æ¨¡å‹\n",
    "        if best_model_state is not None:\n",
    "            model.load_state_dict(best_model_state)\n",
    "            print(f\"\\nâœ“ Loaded best model from epoch {best_epoch+1} \"\n",
    "                  f\"(Recall: {best_recall:.4f})\")\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def train_ensemble(\n",
    "        self,\n",
    "        training_data: Dict,\n",
    "        output_dir: Path\n",
    "    ) -> List[nn.Module]:\n",
    "        \"\"\"\n",
    "        è®­ç»ƒ3-fold ensemble\n",
    "        \n",
    "        åŸæ–‡ï¼ˆSIç¬¬2-3é¡µï¼‰ï¼š\n",
    "        \"The training data was divided randomly into three folds, with perturbation \n",
    "        replicates balanced across the folds. Models were independently trained on \n",
    "        two of three folds.\"\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        training_data: Dict\n",
    "            åŒ…å«X, y, foldsçš„è®­ç»ƒæ•°æ®\n",
    "        output_dir: Path\n",
    "            æ¨¡å‹ä¿å­˜ç›®å½•\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        List[nn.Module]: è®­ç»ƒå¥½çš„3ä¸ªæ¨¡å‹\n",
    "        \"\"\"\n",
    "        X = training_data['X']\n",
    "        y = training_data['y']\n",
    "        folds = training_data['folds']\n",
    "        n_compounds = len(training_data['compound_names'])\n",
    "        \n",
    "        # é¢„å¤„ç†æ•°æ®\n",
    "        X_processed = clip_and_normalize_signature(X)\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ğŸ¯ Training 3-Fold Ensemble\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"  Total samples: {len(X):,}\")\n",
    "        print(f\"  Total compounds: {n_compounds:,}\")\n",
    "        print(f\"  Input features: {X.shape[1]}\")\n",
    "        \n",
    "        models = []\n",
    "        histories = []\n",
    "        \n",
    "        # è®­ç»ƒ3ä¸ªæ¨¡å‹\n",
    "        for fold_id in range(3):\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"Training Fold {fold_id} Model\")\n",
    "            print(f\"{'='*80}\")\n",
    "            \n",
    "            # å‡†å¤‡æ•°æ®\n",
    "            # æ¨¡å‹åœ¨fold_idä¸ŠéªŒè¯ï¼Œåœ¨å…¶ä»–ä¸¤ä¸ªfoldä¸Šè®­ç»ƒ\n",
    "            val_mask = folds == fold_id\n",
    "            train_mask = ~val_mask\n",
    "            \n",
    "            print(f\"  Training samples: {train_mask.sum():,}\")\n",
    "            print(f\"  Validation samples: {val_mask.sum():,}\")\n",
    "            \n",
    "            # åˆ›å»ºæ•°æ®é›†\n",
    "            train_dataset = LINCSDataset(X_processed, y, train_mask)\n",
    "            val_dataset = LINCSDataset(X_processed, y, val_mask)\n",
    "            \n",
    "            train_loader = DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size=self.batch_size,\n",
    "                shuffle=True,\n",
    "                num_workers=4,\n",
    "                pin_memory=True\n",
    "            )\n",
    "            \n",
    "            val_loader = DataLoader(\n",
    "                val_dataset,\n",
    "                batch_size=self.batch_size,\n",
    "                shuffle=False,\n",
    "                num_workers=4,\n",
    "                pin_memory=True\n",
    "            )\n",
    "            \n",
    "            # åˆ›å»ºæ¨¡å‹\n",
    "            model = DrugReflectorModel(\n",
    "                input_size=X.shape[1],\n",
    "                output_size=n_compounds,\n",
    "                dropout_rate=0.64,\n",
    "                batch_norm_momentum=0.1\n",
    "            ).to(self.device)\n",
    "            \n",
    "            print(f\"\\n  Model architecture:\")\n",
    "            print(f\"    Input: {X.shape[1]} â†’ Hidden1: 1024 â†’ Hidden2: 2048 â†’ Output: {n_compounds}\")\n",
    "            print(f\"    Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "            \n",
    "            # è®­ç»ƒæ¨¡å‹\n",
    "            history = self.train_single_model(\n",
    "                model,\n",
    "                train_loader,\n",
    "                val_loader,\n",
    "                fold_id\n",
    "            )\n",
    "            \n",
    "            models.append(model)\n",
    "            histories.append(history)\n",
    "            \n",
    "            # ä¿å­˜æ¨¡å‹\n",
    "            model_path = output_dir / f\"model_fold{fold_id}.pt\"\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'fold_id': fold_id,\n",
    "                'history': history,\n",
    "                'config': {\n",
    "                    'input_size': X.shape[1],\n",
    "                    'output_size': n_compounds,\n",
    "                    'dropout_rate': 0.64,\n",
    "                    'batch_norm_momentum': 0.1\n",
    "                }\n",
    "            }, model_path)\n",
    "            print(f\"\\n  âœ“ Model saved to {model_path}\")\n",
    "        \n",
    "        # ä¿å­˜ensembleå†å²\n",
    "        ensemble_history_path = output_dir / \"ensemble_history.pkl\"\n",
    "        with open(ensemble_history_path, 'wb') as f:\n",
    "            pickle.dump(histories, f)\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"âœ… Ensemble Training Complete!\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"  Models saved to: {output_dir}\")\n",
    "        \n",
    "        return models, histories\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# æ¨¡å‹è¯„ä¼°\n",
    "# ============================================================================\n",
    "\n",
    "class DrugReflectorEvaluator:\n",
    "    \"\"\"æ¨¡å‹è¯„ä¼°å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, models: List[nn.Module], device='cuda'):\n",
    "        self.models = models\n",
    "        self.device = device\n",
    "        for model in self.models:\n",
    "            model.eval()\n",
    "    \n",
    "    def predict_ensemble(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Ensembleé¢„æµ‹\n",
    "        \n",
    "        åŸæ–‡ï¼ˆSIç¬¬2é¡µï¼‰ï¼š\n",
    "        \"The final predicted class probabilities were the softmax probabilities \n",
    "        of the average score over all three folds.\"\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: np.ndarray\n",
    "            è¾“å…¥æ•°æ® (n_samples, 978)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        np.ndarray: å¹³å‡å¾—åˆ† (n_samples, n_compounds)\n",
    "        \"\"\"\n",
    "        X_tensor = torch.FloatTensor(X).to(self.device)\n",
    "        \n",
    "        all_scores = []\n",
    "        with torch.no_grad():\n",
    "            for model in self.models:\n",
    "                scores = model(X_tensor)\n",
    "                all_scores.append(scores.cpu().numpy())\n",
    "        \n",
    "        # å¹³å‡å¾—åˆ†\n",
    "        avg_scores = np.mean(all_scores, axis=0)\n",
    "        \n",
    "        return avg_scores\n",
    "    \n",
    "    def evaluate(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        compound_names: List[str]\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        è¯„ä¼°ensembleæ€§èƒ½\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: np.ndarray\n",
    "            æµ‹è¯•æ•°æ®\n",
    "        y: np.ndarray\n",
    "            çœŸå®æ ‡ç­¾\n",
    "        compound_names: List[str]\n",
    "            åŒ–åˆç‰©åç§°åˆ—è¡¨\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        Dict: è¯„ä¼°æŒ‡æ ‡\n",
    "        \"\"\"\n",
    "        print(f\"\\nğŸ“Š Evaluating ensemble performance...\")\n",
    "        \n",
    "        # é¢„å¤„ç†\n",
    "        X_processed = clip_and_normalize_signature(X)\n",
    "        \n",
    "        # é¢„æµ‹\n",
    "        avg_scores = self.predict_ensemble(X_processed)\n",
    "        probs = torch.softmax(torch.FloatTensor(avg_scores), dim=1).numpy()\n",
    "        preds = np.argmax(avg_scores, axis=1)\n",
    "        \n",
    "        # è®¡ç®—æŒ‡æ ‡\n",
    "        top1_acc = accuracy_score(y, preds)\n",
    "        top10_acc = top_k_accuracy_score(y, probs, k=10)\n",
    "        \n",
    "        # Top 1% recall\n",
    "        top1_percent_k = max(1, int(0.01 * probs.shape[1]))\n",
    "        recall = top_k_accuracy_score(y, probs, k=top1_percent_k)\n",
    "        \n",
    "        print(f\"  Top-1 Accuracy: {top1_acc:.4f}\")\n",
    "        print(f\"  Top-10 Accuracy: {top10_acc:.4f}\")\n",
    "        print(f\"  Top 1% Recall: {recall:.4f}\")\n",
    "        \n",
    "        # è®¡ç®—æ¯ä¸ªåŒ–åˆç‰©çš„recall\n",
    "        compound_recalls = []\n",
    "        for compound_idx in range(len(compound_names)):\n",
    "            mask = y == compound_idx\n",
    "            if mask.sum() == 0:\n",
    "                continue\n",
    "            \n",
    "            compound_probs = probs[mask]\n",
    "            compound_labels = y[mask]\n",
    "            \n",
    "            if len(compound_probs) > 0:\n",
    "                compound_recall = top_k_accuracy_score(\n",
    "                    compound_labels, \n",
    "                    compound_probs, \n",
    "                    k=top1_percent_k\n",
    "                )\n",
    "                compound_recalls.append(compound_recall)\n",
    "        \n",
    "        avg_compound_recall = np.mean(compound_recalls)\n",
    "        \n",
    "        print(f\"  Average per-compound recall: {avg_compound_recall:.4f}\")\n",
    "        \n",
    "        results = {\n",
    "            'top1_accuracy': top1_acc,\n",
    "            'top10_accuracy': top10_acc,\n",
    "            'top1_percent_recall': recall,\n",
    "            'avg_compound_recall': avg_compound_recall,\n",
    "            'compound_recalls': compound_recalls,\n",
    "            'predictions': preds,\n",
    "            'probabilities': probs\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# å¯è§†åŒ–\n",
    "# ============================================================================\n",
    "\n",
    "def plot_training_history(histories: List[Dict], output_dir: Path):\n",
    "    \"\"\"\n",
    "    ç»˜åˆ¶è®­ç»ƒå†å²\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    histories: List[Dict]\n",
    "        3ä¸ªæ¨¡å‹çš„è®­ç»ƒå†å²\n",
    "    output_dir: Path\n",
    "        è¾“å‡ºç›®å½•\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    \n",
    "    metrics = [\n",
    "        ('train_loss', 'Training Loss', 'Loss'),\n",
    "        ('val_loss', 'Validation Loss', 'Loss'),\n",
    "        ('val_recall', 'Validation Recall (Top 1%)', 'Recall'),\n",
    "        ('val_top1_acc', 'Top-1 Accuracy', 'Accuracy'),\n",
    "        ('val_top10_acc', 'Top-10 Accuracy', 'Accuracy'),\n",
    "        ('learning_rates', 'Learning Rate', 'LR')\n",
    "    ]\n",
    "    \n",
    "    for idx, (metric, title, ylabel) in enumerate(metrics):\n",
    "        ax = axes[idx // 3, idx % 3]\n",
    "        \n",
    "        for fold_id, history in enumerate(histories):\n",
    "            epochs = range(1, len(history[metric]) + 1)\n",
    "            ax.plot(epochs, history[metric], label=f'Fold {fold_id}', linewidth=2)\n",
    "        \n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel(ylabel)\n",
    "        ax.set_title(title)\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = output_dir / 'training_history.png'\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\nâœ“ Training history plot saved to {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ä¸»è®­ç»ƒæµç¨‹\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"ä¸»è®­ç»ƒæµç¨‹\"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"ğŸš€ DrugReflector Base Model Training\")\n",
    "    print(\"   Following Science 2025 Supplementary Materials\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # è®¾ç½®è·¯å¾„\n",
    "    data_dir = Path(\"E:/ç§‘ç ”/Models/drugreflector/processed_data\")\n",
    "    output_dir = Path(\"E:/ç§‘ç ”/Models/drugreflector/trained_models\")\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # åŠ è½½æ•°æ®\n",
    "    print(f\"\\nğŸ“‚ Loading preprocessed data...\")\n",
    "    data_file = data_dir / \"training_data_paper_compliant.pkl\"\n",
    "    \n",
    "    if not data_file.exists():\n",
    "        print(f\"âŒ Data file not found: {data_file}\")\n",
    "        print(f\"   Please run the preprocessing script first.\")\n",
    "        return\n",
    "    \n",
    "    with open(data_file, 'rb') as f:\n",
    "        training_data = pickle.load(f)\n",
    "    \n",
    "    print(f\"âœ“ Loaded training data\")\n",
    "    print(f\"  Samples: {len(training_data['X']):,}\")\n",
    "    print(f\"  Compounds: {len(training_data['compound_names']):,}\")\n",
    "    print(f\"  Features: {training_data['X'].shape[1]}\")\n",
    "    \n",
    "    # åˆ›å»ºè®­ç»ƒå™¨\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"\\nğŸ”§ Initializing trainer...\")\n",
    "    print(f\"  Device: {device}\")\n",
    "    \n",
    "    if device == 'cuda':\n",
    "        print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    \n",
    "    trainer = DrugReflectorTrainer(\n",
    "        device=device,\n",
    "        initial_lr=0.0139,\n",
    "        min_lr=0.00001,\n",
    "        weight_decay=1e-5,\n",
    "        t_0=20,\n",
    "        focal_gamma=2.0,\n",
    "        batch_size=256,\n",
    "        num_epochs=50,\n",
    "        early_stop_epoch=20\n",
    "    )\n",
    "    \n",
    "    # è®­ç»ƒensemble\n",
    "    models, histories = trainer.train_ensemble(training_data, output_dir)\n",
    "    \n",
    "    # å¯è§†åŒ–è®­ç»ƒå†å²\n",
    "    print(f\"\\nğŸ“Š Plotting training history...\")\n",
    "    plot_training_history(histories, output_dir)\n",
    "    \n",
    "    # è¯„ä¼°æ¨¡å‹\n",
    "    print(f\"\\nğŸ“Š Evaluating ensemble on validation sets...\")\n",
    "    evaluator = DrugReflectorEvaluator(models, device)\n",
    "    \n",
    "    # åœ¨æ¯ä¸ªfoldä¸Šè¯„ä¼°\n",
    "    for fold_id in range(3):\n",
    "        val_mask = training_data['folds'] == fold_id\n",
    "        X_val = training_data['X'][val_mask]\n",
    "        y_val = training_data['y'][val_mask]\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Fold {fold_id} Validation Performance\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        results = evaluator.evaluate(\n",
    "            X_val,\n",
    "            y_val,\n",
    "            training_data['compound_names']\n",
    "        )\n",
    "        \n",
    "        # ä¿å­˜ç»“æœ\n",
    "        results_path = output_dir / f\"fold{fold_id}_results.pkl\"\n",
    "        with open(results_path, 'wb') as f:\n",
    "            pickle.dump(results, f)\n",
    "        print(f\"  âœ“ Results saved to {results_path}\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"âœ… TRAINING COMPLETE!\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"ğŸ“ All outputs saved to: {output_dir}\")\n",
    "    print(f\"\\nFiles created:\")\n",
    "    print(f\"  â€¢ model_fold0.pt - Fold 0 model checkpoint\")\n",
    "    print(f\"  â€¢ model_fold1.pt - Fold 1 model checkpoint\")\n",
    "    print(f\"  â€¢ model_fold2.pt - Fold 2 model checkpoint\")\n",
    "    print(f\"  â€¢ ensemble_history.pkl - Training history\")\n",
    "    print(f\"  â€¢ training_history.png - Training curves\")\n",
    "    print(f\"  â€¢ fold*_results.pkl - Evaluation results\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7fe306",
   "metadata": {},
   "source": [
    "# DrugReflector åŸºåº§æ¨¡å‹è®­ç»ƒè¯¦ç»†è¯´æ˜\n",
    "\n",
    "## ğŸ“‹ ç›®å½•\n",
    "1. [è®­ç»ƒæµç¨‹æ¦‚è¿°](#è®­ç»ƒæµç¨‹æ¦‚è¿°)\n",
    "2. [å…³é”®æŠ€æœ¯ç»†èŠ‚](#å…³é”®æŠ€æœ¯ç»†èŠ‚)\n",
    "3. [ä»£ç ç»“æ„](#ä»£ç ç»“æ„)\n",
    "4. [ä½¿ç”¨æ–¹æ³•](#ä½¿ç”¨æ–¹æ³•)\n",
    "5. [è¶…å‚æ•°è¯´æ˜](#è¶…å‚æ•°è¯´æ˜)\n",
    "6. [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)\n",
    "7. [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)\n",
    "\n",
    "---\n",
    "\n",
    "## è®­ç»ƒæµç¨‹æ¦‚è¿°\n",
    "\n",
    "DrugReflectoré‡‡ç”¨**3-fold ensemble**ç­–ç•¥è®­ç»ƒåŸºåº§æ¨¡å‹ï¼Œå®Œå…¨æŒ‰ç…§Science 2025è¡¥å……ææ–™ç¬¬2-4é¡µçš„æ–¹æ³•å®ç°ã€‚\n",
    "\n",
    "### æ•´ä½“æµç¨‹\n",
    "\n",
    "```\n",
    "è¾“å…¥æ•°æ® (425,242 Ã— 978)\n",
    "    â†“\n",
    "æ•°æ®æ ‡å‡†åŒ–ï¼ˆclip & normalizeï¼‰\n",
    "    â†“\n",
    "3-fold åˆ’åˆ†\n",
    "    â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Model 0    â”‚  Model 1    â”‚  Model 2    â”‚\n",
    "â”‚ Train: 1,2  â”‚ Train: 0,2  â”‚ Train: 0,1  â”‚\n",
    "â”‚ Val:   0    â”‚ Val:   1    â”‚ Val:   2    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "    â†“\n",
    "Ensembleé¢„æµ‹ï¼ˆå¹³å‡å¾—åˆ†ï¼‰\n",
    "    â†“\n",
    "æœ€ç»ˆè¾“å‡º\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## å…³é”®æŠ€æœ¯ç»†èŠ‚\n",
    "\n",
    "### 1. æ•°æ®é¢„å¤„ç† â­\n",
    "\n",
    "**åŸæ–‡ï¼ˆSI ç¬¬3é¡µï¼‰ï¼š**\n",
    "> \"every transcriptional vector v is clipped to range [-2,2] such that its standard deviation after clipping equals 1\"\n",
    "\n",
    "**å®ç°ï¼š**\n",
    "```python\n",
    "def clip_and_normalize_signature(X, clip_range=(-2, 2)):\n",
    "    X_processed = np.zeros_like(X)\n",
    "    for i in range(X.shape[0]):\n",
    "        # Step 1: è£å‰ªåˆ°[-2, 2]\n",
    "        vec = np.clip(X[i], clip_range[0], clip_range[1])\n",
    "        \n",
    "        # Step 2: æ ‡å‡†åŒ–ä½¿æ ‡å‡†å·®ä¸º1\n",
    "        std = np.std(vec)\n",
    "        if std > 0:\n",
    "            vec = vec / std\n",
    "        \n",
    "        X_processed[i] = vec\n",
    "    return X_processed\n",
    "```\n",
    "\n",
    "**ä¸ºä»€ä¹ˆè¿™æ ·åšï¼Ÿ**\n",
    "- **è£å‰ª**ï¼šå»é™¤æç«¯å¼‚å¸¸å€¼ï¼Œé¿å…æ¨¡å‹è¢«outlierä¸»å¯¼\n",
    "- **æ ‡å‡†åŒ–**ï¼šç¡®ä¿æ‰€æœ‰æ ·æœ¬åœ¨ç›¸åŒå°ºåº¦ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§\n",
    "- **æ··åˆè¡¨ç¤º**ï¼šä»‹äºäºŒå€¼åŒ–ï¼ˆä¸Šè°ƒ/ä¸‹è°ƒï¼‰å’Œè¿ç»­å€¼ä¹‹é—´\n",
    "\n",
    "---\n",
    "\n",
    "### 2. æ¨¡å‹æ¶æ„ ğŸ—ï¸\n",
    "\n",
    "**åŸæ–‡ï¼ˆSI ç¬¬2é¡µï¼‰ï¼š**\n",
    "> \"The input layer has 978 nodes (one for each landmark gene), and the output layer has 9,597 nodes (one for each target LINCS perturbation). The first hidden layer has 1,024 nodes, and the second has 2,048 nodes using rectified linear units (ReLU).\"\n",
    "\n",
    "**å®Œæ•´æ¶æ„ï¼š**\n",
    "\n",
    "```\n",
    "Input (978 genes)\n",
    "    â†“\n",
    "Linear(978 â†’ 1024)\n",
    "    â†“\n",
    "BatchNorm1d(momentum=0.1)\n",
    "    â†“\n",
    "ReLU\n",
    "    â†“\n",
    "Dropout(p=0.64)\n",
    "    â†“\n",
    "Linear(1024 â†’ 2048)\n",
    "    â†“\n",
    "BatchNorm1d(momentum=0.1)\n",
    "    â†“\n",
    "ReLU\n",
    "    â†“\n",
    "Dropout(p=0.64)\n",
    "    â†“\n",
    "Linear(2048 â†’ 9597)\n",
    "    â†“\n",
    "Output (9597 compounds)\n",
    "```\n",
    "\n",
    "**å‚æ•°é‡ï¼š**\n",
    "- Layer 1: 978 Ã— 1024 = 1,001,472\n",
    "- Layer 2: 1024 Ã— 2048 = 2,097,152\n",
    "- Layer 3: 2048 Ã— 9597 = 19,654,656\n",
    "- **æ€»è®¡ï¼š~22.7M å‚æ•°**\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Focal Loss ğŸ¯\n",
    "\n",
    "**åŸæ–‡ï¼ˆSI ç¬¬3é¡µï¼‰ï¼š**\n",
    "> \"The models were trained using a focal loss function with a focusing parameter of Î³ = 2\"\n",
    "\n",
    "**å…¬å¼ï¼š**\n",
    "```\n",
    "FL(pt) = -(1 - pt)^Î³ * log(pt)\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "- pt = softmax(logits)[true_class]\n",
    "- Î³ = 2.0 (focusing parameter)\n",
    "```\n",
    "\n",
    "**ä¸ºä»€ä¹ˆä½¿ç”¨Focal Lossï¼Ÿ**\n",
    "1. **ç±»åˆ«ä¸å¹³è¡¡**ï¼š9,597ä¸ªåŒ–åˆç‰©ï¼Œæ ·æœ¬åˆ†å¸ƒæä¸å‡åŒ€\n",
    "2. **å…³æ³¨éš¾æ ·æœ¬**ï¼š(1-pt)^Î³ æƒé‡ä½¿æ¨¡å‹æ›´å…³æ³¨é”™åˆ†æ ·æœ¬\n",
    "3. **å‡å°‘ç®€å•æ ·æœ¬è´¡çŒ®**ï¼šæ˜“åˆ†æ ·æœ¬çš„lossè¢«å¤§å¹…é™ä½\n",
    "\n",
    "**ä¸äº¤å‰ç†µå¯¹æ¯”ï¼š**\n",
    "```\n",
    "æ ·æœ¬ç½®ä¿¡åº¦    Cross Entropy Loss    Focal Loss (Î³=2)\n",
    "  0.99              0.01                 0.0001  (â†“100x)\n",
    "  0.90              0.11                 0.0011  (â†“100x)\n",
    "  0.50              0.69                 0.1725  (â†“4x)\n",
    "  0.10              2.30                 1.863   (ç›¸è¿‘)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. å­¦ä¹ ç‡è°ƒåº¦ ğŸ“ˆ\n",
    "\n",
    "**åŸæ–‡ï¼ˆSI ç¬¬3-4é¡µï¼‰ï¼š**\n",
    "> \"The learning rate was determined by a cosine annealing schedule with warm restarts, with 20 epochs before the first restart, an initial learning rate of 0.0139, and a minimum learning rate of 0.00001.\"\n",
    "\n",
    "**Cosine Annealing with Warm Restartsï¼š**\n",
    "\n",
    "```python\n",
    "scheduler = CosineAnnealingWarmRestarts(\n",
    "    optimizer,\n",
    "    T_0=20,           # ç¬¬ä¸€æ¬¡é‡å¯å‰çš„epochæ•°\n",
    "    T_mult=1,         # å‘¨æœŸå€å¢å› å­\n",
    "    eta_min=0.00001   # æœ€å°å­¦ä¹ ç‡\n",
    ")\n",
    "```\n",
    "\n",
    "**å­¦ä¹ ç‡å˜åŒ–æ›²çº¿ï¼š**\n",
    "```\n",
    "LR\n",
    " |\n",
    "0.0139 |    â•±â•²         â•±â•²         â•±â•²\n",
    "       |   â•±  â•²       â•±  â•²       â•±  â•²\n",
    "       |  â•±    â•²     â•±    â•²     â•±    â•²\n",
    "0.00001|â”€â•±â”€â”€â”€â”€â”€â”€â•²â”€â”€â”€â•±â”€â”€â”€â”€â”€â”€â•²â”€â”€â”€â•±â”€â”€â”€â”€â”€â”€â•²â”€â”€\n",
    "       |          â•² â•±        â•² â•±        â•²\n",
    "       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€ Epoch\n",
    "       0          20         40         50\n",
    "       \n",
    "       â† T_0=20 â†’  â† T_0=20 â†’  â† T_0=20 â†’\n",
    "```\n",
    "\n",
    "**Warm Restartçš„å¥½å¤„ï¼š**\n",
    "1. **é€ƒç¦»å±€éƒ¨æœ€ä¼˜**ï¼šå‘¨æœŸæ€§æé«˜LRå¸®åŠ©è·³å‡ºå±€éƒ¨æœ€å°å€¼\n",
    "2. **å¤šæ¬¡ç²¾ç»†æœç´¢**ï¼šæ¯æ¬¡é™ä½æ—¶éƒ½èƒ½ç»†åŒ–å‚æ•°\n",
    "3. **ensembleæ•ˆåº”**ï¼šä¸åŒrestarté˜¶æ®µçš„æ¨¡å‹å¯ä»¥äº’è¡¥\n",
    "\n",
    "---\n",
    "\n",
    "### 5. 3-Fold Ensemble ğŸ­\n",
    "\n",
    "**åŸæ–‡ï¼ˆSI ç¬¬2é¡µï¼‰ï¼š**\n",
    "> \"The training data was divided randomly into three folds, with perturbation replicates balanced across the folds. Models were independently trained on two of three folds.\"\n",
    "\n",
    "**åˆ’åˆ†ç­–ç•¥ï¼š**\n",
    "```python\n",
    "# å…³é”®ï¼šåŒä¸€åŒ–åˆç‰©çš„æ‰€æœ‰replicateåœ¨åŒä¸€fold\n",
    "for pert_id in unique_compounds:\n",
    "    pert_indices = np.where(sample_meta['pert_id'] == pert_id)[0]\n",
    "    # éšæœºæ‰“ä¹±å¹¶å‡åŒ€åˆ†é…åˆ°3ä¸ªfold\n",
    "    randomly_assign_to_folds(pert_indices)\n",
    "```\n",
    "\n",
    "**è®­ç»ƒå’ŒéªŒè¯ï¼š**\n",
    "| Model | Training Folds | Validation Fold | Samples (Train) | Samples (Val) |\n",
    "|-------|----------------|-----------------|-----------------|---------------|\n",
    "| 0     | 1, 2           | 0               | ~283k           | ~142k         |\n",
    "| 1     | 0, 2           | 1               | ~283k           | ~142k         |\n",
    "| 2     | 0, 1           | 2               | ~283k           | ~142k         |\n",
    "\n",
    "**Ensembleé¢„æµ‹ï¼š**\n",
    "```python\n",
    "# å¹³å‡3ä¸ªæ¨¡å‹çš„logitsï¼ˆæ³¨æ„ï¼šä¸æ˜¯æ¦‚ç‡ï¼‰\n",
    "avg_scores = (model0_logits + model1_logits + model2_logits) / 3\n",
    "\n",
    "# ç„¶åè®¡ç®—softmax\n",
    "final_probs = softmax(avg_scores)\n",
    "\n",
    "# æ’åºå¾—åˆ°æœ€ç»ˆé¢„æµ‹\n",
    "ranks = argsort(-final_probs)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ä»£ç ç»“æ„\n",
    "\n",
    "```\n",
    "drugreflector_training.py\n",
    "â”œâ”€â”€ clip_and_normalize_signature()    # æ•°æ®é¢„å¤„ç†\n",
    "â”œâ”€â”€ LINCSDataset                       # PyTorchæ•°æ®é›†\n",
    "â”œâ”€â”€ FocalLoss                          # Focal Losså®ç°\n",
    "â”œâ”€â”€ DrugReflectorModel                 # æ¨¡å‹æ¶æ„\n",
    "â”œâ”€â”€ DrugReflectorTrainer               # è®­ç»ƒå™¨\n",
    "â”‚   â”œâ”€â”€ train_single_model()          # å•æ¨¡å‹è®­ç»ƒ\n",
    "â”‚   â””â”€â”€ train_ensemble()              # 3-fold ensembleè®­ç»ƒ\n",
    "â”œâ”€â”€ DrugReflectorEvaluator            # è¯„ä¼°å™¨\n",
    "â”‚   â”œâ”€â”€ predict_ensemble()            # Ensembleé¢„æµ‹\n",
    "â”‚   â””â”€â”€ evaluate()                    # æ€§èƒ½è¯„ä¼°\n",
    "â”œâ”€â”€ plot_training_history()           # å¯è§†åŒ–\n",
    "â””â”€â”€ main()                            # ä¸»æµç¨‹\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ä½¿ç”¨æ–¹æ³•\n",
    "\n",
    "### åŸºç¡€ä½¿ç”¨\n",
    "\n",
    "```bash\n",
    "# 1. ç¡®ä¿å·²ç»å®Œæˆæ•°æ®é¢„å¤„ç†\n",
    "# è¿è¡Œ drugreflector_preprocessing_optimized.py\n",
    "\n",
    "# 2. è¿è¡Œè®­ç»ƒ\n",
    "python drugreflector_training.py\n",
    "```\n",
    "\n",
    "### è‡ªå®šä¹‰è®­ç»ƒ\n",
    "\n",
    "```python\n",
    "from drugreflector_training import DrugReflectorTrainer\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# åŠ è½½æ•°æ®\n",
    "with open('training_data_paper_compliant.pkl', 'rb') as f:\n",
    "    training_data = pickle.load(f)\n",
    "\n",
    "# åˆ›å»ºè®­ç»ƒå™¨ï¼ˆè‡ªå®šä¹‰å‚æ•°ï¼‰\n",
    "trainer = DrugReflectorTrainer(\n",
    "    device='cuda',\n",
    "    initial_lr=0.02,        # æ›´é«˜çš„å­¦ä¹ ç‡\n",
    "    min_lr=0.0001,\n",
    "    weight_decay=1e-4,      # æ›´å¼ºçš„æ­£åˆ™åŒ–\n",
    "    t_0=15,                 # æ›´æ—©çš„restart\n",
    "    focal_gamma=2.5,        # æ›´å¼ºçš„focusing\n",
    "    batch_size=512,         # æ›´å¤§çš„batch\n",
    "    num_epochs=100          # æ›´å¤šepoch\n",
    ")\n",
    "\n",
    "# è®­ç»ƒ\n",
    "output_dir = Path(\"./my_models\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "models, histories = trainer.train_ensemble(training_data, output_dir)\n",
    "```\n",
    "\n",
    "### åŠ è½½å·²è®­ç»ƒæ¨¡å‹\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from drugreflector_training import DrugReflectorModel\n",
    "\n",
    "# åŠ è½½å•ä¸ªæ¨¡å‹\n",
    "checkpoint = torch.load('model_fold0.pt')\n",
    "model = DrugReflectorModel(\n",
    "    input_size=978,\n",
    "    output_size=9597\n",
    ")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# åŠ è½½ensemble\n",
    "models = []\n",
    "for fold_id in range(3):\n",
    "    checkpoint = torch.load(f'model_fold{fold_id}.pt')\n",
    "    model = DrugReflectorModel(input_size=978, output_size=9597)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    models.append(model)\n",
    "```\n",
    "\n",
    "### ä½¿ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹\n",
    "\n",
    "```python\n",
    "from drugreflector_training import DrugReflectorEvaluator\n",
    "import numpy as np\n",
    "\n",
    "# åˆ›å»ºè¯„ä¼°å™¨\n",
    "evaluator = DrugReflectorEvaluator(models, device='cuda')\n",
    "\n",
    "# å‡†å¤‡æŸ¥è¯¢signatureï¼ˆä¾‹å¦‚ï¼šMEP â†’ MPCè½¬æ¢ï¼‰\n",
    "query_signature = np.random.randn(1, 978)  # ç¤ºä¾‹æ•°æ®\n",
    "\n",
    "# é¢„æµ‹\n",
    "scores = evaluator.predict_ensemble(query_signature)\n",
    "probs = torch.softmax(torch.FloatTensor(scores), dim=1).numpy()\n",
    "\n",
    "# è·å–top-ké¢„æµ‹\n",
    "top_k = 100\n",
    "top_indices = np.argsort(-probs[0])[:top_k]\n",
    "top_probs = probs[0, top_indices]\n",
    "\n",
    "# æ˜ å°„åˆ°åŒ–åˆç‰©åç§°\n",
    "top_compounds = [compound_names[idx] for idx in top_indices]\n",
    "\n",
    "for i, (compound, prob) in enumerate(zip(top_compounds, top_probs)):\n",
    "    print(f\"{i+1}. {compound}: {prob:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## è¶…å‚æ•°è¯´æ˜\n",
    "\n",
    "### åŸæ–‡è¶…å‚æ•°ï¼ˆTable S5ï¼‰\n",
    "\n",
    "| å‚æ•° | æœç´¢èŒƒå›´ | æœ€ä¼˜å€¼ | è¯´æ˜ |\n",
    "|------|----------|--------|------|\n",
    "| **Dropout** | (0.2, 0.8] | 0.64 | é˜²æ­¢è¿‡æ‹Ÿåˆ |\n",
    "| **Initial LR** | (1e-4, 1e-1] | 0.0139 | åˆå§‹å­¦ä¹ ç‡ |\n",
    "| **Weight Decay** | (1e-7, 1e-1] | 1e-5 | L2æ­£åˆ™åŒ– |\n",
    "| **T_0** | 10-50 | 20 | ç¬¬ä¸€æ¬¡restartçš„epochæ•° |\n",
    "\n",
    "### è¶…å‚æ•°è°ƒä¼˜å»ºè®®\n",
    "\n",
    "#### 1. **Dropout (0.64)**\n",
    "```python\n",
    "# æ•°æ®é‡å¤§ â†’ å¯ä»¥é™ä½dropout\n",
    "dropout_rate = 0.5  # å¦‚æœæ•°æ®>1Mæ ·æœ¬\n",
    "\n",
    "# æ•°æ®é‡å° â†’ æé«˜dropout\n",
    "dropout_rate = 0.7  # å¦‚æœæ•°æ®<100kæ ·æœ¬\n",
    "```\n",
    "\n",
    "#### 2. **Learning Rate (0.0139)**\n",
    "```python\n",
    "# GPUå†…å­˜è¶³å¤Ÿ â†’ å¢å¤§batch sizeï¼Œæé«˜LR\n",
    "batch_size = 512\n",
    "initial_lr = 0.02\n",
    "\n",
    "# GPUå†…å­˜ä¸è¶³ â†’ å‡å°batch sizeï¼Œé™ä½LR\n",
    "batch_size = 128\n",
    "initial_lr = 0.007\n",
    "```\n",
    "\n",
    "#### 3. **Focal Gamma (2.0)**\n",
    "```python\n",
    "# ç±»åˆ«æä¸å¹³è¡¡ â†’ æé«˜gamma\n",
    "focal_gamma = 2.5\n",
    "\n",
    "# ç±»åˆ«ç›¸å¯¹å‡è¡¡ â†’ é™ä½gamma\n",
    "focal_gamma = 1.5\n",
    "\n",
    "# ä¸éœ€è¦focal loss â†’ è®¾ä¸º0ï¼ˆå˜æˆäº¤å‰ç†µï¼‰\n",
    "focal_gamma = 0.0\n",
    "```\n",
    "\n",
    "#### 4. **Batch Size (256)**\n",
    "```python\n",
    "# æ ¹æ®GPUå†…å­˜è°ƒæ•´\n",
    "GPU_MEMORY_GB = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "\n",
    "if GPU_MEMORY_GB > 24:\n",
    "    batch_size = 512\n",
    "elif GPU_MEMORY_GB > 16:\n",
    "    batch_size = 256\n",
    "elif GPU_MEMORY_GB > 8:\n",
    "    batch_size = 128\n",
    "else:\n",
    "    batch_size = 64\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## å¸¸è§é—®é¢˜\n",
    "\n",
    "### Q1: è®­ç»ƒæ—¶é—´å¤ªé•¿æ€ä¹ˆåŠï¼Ÿ\n",
    "\n",
    "**ä¼˜åŒ–ç­–ç•¥ï¼š**\n",
    "\n",
    "1. **ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ**ï¼ˆé€Ÿåº¦æå‡~2xï¼‰\n",
    "```python\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "# è®­ç»ƒå¾ªç¯ä¸­\n",
    "with autocast():\n",
    "    outputs = model(batch_X)\n",
    "    loss = criterion(outputs, batch_y)\n",
    "\n",
    "scaler.scale(loss).backward()\n",
    "scaler.step(optimizer)\n",
    "scaler.update()\n",
    "```\n",
    "\n",
    "2. **å¢å¤§batch size**\n",
    "```python\n",
    "# åœ¨GPUå†…å­˜å…è®¸çš„æƒ…å†µä¸‹\n",
    "batch_size = 512  # ä»256å¢åŠ åˆ°512\n",
    "```\n",
    "\n",
    "3. **å‡å°‘epochæ•°**\n",
    "```python\n",
    "num_epochs = 30  # ä»50å‡å°‘åˆ°30\n",
    "```\n",
    "\n",
    "**é¢„æœŸè®­ç»ƒæ—¶é—´ï¼š**\n",
    "- **å•ä¸ªGPU (RTX 3090)**: ~6-8å°æ—¶/æ¨¡å‹\n",
    "- **3ä¸ªæ¨¡å‹æ€»è®¡**: ~18-24å°æ—¶\n",
    "- **ä½¿ç”¨æ··åˆç²¾åº¦**: ~9-12å°æ—¶\n",
    "\n",
    "---\n",
    "\n",
    "### Q2: GPUå†…å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ\n",
    "\n",
    "**è§£å†³æ–¹æ¡ˆï¼š**\n",
    "\n",
    "1. **å‡å°batch size**\n",
    "```python\n",
    "batch_size = 128  # æˆ–æ›´å°\n",
    "```\n",
    "\n",
    "2. **æ¢¯åº¦ç´¯ç§¯**\n",
    "```python\n",
    "accumulation_steps = 4  # ç´¯ç§¯4ä¸ªbatchå†æ›´æ–°\n",
    "optimizer.zero_grad()\n",
    "\n",
    "for i, (batch_X, batch_y) in enumerate(train_loader):\n",
    "    outputs = model(batch_X)\n",
    "    loss = criterion(outputs, batch_y) / accumulation_steps\n",
    "    loss.backward()\n",
    "    \n",
    "    if (i + 1) % accumulation_steps == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "```\n",
    "\n",
    "3. **ä½¿ç”¨CPUè®­ç»ƒ**ï¼ˆæ…¢ä½†å¯è¡Œï¼‰\n",
    "```python\n",
    "trainer = DrugReflectorTrainer(device='cpu')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Q3: å¦‚ä½•åˆ¤æ–­æ¨¡å‹æ˜¯å¦æ”¶æ•›ï¼Ÿ\n",
    "\n",
    "**æ£€æŸ¥æŒ‡æ ‡ï¼š**\n",
    "\n",
    "1. **Training LossæŒç»­ä¸‹é™**\n",
    "```python\n",
    "# æ­£å¸¸ï¼šä»~8.0 â†’ ~3.0\n",
    "# å¼‚å¸¸ï¼šéœ‡è¡æˆ–ä¸Šå‡\n",
    "```\n",
    "\n",
    "2. **Validation Recallæå‡**\n",
    "```python\n",
    "# æ­£å¸¸ï¼šä»~0.20 â†’ ~0.45-0.50\n",
    "# å¼‚å¸¸ï¼šä¸æå‡æˆ–ä¸‹é™ï¼ˆè¿‡æ‹Ÿåˆï¼‰\n",
    "```\n",
    "\n",
    "3. **Learning Rateæ›²çº¿**\n",
    "```python\n",
    "# æŸ¥çœ‹warm restartæ˜¯å¦å·¥ä½œ\n",
    "# åº”è¯¥çœ‹åˆ°å‘¨æœŸæ€§çš„ä¸Šå‡å’Œä¸‹é™\n",
    "```\n",
    "\n",
    "4. **æ—©åœæ£€æŸ¥**\n",
    "```python\n",
    "# å¦‚æœvalidation recallåœ¨20 epochåä¸å†æå‡\n",
    "# å¯ä»¥æå‰åœæ­¢è®­ç»ƒ\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Q4: å¦‚ä½•æé«˜æ¨¡å‹æ€§èƒ½ï¼Ÿ\n",
    "\n",
    "**ç­–ç•¥æ¸…å•ï¼š**\n",
    "\n",
    "- [ ] **æ•°æ®è´¨é‡**ï¼šç¡®ä¿é¢„å¤„ç†æ­£ç¡®ï¼ˆå°¤å…¶æ˜¯ç›¸ä¼¼åº¦è¿‡æ»¤ï¼‰\n",
    "- [ ] **æ›´å¤šè®­ç»ƒæ•°æ®**ï¼šæ”¾å®½è¿‡æ»¤æ¡ä»¶è·å¾—æ›´å¤šæ ·æœ¬\n",
    "- [ ] **æ•°æ®å¢å¼º**ï¼šæ·»åŠ é«˜æ–¯å™ªå£°ã€dropoutå¢å¼º\n",
    "- [ ] **æ¨¡å‹å®¹é‡**ï¼šå¢å¤§éšè—å±‚ï¼ˆ1024â†’2048, 2048â†’4096ï¼‰\n",
    "- [ ] **Ensembleå¢å¼º**ï¼šè®­ç»ƒ5-foldæˆ–10-fold ensemble\n",
    "- [ ] **å­¦ä¹ ç‡è°ƒä¼˜**ï¼šä½¿ç”¨å­¦ä¹ ç‡æŸ¥æ‰¾å™¨\n",
    "- [ ] **æ­£åˆ™åŒ–è°ƒæ•´**ï¼šè°ƒæ•´dropoutå’Œweight decay\n",
    "- [ ] **æŸå¤±å‡½æ•°**ï¼šå°è¯•label smoothing\n",
    "\n",
    "---\n",
    "\n",
    "## æ€§èƒ½ä¼˜åŒ–\n",
    "\n",
    "### 1. å¤šGPUè®­ç»ƒ\n",
    "\n",
    "```python\n",
    "# DataParallel (ç®€å•ä½†æ•ˆç‡ä¸é«˜)\n",
    "model = nn.DataParallel(model)\n",
    "\n",
    "# DistributedDataParallel (æ¨è)\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "# åˆå§‹åŒ–è¿›ç¨‹ç»„\n",
    "dist.init_process_group(backend='nccl')\n",
    "local_rank = torch.distributed.get_rank()\n",
    "torch.cuda.set_device(local_rank)\n",
    "\n",
    "# åˆ›å»ºDDPæ¨¡å‹\n",
    "model = DrugReflectorModel(...).to(local_rank)\n",
    "model = DDP(model, device_ids=[local_rank])\n",
    "```\n",
    "\n",
    "### 2. æ•°æ®åŠ è½½ä¼˜åŒ–\n",
    "\n",
    "```python\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    num_workers=8,           # å¢åŠ workeræ•°\n",
    "    pin_memory=True,         # åŠ é€ŸCPUâ†’GPUä¼ è¾“\n",
    "    prefetch_factor=2,       # é¢„å–å› å­\n",
    "    persistent_workers=True  # ä¿æŒworkerè¿›ç¨‹\n",
    ")\n",
    "```\n",
    "\n",
    "### 3. ç¼–è¯‘æ¨¡å‹ï¼ˆPyTorch 2.0+ï¼‰\n",
    "\n",
    "```python\n",
    "# ä½¿ç”¨torch.compileåŠ é€Ÿ\n",
    "model = torch.compile(model, mode='reduce-overhead')\n",
    "```\n",
    "\n",
    "### 4. æ··åˆç²¾åº¦è®­ç»ƒ\n",
    "\n",
    "```python\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "for batch_X, batch_y in train_loader:\n",
    "    with autocast():  # è‡ªåŠ¨æ··åˆç²¾åº¦\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "    \n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## é¢„æœŸç»“æœ\n",
    "\n",
    "### è®­ç»ƒå®Œæˆåçš„æŒ‡æ ‡\n",
    "\n",
    "æ ¹æ®è®ºæ–‡ï¼Œè®­ç»ƒå®Œæˆååº”è¯¥è¾¾åˆ°ï¼š\n",
    "\n",
    "| æŒ‡æ ‡ | CMap Touchstone | sciPlex3 | Internal |\n",
    "|------|-----------------|----------|----------|\n",
    "| **Top 1% Recall** | 0.46-0.50 | 0.30-0.35 | 0.60-0.65 |\n",
    "| **Top-10 Accuracy** | 0.35-0.40 | 0.20-0.25 | 0.45-0.50 |\n",
    "\n",
    "### è®­ç»ƒæ—¥å¿—ç¤ºä¾‹\n",
    "\n",
    "```\n",
    "================================================================================\n",
    "Training Model 0\n",
    "================================================================================\n",
    "\n",
    "Epoch 1/50\n",
    "  Train Loss: 7.2456\n",
    "  Val Loss: 6.8934\n",
    "  Val Recall (top 1%): 0.2134\n",
    "  Val Top-1 Acc: 0.0234\n",
    "  Val Top-10 Acc: 0.1456\n",
    "  Learning Rate: 0.013900\n",
    "  âœ“ New best model! (Recall: 0.2134)\n",
    "\n",
    "Epoch 10/50\n",
    "  Train Loss: 4.5678\n",
    "  Val Loss: 4.9234\n",
    "  Val Recall (top 1%): 0.3567\n",
    "  Val Top-1 Acc: 0.0567\n",
    "  Val Top-10 Acc: 0.2345\n",
    "  Learning Rate: 0.007234\n",
    "\n",
    "...\n",
    "\n",
    "Epoch 50/50\n",
    "  Train Loss: 3.1234\n",
    "  Val Loss: 3.8456\n",
    "  Val Recall (top 1%): 0.4789\n",
    "  Val Top-1 Acc: 0.0934\n",
    "  Val Top-10 Acc: 0.3678\n",
    "  Learning Rate: 0.000056\n",
    "\n",
    "âœ“ Loaded best model from epoch 47 (Recall: 0.4823)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## æ–‡ä»¶è¾“å‡º\n",
    "\n",
    "è®­ç»ƒå®Œæˆåä¼šç”Ÿæˆä»¥ä¸‹æ–‡ä»¶ï¼š\n",
    "\n",
    "```\n",
    "trained_models/\n",
    "â”œâ”€â”€ model_fold0.pt              # Fold 0æ¨¡å‹æ£€æŸ¥ç‚¹\n",
    "â”‚   â”œâ”€â”€ model_state_dict        # æ¨¡å‹æƒé‡\n",
    "â”‚   â”œâ”€â”€ fold_id                 # Fold ID\n",
    "â”‚   â”œâ”€â”€ history                 # è®­ç»ƒå†å²\n",
    "â”‚   â””â”€â”€ config                  # æ¨¡å‹é…ç½®\n",
    "â”œâ”€â”€ model_fold1.pt              # Fold 1æ¨¡å‹æ£€æŸ¥ç‚¹\n",
    "â”œâ”€â”€ model_fold2.pt              # Fold 2æ¨¡å‹æ£€æŸ¥ç‚¹\n",
    "â”œâ”€â”€ ensemble_history.pkl        # 3ä¸ªæ¨¡å‹çš„è®­ç»ƒå†å²\n",
    "â”œâ”€â”€ training_history.png        # è®­ç»ƒæ›²çº¿å›¾\n",
    "â”œâ”€â”€ fold0_results.pkl           # Fold 0è¯„ä¼°ç»“æœ\n",
    "â”œâ”€â”€ fold1_results.pkl           # Fold 1è¯„ä¼°ç»“æœ\n",
    "â””â”€â”€ fold2_results.pkl           # Fold 2è¯„ä¼°ç»“æœ\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## å¼•ç”¨\n",
    "\n",
    "å¦‚æœä½¿ç”¨æ­¤ä»£ç ï¼Œè¯·å¼•ç”¨ï¼š\n",
    "\n",
    "```bibtex\n",
    "@article{demeo2025drugreflector,\n",
    "  title={Active learning framework leveraging transcriptomics identifies modulators of disease phenotypes},\n",
    "  author={DeMeo, Benjamin and others},\n",
    "  journal={Science},\n",
    "  year={2025},\n",
    "  doi={10.1126/science.adi8577}\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## è”ç³»ä¸æ”¯æŒ\n",
    "\n",
    "å¦‚é‡é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼š\n",
    "1. æ•°æ®é¢„å¤„ç†æ˜¯å¦æ­£ç¡®å®Œæˆ\n",
    "2. GPUé©±åŠ¨å’ŒCUDAç‰ˆæœ¬æ˜¯å¦å…¼å®¹\n",
    "3. PyTorchç‰ˆæœ¬ â‰¥ 1.12.0\n",
    "4. è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´ï¼ˆè‡³å°‘50GBï¼‰\n",
    "\n",
    "**éœ€è¦å¸®åŠ©ï¼Ÿ**\n",
    "- æ£€æŸ¥é”™è¯¯æ—¥å¿—\n",
    "- ç¡®è®¤æ•°æ®æ ¼å¼\n",
    "- éªŒè¯è¶…å‚æ•°è®¾ç½®"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
