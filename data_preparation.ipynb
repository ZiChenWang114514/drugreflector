{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "106ce8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸš€ LOADING FROM CACHED FILES\n",
      "======================================================================\n",
      "\n",
      "âœ“ Found decompressed GCTX: GSE92742_Broad_LINCS_Level4_ZSPCINF_mlr12k_n1319138x12328.gctx\n",
      "\n",
      "======================================================================\n",
      "STEP 1: Loading gene and cell information\n",
      "======================================================================\n",
      "Total genes: 12328\n",
      "Landmark genes: 978\n",
      "Total cell lines: 98\n",
      "Unique cell IDs: 98\n",
      "\n",
      "======================================================================\n",
      "STEP 2: Reading cached GCTX file\n",
      "======================================================================\n",
      "\n",
      "ğŸ“– Reading GCTX file: GSE92742_Broad_LINCS_Level4_ZSPCINF_mlr12k_n1319138x12328.gctx\n",
      "ğŸ“Š Loading matrix from HDF5...\n",
      "âœ“ Matrix shape: (1319138, 12328)\n",
      "ğŸ“‹ Loading metadata...\n",
      "\n",
      "âœ“ Data loaded successfully:\n",
      "  Matrix: (1319138, 12328) (samples Ã— genes)\n",
      "  Samples: 1319138\n",
      "  Genes: 12328\n",
      "\n",
      "======================================================================\n",
      "STEP 3: Filtering to landmark genes\n",
      "======================================================================\n",
      "âœ“ Filtered matrix shape: (1319138, 978)\n",
      "\n",
      "======================================================================\n",
      "STEP 4: Applying quality filters\n",
      "======================================================================\n",
      "Filter parameters:\n",
      "   â€¢ min_replicates: 5\n",
      "   â€¢ min_cell_lines: 2\n",
      "   â€¢ min_obs_per_compound: 10\n",
      "   â€¢ min_compounds_per_cell: 200\n",
      "   â€¢ min_replicate_similarity: 0.12\n",
      "\n",
      "======================================================================\n",
      "ğŸ” APPLYING QUALITY CONTROL FILTERS\n",
      "======================================================================\n",
      "Initial samples: 1319138\n",
      "\n",
      "Filter criteria:\n",
      "  â€¢ Replicates per compound â‰¥ 5\n",
      "  â€¢ Cell lines per compound â‰¥ 2\n",
      "  â€¢ Observations per compound â‰¥ 10\n",
      "  â€¢ Compounds per cell line â‰¥ 200\n",
      "  â€¢ Replicate similarity â‰¥ 0.12\n",
      "\n",
      "[INFO] 'pert_id' or 'cell_id' not found in row_meta, parsing from 'id' column...\n",
      "       Added columns:\n",
      "         â€¢ pert_id <- id.split('_')[0]\n",
      "         â€¢ cell_id <- id.split('_')[1]\n",
      "       ç¤ºä¾‹ï¼š                                      id pert_id cell_id\n",
      "0  CPC005_A375_6H_X1_B3_DUO52HI53LO:K06  CPC005    A375\n",
      "1  CPC005_A375_6H_X2_B3_DUO52HI53LO:K06  CPC005    A375\n",
      "2  CPC005_A375_6H_X3_B3_DUO52HI53LO:K06  CPC005    A375\n",
      "\n",
      "ğŸ“Š Pre-filter statistics:\n",
      "   Total unique compounds: 233\n",
      "   Total unique cell lines: 76\n",
      "\n",
      "   Compounds per cell line distribution:\n",
      "      Min: 1\n",
      "      Max: 122\n",
      "      Mean: 12.9\n",
      "      Median: 1\n",
      "      75th percentile: 4\n",
      "      90th percentile: 62\n",
      "\n",
      "   Samples per compound distribution:\n",
      "      Min: 43\n",
      "      Max: 62611\n",
      "      Mean: 5661.5\n",
      "      Median: 2449\n",
      "\n",
      "======================================================================\n",
      "FILTER 1: Compounds per cell line\n",
      "======================================================================\n",
      "  Cell lines with â‰¥200 compounds: 0/76\n",
      "  âš ï¸  WARNING: No cell lines meet the criteria!\n",
      "  Suggestion: Lower min_compounds_per_cell to 1 (median) or 4 (75th percentile)\n",
      "  Remaining samples: 1319138\n",
      "  Remaining compounds: 233\n",
      "\n",
      "======================================================================\n",
      "FILTER 2: Replicates per compound\n",
      "======================================================================\n",
      "  Compounds with â‰¥5 replicates: 233/233\n",
      "  Remaining samples: 1319138\n",
      "  Remaining compounds: 233\n",
      "\n",
      "======================================================================\n",
      "FILTER 3: Cell line diversity per compound\n",
      "======================================================================\n",
      "  Compounds in â‰¥2 cell lines: 134/233\n",
      "  Remaining samples: 1208775\n",
      "  Remaining compounds: 134\n",
      "\n",
      "======================================================================\n",
      "FILTER 4: Total observations per compound\n",
      "======================================================================\n",
      "  Compounds with â‰¥10 observations: 134/134\n",
      "  Remaining samples: 1208775\n",
      "  Remaining compounds: 134\n",
      "\n",
      "======================================================================\n",
      "FILTER 5: Signature-level replicate CC (â‰ˆ distil_cc_q75)\n",
      "======================================================================\n",
      "\n",
      "ğŸ“Š Calculating signature-level replicate CC (corr=spearman, q=0.75)...\n",
      "   Grouping by: ['pert_id', 'cell_id']\n",
      "   Total signatures to process: 885\n",
      "   Progress: 88/885 (9.9%)\n",
      "   Progress: 176/885 (19.9%)\n",
      "   Progress: 264/885 (29.8%)\n",
      "   Progress: 352/885 (39.8%)\n",
      "   Progress: 440/885 (49.7%)\n",
      "   Progress: 528/885 (59.7%)\n",
      "   Progress: 616/885 (69.6%)\n",
      "   Progress: 704/885 (79.5%)\n",
      "   Progress: 792/885 (89.5%)\n",
      "   Progress: 880/885 (99.4%)\n",
      "\n",
      "   âœ“ Calculation completed!\n",
      "   Signatures with â‰¥2 replicates: 884\n",
      "   Mean CC(q75): 0.1039\n",
      "   Median CC: 0.1017\n",
      "  Using CC threshold (min_replicate_similarity): 0.12\n",
      "  Signatures with CC â‰¥ 0.12: 219/884\n",
      "\n",
      "âŒ Error loading from cache: Unable to allocate 974. MiB for an array with shape (261031, 978) and data type float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\11234\\AppData\\Local\\Temp\\ipykernel_33336\\2566023164.py\", line 919, in load_from_cache\n",
      "    training_data = loader.prepare_training_data(\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\11234\\AppData\\Local\\Temp\\ipykernel_33336\\2566023164.py\", line 557, in prepare_training_data\n",
      "    working_matrix = working_matrix[keep_idx]\n",
      "                     ~~~~~~~~~~~~~~^^^^^^^^^^\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 974. MiB for an array with shape (261031, 978) and data type float32\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "LINCSæ•°æ®åŠ è½½å’Œé¢„å¤„ç†è„šæœ¬\n",
    "å°†GCTXæ ¼å¼è½¬æ¢ä¸ºè®­ç»ƒæ‰€éœ€çš„æ ¼å¼\n",
    "å®Œå…¨ä¿®å¤ç‰ˆæœ¬ï¼šæ­£ç¡®å¤„ç†GCTXæ ¼å¼çš„åç›´è§‰å…ƒæ•°æ®æ˜ å°„\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import gzip\n",
    "import shutil\n",
    "import glob\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "class LINCSDataLoader:\n",
    "    \"\"\"åŠ è½½å’Œé¢„å¤„ç†LINCS L1000æ•°æ®\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.gene_info = None\n",
    "        self.cell_info = None\n",
    "        self.signatures = None\n",
    "        self.decompressed_files = []\n",
    "        \n",
    "    def load_gene_info(self, dataset='GSE92742'):\n",
    "        \"\"\"åŠ è½½åŸºå› ä¿¡æ¯ï¼Œè·å–978ä¸ªlandmark genes\"\"\"\n",
    "        gene_file = self.data_dir / f\"{dataset}_Broad_LINCS_gene_info.txt.gz\"\n",
    "        \n",
    "        gene_info = pd.read_csv(gene_file, sep='\\t', compression='gzip')\n",
    "        \n",
    "        # ç­›é€‰landmark genes (pr_is_lm == 1)\n",
    "        landmark_genes = gene_info[gene_info['pr_is_lm'] == 1].copy()\n",
    "        \n",
    "        print(f\"Total genes: {len(gene_info)}\")\n",
    "        print(f\"Landmark genes: {len(landmark_genes)}\")\n",
    "        \n",
    "        self.gene_info = landmark_genes\n",
    "        return landmark_genes\n",
    "    \n",
    "    def load_cell_info(self, dataset='GSE92742'):\n",
    "        \"\"\"åŠ è½½ç»†èƒç³»ä¿¡æ¯\"\"\"\n",
    "        cell_file = self.data_dir / f\"{dataset}_Broad_LINCS_cell_info.txt.gz\"\n",
    "        \n",
    "        cell_info = pd.read_csv(cell_file, sep='\\t', compression='gzip')\n",
    "        \n",
    "        print(f\"Total cell lines: {len(cell_info)}\")\n",
    "        print(f\"Unique cell IDs: {cell_info['cell_id'].nunique()}\")\n",
    "        \n",
    "        self.cell_info = cell_info\n",
    "        return cell_info\n",
    "    \n",
    "    def find_decompressed_file(self, original_gz_file):\n",
    "        \"\"\"æŸ¥æ‰¾å·²è§£å‹çš„æ–‡ä»¶\"\"\"\n",
    "        gz_path = Path(original_gz_file)\n",
    "        decompressed_dir = self.data_dir / \"_decompressed\"\n",
    "        decompressed_file = decompressed_dir / gz_path.stem  # å»æ‰.gzåç¼€\n",
    "        \n",
    "        if decompressed_file.exists():\n",
    "            return str(decompressed_file)\n",
    "        return None\n",
    "    \n",
    "    def decompress_gzip_file(self, gzip_file):\n",
    "        \"\"\"\n",
    "        è§£å‹gzipæ–‡ä»¶åˆ°_decompressedæ–‡ä»¶å¤¹\n",
    "        è¿”å›è§£å‹åçš„æ–‡ä»¶è·¯å¾„\n",
    "        \"\"\"\n",
    "        import time\n",
    "        \n",
    "        gzip_file = str(gzip_file)\n",
    "        \n",
    "        # åˆ›å»ºè§£å‹æ–‡ä»¶å¤¹\n",
    "        decompressed_dir = Path(self.data_dir) / \"_decompressed\"\n",
    "        decompressed_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # ä½¿ç”¨åŸæ–‡ä»¶åï¼ˆå»æ‰.gzåç¼€ï¼‰\n",
    "        original_name = Path(gzip_file).stem\n",
    "        output_path = decompressed_dir / original_name\n",
    "        \n",
    "        # å¦‚æœæ–‡ä»¶å·²ç»å­˜åœ¨ï¼Œç›´æ¥ä½¿ç”¨\n",
    "        if output_path.exists():\n",
    "            print(f\"âœ“ Found existing decompressed file: {output_path.name}\")\n",
    "            self.decompressed_files.append(output_path)\n",
    "            return str(output_path)\n",
    "        \n",
    "        print(f\"ğŸ’¾ Decompressing to: {decompressed_dir}\")\n",
    "        \n",
    "        try:\n",
    "            print(f\"ğŸ“¦ Decompressing {Path(gzip_file).name}...\")\n",
    "            source_size = Path(gzip_file).stat().st_size / (1024**3)\n",
    "            print(f\"   Source size: ~{source_size:.1f} GB\")\n",
    "            \n",
    "            # ä½¿ç”¨8MBç¼“å†²åŒº\n",
    "            with gzip.open(gzip_file, 'rb') as f_in:\n",
    "                with open(output_path, 'wb') as f_out:\n",
    "                    shutil.copyfileobj(f_in, f_out, length=8*1024*1024)\n",
    "            \n",
    "            output_size = output_path.stat().st_size / (1024**3)\n",
    "            print(f\"âœ“ Decompressed successfully!\")\n",
    "            print(f\"âœ“ Output file: {output_path.name}\")\n",
    "            print(f\"âœ“ Output size: ~{output_size:.1f} GB\")\n",
    "            \n",
    "            self.decompressed_files.append(output_path)\n",
    "            return str(output_path)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"\\nâŒ Decompression failed: {e}\")\n",
    "            # æ¸…ç†å¤±è´¥çš„æ–‡ä»¶\n",
    "            if output_path.exists():\n",
    "                try:\n",
    "                    output_path.unlink()\n",
    "                except:\n",
    "                    pass\n",
    "            raise RuntimeError(f\"Failed to decompress {gzip_file}: {e}\")\n",
    "    \n",
    "    def read_gctx(self, gctx_file):\n",
    "        \"\"\"\n",
    "        è¯»å–GCTXæ–‡ä»¶ (HDF5æ ¼å¼)\n",
    "        \n",
    "        âš ï¸ CRITICAL: GCTXæ ¼å¼çš„åç›´è§‰è®¾è®¡\n",
    "        - æ–‡ä»¶å nXXXXxYYYY.gctx è¡¨ç¤º XXXXæ ·æœ¬ Ã— YYYYåŸºå› \n",
    "        - çŸ©é˜µå­˜å‚¨ä¸º (XXXX, YYYY) å½¢çŠ¶\n",
    "        - ä½†å…ƒæ•°æ®å‘½ååç›´è§‰ï¼š\n",
    "          * /0/META/ROW å­˜å‚¨çš„æ˜¯åŸºå› ä¿¡æ¯ï¼ˆå¯¹åº”çŸ©é˜µçš„åˆ—ï¼‰\n",
    "          * /0/META/COL å­˜å‚¨çš„æ˜¯æ ·æœ¬ä¿¡æ¯ï¼ˆå¯¹åº”çŸ©é˜µçš„è¡Œï¼‰\n",
    "        \"\"\"\n",
    "        gctx_file = str(gctx_file)\n",
    "        print(f\"\\nğŸ“– Reading GCTX file: {Path(gctx_file).name}\")\n",
    "        \n",
    "        # æ£€æŸ¥æ˜¯å¦æ˜¯gzipæ–‡ä»¶\n",
    "        if gctx_file.endswith('.gz'):\n",
    "            print(\"âš ï¸  Detected gzip compressed file\")\n",
    "            gctx_file = self.decompress_gzip_file(gctx_file)\n",
    "        \n",
    "        with h5py.File(gctx_file, 'r') as f:\n",
    "            # è¯»å–æ•°æ®çŸ©é˜µ\n",
    "            print(f\"ğŸ“Š Loading matrix from HDF5...\")\n",
    "            matrix = f['/0/DATA/0/matrix'][:]\n",
    "            print(f\"âœ“ Matrix shape: {matrix.shape}\")\n",
    "            \n",
    "            # ğŸ”§ å…³é”®ä¿®å¤ï¼šäº¤æ¢ ROW å’Œ COL çš„ç†è§£\n",
    "            # ROW åœ¨ GCTX ä¸­å®é™…å¯¹åº”åŸºå› ï¼ˆçŸ©é˜µçš„åˆ—ï¼‰\n",
    "            # COL åœ¨ GCTX ä¸­å®é™…å¯¹åº”æ ·æœ¬ï¼ˆçŸ©é˜µçš„è¡Œï¼‰\n",
    "            \n",
    "            print(f\"ğŸ“‹ Loading metadata...\")\n",
    "            \n",
    "            # è¯»å–åŸºå› å…ƒæ•°æ®ï¼ˆæ¥è‡ª ROWï¼‰\n",
    "            gene_meta = {}\n",
    "            for key in f['/0/META/ROW'].keys():\n",
    "                data = f[f'/0/META/ROW/{key}'][:]\n",
    "                if data.dtype.kind in ['S', 'O']:\n",
    "                    gene_meta[key] = np.char.decode(data.astype('S'), 'utf-8')\n",
    "                else:\n",
    "                    gene_meta[key] = data.astype(str)\n",
    "            \n",
    "            # è¯»å–æ ·æœ¬å…ƒæ•°æ®ï¼ˆæ¥è‡ª COLï¼‰\n",
    "            sample_meta = {}\n",
    "            for key in f['/0/META/COL'].keys():\n",
    "                data = f[f'/0/META/COL/{key}'][:]\n",
    "                if data.dtype.kind in ['S', 'O']:\n",
    "                    sample_meta[key] = np.char.decode(data.astype('S'), 'utf-8')\n",
    "                else:\n",
    "                    sample_meta[key] = data.astype(str)\n",
    "        \n",
    "        # è½¬æ¢ä¸ºDataFrame\n",
    "        sample_df = pd.DataFrame(sample_meta)\n",
    "        gene_df = pd.DataFrame(gene_meta)\n",
    "        \n",
    "        print(f\"\\nâœ“ Data loaded successfully:\")\n",
    "        print(f\"  Matrix: {matrix.shape} (samples Ã— genes)\")\n",
    "        print(f\"  Samples: {len(sample_df)}\")\n",
    "        print(f\"  Genes: {len(gene_df)}\")\n",
    "        \n",
    "        # éªŒè¯ç»´åº¦ä¸€è‡´æ€§\n",
    "        assert matrix.shape[0] == len(sample_df), \\\n",
    "            f\"Matrix rows ({matrix.shape[0]}) != sample metadata ({len(sample_df)})\"\n",
    "        assert matrix.shape[1] == len(gene_df), \\\n",
    "            f\"Matrix cols ({matrix.shape[1]}) != gene metadata ({len(gene_df)})\"\n",
    "        \n",
    "        return matrix, sample_df, gene_df\n",
    "    \n",
    "    def load_level4_signatures(self, dataset='GSE92742'):\n",
    "        \"\"\"åŠ è½½Level 4æ•°æ® (Z-score normalized)\"\"\"\n",
    "        level4_pattern = self.data_dir / f\"{dataset}_Broad_LINCS_Level4_ZSPCINF_mlr12k_n*x12328.gctx.gz\"\n",
    "        \n",
    "        print(f\"ğŸ” Searching for Level 4 file...\")\n",
    "        print(f\"   Pattern: {level4_pattern.name}\")\n",
    "        files = glob.glob(str(level4_pattern))\n",
    "        \n",
    "        if not files:\n",
    "            raise FileNotFoundError(\n",
    "                f\"âŒ No Level 4 file found matching {level4_pattern}\"\n",
    "            )\n",
    "        \n",
    "        level4_file = files[0]\n",
    "        print(f\"âœ“ Found: {Path(level4_file).name}\")\n",
    "        \n",
    "        # è¯»å–GCTX\n",
    "        matrix, sample_meta, gene_meta = self.read_gctx(level4_file)\n",
    "        \n",
    "        # åªä¿ç•™landmark genes\n",
    "        print(f\"\\nğŸ”¬ Filtering to landmark genes...\")\n",
    "        if self.gene_info is None:\n",
    "            self.load_gene_info(dataset)\n",
    "        \n",
    "        landmark_ids = set(self.gene_info['pr_gene_id'].astype(str).values)\n",
    "        print(f\"   Landmark genes to find: {len(landmark_ids)}\")\n",
    "        print(f\"   Total genes in data: {len(gene_meta)}\")\n",
    "        \n",
    "        # åˆ›å»ºåŸºå› mask\n",
    "        gene_mask = gene_meta['id'].isin(landmark_ids)\n",
    "        n_matched = gene_mask.sum()\n",
    "        print(f\"   âœ“ Matched: {n_matched} genes\")\n",
    "        \n",
    "        if n_matched == 0:\n",
    "            raise ValueError(\"No landmark genes matched!\")\n",
    "        \n",
    "        # è¿‡æ»¤çŸ©é˜µå’Œå…ƒæ•°æ®\n",
    "        print(f\"\\nğŸ¯ Applying filter...\")\n",
    "        matrix = matrix[:, gene_mask]\n",
    "        gene_meta = gene_meta[gene_mask].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"   âœ“ Final matrix shape: {matrix.shape}\")\n",
    "        \n",
    "        self.signatures = {\n",
    "            'matrix': matrix,\n",
    "            'row_meta': sample_meta,\n",
    "            'col_meta': gene_meta\n",
    "        }\n",
    "        \n",
    "        return matrix, sample_meta, gene_meta\n",
    "    \n",
    "    def calculate_replicate_similarity(\n",
    "        self,\n",
    "        matrix,\n",
    "        row_meta,\n",
    "        group_cols=None,\n",
    "        corr_type: str = \"spearman\",\n",
    "        quantile: float = 0.75,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        è¿‘ä¼¼å¤åˆ» CMap çš„ replicate CCï¼ˆdistil_cc_q75ï¼‰- é«˜æ•ˆå‘é‡åŒ–ç‰ˆæœ¬\n",
    "        - æŒ‰\"signature\"ç»´åº¦åˆ†ç»„ (pert_id + cell_id + dose + time ç­‰)\n",
    "        - å¯¹æ¯ä¸ª signature å†…æ‰€æœ‰ Level 4 replicate åšä¸¤ä¸¤ç›¸å…³\n",
    "        - é»˜è®¤ä½¿ç”¨ Spearman ç›¸å…³ (corr_type='spearman')\n",
    "        - å– pairwise ç›¸å…³çš„ 75% åˆ†ä½æ•°ä½œä¸ºè¯¥ signature çš„ CC å€¼\n",
    "\n",
    "        è¿”å›: DataFrame, æ¯è¡Œæ˜¯ä¸€ä¸ª signature åŠå…¶ n_reps å’Œ rep_cc_q(â‰ˆdistil_cc_q75)\n",
    "        \n",
    "        âš¡ æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨å‘é‡åŒ–çš„ç›¸å…³çŸ©é˜µè®¡ç®—ï¼Œæ¯”åŸç‰ˆå¿« 10-100 å€\n",
    "        \"\"\"\n",
    "        from scipy.stats import rankdata\n",
    "        \n",
    "        # 1. è®¾å®šç”¨äºå®šä¹‰ signature çš„åˆ†ç»„åˆ—\n",
    "        if group_cols is None:\n",
    "            group_cols = ['pert_id', 'cell_id', 'pert_time', 'pert_dose']\n",
    "\n",
    "        # åªä¿ç•™åœ¨ row_meta é‡ŒçœŸå®å­˜åœ¨çš„åˆ—\n",
    "        group_cols = [c for c in group_cols if c in row_meta.columns]\n",
    "        if not group_cols:\n",
    "            raise ValueError(\"No valid grouping columns found in row_meta for replicate QC.\")\n",
    "\n",
    "        df = row_meta.copy()\n",
    "        df['_row_idx'] = np.arange(len(df))\n",
    "\n",
    "        print(f\"\\nğŸ“Š Calculating signature-level replicate CC \"\n",
    "            f\"(corr={corr_type}, q={quantile})...\")\n",
    "        print(f\"   Grouping by: {group_cols}\")\n",
    "\n",
    "        sig_records = []\n",
    "        grouped = df.groupby(group_cols)\n",
    "        \n",
    "        total_groups = len(grouped)\n",
    "        print(f\"   Total signatures to process: {total_groups:,}\")\n",
    "\n",
    "        # è¿›åº¦è·Ÿè¸ª\n",
    "        processed = 0\n",
    "        progress_interval = max(1, total_groups // 10)  # æ¯10%æ˜¾ç¤ºä¸€æ¬¡\n",
    "\n",
    "        for key, sub in grouped:\n",
    "            processed += 1\n",
    "            \n",
    "            # æ˜¾ç¤ºè¿›åº¦ï¼ˆæ¯å¤„ç†10%æˆ–æ¯1000ä¸ªï¼‰\n",
    "            if processed % 1000 == 0 or processed % progress_interval == 0:\n",
    "                print(f\"   Progress: {processed:,}/{total_groups:,} \"\n",
    "                    f\"({100 * processed / total_groups:.1f}%)\")\n",
    "            \n",
    "            idx = sub['_row_idx'].to_numpy()\n",
    "            n_rep = len(idx)\n",
    "            \n",
    "            # è‡³å°‘è¦æœ‰2ä¸ªreplicateæ‰è°ˆå¾—ä¸Šç›¸å…³\n",
    "            if n_rep < 2:\n",
    "                continue\n",
    "\n",
    "            profiles = matrix[idx]  # (n_rep, n_genes)\n",
    "\n",
    "            # ğŸš€ å…³é”®ä¼˜åŒ–ï¼šä½¿ç”¨å‘é‡åŒ–çš„ç›¸å…³çŸ©é˜µè®¡ç®—\n",
    "            try:\n",
    "                if corr_type == \"spearman\":\n",
    "                    # Spearman: å…ˆå¯¹æ¯ä¸ªæ ·æœ¬ï¼ˆè¡Œï¼‰è¿›è¡Œ rank è½¬æ¢\n",
    "                    # ç„¶åè®¡ç®— Pearson ç›¸å…³ï¼ˆrank åçš„ Pearson = Spearmanï¼‰\n",
    "                    ranked_profiles = np.apply_along_axis(\n",
    "                        lambda x: rankdata(x), \n",
    "                        axis=1, \n",
    "                        arr=profiles\n",
    "                    )\n",
    "                    # è®¡ç®— rank åçš„ Pearson ç›¸å…³çŸ©é˜µ\n",
    "                    corr_matrix = np.corrcoef(ranked_profiles)\n",
    "                else:\n",
    "                    # Pearson: ç›´æ¥è®¡ç®—ç›¸å…³çŸ©é˜µ\n",
    "                    corr_matrix = np.corrcoef(profiles)\n",
    "                \n",
    "                # æå–ä¸Šä¸‰è§’éƒ¨åˆ†ï¼ˆä¸åŒ…æ‹¬å¯¹è§’çº¿ï¼‰\n",
    "                # np.triu_indices(n, k=1) è¿”å›ä¸Šä¸‰è§’çš„ç´¢å¼•\n",
    "                triu_indices = np.triu_indices(n_rep, k=1)\n",
    "                pairwise_corrs = corr_matrix[triu_indices]\n",
    "                \n",
    "                # è¿‡æ»¤æ‰ NaN å€¼\n",
    "                pairwise_corrs = pairwise_corrs[~np.isnan(pairwise_corrs)]\n",
    "                \n",
    "                if len(pairwise_corrs) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # è®¡ç®—æŒ‡å®šåˆ†ä½æ•°\n",
    "                cc_q = np.quantile(pairwise_corrs, quantile)\n",
    "                \n",
    "            except Exception as e:\n",
    "                # å¦‚æœå‘é‡åŒ–è®¡ç®—å¤±è´¥ï¼Œè·³è¿‡è¿™ä¸ª signature\n",
    "                if processed <= 5:  # åªæ˜¾ç¤ºå‰å‡ ä¸ªé”™è¯¯\n",
    "                    print(f\"   âš ï¸  Warning: Failed to calculate CC for signature {key}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # key å¯èƒ½æ˜¯ tupleï¼Œä¹Ÿå¯èƒ½æ˜¯å•å€¼\n",
    "            if not isinstance(key, tuple):\n",
    "                key = (key,)\n",
    "            sig_records.append((*key, n_rep, cc_q))\n",
    "\n",
    "        if not sig_records:\n",
    "            print(\"  âš ï¸ No signatures with â‰¥2 replicates found for QC.\")\n",
    "            return pd.DataFrame(columns=group_cols + ['n_reps', 'rep_cc_q'])\n",
    "\n",
    "        cc_cols = group_cols + ['n_reps', 'rep_cc_q']\n",
    "        sig_cc_df = pd.DataFrame(sig_records, columns=cc_cols)\n",
    "\n",
    "        print(f\"\\n   âœ“ Calculation completed!\")\n",
    "        print(f\"   Signatures with â‰¥2 replicates: {len(sig_cc_df):,}\")\n",
    "        print(f\"   Mean CC(q{int(quantile*100)}): {sig_cc_df['rep_cc_q'].mean():.4f}\")\n",
    "        print(f\"   Median CC: {sig_cc_df['rep_cc_q'].median():.4f}\")\n",
    "\n",
    "        return sig_cc_df\n",
    "    \n",
    "    def prepare_training_data(self, \n",
    "                            min_replicates=5, \n",
    "                            min_cell_lines=2,\n",
    "                            min_obs_per_compound=10,\n",
    "                            min_compounds_per_cell=200,\n",
    "                            min_replicate_similarity=0.12):\n",
    "        \"\"\"\n",
    "        å‡†å¤‡è®­ç»ƒæ•°æ®ï¼Œåº”ç”¨ä¸¥æ ¼çš„è´¨é‡æ§åˆ¶ç­›é€‰\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        min_replicates: int\n",
    "            æ¯ä¸ªåŒ–åˆç‰©çš„æœ€å°replicateæ•° (default: 5)\n",
    "        min_cell_lines: int\n",
    "            æ¯ä¸ªåŒ–åˆç‰©æµ‹è¯•çš„æœ€å°ç»†èƒç³»æ•° (default: 2)\n",
    "        min_obs_per_compound: int\n",
    "            æ¯ä¸ªåŒ–åˆç‰©çš„æœ€å°è§‚æµ‹æ•° (default: 10)\n",
    "        min_compounds_per_cell: int\n",
    "            æ¯ä¸ªç»†èƒç³»çš„æœ€å°åŒ–åˆç‰©æ•° (default: 200)\n",
    "        min_replicate_similarity: float\n",
    "            replicateä¹‹é—´çš„æœ€å°ç›¸ä¼¼åº¦ (default: 0.12)\n",
    "        \"\"\"\n",
    "        if self.signatures is None:\n",
    "            raise ValueError(\"Please load signatures first using load_level4_signatures()\")\n",
    "        \n",
    "        matrix = self.signatures['matrix']\n",
    "        row_meta = self.signatures['row_meta']\n",
    "        col_meta = self.signatures['col_meta']\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"ğŸ” APPLYING QUALITY CONTROL FILTERS\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"Initial samples: {len(row_meta)}\")\n",
    "        print(f\"\\nFilter criteria:\")\n",
    "        print(f\"  â€¢ Replicates per compound â‰¥ {min_replicates}\")\n",
    "        print(f\"  â€¢ Cell lines per compound â‰¥ {min_cell_lines}\")\n",
    "        print(f\"  â€¢ Observations per compound â‰¥ {min_obs_per_compound}\")\n",
    "        print(f\"  â€¢ Compounds per cell line â‰¥ {min_compounds_per_cell}\")\n",
    "        print(f\"  â€¢ Replicate similarity â‰¥ {min_replicate_similarity}\")\n",
    "        \n",
    "        # åˆ›å»ºå·¥ä½œå‰¯æœ¬\n",
    "        working_matrix = matrix.copy()\n",
    "        working_meta = row_meta.copy()\n",
    "        \n",
    "        if ('pert_id' not in working_meta.columns) or ('cell_id' not in working_meta.columns):\n",
    "            print(\"\\n[INFO] 'pert_id' or 'cell_id' not found in row_meta, parsing from 'id' column...\")\n",
    "            if 'id' not in working_meta.columns:\n",
    "                raise KeyError(\n",
    "                    \"row_meta ä¸­æ—¢æ²¡æœ‰ 'pert_id'/'cell_id'ï¼Œä¹Ÿæ²¡æœ‰ 'id' åˆ—ï¼Œæ— æ³•è§£æã€‚\"\n",
    "                )\n",
    "            \n",
    "            # æŒ‰ä¸‹åˆ’çº¿æ‹†åˆ†\n",
    "            parts = working_meta['id'].str.split('_', expand=True)\n",
    "            if parts.shape[1] < 2:\n",
    "                raise ValueError(\n",
    "                    f\"'id' åˆ—æ— æ³•æŒ‰ '_' æ‹†åˆ†ä¸ºè‡³å°‘ä¸¤æ®µï¼Œæ— æ³•æ¨æ–­ pert_id / cell_idï¼Œç¤ºä¾‹: {working_meta['id'].iloc[0]}\"\n",
    "                )\n",
    "            \n",
    "            # çº¦å®šï¼šç¬¬ 0 æ®µæ˜¯ pert_idï¼Œç¬¬ 1 æ®µæ˜¯ cell_id\n",
    "            working_meta['pert_id'] = parts[0]\n",
    "            working_meta['cell_id'] = parts[1]\n",
    "            \n",
    "            print(\"       Added columns:\")\n",
    "            print(\"         â€¢ pert_id <- id.split('_')[0]\")\n",
    "            print(\"         â€¢ cell_id <- id.split('_')[1]\")\n",
    "            print(\"       ç¤ºä¾‹ï¼š\", working_meta[['id', 'pert_id', 'cell_id']].head(3))\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Pre-filter statistics:\")\n",
    "        print(f\"   Total unique compounds: {working_meta['pert_id'].nunique()}\")\n",
    "        print(f\"   Total unique cell lines: {working_meta['cell_id'].nunique()}\")\n",
    "\n",
    "        # æ˜¾ç¤ºæ¯ä¸ªç»†èƒç³»çš„åŒ–åˆç‰©æ•°åˆ†å¸ƒ\n",
    "        cell_compound_counts = working_meta.groupby('cell_id')['pert_id'].nunique()\n",
    "        print(f\"\\n   Compounds per cell line distribution:\")\n",
    "        print(f\"      Min: {cell_compound_counts.min()}\")\n",
    "        print(f\"      Max: {cell_compound_counts.max()}\")\n",
    "        print(f\"      Mean: {cell_compound_counts.mean():.1f}\")\n",
    "        print(f\"      Median: {cell_compound_counts.median():.0f}\")\n",
    "        print(f\"      75th percentile: {cell_compound_counts.quantile(0.75):.0f}\")\n",
    "        print(f\"      90th percentile: {cell_compound_counts.quantile(0.90):.0f}\")\n",
    "\n",
    "        # æ˜¾ç¤ºåŒ–åˆç‰©åˆ†å¸ƒ\n",
    "        compound_counts = working_meta.groupby('pert_id').size()\n",
    "        print(f\"\\n   Samples per compound distribution:\")\n",
    "        print(f\"      Min: {compound_counts.min()}\")\n",
    "        print(f\"      Max: {compound_counts.max()}\")\n",
    "        print(f\"      Mean: {compound_counts.mean():.1f}\")\n",
    "        print(f\"      Median: {compound_counts.median():.0f}\")\n",
    "        \n",
    "        # ========== Filter 1: Cell line filter ==========\n",
    "        if min_compounds_per_cell is not None and min_compounds_per_cell > 0:\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"FILTER 1: Compounds per cell line\")\n",
    "            print(f\"{'='*70}\")\n",
    "            \n",
    "            cell_compound_counts = working_meta.groupby('cell_id')['pert_id'].nunique()\n",
    "            valid_cells = cell_compound_counts[cell_compound_counts >= min_compounds_per_cell].index\n",
    "            \n",
    "            print(f\"  Cell lines with â‰¥{min_compounds_per_cell} compounds: {len(valid_cells)}/{len(cell_compound_counts)}\")\n",
    "            \n",
    "            if len(valid_cells) == 0:\n",
    "                print(f\"  âš ï¸  WARNING: No cell lines meet the criteria!\")\n",
    "                print(f\"  Suggestion: Lower min_compounds_per_cell to {cell_compound_counts.quantile(0.5):.0f} (median) or {cell_compound_counts.quantile(0.75):.0f} (75th percentile)\")\n",
    "                # ä¸è¦ç›´æ¥è¿‡æ»¤ï¼Œç»§ç»­ä½¿ç”¨æ‰€æœ‰æ•°æ®\n",
    "            else:\n",
    "                cell_mask = working_meta['cell_id'].isin(valid_cells)\n",
    "                working_matrix = working_matrix[cell_mask]\n",
    "                working_meta = working_meta[cell_mask].reset_index(drop=True)\n",
    "            \n",
    "            print(f\"  Remaining samples: {len(working_meta)}\")\n",
    "            print(f\"  Remaining compounds: {working_meta['pert_id'].nunique()}\")\n",
    "        else:\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"FILTER 1: Cell line filter [SKIPPED]\")\n",
    "            print(f\"{'='*70}\")\n",
    "        \n",
    "        # ========== Filter 2: Replicate count ==========\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"FILTER 2: Replicates per compound\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        replicate_counts = working_meta.groupby('pert_id').size()\n",
    "        valid_perts_rep = replicate_counts[replicate_counts >= min_replicates].index\n",
    "        \n",
    "        print(f\"  Compounds with â‰¥{min_replicates} replicates: {len(valid_perts_rep)}/{len(replicate_counts)}\")\n",
    "        \n",
    "        rep_mask = working_meta['pert_id'].isin(valid_perts_rep)\n",
    "        working_matrix = working_matrix[rep_mask]\n",
    "        working_meta = working_meta[rep_mask].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"  Remaining samples: {len(working_meta)}\")\n",
    "        print(f\"  Remaining compounds: {working_meta['pert_id'].nunique()}\")\n",
    "        \n",
    "        # ========== Filter 3: Cell line diversity ==========\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"FILTER 3: Cell line diversity per compound\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        cell_line_counts = working_meta.groupby('pert_id')['cell_id'].nunique()\n",
    "        valid_perts_cell = cell_line_counts[cell_line_counts >= min_cell_lines].index\n",
    "        \n",
    "        print(f\"  Compounds in â‰¥{min_cell_lines} cell lines: {len(valid_perts_cell)}/{len(cell_line_counts)}\")\n",
    "        \n",
    "        cell_div_mask = working_meta['pert_id'].isin(valid_perts_cell)\n",
    "        working_matrix = working_matrix[cell_div_mask]\n",
    "        working_meta = working_meta[cell_div_mask].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"  Remaining samples: {len(working_meta)}\")\n",
    "        print(f\"  Remaining compounds: {working_meta['pert_id'].nunique()}\")\n",
    "        \n",
    "        # ========== Filter 4: Observations per compound ==========\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"FILTER 4: Total observations per compound\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        obs_counts = working_meta.groupby('pert_id').size()\n",
    "        valid_perts_obs = obs_counts[obs_counts >= min_obs_per_compound].index\n",
    "        \n",
    "        print(f\"  Compounds with â‰¥{min_obs_per_compound} observations: {len(valid_perts_obs)}/{len(obs_counts)}\")\n",
    "        \n",
    "        obs_mask = working_meta['pert_id'].isin(valid_perts_obs)\n",
    "        working_matrix = working_matrix[obs_mask]\n",
    "        working_meta = working_meta[obs_mask].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"  Remaining samples: {len(working_meta)}\")\n",
    "        print(f\"  Remaining compounds: {working_meta['pert_id'].nunique()}\")\n",
    "        \n",
    "        # ========== Filter 5: Signature-level replicate CC (distil_cc_q75-like) ==========\n",
    "        if min_replicate_similarity is not None and min_replicate_similarity > 0:\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"FILTER 5: Signature-level replicate CC (â‰ˆ distil_cc_q75)\")\n",
    "            print(f\"{'='*70}\")\n",
    "\n",
    "            # 1. è®¡ç®—æ¯ä¸ª signature çš„ CCï¼ˆq75 Spearmanï¼‰\n",
    "            sig_cc_df = self.calculate_replicate_similarity(\n",
    "                working_matrix,\n",
    "                working_meta,\n",
    "                group_cols=['pert_id', 'cell_id', 'pert_time', 'pert_dose'],\n",
    "                corr_type='spearman',\n",
    "                quantile=0.75,\n",
    "            )\n",
    "\n",
    "            if sig_cc_df.empty:\n",
    "                print(\"  âš ï¸ No signatures with â‰¥2 replicates, skipping CC-based filter.\")\n",
    "                replicate_similarities = None\n",
    "            else:\n",
    "                print(f\"  Using CC threshold (min_replicate_similarity): {min_replicate_similarity}\")\n",
    "                # 2. æŒ‰ CC é˜ˆå€¼ç­›é€‰ signature\n",
    "                valid_sigs = sig_cc_df[sig_cc_df['rep_cc_q'] >= min_replicate_similarity]\n",
    "                print(f\"  Signatures with CC â‰¥ {min_replicate_similarity}: \"\n",
    "                    f\"{len(valid_sigs)}/{len(sig_cc_df)}\")\n",
    "\n",
    "                # 3. æŠŠé€šè¿‡ QC çš„ signature æ˜ å°„å›å…·ä½“çš„è¡Œï¼ˆsamplesï¼‰\n",
    "                tmp_meta = working_meta.copy()\n",
    "                tmp_meta['_row_idx'] = np.arange(len(tmp_meta))\n",
    "\n",
    "                sig_cols = [c for c in ['pert_id','cell_id','pert_time','pert_dose']\n",
    "                            if c in working_meta.columns]\n",
    "                valid_sigs_for_merge = valid_sigs[sig_cols + ['rep_cc_q']]\n",
    "\n",
    "                merged = tmp_meta.merge(valid_sigs_for_merge, on=sig_cols, how='inner')\n",
    "                keep_idx = merged['_row_idx'].to_numpy()\n",
    "\n",
    "                working_matrix = working_matrix[keep_idx]\n",
    "                working_meta = working_meta.iloc[keep_idx].reset_index(drop=True)\n",
    "\n",
    "                print(f\"  Remaining samples after CC filter: {len(working_meta)}\")\n",
    "                print(f\"  Remaining compounds: {working_meta['pert_id'].nunique()}\")\n",
    "\n",
    "                # ï¼ˆå¯é€‰ï¼‰æŠŠ signature CC èšåˆåˆ°åŒ–åˆç‰©å±‚é¢ï¼Œä¾¿äºåç»­åˆ†æ\n",
    "                replicate_similarities = (\n",
    "                    sig_cc_df\n",
    "                    .groupby('pert_id')['rep_cc_q']\n",
    "                    .mean()\n",
    "                    .sort_values(ascending=False)\n",
    "                )\n",
    "        else:\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"FILTER 5: Replicate similarity [SKIPPED]\")\n",
    "            print(f\"{'='*70}\")\n",
    "            replicate_similarities = None\n",
    "\n",
    "        \n",
    "        # ========== Create final dataset ==========\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"âœ… FINAL DATASET\")\n",
    "        print(f\"{'='*70}\")\n",
    "\n",
    "        # ğŸ”§ å…ˆåˆ›å»ºåŒ–åˆç‰©æ ‡ç­¾ç¼–ç \n",
    "        unique_perts = sorted(working_meta['pert_id'].unique())\n",
    "        pert_to_idx = {pert: idx for idx, pert in enumerate(unique_perts)}\n",
    "        labels = np.array([pert_to_idx[p] for p in working_meta['pert_id']])\n",
    "\n",
    "        # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®å‰©ä½™\n",
    "        if len(unique_perts) == 0:\n",
    "            print(f\"  âŒ No compounds left after filtering!\")\n",
    "            print(f\"  Suggestions:\")\n",
    "            print(f\"     1. Lower the filtering thresholds\")\n",
    "            print(f\"     2. Check the data format (pert_id/cell_id parsing)\")\n",
    "            print(f\"     3. Run with debug info to see distribution\")\n",
    "            raise ValueError(\"No compounds remain after filtering. Please adjust filter criteria.\")\n",
    "\n",
    "        print(f\"  Total samples: {len(working_matrix)}\")\n",
    "        print(f\"  Total compounds: {len(unique_perts)}\")\n",
    "        print(f\"  Gene features: {working_matrix.shape[1]}\")\n",
    "        print(f\"  Samples per compound (mean): {len(working_matrix) / len(unique_perts):.1f}\")\n",
    "\n",
    "        # ç»Ÿè®¡ç»†èƒç³»åˆ†å¸ƒ\n",
    "        cell_dist = working_meta.groupby('cell_id').size()\n",
    "        print(f\"\\n  Cell line distribution:\")\n",
    "        print(f\"    Unique cell lines: {len(cell_dist)}\")\n",
    "        print(f\"    Samples per cell line (mean): {cell_dist.mean():.1f}\")\n",
    "        print(f\"    Samples per cell line (median): {cell_dist.median():.0f}\")\n",
    "\n",
    "        # æ„å»ºè®­ç»ƒæ•°æ®å­—å…¸\n",
    "        training_data = {\n",
    "            'X': working_matrix,\n",
    "            'y': labels,\n",
    "            'sample_meta': working_meta,\n",
    "            'gene_names': col_meta['id'].values,\n",
    "            'compound_names': unique_perts,\n",
    "            'pert_to_idx': pert_to_idx\n",
    "        }\n",
    "\n",
    "        # ğŸ”§ åªæœ‰åœ¨è®¡ç®—äº†ç›¸ä¼¼åº¦çš„æƒ…å†µä¸‹æ‰æ·»åŠ \n",
    "        if replicate_similarities is not None:\n",
    "            training_data['replicate_similarities'] = replicate_similarities\n",
    "\n",
    "        return training_data\n",
    "    \n",
    "    def create_3fold_splits(self, training_data, random_state=42):\n",
    "        \"\"\"åˆ›å»º3æŠ˜äº¤å‰éªŒè¯åˆ’åˆ†\"\"\"\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        sample_meta = training_data['sample_meta']\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"ğŸ² Creating 3-fold cross-validation splits\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        folds = np.zeros(len(sample_meta), dtype=int)\n",
    "        \n",
    "        for pert_id in sample_meta['pert_id'].unique():\n",
    "            pert_mask = sample_meta['pert_id'] == pert_id\n",
    "            pert_indices = np.where(pert_mask)[0]\n",
    "            \n",
    "            np.random.shuffle(pert_indices)\n",
    "            n_samples = len(pert_indices)\n",
    "            \n",
    "            fold_sizes = [n_samples // 3] * 3\n",
    "            for i in range(n_samples % 3):\n",
    "                fold_sizes[i] += 1\n",
    "            \n",
    "            start_idx = 0\n",
    "            for fold_id, size in enumerate(fold_sizes):\n",
    "                end_idx = start_idx + size\n",
    "                folds[pert_indices[start_idx:end_idx]] = fold_id\n",
    "                start_idx = end_idx\n",
    "        \n",
    "        training_data['folds'] = folds\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Fold statistics:\")\n",
    "        for fold_id in range(3):\n",
    "            fold_mask = folds == fold_id\n",
    "            n_samples = fold_mask.sum()\n",
    "            n_compounds = sample_meta[fold_mask]['pert_id'].nunique()\n",
    "            print(f\"   Fold {fold_id}: {n_samples:,} samples, {n_compounds:,} compounds\")\n",
    "        \n",
    "        return training_data\n",
    "\n",
    "    def explore_data_distribution(self):\n",
    "        \"\"\"æ¢ç´¢æ•°æ®åˆ†å¸ƒï¼Œå¸®åŠ©ç¡®å®šåˆé€‚çš„ç­›é€‰é˜ˆå€¼\"\"\"\n",
    "        if self.signatures is None:\n",
    "            raise ValueError(\"Please load signatures first\")\n",
    "        \n",
    "        matrix = self.signatures['matrix']\n",
    "        row_meta = self.signatures['row_meta']\n",
    "        \n",
    "        # è§£æ pert_id å’Œ cell_idï¼ˆå¦‚æœéœ€è¦ï¼‰\n",
    "        working_meta = row_meta.copy()\n",
    "        if 'pert_id' not in working_meta.columns:\n",
    "            parts = working_meta['id'].str.split('_', expand=True)\n",
    "            working_meta['pert_id'] = parts[0]\n",
    "            working_meta['cell_id'] = parts[1]\n",
    "        \n",
    "        print(\"=\" * 70)\n",
    "        print(\"ğŸ“Š DATA DISTRIBUTION ANALYSIS\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # åŒ–åˆç‰©ç»Ÿè®¡\n",
    "        compound_counts = working_meta.groupby('pert_id').size()\n",
    "        print(f\"\\n1. Samples per compound:\")\n",
    "        print(f\"   Total compounds: {len(compound_counts)}\")\n",
    "        print(f\"   Percentiles:\")\n",
    "        for p in [10, 25, 50, 75, 90, 95]:\n",
    "            print(f\"      {p}%: {compound_counts.quantile(p/100):.0f}\")\n",
    "        \n",
    "        # ç»†èƒç³»ç»Ÿè®¡\n",
    "        cell_counts = working_meta.groupby('cell_id').size()\n",
    "        print(f\"\\n2. Samples per cell line:\")\n",
    "        print(f\"   Total cell lines: {len(cell_counts)}\")\n",
    "        print(f\"   Percentiles:\")\n",
    "        for p in [10, 25, 50, 75, 90, 95]:\n",
    "            print(f\"      {p}%: {cell_counts.quantile(p/100):.0f}\")\n",
    "        \n",
    "        # ç»†èƒç³»çš„åŒ–åˆç‰©æ•°\n",
    "        cell_compound_counts = working_meta.groupby('cell_id')['pert_id'].nunique()\n",
    "        print(f\"\\n3. Compounds per cell line:\")\n",
    "        print(f\"   Percentiles:\")\n",
    "        for p in [10, 25, 50, 75, 90, 95]:\n",
    "            print(f\"      {p}%: {cell_compound_counts.quantile(p/100):.0f}\")\n",
    "        \n",
    "        # åŒ–åˆç‰©çš„ç»†èƒç³»æ•°\n",
    "        compound_cell_counts = working_meta.groupby('pert_id')['cell_id'].nunique()\n",
    "        print(f\"\\n4. Cell lines per compound:\")\n",
    "        print(f\"   Percentiles:\")\n",
    "        for p in [10, 25, 50, 75, 90, 95]:\n",
    "            print(f\"      {p}%: {compound_cell_counts.quantile(p/100):.0f}\")\n",
    "        \n",
    "        print(f\"\\n\" + \"=\" * 70)\n",
    "        print(f\"ğŸ’¡ RECOMMENDED THRESHOLDS:\")\n",
    "        print(f\"=\" * 70)\n",
    "        print(f\"   min_replicates: {max(3, int(compound_counts.quantile(0.25)))}\")\n",
    "        print(f\"   min_cell_lines: {max(1, int(compound_cell_counts.quantile(0.25)))}\")\n",
    "        print(f\"   min_obs_per_compound: {max(5, int(compound_counts.quantile(0.25)))}\")\n",
    "        print(f\"   min_compounds_per_cell: {max(20, int(cell_compound_counts.quantile(0.25)))}\")\n",
    "\n",
    "# ========== ä¸»ç¨‹åº ==========\n",
    "\n",
    "def main():\n",
    "    \"\"\"ä¸»ç¨‹åº\"\"\"\n",
    "    loader = LINCSDataLoader(\"E:/ç§‘ç ”/Models/drugreflector/datasets\")\n",
    "    \n",
    "    try:\n",
    "        # Step 1: åŠ è½½åŸºå› å’Œç»†èƒä¿¡æ¯\n",
    "        print(\"=\" * 70)\n",
    "        print(\"STEP 1: Loading gene and cell information\")\n",
    "        print(\"=\" * 70)\n",
    "        gene_info = loader.load_gene_info('GSE92742')\n",
    "        cell_info = loader.load_cell_info('GSE92742')\n",
    "        \n",
    "        # Step 2: åŠ è½½Level 4 signatures\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"STEP 2: Loading Level 4 signatures\")\n",
    "        print(\"=\" * 70)\n",
    "        matrix, row_meta, col_meta = loader.load_level4_signatures('GSE92742')\n",
    "        \n",
    "        # Step 3: å‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆåº”ç”¨æ‰€æœ‰ç­›é€‰æ¡ä»¶ï¼‰\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"STEP 3: Preparing training data with quality filters\")\n",
    "        print(\"=\" * 70)\n",
    "        training_data = loader.prepare_training_data(\n",
    "            min_replicates=5,                    # æ¯ä¸ªåŒ–åˆç‰©è‡³å°‘5ä¸ªreplicate\n",
    "            min_cell_lines=2,                    # æ¯ä¸ªåŒ–åˆç‰©è‡³å°‘åœ¨2ä¸ªç»†èƒç³»æµ‹è¯•\n",
    "            min_obs_per_compound=10,              # æ¯ä¸ªåŒ–åˆç‰©è‡³å°‘10ä¸ªè§‚æµ‹\n",
    "            min_compounds_per_cell=200,            # æ¯ä¸ªç»†èƒç³»è‡³å°‘æœ‰200ä¸ªåŒ–åˆç‰©\n",
    "            min_replicate_similarity=0.12        # replicateç›¸ä¼¼åº¦è‡³å°‘0.12\n",
    "        )\n",
    "        \n",
    "        # Step 4: åˆ›å»º3æŠ˜åˆ’åˆ†\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"STEP 4: Creating 3-fold splits\")\n",
    "        print(\"=\" * 70)\n",
    "        training_data = loader.create_3fold_splits(training_data)\n",
    "        \n",
    "        # Step 5: ä¿å­˜å¤„ç†åçš„æ•°æ®\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"STEP 5: Saving processed data\")\n",
    "        print(\"=\" * 70)\n",
    "        output_dir = Path(\"E:/ç§‘ç ”/Models/drugreflector/processed_data\")\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        output_file = output_dir / \"training_data_filtered.pkl\"\n",
    "        print(f\"ğŸ’¾ Saving to: {output_file}\")\n",
    "        \n",
    "        with open(output_file, 'wb') as f:\n",
    "            pickle.dump(training_data, f)\n",
    "        \n",
    "        print(f\"âœ“ Saved successfully!\")\n",
    "        \n",
    "        # æ‰“å°æœ€ç»ˆæ‘˜è¦\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"âœ… DATA PREPARATION COMPLETE!\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"ğŸ“ Output file: {output_file}\")\n",
    "        print(f\"\\nğŸ“Š Final dataset summary:\")\n",
    "        print(f\"   â€¢ Total samples: {len(training_data['X']):,}\")\n",
    "        print(f\"   â€¢ Total compounds: {len(training_data['compound_names']):,}\")\n",
    "        print(f\"   â€¢ Gene features: {training_data['X'].shape[1]}\")\n",
    "        print(f\"   â€¢ Data shape: {training_data['X'].shape}\")\n",
    "        print(f\"   â€¢ Average samples per compound: {len(training_data['X']) / len(training_data['compound_names']):.1f}\")\n",
    "        \n",
    "        # è§£å‹æ–‡ä»¶ä¿¡æ¯\n",
    "        if loader.decompressed_files:\n",
    "            print(f\"\\nğŸ“¦ Decompressed files saved at:\")\n",
    "            for f in loader.decompressed_files:\n",
    "                print(f\"   â€¢ {f}\")\n",
    "        \n",
    "        return training_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n\" + \"=\" * 70)\n",
    "        print(\"âŒ ERROR DURING DATA PREPARATION\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"   Type: {type(e).__name__}\")\n",
    "        print(f\"   Message: {e}\")\n",
    "        print(f\"\\nğŸ“‹ Full traceback:\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_from_cache(load_filtered_data=True, \n",
    "                   filtered_data_path=\"E:/ç§‘ç ”/Models/drugreflector/processed_data/training_data_filtered.pkl\",\n",
    "                   min_replicates=5,\n",
    "                   min_cell_lines=2,\n",
    "                   min_obs_per_compound=10,\n",
    "                   min_compounds_per_cell=50,  # é™ä½é»˜è®¤å€¼\n",
    "                   min_replicate_similarity=None):  # é»˜è®¤å…³é—­\n",
    "    \"\"\"\n",
    "    ç›´æ¥ä»è§£å‹æ–‡ä»¶æˆ–è¿‡æ»¤åçš„pklæ–‡ä»¶åŠ è½½æ•°æ®\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    load_filtered_data: bool\n",
    "        å¦‚æœä¸ºTrueä¸”pklæ–‡ä»¶å­˜åœ¨ï¼Œç›´æ¥è¯»å–è¿‡æ»¤åçš„æ•°æ®ï¼ˆæœ€å¿«ï¼‰\n",
    "    filtered_data_path: str\n",
    "        è¿‡æ»¤åæ•°æ®çš„pklæ–‡ä»¶è·¯å¾„\n",
    "    å…¶ä»–å‚æ•°: ç­›é€‰æ¡ä»¶ï¼ˆä»…åœ¨ load_filtered_data=False æˆ– pkl ä¸å­˜åœ¨æ—¶ä½¿ç”¨ï¼‰\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    training_data: dict or None\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"ğŸš€ LOADING FROM CACHED FILES\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # ========== ä¼˜å…ˆçº§1: å°è¯•åŠ è½½è¿‡æ»¤åçš„pklæ–‡ä»¶ ==========\n",
    "    filtered_pkl = Path(filtered_data_path)\n",
    "    \n",
    "    if load_filtered_data and filtered_pkl.exists():\n",
    "        print(f\"\\nğŸ“¦ Found filtered data cache: {filtered_pkl.name}\")\n",
    "        print(f\"   Loading pre-processed data (fastest method)...\")\n",
    "        \n",
    "        try:\n",
    "            with open(filtered_pkl, 'rb') as f:\n",
    "                training_data = pickle.load(f)\n",
    "            \n",
    "            print(f\"\\nâœ… Filtered data loaded successfully!\")\n",
    "            print(f\"   â€¢ Samples: {len(training_data['X']):,}\")\n",
    "            print(f\"   â€¢ Compounds: {len(training_data['compound_names']):,}\")\n",
    "            print(f\"   â€¢ Features: {training_data['X'].shape[1]}\")\n",
    "            print(f\"   â€¢ Folds: {'Yes' if 'folds' in training_data else 'No'}\")\n",
    "            \n",
    "            print(f\"\\nğŸ’¡ Tip: To reprocess data with different filters, use:\")\n",
    "            print(f\"   load_from_cache(load_filtered_data=False)\")\n",
    "            \n",
    "            return training_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nâš ï¸  Failed to load filtered data: {e}\")\n",
    "            print(f\"   Falling back to full processing...\")\n",
    "    \n",
    "    elif load_filtered_data and not filtered_pkl.exists():\n",
    "        print(f\"\\nâš ï¸  Filtered data cache not found: {filtered_pkl}\")\n",
    "        print(f\"   Will process from raw GCTX file...\")\n",
    "    \n",
    "    # ========== ä¼˜å…ˆçº§2: ä»è§£å‹çš„GCTXæ–‡ä»¶åŠ è½½å¹¶å¤„ç† ==========\n",
    "    loader = LINCSDataLoader(\"E:/ç§‘ç ”/Models/drugreflector/datasets\")\n",
    "    \n",
    "    # æ£€æŸ¥è§£å‹çš„GCTXç¼“å­˜æ–‡ä»¶\n",
    "    decompressed_dir = Path(\"E:/ç§‘ç ”/Models/drugreflector/datasets/_decompressed\")\n",
    "    cached_gctx = decompressed_dir / \"GSE92742_Broad_LINCS_Level4_ZSPCINF_mlr12k_n1319138x12328.gctx\"\n",
    "    \n",
    "    if not cached_gctx.exists():\n",
    "        print(f\"\\nâŒ Decompressed GCTX file not found: {cached_gctx}\")\n",
    "        print(f\"   Please run main() first to decompress the data.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nâœ“ Found decompressed GCTX: {cached_gctx.name}\")\n",
    "    \n",
    "    try:\n",
    "        # åŠ è½½åŸºå› ä¿¡æ¯\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"STEP 1: Loading gene and cell information\")\n",
    "        print(\"=\" * 70)\n",
    "        loader.load_gene_info('GSE92742')\n",
    "        loader.load_cell_info('GSE92742')\n",
    "        \n",
    "        # ç›´æ¥è¯»å–è§£å‹æ–‡ä»¶\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"STEP 2: Reading cached GCTX file\")\n",
    "        print(\"=\" * 70)\n",
    "        matrix, sample_meta, gene_meta = loader.read_gctx(str(cached_gctx))\n",
    "        \n",
    "        # è¿‡æ»¤landmark genes\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"STEP 3: Filtering to landmark genes\")\n",
    "        print(\"=\" * 70)\n",
    "        landmark_ids = set(loader.gene_info['pr_gene_id'].astype(str).values)\n",
    "        gene_mask = gene_meta['id'].isin(landmark_ids)\n",
    "        \n",
    "        matrix = matrix[:, gene_mask]\n",
    "        gene_meta = gene_meta[gene_mask].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"âœ“ Filtered matrix shape: {matrix.shape}\")\n",
    "        \n",
    "        loader.signatures = {\n",
    "            'matrix': matrix,\n",
    "            'row_meta': sample_meta,\n",
    "            'col_meta': gene_meta\n",
    "        }\n",
    "        \n",
    "        # åº”ç”¨ç­›é€‰\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"STEP 4: Applying quality filters\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"Filter parameters:\")\n",
    "        print(f\"   â€¢ min_replicates: {min_replicates}\")\n",
    "        print(f\"   â€¢ min_cell_lines: {min_cell_lines}\")\n",
    "        print(f\"   â€¢ min_obs_per_compound: {min_obs_per_compound}\")\n",
    "        print(f\"   â€¢ min_compounds_per_cell: {min_compounds_per_cell if min_compounds_per_cell else 'DISABLED'}\")\n",
    "        print(f\"   â€¢ min_replicate_similarity: {min_replicate_similarity if min_replicate_similarity else 'DISABLED'}\")\n",
    "        \n",
    "        training_data = loader.prepare_training_data(\n",
    "            min_replicates=min_replicates,\n",
    "            min_cell_lines=min_cell_lines,\n",
    "            min_obs_per_compound=min_obs_per_compound,\n",
    "            min_compounds_per_cell=min_compounds_per_cell,\n",
    "            min_replicate_similarity=min_replicate_similarity\n",
    "        )\n",
    "        \n",
    "        # åˆ›å»ºfoldåˆ’åˆ†\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"STEP 5: Creating 3-fold splits\")\n",
    "        print(\"=\" * 70)\n",
    "        training_data = loader.create_3fold_splits(training_data)\n",
    "        \n",
    "        # ä¿å­˜è¿‡æ»¤åçš„æ•°æ®\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"STEP 6: Saving filtered data for future use\")\n",
    "        print(\"=\" * 70)\n",
    "        filtered_pkl.parent.mkdir(exist_ok=True)\n",
    "        \n",
    "        print(f\"ğŸ’¾ Saving to: {filtered_pkl}\")\n",
    "        with open(filtered_pkl, 'wb') as f:\n",
    "            pickle.dump(training_data, f)\n",
    "        \n",
    "        print(f\"âœ“ Saved successfully!\")\n",
    "        print(f\"\\nğŸ’¡ Next time, this file will be loaded directly (much faster!)\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"âœ… Data loaded and processed successfully!\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        return training_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Error loading from cache: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ç¬¬ä¸€æ¬¡è¿è¡Œï¼šè§£å‹å¹¶å¤„ç†æ•°æ®\n",
    "    # training_data = main()\n",
    "    \n",
    "    # åç»­è¿è¡Œï¼šç›´æ¥ä»ç¼“å­˜åŠ è½½\n",
    "    training_data = load_from_cache(\n",
    "        load_filtered_data=False,  # ä¼šå°è¯•è¯»å–pklï¼Œä½†ä¸å­˜åœ¨\n",
    "        min_replicates=5,\n",
    "        min_cell_lines=2,\n",
    "        min_obs_per_compound=10,\n",
    "        min_compounds_per_cell=200,\n",
    "        min_replicate_similarity=0.12\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672f194d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf14b519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸš€ LOADING FROM CACHED FILES\n",
      "======================================================================\n",
      "\n",
      "ğŸ“¦ Found filtered data cache: training_data_filtered.pkl\n",
      "   Loading pre-processed data (fastest method)...\n",
      "\n",
      "âœ… Filtered data loaded successfully!\n",
      "   â€¢ Samples: 1,014,503\n",
      "   â€¢ Compounds: 118\n",
      "   â€¢ Features: 978\n",
      "   â€¢ Folds: Yes\n",
      "\n",
      "ğŸ’¡ Tip: To reprocess data with different filters, use:\n",
      "   load_from_cache(load_filtered_data=False)\n"
     ]
    }
   ],
   "source": [
    "training_data = load_from_cache(\n",
    "        load_filtered_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd57492a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, ..., 1, 2, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data['folds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d572f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸš€ LOADING FROM CACHED FILES\n",
      "======================================================================\n",
      "âœ“ Found cached file: GSE92742_Broad_LINCS_Level4_ZSPCINF_mlr12k_n1319138x12328.gctx\n",
      "\n",
      "Loading gene and cell information...\n",
      "Total genes: 12328\n",
      "Landmark genes: 978\n",
      "Total cell lines: 98\n",
      "Unique cell IDs: 98\n",
      "\n",
      "Reading from cached GCTX file...\n",
      "\n",
      "ğŸ“– Reading GCTX file: GSE92742_Broad_LINCS_Level4_ZSPCINF_mlr12k_n1319138x12328.gctx\n",
      "ğŸ“Š Loading matrix from HDF5...\n",
      "âœ“ Matrix shape: (1319138, 12328)\n",
      "ğŸ“‹ Loading metadata...\n",
      "\n",
      "âœ“ Data loaded successfully:\n",
      "  Matrix: (1319138, 12328) (samples Ã— genes)\n",
      "  Samples: 1319138\n",
      "  Genes: 12328\n",
      "\n",
      "Filtering to landmark genes...\n",
      "âœ“ Filtered matrix shape: (1319138, 978)\n",
      "ğŸ’¾ Saved raw (unfiltered) signatures to: E:\\ç§‘ç ”\\Models\\drugreflector\\datasets\\_cache\\GSE92742_level4_landmark_signatures.pkl\n",
      "\n",
      "Applying quality filters...\n",
      "\n",
      "======================================================================\n",
      "ğŸ” APPLYING QUALITY CONTROL FILTERS\n",
      "======================================================================\n",
      "Initial samples: 1319138\n",
      "\n",
      "Filter criteria:\n",
      "  â€¢ Replicates per compound â‰¥ 5\n",
      "  â€¢ Cell lines per compound â‰¥ 2\n",
      "  â€¢ Observations per compound â‰¥ 10\n",
      "  â€¢ Compounds per cell line â‰¥ 200\n",
      "  â€¢ Replicate similarity â‰¥ 0.12\n",
      "\n",
      "[INFO] 'pert_id' or 'cell_id' not found in row_meta, parsing from 'id' column...\n",
      "       Added columns:\n",
      "         â€¢ pert_id <- id.split('_')[0]\n",
      "         â€¢ cell_id <- id.split('_')[1]\n",
      "       ç¤ºä¾‹ï¼š                                      id pert_id cell_id\n",
      "0  CPC005_A375_6H_X1_B3_DUO52HI53LO:K06  CPC005    A375\n",
      "1  CPC005_A375_6H_X2_B3_DUO52HI53LO:K06  CPC005    A375\n",
      "2  CPC005_A375_6H_X3_B3_DUO52HI53LO:K06  CPC005    A375\n",
      "\n",
      "======================================================================\n",
      "FILTER 1: Compounds per cell line\n",
      "======================================================================\n",
      "  Cell lines with â‰¥200 compounds: 0/76\n",
      "  Remaining samples: 0\n",
      "  Remaining compounds: 0\n",
      "\n",
      "======================================================================\n",
      "FILTER 2: Replicates per compound\n",
      "======================================================================\n",
      "  Compounds with â‰¥5 replicates: 0/0\n",
      "  Remaining samples: 0\n",
      "  Remaining compounds: 0\n",
      "\n",
      "======================================================================\n",
      "FILTER 3: Cell line diversity per compound\n",
      "======================================================================\n",
      "  Compounds in â‰¥2 cell lines: 0/0\n",
      "  Remaining samples: 0\n",
      "  Remaining compounds: 0\n",
      "\n",
      "======================================================================\n",
      "FILTER 4: Total observations per compound\n",
      "======================================================================\n",
      "  Compounds with â‰¥10 observations: 0/0\n",
      "  Remaining samples: 0\n",
      "  Remaining compounds: 0\n",
      "\n",
      "======================================================================\n",
      "FILTER 5: Replicate similarity\n",
      "======================================================================\n",
      "\n",
      "ğŸ“Š Calculating replicate similarities...\n",
      "   Processing 0 compounds...\n",
      "   âœ“ Calculated similarities for 0 compounds\n",
      "  Compounds with similarity â‰¥0.12: 0/0\n",
      "  Mean similarity: nan\n",
      "  Median similarity: nan\n",
      "  Remaining samples: 0\n",
      "  Remaining compounds: 0\n",
      "\n",
      "======================================================================\n",
      "âœ… FINAL DATASET\n",
      "======================================================================\n",
      "  Total samples: 0\n",
      "  Total compounds: 0\n",
      "  Gene features: 978\n",
      "\n",
      "âŒ Error loading from cache: division by zero\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\11234\\AppData\\Local\\Temp\\ipykernel_33336\\1495855035.py\", line 656, in load_from_cache\n",
      "    training_data = loader.prepare_training_data(\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\11234\\AppData\\Local\\Temp\\ipykernel_33336\\1495855035.py\", line 453, in prepare_training_data\n",
      "    print(f\"  Samples per compound (mean): {len(working_matrix) / len(unique_perts):.1f}\")\n",
      "                                            ~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~\n",
      "ZeroDivisionError: division by zero\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "LINCSæ•°æ®åŠ è½½å’Œé¢„å¤„ç†è„šæœ¬\n",
    "å°†GCTXæ ¼å¼è½¬æ¢ä¸ºè®­ç»ƒæ‰€éœ€çš„æ ¼å¼\n",
    "å®Œå…¨ä¿®å¤ç‰ˆæœ¬ï¼šæ­£ç¡®å¤„ç†GCTXæ ¼å¼çš„åç›´è§‰å…ƒæ•°æ®æ˜ å°„\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import gzip\n",
    "import shutil\n",
    "import glob\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class LINCSDataLoader:\n",
    "    \"\"\"åŠ è½½å’Œé¢„å¤„ç†LINCS L1000æ•°æ®\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.gene_info = None\n",
    "        self.cell_info = None\n",
    "        self.signatures = None\n",
    "        self.decompressed_files = []\n",
    "        \n",
    "    def load_gene_info(self, dataset='GSE92742'):\n",
    "        \"\"\"åŠ è½½åŸºå› ä¿¡æ¯ï¼Œè·å–978ä¸ªlandmark genes\"\"\"\n",
    "        gene_file = self.data_dir / f\"{dataset}_Broad_LINCS_gene_info.txt.gz\"\n",
    "        \n",
    "        gene_info = pd.read_csv(gene_file, sep='\\t', compression='gzip')\n",
    "        \n",
    "        # ç­›é€‰landmark genes (pr_is_lm == 1)\n",
    "        landmark_genes = gene_info[gene_info['pr_is_lm'] == 1].copy()\n",
    "        \n",
    "        print(f\"Total genes: {len(gene_info)}\")\n",
    "        print(f\"Landmark genes: {len(landmark_genes)}\")\n",
    "        \n",
    "        self.gene_info = landmark_genes\n",
    "        return landmark_genes\n",
    "    \n",
    "    def load_cell_info(self, dataset='GSE92742'):\n",
    "        \"\"\"åŠ è½½ç»†èƒç³»ä¿¡æ¯\"\"\"\n",
    "        cell_file = self.data_dir / f\"{dataset}_Broad_LINCS_cell_info.txt.gz\"\n",
    "        \n",
    "        cell_info = pd.read_csv(cell_file, sep='\\t', compression='gzip')\n",
    "        \n",
    "        print(f\"Total cell lines: {len(cell_info)}\")\n",
    "        print(f\"Unique cell IDs: {cell_info['cell_id'].nunique()}\")\n",
    "        \n",
    "        self.cell_info = cell_info\n",
    "        return cell_info\n",
    "    \n",
    "    def find_decompressed_file(self, original_gz_file):\n",
    "        \"\"\"æŸ¥æ‰¾å·²è§£å‹çš„æ–‡ä»¶\"\"\"\n",
    "        gz_path = Path(original_gz_file)\n",
    "        decompressed_dir = self.data_dir / \"_decompressed\"\n",
    "        decompressed_file = decompressed_dir / gz_path.stem  # å»æ‰.gzåç¼€\n",
    "        \n",
    "        if decompressed_file.exists():\n",
    "            return str(decompressed_file)\n",
    "        return None\n",
    "    \n",
    "    def decompress_gzip_file(self, gzip_file):\n",
    "        \"\"\"\n",
    "        è§£å‹gzipæ–‡ä»¶åˆ°_decompressedæ–‡ä»¶å¤¹\n",
    "        è¿”å›è§£å‹åçš„æ–‡ä»¶è·¯å¾„\n",
    "        \"\"\"\n",
    "        import time\n",
    "        \n",
    "        gzip_file = str(gzip_file)\n",
    "        \n",
    "        # åˆ›å»ºè§£å‹æ–‡ä»¶å¤¹\n",
    "        decompressed_dir = Path(self.data_dir) / \"_decompressed\"\n",
    "        decompressed_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # ä½¿ç”¨åŸæ–‡ä»¶åï¼ˆå»æ‰.gzåç¼€ï¼‰\n",
    "        original_name = Path(gzip_file).stem\n",
    "        output_path = decompressed_dir / original_name\n",
    "        \n",
    "        # å¦‚æœæ–‡ä»¶å·²ç»å­˜åœ¨ï¼Œç›´æ¥ä½¿ç”¨\n",
    "        if output_path.exists():\n",
    "            print(f\"âœ“ Found existing decompressed file: {output_path.name}\")\n",
    "            self.decompressed_files.append(output_path)\n",
    "            return str(output_path)\n",
    "        \n",
    "        print(f\"ğŸ’¾ Decompressing to: {decompressed_dir}\")\n",
    "        \n",
    "        try:\n",
    "            print(f\"ğŸ“¦ Decompressing {Path(gzip_file).name}...\")\n",
    "            source_size = Path(gzip_file).stat().st_size / (1024**3)\n",
    "            print(f\"   Source size: ~{source_size:.1f} GB\")\n",
    "            \n",
    "            # ä½¿ç”¨8MBç¼“å†²åŒº\n",
    "            with gzip.open(gzip_file, 'rb') as f_in:\n",
    "                with open(output_path, 'wb') as f_out:\n",
    "                    shutil.copyfileobj(f_in, f_out, length=8*1024*1024)\n",
    "            \n",
    "            output_size = output_path.stat().st_size / (1024**3)\n",
    "            print(f\"âœ“ Decompressed successfully!\")\n",
    "            print(f\"âœ“ Output file: {output_path.name}\")\n",
    "            print(f\"âœ“ Output size: ~{output_size:.1f} GB\")\n",
    "            \n",
    "            self.decompressed_files.append(output_path)\n",
    "            return str(output_path)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"\\nâŒ Decompression failed: {e}\")\n",
    "            # æ¸…ç†å¤±è´¥çš„æ–‡ä»¶\n",
    "            if output_path.exists():\n",
    "                try:\n",
    "                    output_path.unlink()\n",
    "                except:\n",
    "                    pass\n",
    "            raise RuntimeError(f\"Failed to decompress {gzip_file}: {e}\")\n",
    "    \n",
    "    def read_gctx(self, gctx_file):\n",
    "        \"\"\"\n",
    "        è¯»å–GCTXæ–‡ä»¶ (HDF5æ ¼å¼)\n",
    "        \n",
    "        âš ï¸ CRITICAL: GCTXæ ¼å¼çš„åç›´è§‰è®¾è®¡\n",
    "        - æ–‡ä»¶å nXXXXxYYYY.gctx è¡¨ç¤º XXXXæ ·æœ¬ Ã— YYYYåŸºå› \n",
    "        - çŸ©é˜µå­˜å‚¨ä¸º (XXXX, YYYY) å½¢çŠ¶\n",
    "        - ä½†å…ƒæ•°æ®å‘½ååç›´è§‰ï¼š\n",
    "          * /0/META/ROW å­˜å‚¨çš„æ˜¯åŸºå› ä¿¡æ¯ï¼ˆå¯¹åº”çŸ©é˜µçš„åˆ—ï¼‰\n",
    "          * /0/META/COL å­˜å‚¨çš„æ˜¯æ ·æœ¬ä¿¡æ¯ï¼ˆå¯¹åº”çŸ©é˜µçš„è¡Œï¼‰\n",
    "        \"\"\"\n",
    "        gctx_file = str(gctx_file)\n",
    "        print(f\"\\nğŸ“– Reading GCTX file: {Path(gctx_file).name}\")\n",
    "        \n",
    "        # æ£€æŸ¥æ˜¯å¦æ˜¯gzipæ–‡ä»¶\n",
    "        if gctx_file.endswith('.gz'):\n",
    "            print(\"âš ï¸  Detected gzip compressed file\")\n",
    "            gctx_file = self.decompress_gzip_file(gctx_file)\n",
    "        \n",
    "        with h5py.File(gctx_file, 'r') as f:\n",
    "            # è¯»å–æ•°æ®çŸ©é˜µ\n",
    "            print(f\"ğŸ“Š Loading matrix from HDF5...\")\n",
    "            matrix = f['/0/DATA/0/matrix'][:]\n",
    "            print(f\"âœ“ Matrix shape: {matrix.shape}\")\n",
    "            \n",
    "            # ğŸ”§ å…³é”®ä¿®å¤ï¼šäº¤æ¢ ROW å’Œ COL çš„ç†è§£\n",
    "            # ROW åœ¨ GCTX ä¸­å®é™…å¯¹åº”åŸºå› ï¼ˆçŸ©é˜µçš„åˆ—ï¼‰\n",
    "            # COL åœ¨ GCTX ä¸­å®é™…å¯¹åº”æ ·æœ¬ï¼ˆçŸ©é˜µçš„è¡Œï¼‰\n",
    "            \n",
    "            print(f\"ğŸ“‹ Loading metadata...\")\n",
    "            \n",
    "            # è¯»å–åŸºå› å…ƒæ•°æ®ï¼ˆæ¥è‡ª ROWï¼‰\n",
    "            gene_meta = {}\n",
    "            for key in f['/0/META/ROW'].keys():\n",
    "                data = f[f'/0/META/ROW/{key}'][:]\n",
    "                if data.dtype.kind in ['S', 'O']:\n",
    "                    gene_meta[key] = np.char.decode(data.astype('S'), 'utf-8')\n",
    "                else:\n",
    "                    gene_meta[key] = data.astype(str)\n",
    "            \n",
    "            # è¯»å–æ ·æœ¬å…ƒæ•°æ®ï¼ˆæ¥è‡ª COLï¼‰\n",
    "            sample_meta = {}\n",
    "            for key in f['/0/META/COL'].keys():\n",
    "                data = f[f'/0/META/COL/{key}'][:]\n",
    "                if data.dtype.kind in ['S', 'O']:\n",
    "                    sample_meta[key] = np.char.decode(data.astype('S'), 'utf-8')\n",
    "                else:\n",
    "                    sample_meta[key] = data.astype(str)\n",
    "        \n",
    "        # è½¬æ¢ä¸ºDataFrame\n",
    "        sample_df = pd.DataFrame(sample_meta)\n",
    "        gene_df = pd.DataFrame(gene_meta)\n",
    "        \n",
    "        print(f\"\\nâœ“ Data loaded successfully:\")\n",
    "        print(f\"  Matrix: {matrix.shape} (samples Ã— genes)\")\n",
    "        print(f\"  Samples: {len(sample_df)}\")\n",
    "        print(f\"  Genes: {len(gene_df)}\")\n",
    "        \n",
    "        # éªŒè¯ç»´åº¦ä¸€è‡´æ€§\n",
    "        assert matrix.shape[0] == len(sample_df), \\\n",
    "            f\"Matrix rows ({matrix.shape[0]}) != sample metadata ({len(sample_df)})\"\n",
    "        assert matrix.shape[1] == len(gene_df), \\\n",
    "            f\"Matrix cols ({matrix.shape[1]}) != gene metadata ({len(gene_df)})\"\n",
    "        \n",
    "        return matrix, sample_df, gene_df\n",
    "    \n",
    "    def load_level4_signatures(self, dataset='GSE92742'):\n",
    "        \"\"\"åŠ è½½Level 4æ•°æ® (Z-score normalized)\"\"\"\n",
    "        level4_pattern = self.data_dir / f\"{dataset}_Broad_LINCS_Level4_ZSPCINF_mlr12k_n*x12328.gctx.gz\"\n",
    "        \n",
    "        print(f\"ğŸ” Searching for Level 4 file...\")\n",
    "        print(f\"   Pattern: {level4_pattern.name}\")\n",
    "        files = glob.glob(str(level4_pattern))\n",
    "        \n",
    "        if not files:\n",
    "            raise FileNotFoundError(\n",
    "                f\"âŒ No Level 4 file found matching {level4_pattern}\"\n",
    "            )\n",
    "        \n",
    "        level4_file = files[0]\n",
    "        print(f\"âœ“ Found: {Path(level4_file).name}\")\n",
    "        \n",
    "        # è¯»å–GCTX\n",
    "        matrix, sample_meta, gene_meta = self.read_gctx(level4_file)\n",
    "        \n",
    "        # åªä¿ç•™landmark genes\n",
    "        print(f\"\\nğŸ”¬ Filtering to landmark genes...\")\n",
    "        if self.gene_info is None:\n",
    "            self.load_gene_info(dataset)\n",
    "        \n",
    "        landmark_ids = set(self.gene_info['pr_gene_id'].astype(str).values)\n",
    "        print(f\"   Landmark genes to find: {len(landmark_ids)}\")\n",
    "        print(f\"   Total genes in data: {len(gene_meta)}\")\n",
    "        \n",
    "        # åˆ›å»ºåŸºå› mask\n",
    "        gene_mask = gene_meta['id'].isin(landmark_ids)\n",
    "        n_matched = gene_mask.sum()\n",
    "        print(f\"   âœ“ Matched: {n_matched} genes\")\n",
    "        \n",
    "        if n_matched == 0:\n",
    "            raise ValueError(\"No landmark genes matched!\")\n",
    "        \n",
    "        # è¿‡æ»¤çŸ©é˜µå’Œå…ƒæ•°æ®\n",
    "        print(f\"\\nğŸ¯ Applying filter...\")\n",
    "        matrix = matrix[:, gene_mask]\n",
    "        gene_meta = gene_meta[gene_mask].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"   âœ“ Final matrix shape: {matrix.shape}\")\n",
    "        \n",
    "        self.signatures = {\n",
    "            'matrix': matrix,\n",
    "            'row_meta': sample_meta,\n",
    "            'col_meta': gene_meta\n",
    "        }\n",
    "        \n",
    "        return matrix, sample_meta, gene_meta\n",
    "    \n",
    "    def calculate_replicate_similarity(self, matrix, pert_ids):\n",
    "        \"\"\"\n",
    "        è®¡ç®—æ¯ä¸ªåŒ–åˆç‰©çš„replicateç›¸ä¼¼åº¦ï¼ˆå¹³å‡Pearsonç›¸å…³ç³»æ•°ï¼‰\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        matrix: np.ndarray\n",
    "            è¡¨è¾¾çŸ©é˜µ (n_samples, n_genes)\n",
    "        pert_ids: pd.Series\n",
    "            åŒ–åˆç‰©ID\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.Series: æ¯ä¸ªåŒ–åˆç‰©çš„å¹³å‡replicateç›¸ä¼¼åº¦\n",
    "        \"\"\"\n",
    "        print(f\"\\nğŸ“Š Calculating replicate similarities...\")\n",
    "        similarities = {}\n",
    "        \n",
    "        unique_perts = pert_ids.unique()\n",
    "        print(f\"   Processing {len(unique_perts)} compounds...\")\n",
    "        \n",
    "        for i, pert_id in enumerate(unique_perts):\n",
    "            if (i + 1) % 1000 == 0:\n",
    "                print(f\"   Progress: {i + 1}/{len(unique_perts)}\")\n",
    "            \n",
    "            # è·å–è¯¥åŒ–åˆç‰©çš„æ‰€æœ‰replicate\n",
    "            pert_mask = pert_ids == pert_id\n",
    "            pert_data = matrix[pert_mask]\n",
    "            \n",
    "            if len(pert_data) < 2:\n",
    "                similarities[pert_id] = 0.0\n",
    "                continue\n",
    "            \n",
    "            # è®¡ç®—æˆå¯¹ç›¸å…³ç³»æ•°\n",
    "            correlations = []\n",
    "            for j in range(len(pert_data)):\n",
    "                for k in range(j + 1, len(pert_data)):\n",
    "                    corr, _ = pearsonr(pert_data[j], pert_data[k])\n",
    "                    if not np.isnan(corr):\n",
    "                        correlations.append(corr)\n",
    "            \n",
    "            similarities[pert_id] = np.mean(correlations) if correlations else 0.0\n",
    "        \n",
    "        print(f\"   âœ“ Calculated similarities for {len(similarities)} compounds\")\n",
    "        return pd.Series(similarities)\n",
    "    \n",
    "    def prepare_training_data(self, \n",
    "                            min_replicates=5, \n",
    "                            min_cell_lines=2,\n",
    "                            min_obs_per_compound=10,\n",
    "                            min_compounds_per_cell=200,\n",
    "                            min_replicate_similarity=0.12):\n",
    "        \"\"\"\n",
    "        å‡†å¤‡è®­ç»ƒæ•°æ®ï¼Œåº”ç”¨ä¸¥æ ¼çš„è´¨é‡æ§åˆ¶ç­›é€‰\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        min_replicates: int\n",
    "            æ¯ä¸ªåŒ–åˆç‰©çš„æœ€å°replicateæ•° (default: 5)\n",
    "        min_cell_lines: int\n",
    "            æ¯ä¸ªåŒ–åˆç‰©æµ‹è¯•çš„æœ€å°ç»†èƒç³»æ•° (default: 2)\n",
    "        min_obs_per_compound: int\n",
    "            æ¯ä¸ªåŒ–åˆç‰©çš„æœ€å°è§‚æµ‹æ•° (default: 10)\n",
    "        min_compounds_per_cell: int\n",
    "            æ¯ä¸ªç»†èƒç³»çš„æœ€å°åŒ–åˆç‰©æ•° (default: 200)\n",
    "        min_replicate_similarity: float\n",
    "            replicateä¹‹é—´çš„æœ€å°ç›¸ä¼¼åº¦ (default: 0.12)\n",
    "        \"\"\"\n",
    "        if self.signatures is None:\n",
    "            raise ValueError(\"Please load signatures first using load_level4_signatures()\")\n",
    "        \n",
    "        matrix = self.signatures['matrix']\n",
    "        row_meta = self.signatures['row_meta']\n",
    "        col_meta = self.signatures['col_meta']\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"ğŸ” APPLYING QUALITY CONTROL FILTERS\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"Initial samples: {len(row_meta)}\")\n",
    "        print(f\"\\nFilter criteria:\")\n",
    "        print(f\"  â€¢ Replicates per compound â‰¥ {min_replicates}\")\n",
    "        print(f\"  â€¢ Cell lines per compound â‰¥ {min_cell_lines}\")\n",
    "        print(f\"  â€¢ Observations per compound â‰¥ {min_obs_per_compound}\")\n",
    "        print(f\"  â€¢ Compounds per cell line â‰¥ {min_compounds_per_cell}\")\n",
    "        print(f\"  â€¢ Replicate similarity â‰¥ {min_replicate_similarity}\")\n",
    "        \n",
    "        # åˆ›å»ºå·¥ä½œå‰¯æœ¬\n",
    "        working_matrix = matrix.copy()\n",
    "        working_meta = row_meta.copy()\n",
    "        \n",
    "        if ('pert_id' not in working_meta.columns) or ('cell_id' not in working_meta.columns):\n",
    "            print(\"\\n[INFO] 'pert_id' or 'cell_id' not found in row_meta, parsing from 'id' column...\")\n",
    "            if 'id' not in working_meta.columns:\n",
    "                raise KeyError(\n",
    "                    \"row_meta ä¸­æ—¢æ²¡æœ‰ 'pert_id'/'cell_id'ï¼Œä¹Ÿæ²¡æœ‰ 'id' åˆ—ï¼Œæ— æ³•è§£æã€‚\"\n",
    "                )\n",
    "            \n",
    "            # æŒ‰ä¸‹åˆ’çº¿æ‹†åˆ†\n",
    "            parts = working_meta['id'].str.split('_', expand=True)\n",
    "            if parts.shape[1] < 2:\n",
    "                raise ValueError(\n",
    "                    f\"'id' åˆ—æ— æ³•æŒ‰ '_' æ‹†åˆ†ä¸ºè‡³å°‘ä¸¤æ®µï¼Œæ— æ³•æ¨æ–­ pert_id / cell_idï¼Œç¤ºä¾‹: {working_meta['id'].iloc[0]}\"\n",
    "                )\n",
    "            \n",
    "            # çº¦å®šï¼šç¬¬ 0 æ®µæ˜¯ pert_idï¼Œç¬¬ 1 æ®µæ˜¯ cell_id\n",
    "            working_meta['pert_id'] = parts[0]\n",
    "            working_meta['cell_id'] = parts[1]\n",
    "            \n",
    "            print(\"       Added columns:\")\n",
    "            print(\"         â€¢ pert_id <- id.split('_')[0]\")\n",
    "            print(\"         â€¢ cell_id <- id.split('_')[1]\")\n",
    "            print(\"       ç¤ºä¾‹ï¼š\", working_meta[['id', 'pert_id', 'cell_id']].head(3))\n",
    "        \n",
    "        # ========== Filter 1: Cell line filter ==========\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"FILTER 1: Compounds per cell line\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        cell_compound_counts = working_meta.groupby('cell_id')['pert_id'].nunique()\n",
    "        valid_cells = cell_compound_counts[cell_compound_counts >= min_compounds_per_cell].index\n",
    "        \n",
    "        print(f\"  Cell lines with â‰¥{min_compounds_per_cell} compounds: {len(valid_cells)}/{len(cell_compound_counts)}\")\n",
    "        \n",
    "        cell_mask = working_meta['cell_id'].isin(valid_cells)\n",
    "        working_matrix = working_matrix[cell_mask]\n",
    "        working_meta = working_meta[cell_mask].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"  Remaining samples: {len(working_meta)}\")\n",
    "        print(f\"  Remaining compounds: {working_meta['pert_id'].nunique()}\")\n",
    "        \n",
    "        # ========== Filter 2: Replicate count ==========\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"FILTER 2: Replicates per compound\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        replicate_counts = working_meta.groupby('pert_id').size()\n",
    "        valid_perts_rep = replicate_counts[replicate_counts >= min_replicates].index\n",
    "        \n",
    "        print(f\"  Compounds with â‰¥{min_replicates} replicates: {len(valid_perts_rep)}/{len(replicate_counts)}\")\n",
    "        \n",
    "        rep_mask = working_meta['pert_id'].isin(valid_perts_rep)\n",
    "        working_matrix = working_matrix[rep_mask]\n",
    "        working_meta = working_meta[rep_mask].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"  Remaining samples: {len(working_meta)}\")\n",
    "        print(f\"  Remaining compounds: {working_meta['pert_id'].nunique()}\")\n",
    "        \n",
    "        # ========== Filter 3: Cell line diversity ==========\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"FILTER 3: Cell line diversity per compound\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        cell_line_counts = working_meta.groupby('pert_id')['cell_id'].nunique()\n",
    "        valid_perts_cell = cell_line_counts[cell_line_counts >= min_cell_lines].index\n",
    "        \n",
    "        print(f\"  Compounds in â‰¥{min_cell_lines} cell lines: {len(valid_perts_cell)}/{len(cell_line_counts)}\")\n",
    "        \n",
    "        cell_div_mask = working_meta['pert_id'].isin(valid_perts_cell)\n",
    "        working_matrix = working_matrix[cell_div_mask]\n",
    "        working_meta = working_meta[cell_div_mask].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"  Remaining samples: {len(working_meta)}\")\n",
    "        print(f\"  Remaining compounds: {working_meta['pert_id'].nunique()}\")\n",
    "        \n",
    "        # ========== Filter 4: Observations per compound ==========\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"FILTER 4: Total observations per compound\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        obs_counts = working_meta.groupby('pert_id').size()\n",
    "        valid_perts_obs = obs_counts[obs_counts >= min_obs_per_compound].index\n",
    "        \n",
    "        print(f\"  Compounds with â‰¥{min_obs_per_compound} observations: {len(valid_perts_obs)}/{len(obs_counts)}\")\n",
    "        \n",
    "        obs_mask = working_meta['pert_id'].isin(valid_perts_obs)\n",
    "        working_matrix = working_matrix[obs_mask]\n",
    "        working_meta = working_meta[obs_mask].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"  Remaining samples: {len(working_meta)}\")\n",
    "        print(f\"  Remaining compounds: {working_meta['pert_id'].nunique()}\")\n",
    "        \n",
    "        # ========== Filter 5: Replicate similarity ==========\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"FILTER 5: Replicate similarity\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        similarities = self.calculate_replicate_similarity(\n",
    "            working_matrix, \n",
    "            working_meta['pert_id']\n",
    "        )\n",
    "        \n",
    "        valid_perts_sim = similarities[similarities >= min_replicate_similarity].index\n",
    "        \n",
    "        print(f\"  Compounds with similarity â‰¥{min_replicate_similarity}: {len(valid_perts_sim)}/{len(similarities)}\")\n",
    "        print(f\"  Mean similarity: {similarities.mean():.4f}\")\n",
    "        print(f\"  Median similarity: {similarities.median():.4f}\")\n",
    "        \n",
    "        sim_mask = working_meta['pert_id'].isin(valid_perts_sim)\n",
    "        working_matrix = working_matrix[sim_mask]\n",
    "        working_meta = working_meta[sim_mask].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"  Remaining samples: {len(working_meta)}\")\n",
    "        print(f\"  Remaining compounds: {working_meta['pert_id'].nunique()}\")\n",
    "        \n",
    "        # ========== Create final dataset ==========\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"âœ… FINAL DATASET\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # åˆ›å»ºåŒ–åˆç‰©æ ‡ç­¾ç¼–ç \n",
    "        unique_perts = sorted(working_meta['pert_id'].unique())\n",
    "        pert_to_idx = {pert: idx for idx, pert in enumerate(unique_perts)}\n",
    "        labels = np.array([pert_to_idx[p] for p in working_meta['pert_id']])\n",
    "        \n",
    "        print(f\"  Total samples: {len(working_matrix)}\")\n",
    "        print(f\"  Total compounds: {len(unique_perts)}\")\n",
    "        print(f\"  Gene features: {working_matrix.shape[1]}\")\n",
    "        print(f\"  Samples per compound (mean): {len(working_matrix) / len(unique_perts):.1f}\")\n",
    "        \n",
    "        # ç»Ÿè®¡ç»†èƒç³»åˆ†å¸ƒ\n",
    "        cell_dist = working_meta.groupby('cell_id').size()\n",
    "        print(f\"\\n  Cell line distribution:\")\n",
    "        print(f\"    Unique cell lines: {len(cell_dist)}\")\n",
    "        print(f\"    Samples per cell line (mean): {cell_dist.mean():.1f}\")\n",
    "        print(f\"    Samples per cell line (median): {cell_dist.median():.0f}\")\n",
    "        \n",
    "        training_data = {\n",
    "            'X': working_matrix,\n",
    "            'y': labels,\n",
    "            'sample_meta': working_meta,\n",
    "            'gene_names': col_meta['id'].values,\n",
    "            'compound_names': unique_perts,\n",
    "            'pert_to_idx': pert_to_idx,\n",
    "            'replicate_similarities': similarities[valid_perts_sim]\n",
    "        }\n",
    "        \n",
    "        return training_data\n",
    "    \n",
    "    def create_3fold_splits(self, training_data, random_state=42):\n",
    "        \"\"\"åˆ›å»º3æŠ˜äº¤å‰éªŒè¯åˆ’åˆ†\"\"\"\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        sample_meta = training_data['sample_meta']\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"ğŸ² Creating 3-fold cross-validation splits\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        folds = np.zeros(len(sample_meta), dtype=int)\n",
    "        \n",
    "        for pert_id in sample_meta['pert_id'].unique():\n",
    "            pert_mask = sample_meta['pert_id'] == pert_id\n",
    "            pert_indices = np.where(pert_mask)[0]\n",
    "            \n",
    "            np.random.shuffle(pert_indices)\n",
    "            n_samples = len(pert_indices)\n",
    "            \n",
    "            fold_sizes = [n_samples // 3] * 3\n",
    "            for i in range(n_samples % 3):\n",
    "                fold_sizes[i] += 1\n",
    "            \n",
    "            start_idx = 0\n",
    "            for fold_id, size in enumerate(fold_sizes):\n",
    "                end_idx = start_idx + size\n",
    "                folds[pert_indices[start_idx:end_idx]] = fold_id\n",
    "                start_idx = end_idx\n",
    "        \n",
    "        training_data['folds'] = folds\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Fold statistics:\")\n",
    "        for fold_id in range(3):\n",
    "            fold_mask = folds == fold_id\n",
    "            n_samples = fold_mask.sum()\n",
    "            n_compounds = sample_meta[fold_mask]['pert_id'].nunique()\n",
    "            print(f\"   Fold {fold_id}: {n_samples:,} samples, {n_compounds:,} compounds\")\n",
    "        \n",
    "        return training_data\n",
    "\n",
    "\n",
    "# ========== ä¸»ç¨‹åº ==========\n",
    "\n",
    "def main():\n",
    "    \"\"\"ä¸»ç¨‹åº\"\"\"\n",
    "    loader = LINCSDataLoader(\"E:/ç§‘ç ”/Models/drugreflector/datasets\")\n",
    "    \n",
    "    try:\n",
    "        # Step 1: åŠ è½½åŸºå› å’Œç»†èƒä¿¡æ¯\n",
    "        print(\"=\" * 70)\n",
    "        print(\"STEP 1: Loading gene and cell information\")\n",
    "        print(\"=\" * 70)\n",
    "        gene_info = loader.load_gene_info('GSE92742')\n",
    "        cell_info = loader.load_cell_info('GSE92742')\n",
    "        \n",
    "        # Step 2: åŠ è½½Level 4 signatures\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"STEP 2: Loading Level 4 signatures\")\n",
    "        print(\"=\" * 70)\n",
    "        matrix, row_meta, col_meta = loader.load_level4_signatures('GSE92742')\n",
    "        \n",
    "        # Step 3: å‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆåº”ç”¨æ‰€æœ‰ç­›é€‰æ¡ä»¶ï¼‰\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"STEP 3: Preparing training data with quality filters\")\n",
    "        print(\"=\" * 70)\n",
    "        training_data = loader.prepare_training_data(\n",
    "            min_replicates=5,                    # æ¯ä¸ªåŒ–åˆç‰©è‡³å°‘5ä¸ªreplicate\n",
    "            min_cell_lines=2,                    # æ¯ä¸ªåŒ–åˆç‰©è‡³å°‘åœ¨2ä¸ªç»†èƒç³»æµ‹è¯•\n",
    "            min_obs_per_compound=10,              # æ¯ä¸ªåŒ–åˆç‰©è‡³å°‘10ä¸ªè§‚æµ‹\n",
    "            min_compounds_per_cell=200,            # æ¯ä¸ªç»†èƒç³»è‡³å°‘æœ‰200ä¸ªåŒ–åˆç‰©\n",
    "            min_replicate_similarity=0.12        # replicateç›¸ä¼¼åº¦è‡³å°‘0.12\n",
    "        )\n",
    "        \n",
    "        # Step 4: åˆ›å»º3æŠ˜åˆ’åˆ†\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"STEP 4: Creating 3-fold splits\")\n",
    "        print(\"=\" * 70)\n",
    "        training_data = loader.create_3fold_splits(training_data)\n",
    "        \n",
    "        # Step 5: ä¿å­˜å¤„ç†åçš„æ•°æ®\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"STEP 5: Saving processed data\")\n",
    "        print(\"=\" * 70)\n",
    "        output_dir = Path(\"E:/ç§‘ç ”/Models/drugreflector/processed_data\")\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        output_file = output_dir / \"training_data_filtered.pkl\"\n",
    "        print(f\"ğŸ’¾ Saving to: {output_file}\")\n",
    "        \n",
    "        with open(output_file, 'wb') as f:\n",
    "            pickle.dump(training_data, f)\n",
    "        \n",
    "        print(f\"âœ“ Saved successfully!\")\n",
    "        \n",
    "        # æ‰“å°æœ€ç»ˆæ‘˜è¦\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"âœ… DATA PREPARATION COMPLETE!\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"ğŸ“ Output file: {output_file}\")\n",
    "        print(f\"\\nğŸ“Š Final dataset summary:\")\n",
    "        print(f\"   â€¢ Total samples: {len(training_data['X']):,}\")\n",
    "        print(f\"   â€¢ Total compounds: {len(training_data['compound_names']):,}\")\n",
    "        print(f\"   â€¢ Gene features: {training_data['X'].shape[1]}\")\n",
    "        print(f\"   â€¢ Data shape: {training_data['X'].shape}\")\n",
    "        print(f\"   â€¢ Average samples per compound: {len(training_data['X']) / len(training_data['compound_names']):.1f}\")\n",
    "        \n",
    "        # è§£å‹æ–‡ä»¶ä¿¡æ¯\n",
    "        if loader.decompressed_files:\n",
    "            print(f\"\\nğŸ“¦ Decompressed files saved at:\")\n",
    "            for f in loader.decompressed_files:\n",
    "                print(f\"   â€¢ {f}\")\n",
    "        \n",
    "        return training_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n\" + \"=\" * 70)\n",
    "        print(\"âŒ ERROR DURING DATA PREPARATION\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"   Type: {type(e).__name__}\")\n",
    "        print(f\"   Message: {e}\")\n",
    "        print(f\"\\nğŸ“‹ Full traceback:\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_from_cache():\n",
    "    \"\"\"ç›´æ¥ä»è§£å‹æ–‡ä»¶åŠ è½½æ•°æ®ï¼ˆè·³è¿‡è§£å‹æ­¥éª¤ï¼‰\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"ğŸš€ LOADING FROM CACHED FILES\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    loader = LINCSDataLoader(\"E:/ç§‘ç ”/Models/drugreflector/datasets\")\n",
    "    \n",
    "    # æ£€æŸ¥ç¼“å­˜æ–‡ä»¶\n",
    "    decompressed_dir = Path(\"E:/ç§‘ç ”/Models/drugreflector/datasets/_decompressed\")\n",
    "    cached_file = decompressed_dir / \"GSE92742_Broad_LINCS_Level4_ZSPCINF_mlr12k_n1319138x12328.gctx\"\n",
    "    \n",
    "    if not cached_file.exists():\n",
    "        print(f\"âŒ Cached file not found: {cached_file}\")\n",
    "        print(f\"   Please run main() first to decompress the data.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"âœ“ Found cached file: {cached_file.name}\")\n",
    "    \n",
    "    try:\n",
    "        # åŠ è½½åŸºå› ä¿¡æ¯\n",
    "        print(\"\\nLoading gene and cell information...\")\n",
    "        loader.load_gene_info('GSE92742')\n",
    "        loader.load_cell_info('GSE92742')\n",
    "        \n",
    "        # ç›´æ¥è¯»å–è§£å‹æ–‡ä»¶\n",
    "        print(f\"\\nReading from cached GCTX file...\")\n",
    "        matrix, sample_meta, gene_meta = loader.read_gctx(str(cached_file))\n",
    "        \n",
    "        # è¿‡æ»¤landmark genes\n",
    "        print(f\"\\nFiltering to landmark genes...\")\n",
    "        landmark_ids = set(loader.gene_info['pr_gene_id'].astype(str).values)\n",
    "        gene_mask = gene_meta['id'].isin(landmark_ids)\n",
    "        \n",
    "        matrix = matrix[:, gene_mask]\n",
    "        gene_meta = gene_meta[gene_mask].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"âœ“ Filtered matrix shape: {matrix.shape}\")\n",
    "        \n",
    "        loader.signatures = {\n",
    "            'matrix': matrix,\n",
    "            'row_meta': sample_meta,\n",
    "            'col_meta': gene_meta\n",
    "        }\n",
    "        \n",
    "        # ========= æ–°å¢éƒ¨åˆ†ï¼šåœ¨æ‰€æœ‰ QC filter ä¹‹å‰ï¼ŒæŠŠå®Œæ•´ signatures ç¼“å­˜åˆ°ç£ç›˜ =========\n",
    "        cache_dir = Path(\"E:/ç§‘ç ”/Models/drugreflector/datasets/_cache\")\n",
    "        cache_dir.mkdir(exist_ok=True)\n",
    "        cache_file = cache_dir / \"GSE92742_level4_landmark_signatures.pkl\"\n",
    "        with open(cache_file, \"wb\") as f:\n",
    "            pickle.dump(loader.signatures, f)\n",
    "        print(f\"ğŸ’¾ Saved raw (unfiltered) signatures to: {cache_file}\")\n",
    "        # ======================================================================\n",
    "\n",
    "        # åº”ç”¨ç­›é€‰\n",
    "        print(\"\\nApplying quality filters...\")\n",
    "        training_data = loader.prepare_training_data(\n",
    "            min_replicates=5,\n",
    "            min_cell_lines=2,\n",
    "            min_obs_per_compound=10,\n",
    "            min_compounds_per_cell=200,\n",
    "            min_replicate_similarity=0.12\n",
    "        )\n",
    "        \n",
    "        # åˆ›å»ºfoldåˆ’åˆ†\n",
    "        training_data = loader.create_3fold_splits(training_data)\n",
    "        \n",
    "        print(\"\\nâœ… Data loaded from cache successfully!\")\n",
    "        return training_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Error loading from cache: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ç¬¬ä¸€æ¬¡è¿è¡Œï¼šè§£å‹å¹¶å¤„ç†æ•°æ®\n",
    "    # training_data = main()\n",
    "    \n",
    "    # åç»­è¿è¡Œï¼šç›´æ¥ä»ç¼“å­˜åŠ è½½\n",
    "    training_data = load_from_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8769fd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "LINCSæ•°æ®åŠ è½½å’Œé¢„å¤„ç†è„šæœ¬\n",
    "å°†GCTXæ ¼å¼è½¬æ¢ä¸ºè®­ç»ƒæ‰€éœ€çš„æ ¼å¼\n",
    "å®Œå…¨ä¿®å¤ç‰ˆæœ¬ï¼šæ­£ç¡®å¤„ç†GCTXæ ¼å¼çš„åç›´è§‰å…ƒæ•°æ®æ˜ å°„\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import gzip\n",
    "import shutil\n",
    "import glob\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "class LINCSDataLoader:\n",
    "    \"\"\"åŠ è½½å’Œé¢„å¤„ç†LINCS L1000æ•°æ®\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.gene_info = None\n",
    "        self.cell_info = None\n",
    "        self.signatures = None\n",
    "        self.decompressed_files = []\n",
    "        \n",
    "    def load_gene_info(self, dataset='GSE92742'):\n",
    "        \"\"\"åŠ è½½åŸºå› ä¿¡æ¯ï¼Œè·å–978ä¸ªlandmark genes\"\"\"\n",
    "        gene_file = self.data_dir / f\"{dataset}_Broad_LINCS_gene_info.txt.gz\"\n",
    "        \n",
    "        gene_info = pd.read_csv(gene_file, sep='\\t', compression='gzip')\n",
    "        \n",
    "        # ç­›é€‰landmark genes (pr_is_lm == 1)\n",
    "        landmark_genes = gene_info[gene_info['pr_is_lm'] == 1].copy()\n",
    "        \n",
    "        print(f\"Total genes: {len(gene_info)}\")\n",
    "        print(f\"Landmark genes: {len(landmark_genes)}\")\n",
    "        \n",
    "        self.gene_info = landmark_genes\n",
    "        return landmark_genes\n",
    "    \n",
    "    def load_cell_info(self, dataset='GSE92742'):\n",
    "        \"\"\"åŠ è½½ç»†èƒç³»ä¿¡æ¯\"\"\"\n",
    "        cell_file = self.data_dir / f\"{dataset}_Broad_LINCS_cell_info.txt.gz\"\n",
    "        \n",
    "        cell_info = pd.read_csv(cell_file, sep='\\t', compression='gzip')\n",
    "        \n",
    "        print(f\"Total cell lines: {len(cell_info)}\")\n",
    "        print(f\"Unique cell IDs: {cell_info['cell_id'].nunique()}\")\n",
    "        \n",
    "        self.cell_info = cell_info\n",
    "        return cell_info\n",
    "    \n",
    "    def find_decompressed_file(self, original_gz_file):\n",
    "        \"\"\"æŸ¥æ‰¾å·²è§£å‹çš„æ–‡ä»¶\"\"\"\n",
    "        gz_path = Path(original_gz_file)\n",
    "        decompressed_dir = self.data_dir / \"_decompressed\"\n",
    "        decompressed_file = decompressed_dir / gz_path.stem  # å»æ‰.gzåç¼€\n",
    "        \n",
    "        if decompressed_file.exists():\n",
    "            return str(decompressed_file)\n",
    "        return None\n",
    "    \n",
    "    def decompress_gzip_file(self, gzip_file):\n",
    "        \"\"\"\n",
    "        è§£å‹gzipæ–‡ä»¶åˆ°_decompressedæ–‡ä»¶å¤¹\n",
    "        è¿”å›è§£å‹åçš„æ–‡ä»¶è·¯å¾„\n",
    "        \"\"\"\n",
    "        import time\n",
    "        \n",
    "        gzip_file = str(gzip_file)\n",
    "        \n",
    "        # åˆ›å»ºè§£å‹æ–‡ä»¶å¤¹\n",
    "        decompressed_dir = Path(self.data_dir) / \"_decompressed\"\n",
    "        decompressed_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # ä½¿ç”¨åŸæ–‡ä»¶åï¼ˆå»æ‰.gzåç¼€ï¼‰\n",
    "        original_name = Path(gzip_file).stem\n",
    "        output_path = decompressed_dir / original_name\n",
    "        \n",
    "        # å¦‚æœæ–‡ä»¶å·²ç»å­˜åœ¨ï¼Œç›´æ¥ä½¿ç”¨\n",
    "        if output_path.exists():\n",
    "            print(f\"âœ“ Found existing decompressed file: {output_path.name}\")\n",
    "            self.decompressed_files.append(output_path)\n",
    "            return str(output_path)\n",
    "        \n",
    "        print(f\"ğŸ’¾ Decompressing to: {decompressed_dir}\")\n",
    "        \n",
    "        try:\n",
    "            print(f\"ğŸ“¦ Decompressing {Path(gzip_file).name}...\")\n",
    "            source_size = Path(gzip_file).stat().st_size / (1024**3)\n",
    "            print(f\"   Source size: ~{source_size:.1f} GB\")\n",
    "            \n",
    "            # ä½¿ç”¨8MBç¼“å†²åŒº\n",
    "            with gzip.open(gzip_file, 'rb') as f_in:\n",
    "                with open(output_path, 'wb') as f_out:\n",
    "                    shutil.copyfileobj(f_in, f_out, length=8*1024*1024)\n",
    "            \n",
    "            output_size = output_path.stat().st_size / (1024**3)\n",
    "            print(f\"âœ“ Decompressed successfully!\")\n",
    "            print(f\"âœ“ Output file: {output_path.name}\")\n",
    "            print(f\"âœ“ Output size: ~{output_size:.1f} GB\")\n",
    "            \n",
    "            self.decompressed_files.append(output_path)\n",
    "            return str(output_path)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"\\nâŒ Decompression failed: {e}\")\n",
    "            # æ¸…ç†å¤±è´¥çš„æ–‡ä»¶\n",
    "            if output_path.exists():\n",
    "                try:\n",
    "                    output_path.unlink()\n",
    "                except:\n",
    "                    pass\n",
    "            raise RuntimeError(f\"Failed to decompress {gzip_file}: {e}\")\n",
    "    \n",
    "    def read_gctx(self, gctx_file):\n",
    "        \"\"\"\n",
    "        è¯»å–GCTXæ–‡ä»¶ (HDF5æ ¼å¼)\n",
    "        \n",
    "        âš ï¸ CRITICAL: GCTXæ ¼å¼çš„åç›´è§‰è®¾è®¡\n",
    "        - æ–‡ä»¶å nXXXXxYYYY.gctx è¡¨ç¤º XXXXæ ·æœ¬ Ã— YYYYåŸºå› \n",
    "        - çŸ©é˜µå­˜å‚¨ä¸º (XXXX, YYYY) å½¢çŠ¶\n",
    "        - ä½†å…ƒæ•°æ®å‘½ååç›´è§‰ï¼š\n",
    "          * /0/META/ROW å­˜å‚¨çš„æ˜¯åŸºå› ä¿¡æ¯ï¼ˆå¯¹åº”çŸ©é˜µçš„åˆ—ï¼‰\n",
    "          * /0/META/COL å­˜å‚¨çš„æ˜¯æ ·æœ¬ä¿¡æ¯ï¼ˆå¯¹åº”çŸ©é˜µçš„è¡Œï¼‰\n",
    "        \"\"\"\n",
    "        gctx_file = str(gctx_file)\n",
    "        print(f\"\\nğŸ“– Reading GCTX file: {Path(gctx_file).name}\")\n",
    "        \n",
    "        # æ£€æŸ¥æ˜¯å¦æ˜¯gzipæ–‡ä»¶\n",
    "        if gctx_file.endswith('.gz'):\n",
    "            print(\"âš ï¸  Detected gzip compressed file\")\n",
    "            gctx_file = self.decompress_gzip_file(gctx_file)\n",
    "        \n",
    "        with h5py.File(gctx_file, 'r') as f:\n",
    "            # è¯»å–æ•°æ®çŸ©é˜µ\n",
    "            print(f\"ğŸ“Š Loading matrix from HDF5...\")\n",
    "            matrix = f['/0/DATA/0/matrix'][:]\n",
    "            print(f\"âœ“ Matrix shape: {matrix.shape}\")\n",
    "            \n",
    "            # ğŸ”§ å…³é”®ä¿®å¤ï¼šäº¤æ¢ ROW å’Œ COL çš„ç†è§£\n",
    "            # ROW åœ¨ GCTX ä¸­å®é™…å¯¹åº”åŸºå› ï¼ˆçŸ©é˜µçš„åˆ—ï¼‰\n",
    "            # COL åœ¨ GCTX ä¸­å®é™…å¯¹åº”æ ·æœ¬ï¼ˆçŸ©é˜µçš„è¡Œï¼‰\n",
    "            \n",
    "            print(f\"ğŸ“‹ Loading metadata...\")\n",
    "            \n",
    "            # è¯»å–åŸºå› å…ƒæ•°æ®ï¼ˆæ¥è‡ª ROWï¼‰\n",
    "            gene_meta = {}\n",
    "            for key in f['/0/META/ROW'].keys():\n",
    "                data = f[f'/0/META/ROW/{key}'][:]\n",
    "                if data.dtype.kind in ['S', 'O']:\n",
    "                    gene_meta[key] = np.char.decode(data.astype('S'), 'utf-8')\n",
    "                else:\n",
    "                    gene_meta[key] = data.astype(str)\n",
    "            \n",
    "            # è¯»å–æ ·æœ¬å…ƒæ•°æ®ï¼ˆæ¥è‡ª COLï¼‰\n",
    "            sample_meta = {}\n",
    "            for key in f['/0/META/COL'].keys():\n",
    "                data = f[f'/0/META/COL/{key}'][:]\n",
    "                if data.dtype.kind in ['S', 'O']:\n",
    "                    sample_meta[key] = np.char.decode(data.astype('S'), 'utf-8')\n",
    "                else:\n",
    "                    sample_meta[key] = data.astype(str)\n",
    "        \n",
    "        # è½¬æ¢ä¸ºDataFrame\n",
    "        sample_df = pd.DataFrame(sample_meta)\n",
    "        gene_df = pd.DataFrame(gene_meta)\n",
    "        \n",
    "        print(f\"\\nâœ“ Data loaded successfully:\")\n",
    "        print(f\"  Matrix: {matrix.shape} (samples Ã— genes)\")\n",
    "        print(f\"  Samples: {len(sample_df)}\")\n",
    "        print(f\"  Genes: {len(gene_df)}\")\n",
    "        \n",
    "        # éªŒè¯ç»´åº¦ä¸€è‡´æ€§\n",
    "        assert matrix.shape[0] == len(sample_df), \\\n",
    "            f\"Matrix rows ({matrix.shape[0]}) != sample metadata ({len(sample_df)})\"\n",
    "        assert matrix.shape[1] == len(gene_df), \\\n",
    "            f\"Matrix cols ({matrix.shape[1]}) != gene metadata ({len(gene_df)})\"\n",
    "        \n",
    "        return matrix, sample_df, gene_df\n",
    "    \n",
    "    def load_level4_signatures(self, dataset='GSE92742'):\n",
    "        \"\"\"åŠ è½½Level 4æ•°æ® (Z-score normalized)\"\"\"\n",
    "        level4_pattern = self.data_dir / f\"{dataset}_Broad_LINCS_Level4_ZSPCINF_mlr12k_n*x12328.gctx.gz\"\n",
    "        \n",
    "        print(f\"ğŸ” Searching for Level 4 file...\")\n",
    "        print(f\"   Pattern: {level4_pattern.name}\")\n",
    "        files = glob.glob(str(level4_pattern))\n",
    "        \n",
    "        if not files:\n",
    "            raise FileNotFoundError(\n",
    "                f\"âŒ No Level 4 file found matching {level4_pattern}\"\n",
    "            )\n",
    "        \n",
    "        level4_file = files[0]\n",
    "        print(f\"âœ“ Found: {Path(level4_file).name}\")\n",
    "        \n",
    "        # è¯»å–GCTX\n",
    "        matrix, sample_meta, gene_meta = self.read_gctx(level4_file)\n",
    "        \n",
    "        # åªä¿ç•™landmark genes\n",
    "        print(f\"\\nğŸ”¬ Filtering to landmark genes...\")\n",
    "        if self.gene_info is None:\n",
    "            self.load_gene_info(dataset)\n",
    "        \n",
    "        landmark_ids = set(self.gene_info['pr_gene_id'].astype(str).values)\n",
    "        print(f\"   Landmark genes to find: {len(landmark_ids)}\")\n",
    "        print(f\"   Total genes in data: {len(gene_meta)}\")\n",
    "        \n",
    "        # åˆ›å»ºåŸºå› mask\n",
    "        gene_mask = gene_meta['id'].isin(landmark_ids)\n",
    "        n_matched = gene_mask.sum()\n",
    "        print(f\"   âœ“ Matched: {n_matched} genes\")\n",
    "        \n",
    "        if n_matched == 0:\n",
    "            raise ValueError(\"No landmark genes matched!\")\n",
    "        \n",
    "        # è¿‡æ»¤çŸ©é˜µå’Œå…ƒæ•°æ®\n",
    "        print(f\"\\nğŸ¯ Applying filter...\")\n",
    "        matrix = matrix[:, gene_mask]\n",
    "        gene_meta = gene_meta[gene_mask].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"   âœ“ Final matrix shape: {matrix.shape}\")\n",
    "        \n",
    "        self.signatures = {\n",
    "            'matrix': matrix,\n",
    "            'row_meta': sample_meta,\n",
    "            'col_meta': gene_meta\n",
    "        }\n",
    "        \n",
    "        return matrix, sample_meta, gene_meta\n",
    "    \n",
    "    def calculate_replicate_similarity(self, matrix, pert_ids):\n",
    "        \"\"\"\n",
    "        è®¡ç®—æ¯ä¸ªåŒ–åˆç‰©çš„replicateç›¸ä¼¼åº¦ï¼ˆå¹³å‡Pearsonç›¸å…³ç³»æ•°ï¼‰\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        matrix: np.ndarray\n",
    "            è¡¨è¾¾çŸ©é˜µ (n_samples, n_genes)\n",
    "        pert_ids: pd.Series\n",
    "            åŒ–åˆç‰©ID\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.Series: æ¯ä¸ªåŒ–åˆç‰©çš„å¹³å‡replicateç›¸ä¼¼åº¦\n",
    "        \"\"\"\n",
    "        print(f\"\\nğŸ“Š Calculating replicate similarities...\")\n",
    "        similarities = {}\n",
    "        \n",
    "        unique_perts = pert_ids.unique()\n",
    "        print(f\"   Processing {len(unique_perts)} compounds...\")\n",
    "        \n",
    "        for i, pert_id in enumerate(unique_perts):\n",
    "            if (i + 1) % 1000 == 0:\n",
    "                print(f\"   Progress: {i + 1}/{len(unique_perts)}\")\n",
    "            \n",
    "            # è·å–è¯¥åŒ–åˆç‰©çš„æ‰€æœ‰replicate\n",
    "            pert_mask = pert_ids == pert_id\n",
    "            pert_data = matrix[pert_mask]\n",
    "            \n",
    "            if len(pert_data) < 2:\n",
    "                similarities[pert_id] = 0.0\n",
    "                continue\n",
    "            \n",
    "            # è®¡ç®—æˆå¯¹ç›¸å…³ç³»æ•°\n",
    "            correlations = []\n",
    "            for j in range(len(pert_data)):\n",
    "                for k in range(j + 1, len(pert_data)):\n",
    "                    corr, _ = pearsonr(pert_data[j], pert_data[k])\n",
    "                    if not np.isnan(corr):\n",
    "                        correlations.append(corr)\n",
    "            \n",
    "            similarities[pert_id] = np.mean(correlations) if correlations else 0.0\n",
    "        \n",
    "        print(f\"   âœ“ Calculated similarities for {len(similarities)} compounds\")\n",
    "        return pd.Series(similarities)\n",
    "    \n",
    "    def prepare_training_data(self, \n",
    "                            min_replicates=5, \n",
    "                            min_cell_lines=2,\n",
    "                            min_obs_per_compound=10,\n",
    "                            min_compounds_per_cell=200,\n",
    "                            min_replicate_similarity=0.12):\n",
    "        \"\"\"\n",
    "        å‡†å¤‡è®­ç»ƒæ•°æ®ï¼Œåº”ç”¨ä¸¥æ ¼çš„è´¨é‡æ§åˆ¶ç­›é€‰\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        min_replicates: int\n",
    "            æ¯ä¸ªåŒ–åˆç‰©çš„æœ€å°replicateæ•° (default: 5)\n",
    "        min_cell_lines: int\n",
    "            æ¯ä¸ªåŒ–åˆç‰©æµ‹è¯•çš„æœ€å°ç»†èƒç³»æ•° (default: 2)\n",
    "        min_obs_per_compound: int\n",
    "            æ¯ä¸ªåŒ–åˆç‰©çš„æœ€å°è§‚æµ‹æ•° (default: 10)\n",
    "        min_compounds_per_cell: int\n",
    "            æ¯ä¸ªç»†èƒç³»çš„æœ€å°åŒ–åˆç‰©æ•° (default: 200)\n",
    "        min_replicate_similarity: float\n",
    "            replicateä¹‹é—´çš„æœ€å°ç›¸ä¼¼åº¦ (default: 0.12)\n",
    "        \"\"\"\n",
    "        if self.signatures is None:\n",
    "            raise ValueError(\"Please load signatures first using load_level4_signatures()\")\n",
    "        \n",
    "        matrix = self.signatures['matrix']\n",
    "        row_meta = self.signatures['row_meta']\n",
    "        col_meta = self.signatures['col_meta']\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"ğŸ” APPLYING QUALITY CONTROL FILTERS\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"Initial samples: {len(row_meta)}\")\n",
    "        print(f\"\\nFilter criteria:\")\n",
    "        print(f\"  â€¢ Replicates per compound â‰¥ {min_replicates}\")\n",
    "        print(f\"  â€¢ Cell lines per compound â‰¥ {min_cell_lines}\")\n",
    "        print(f\"  â€¢ Observations per compound â‰¥ {min_obs_per_compound}\")\n",
    "        print(f\"  â€¢ Compounds per cell line â‰¥ {min_compounds_per_cell}\")\n",
    "        print(f\"  â€¢ Replicate similarity â‰¥ {min_replicate_similarity}\")\n",
    "        \n",
    "        # åˆ›å»ºå·¥ä½œå‰¯æœ¬\n",
    "        working_matrix = matrix.copy()\n",
    "        working_meta = row_meta.copy()\n",
    "        \n",
    "        if ('pert_id' not in working_meta.columns) or ('cell_id' not in working_meta.columns):\n",
    "            print(\"\\n[INFO] 'pert_id' or 'cell_id' not found in row_meta, parsing from 'id' column...\")\n",
    "            if 'id' not in working_meta.columns:\n",
    "                raise KeyError(\n",
    "                    \"row_meta ä¸­æ—¢æ²¡æœ‰ 'pert_id'/'cell_id'ï¼Œä¹Ÿæ²¡æœ‰ 'id' åˆ—ï¼Œæ— æ³•è§£æã€‚\"\n",
    "                )\n",
    "            \n",
    "            # æŒ‰ä¸‹åˆ’çº¿æ‹†åˆ†\n",
    "            parts = working_meta['id'].str.split('_', expand=True)\n",
    "            if parts.shape[1] < 2:\n",
    "                raise ValueError(\n",
    "                    f\"'id' åˆ—æ— æ³•æŒ‰ '_' æ‹†åˆ†ä¸ºè‡³å°‘ä¸¤æ®µï¼Œæ— æ³•æ¨æ–­ pert_id / cell_idï¼Œç¤ºä¾‹: {working_meta['id'].iloc[0]}\"\n",
    "                )\n",
    "            \n",
    "            # çº¦å®šï¼šç¬¬ 0 æ®µæ˜¯ pert_idï¼Œç¬¬ 1 æ®µæ˜¯ cell_id\n",
    "            working_meta['pert_id'] = parts[0]\n",
    "            working_meta['cell_id'] = parts[1]\n",
    "            \n",
    "            print(\"       Added columns:\")\n",
    "            print(\"         â€¢ pert_id <- id.split('_')[0]\")\n",
    "            print(\"         â€¢ cell_id <- id.split('_')[1]\")\n",
    "            print(\"       ç¤ºä¾‹ï¼š\", working_meta[['id', 'pert_id', 'cell_id']].head(3))\n",
    "        \n",
    "        # ========== Filter 1: Cell line filter ==========\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"FILTER 1: Compounds per cell line\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        cell_compound_counts = working_meta.groupby('cell_id')['pert_id'].nunique()\n",
    "        valid_cells = cell_compound_counts[cell_compound_counts >= min_compounds_per_cell].index\n",
    "        \n",
    "        print(f\"  Cell lines with â‰¥{min_compounds_per_cell} compounds: {len(valid_cells)}/{len(cell_compound_counts)}\")\n",
    "        \n",
    "        cell_mask = working_meta['cell_id'].isin(valid_cells)\n",
    "        working_matrix = working_matrix[cell_mask]\n",
    "        working_meta = working_meta[cell_mask].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"  Remaining samples: {len(working_meta)}\")\n",
    "        print(f\"  Remaining compounds: {working_meta['pert_id'].nunique()}\")\n",
    "        \n",
    "        # ========== Filter 2: Replicate count ==========\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"FILTER 2: Replicates per compound\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        replicate_counts = working_meta.groupby('pert_id').size()\n",
    "        valid_perts_rep = replicate_counts[replicate_counts >= min_replicates].index\n",
    "        \n",
    "        print(f\"  Compounds with â‰¥{min_replicates} replicates: {len(valid_perts_rep)}/{len(replicate_counts)}\")\n",
    "        \n",
    "        rep_mask = working_meta['pert_id'].isin(valid_perts_rep)\n",
    "        working_matrix = working_matrix[rep_mask]\n",
    "        working_meta = working_meta[rep_mask].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"  Remaining samples: {len(working_meta)}\")\n",
    "        print(f\"  Remaining compounds: {working_meta['pert_id'].nunique()}\")\n",
    "        \n",
    "        # ========== Filter 3: Cell line diversity ==========\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"FILTER 3: Cell line diversity per compound\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        cell_line_counts = working_meta.groupby('pert_id')['cell_id'].nunique()\n",
    "        valid_perts_cell = cell_line_counts[cell_line_counts >= min_cell_lines].index\n",
    "        \n",
    "        print(f\"  Compounds in â‰¥{min_cell_lines} cell lines: {len(valid_perts_cell)}/{len(cell_line_counts)}\")\n",
    "        \n",
    "        cell_div_mask = working_meta['pert_id'].isin(valid_perts_cell)\n",
    "        working_matrix = working_matrix[cell_div_mask]\n",
    "        working_meta = working_meta[cell_div_mask].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"  Remaining samples: {len(working_meta)}\")\n",
    "        print(f\"  Remaining compounds: {working_meta['pert_id'].nunique()}\")\n",
    "        \n",
    "        # ========== Filter 4: Observations per compound ==========\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"FILTER 4: Total observations per compound\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        obs_counts = working_meta.groupby('pert_id').size()\n",
    "        valid_perts_obs = obs_counts[obs_counts >= min_obs_per_compound].index\n",
    "        \n",
    "        print(f\"  Compounds with â‰¥{min_obs_per_compound} observations: {len(valid_perts_obs)}/{len(obs_counts)}\")\n",
    "        \n",
    "        obs_mask = working_meta['pert_id'].isin(valid_perts_obs)\n",
    "        working_matrix = working_matrix[obs_mask]\n",
    "        working_meta = working_meta[obs_mask].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"  Remaining samples: {len(working_meta)}\")\n",
    "        print(f\"  Remaining compounds: {working_meta['pert_id'].nunique()}\")\n",
    "        \n",
    "        # ========== Filter 5: Replicate similarity ==========\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"FILTER 5: Replicate similarity\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        similarities = self.calculate_replicate_similarity(\n",
    "            working_matrix, \n",
    "            working_meta['pert_id']\n",
    "        )\n",
    "        \n",
    "        valid_perts_sim = similarities[similarities >= min_replicate_similarity].index\n",
    "        \n",
    "        print(f\"  Compounds with similarity â‰¥{min_replicate_similarity}: {len(valid_perts_sim)}/{len(similarities)}\")\n",
    "        print(f\"  Mean similarity: {similarities.mean():.4f}\")\n",
    "        print(f\"  Median similarity: {similarities.median():.4f}\")\n",
    "        \n",
    "        sim_mask = working_meta['pert_id'].isin(valid_perts_sim)\n",
    "        working_matrix = working_matrix[sim_mask]\n",
    "        working_meta = working_meta[sim_mask].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"  Remaining samples: {len(working_meta)}\")\n",
    "        print(f\"  Remaining compounds: {working_meta['pert_id'].nunique()}\")\n",
    "        \n",
    "        # ========== Create final dataset ==========\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"âœ… FINAL DATASET\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # åˆ›å»ºåŒ–åˆç‰©æ ‡ç­¾ç¼–ç \n",
    "        unique_perts = sorted(working_meta['pert_id'].unique())\n",
    "        pert_to_idx = {pert: idx for idx, pert in enumerate(unique_perts)}\n",
    "        labels = np.array([pert_to_idx[p] for p in working_meta['pert_id']])\n",
    "        \n",
    "        print(f\"  Total samples: {len(working_matrix)}\")\n",
    "        print(f\"  Total compounds: {len(unique_perts)}\")\n",
    "        print(f\"  Gene features: {working_matrix.shape[1]}\")\n",
    "        print(f\"  Samples per compound (mean): {len(working_matrix) / len(unique_perts):.1f}\")\n",
    "        \n",
    "        # ç»Ÿè®¡ç»†èƒç³»åˆ†å¸ƒ\n",
    "        cell_dist = working_meta.groupby('cell_id').size()\n",
    "        print(f\"\\n  Cell line distribution:\")\n",
    "        print(f\"    Unique cell lines: {len(cell_dist)}\")\n",
    "        print(f\"    Samples per cell line (mean): {cell_dist.mean():.1f}\")\n",
    "        print(f\"    Samples per cell line (median): {cell_dist.median():.0f}\")\n",
    "        \n",
    "        training_data = {\n",
    "            'X': working_matrix,\n",
    "            'y': labels,\n",
    "            'sample_meta': working_meta,\n",
    "            'gene_names': col_meta['id'].values,\n",
    "            'compound_names': unique_perts,\n",
    "            'pert_to_idx': pert_to_idx,\n",
    "            'replicate_similarities': similarities[valid_perts_sim]\n",
    "        }\n",
    "        \n",
    "        return training_data\n",
    "    \n",
    "    def debug_filter_distributions(self):\n",
    "        \"\"\"\n",
    "        è°ƒè¯•ç”¨ï¼šåœ¨ä¸çœŸæ­£è¿‡æ»¤æ•°æ®çš„å‰æä¸‹ï¼Œç›´æ¥è§‚å¯Ÿå‰ 4 ä¸ªè¿‡æ»¤æ¡ä»¶å¯¹åº”çš„åˆ†å¸ƒï¼Œ\n",
    "        å¹¶ç”»å›¾/å¯¼å‡º CSVï¼š\n",
    "        1) æ¯ä¸ª cell line æ‹¥æœ‰çš„ compound æ•°é‡\n",
    "        2) æ¯ä¸ª compound çš„ replicate æ•°\n",
    "        3) æ¯ä¸ª compound æ¶µç›–çš„ cell line æ•°\n",
    "        4) æ¯ä¸ª compound çš„è§‚æµ‹æ•°ï¼ˆç›®å‰ä¸ replicate æ•°ç›¸åŒï¼‰\n",
    "\n",
    "        æ³¨æ„ï¼šä¸è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆFilter 5ï¼‰ã€‚\n",
    "        \"\"\"\n",
    "        if self.signatures is None:\n",
    "            raise ValueError(\"self.signatures ä¸ºç©ºï¼Œè¯·å…ˆé€šè¿‡ load_level4_signatures æˆ–ç¼“å­˜åŠ è½½ã€‚\")\n",
    "\n",
    "        matrix = self.signatures['matrix']\n",
    "        row_meta = self.signatures['row_meta']\n",
    "        col_meta = self.signatures['col_meta']\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"ğŸ“Š DEBUG FILTER DISTRIBUTIONS (on RAW signatures, no QC applied yet)\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"  Matrix shape: {matrix.shape} (samples Ã— genes)\")\n",
    "        print(f\"  row_meta shape: {row_meta.shape}\")\n",
    "        print(f\"  col_meta shape: {col_meta.shape}\")\n",
    "\n",
    "        # å¤åˆ¶ä¸€ä»½ï¼Œé¿å…æ±¡æŸ“åŸå§‹å…ƒæ•°æ®\n",
    "        working_matrix = matrix.copy()\n",
    "        working_meta = row_meta.copy()\n",
    "\n",
    "        # ---------- è§£æ pert_id / cell_idï¼ˆä¸ prepare_training_data ä¿æŒä¸€è‡´ï¼‰ ----------\n",
    "        if ('pert_id' not in working_meta.columns) or ('cell_id' not in working_meta.columns):\n",
    "            print(\"\\n[INFO] 'pert_id' or 'cell_id' not found in row_meta, parsing from 'id' column...\")\n",
    "            if 'id' not in working_meta.columns:\n",
    "                raise KeyError(\"row_meta ä¸­æ—¢æ²¡æœ‰ 'pert_id'/'cell_id'ï¼Œä¹Ÿæ²¡æœ‰ 'id' åˆ—ï¼Œæ— æ³•è§£æã€‚\")\n",
    "\n",
    "            parts = working_meta['id'].str.split('_', expand=True)\n",
    "            if parts.shape[1] < 2:\n",
    "                raise ValueError(\n",
    "                    f\"'id' åˆ—æ— æ³•æŒ‰ '_' æ‹†åˆ†ä¸ºè‡³å°‘ä¸¤æ®µï¼Œæ— æ³•æ¨æ–­ pert_id / cell_idï¼Œç¤ºä¾‹: {working_meta['id'].iloc[0]}\"\n",
    "                )\n",
    "\n",
    "            working_meta['pert_id'] = parts[0]\n",
    "            working_meta['cell_id'] = parts[1]\n",
    "\n",
    "            print(\"       Added columns:\")\n",
    "            print(\"         â€¢ pert_id <- id.split('_')[0]\")\n",
    "            print(\"         â€¢ cell_id <- id.split('_')[1]\")\n",
    "            print(\"       ç¤ºä¾‹ï¼š\")\n",
    "            print(working_meta[['id', 'pert_id', 'cell_id']].head(5))\n",
    "\n",
    "        # ---------- åˆ›å»º debug è¾“å‡ºç›®å½• ----------\n",
    "        debug_dir = self.data_dir / \"_debug_filters\"\n",
    "        debug_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        # ========== Filter 1: Compounds per cell line ==========\n",
    "        print(\"\\n\" + \"-\" * 70)\n",
    "        print(\"ğŸ” [Filter 1] Compounds per cell line (æ¯ä¸ª cell æ‹¥æœ‰çš„ compound æ•°é‡)\")\n",
    "        print(\"-\" * 70)\n",
    "        cell_compound_counts = working_meta.groupby('cell_id')['pert_id'].nunique()\n",
    "        print(\"  åŸºæœ¬ç»Ÿè®¡ï¼š\")\n",
    "        print(cell_compound_counts.describe(percentiles=[0.5, 0.75, 0.9, 0.99]))\n",
    "        print(\"\\n  Top 20 cell lines by #compounds:\")\n",
    "        print(cell_compound_counts.sort_values(ascending=False).head(20))\n",
    "\n",
    "        # ä¿å­˜ CSV\n",
    "        f1_csv = debug_dir / \"filter1_compounds_per_cell.csv\"\n",
    "        cell_compound_counts.to_csv(f1_csv, header=[\"n_compounds\"])\n",
    "        print(f\"  ğŸ‘‰ Saved full distribution to {f1_csv}\")\n",
    "\n",
    "        # ç”»ç›´æ–¹å›¾\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.hist(cell_compound_counts.values, bins=50)\n",
    "        plt.xlabel(\"Number of compounds per cell line\")\n",
    "        plt.ylabel(\"Count of cell lines\")\n",
    "        plt.title(\"Filter 1: Compounds per cell line\")\n",
    "        plt.yscale(\"log\")  # cell line æ•°ç›¸å¯¹è¾ƒå°‘ï¼Œä½†åˆ†å¸ƒå¯èƒ½é•¿å°¾ï¼Œç”¨ log çœ‹æ›´æ¸…æ™°\n",
    "        plt.tight_layout()\n",
    "        f1_png = debug_dir / \"filter1_compounds_per_cell_hist.png\"\n",
    "        plt.savefig(str(f1_png), dpi=150)\n",
    "        plt.close()\n",
    "        print(f\"  ğŸ“ˆ Histogram saved to {f1_png}\")\n",
    "\n",
    "        # ========== Filter 2: Replicates per compound ==========\n",
    "        print(\"\\n\" + \"-\" * 70)\n",
    "        print(\"ğŸ” [Filter 2] Replicates per compound (æ¯ä¸ª compound çš„ replicate æ•°)\")\n",
    "        print(\"-\" * 70)\n",
    "        replicate_counts = working_meta.groupby('pert_id').size()\n",
    "        print(\"  åŸºæœ¬ç»Ÿè®¡ï¼š\")\n",
    "        print(replicate_counts.describe(percentiles=[0.5, 0.75, 0.9, 0.99]))\n",
    "        print(\"\\n  Value counts (å‰ 20 ä¸ª replicate æ•°å¯¹åº”çš„ compound ä¸ªæ•°)ï¼š\")\n",
    "        print(replicate_counts.value_counts().sort_index().head(20))\n",
    "\n",
    "        f2_csv = debug_dir / \"filter2_replicates_per_compound.csv\"\n",
    "        replicate_counts.to_csv(f2_csv, header=[\"n_replicates\"])\n",
    "        print(f\"  ğŸ‘‰ Saved full distribution to {f2_csv}\")\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.hist(replicate_counts.values, bins=50)\n",
    "        plt.xlabel(\"Replicates per compound\")\n",
    "        plt.ylabel(\"Count of compounds\")\n",
    "        plt.title(\"Filter 2: Replicates per compound\")\n",
    "        plt.yscale(\"log\")\n",
    "        plt.tight_layout()\n",
    "        f2_png = debug_dir / \"filter2_replicates_per_compound_hist.png\"\n",
    "        plt.savefig(str(f2_png), dpi=150)\n",
    "        plt.close()\n",
    "        print(f\"  ğŸ“ˆ Histogram saved to {f2_png}\")\n",
    "\n",
    "        # ========== Filter 3: Cell line diversity per compound ==========\n",
    "        print(\"\\n\" + \"-\" * 70)\n",
    "        print(\"ğŸ” [Filter 3] Cell line diversity per compound (æ¯ä¸ª compound æ¶µç›–çš„ cell line ä¸ªæ•°)\")\n",
    "        print(\"-\" * 70)\n",
    "        cell_line_counts = working_meta.groupby('pert_id')['cell_id'].nunique()\n",
    "        print(\"  åŸºæœ¬ç»Ÿè®¡ï¼š\")\n",
    "        print(cell_line_counts.describe(percentiles=[0.5, 0.75, 0.9, 0.99]))\n",
    "        print(\"\\n  Value counts (å‰ 20 ä¸ª cell line ä¸ªæ•°å¯¹åº”çš„ compound ä¸ªæ•°)ï¼š\")\n",
    "        print(cell_line_counts.value_counts().sort_index().head(20))\n",
    "\n",
    "        f3_csv = debug_dir / \"filter3_cell_lines_per_compound.csv\"\n",
    "        cell_line_counts.to_csv(f3_csv, header=[\"n_cell_lines\"])\n",
    "        print(f\"  ğŸ‘‰ Saved full distribution to {f3_csv}\")\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.hist(cell_line_counts.values, bins=30)\n",
    "        plt.xlabel(\"Number of cell lines per compound\")\n",
    "        plt.ylabel(\"Count of compounds\")\n",
    "        plt.title(\"Filter 3: Cell line diversity per compound\")\n",
    "        plt.yscale(\"log\")\n",
    "        plt.tight_layout()\n",
    "        f3_png = debug_dir / \"filter3_cell_lines_per_compound_hist.png\"\n",
    "        plt.savefig(str(f3_png), dpi=150)\n",
    "        plt.close()\n",
    "        print(f\"  ğŸ“ˆ Histogram saved to {f3_png}\")\n",
    "\n",
    "        # ========== Filter 4: Observations per compound ==========\n",
    "        # åœ¨å½“å‰æ•°æ®ç»“æ„ä¸­ï¼Œè§‚æµ‹æ•° == replicate æ•°ï¼Œè¿™é‡Œä»ç„¶å•ç‹¬æ‰“å‡ºæ¥æ–¹ä¾¿å’Œ Filter4 é˜ˆå€¼å¯¹åº”\n",
    "        print(\"\\n\" + \"-\" * 70)\n",
    "        print(\"ğŸ” [Filter 4] Observations per compound (æ¯ä¸ª compound çš„æ€»è§‚æµ‹æ•°)\")\n",
    "        print(\"-\" * 70)\n",
    "        obs_counts = replicate_counts  # å¦‚æœå°†æ¥å¯¹â€œè§‚æµ‹â€æœ‰ä¸åŒå®šä¹‰ï¼Œå¯ä»¥åœ¨æ­¤å¤„ä¿®æ”¹\n",
    "        print(\"  åŸºæœ¬ç»Ÿè®¡ï¼š\")\n",
    "        print(obs_counts.describe(percentiles=[0.5, 0.75, 0.9, 0.99]))\n",
    "        print(\"\\n  Value counts (å‰ 20 ä¸ªè§‚æµ‹æ•°å¯¹åº”çš„ compound ä¸ªæ•°)ï¼š\")\n",
    "        print(obs_counts.value_counts().sort_index().head(20))\n",
    "\n",
    "        f4_csv = debug_dir / \"filter4_observations_per_compound.csv\"\n",
    "        obs_counts.to_csv(f4_csv, header=[\"n_obs\"])\n",
    "        print(f\"  ğŸ‘‰ Saved full distribution to {f4_csv}\")\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.hist(obs_counts.values, bins=50)\n",
    "        plt.xlabel(\"Observations per compound\")\n",
    "        plt.ylabel(\"Count of compounds\")\n",
    "        plt.title(\"Filter 4: Observations per compound\")\n",
    "        plt.yscale(\"log\")\n",
    "        plt.tight_layout()\n",
    "        f4_png = debug_dir / \"filter4_observations_per_compound_hist.png\"\n",
    "        plt.savefig(str(f4_png), dpi=150)\n",
    "        plt.close()\n",
    "        print(f\"  ğŸ“ˆ Histogram saved to {f4_png}\")\n",
    "\n",
    "        print(\"\\nâœ… DONE: Filter 1â€“4 distributions printed and saved to\", debug_dir)\n",
    "\n",
    "\n",
    "    def create_3fold_splits(self, training_data, random_state=42):\n",
    "        \"\"\"åˆ›å»º3æŠ˜äº¤å‰éªŒè¯åˆ’åˆ†\"\"\"\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        sample_meta = training_data['sample_meta']\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"ğŸ² Creating 3-fold cross-validation splits\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        folds = np.zeros(len(sample_meta), dtype=int)\n",
    "        \n",
    "        for pert_id in sample_meta['pert_id'].unique():\n",
    "            pert_mask = sample_meta['pert_id'] == pert_id\n",
    "            pert_indices = np.where(pert_mask)[0]\n",
    "            \n",
    "            np.random.shuffle(pert_indices)\n",
    "            n_samples = len(pert_indices)\n",
    "            \n",
    "            fold_sizes = [n_samples // 3] * 3\n",
    "            for i in range(n_samples % 3):\n",
    "                fold_sizes[i] += 1\n",
    "            \n",
    "            start_idx = 0\n",
    "            for fold_id, size in enumerate(fold_sizes):\n",
    "                end_idx = start_idx + size\n",
    "                folds[pert_indices[start_idx:end_idx]] = fold_id\n",
    "                start_idx = end_idx\n",
    "        \n",
    "        training_data['folds'] = folds\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Fold statistics:\")\n",
    "        for fold_id in range(3):\n",
    "            fold_mask = folds == fold_id\n",
    "            n_samples = fold_mask.sum()\n",
    "            n_compounds = sample_meta[fold_mask]['pert_id'].nunique()\n",
    "            print(f\"   Fold {fold_id}: {n_samples:,} samples, {n_compounds:,} compounds\")\n",
    "        \n",
    "        return training_data\n",
    "\n",
    "\n",
    "# ========== ä¸»ç¨‹åº ==========\n",
    "\n",
    "def main():\n",
    "    \"\"\"ä¸»ç¨‹åº\"\"\"\n",
    "    loader = LINCSDataLoader(\"E:/ç§‘ç ”/Models/drugreflector/datasets\")\n",
    "    \n",
    "    try:\n",
    "        # Step 1: åŠ è½½åŸºå› å’Œç»†èƒä¿¡æ¯\n",
    "        print(\"=\" * 70)\n",
    "        print(\"STEP 1: Loading gene and cell information\")\n",
    "        print(\"=\" * 70)\n",
    "        gene_info = loader.load_gene_info('GSE92742')\n",
    "        cell_info = loader.load_cell_info('GSE92742')\n",
    "        \n",
    "        # Step 2: åŠ è½½Level 4 signatures\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"STEP 2: Loading Level 4 signatures\")\n",
    "        print(\"=\" * 70)\n",
    "        matrix, row_meta, col_meta = loader.load_level4_signatures('GSE92742')\n",
    "        \n",
    "        # Step 3: å‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆåº”ç”¨æ‰€æœ‰ç­›é€‰æ¡ä»¶ï¼‰\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"STEP 3: Preparing training data with quality filters\")\n",
    "        print(\"=\" * 70)\n",
    "        training_data = loader.prepare_training_data(\n",
    "            min_replicates=5,                    # æ¯ä¸ªåŒ–åˆç‰©è‡³å°‘5ä¸ªreplicate\n",
    "            min_cell_lines=2,                    # æ¯ä¸ªåŒ–åˆç‰©è‡³å°‘åœ¨2ä¸ªç»†èƒç³»æµ‹è¯•\n",
    "            min_obs_per_compound=10,              # æ¯ä¸ªåŒ–åˆç‰©è‡³å°‘10ä¸ªè§‚æµ‹\n",
    "            min_compounds_per_cell=200,            # æ¯ä¸ªç»†èƒç³»è‡³å°‘æœ‰200ä¸ªåŒ–åˆç‰©\n",
    "            min_replicate_similarity=0.12        # replicateç›¸ä¼¼åº¦è‡³å°‘0.12\n",
    "        )\n",
    "        \n",
    "        # Step 4: åˆ›å»º3æŠ˜åˆ’åˆ†\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"STEP 4: Creating 3-fold splits\")\n",
    "        print(\"=\" * 70)\n",
    "        training_data = loader.create_3fold_splits(training_data)\n",
    "        \n",
    "        # Step 5: ä¿å­˜å¤„ç†åçš„æ•°æ®\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"STEP 5: Saving processed data\")\n",
    "        print(\"=\" * 70)\n",
    "        output_dir = Path(\"E:/ç§‘ç ”/Models/drugreflector/processed_data\")\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        output_file = output_dir / \"training_data_filtered.pkl\"\n",
    "        print(f\"ğŸ’¾ Saving to: {output_file}\")\n",
    "        \n",
    "        with open(output_file, 'wb') as f:\n",
    "            pickle.dump(training_data, f)\n",
    "        \n",
    "        print(f\"âœ“ Saved successfully!\")\n",
    "        \n",
    "        # æ‰“å°æœ€ç»ˆæ‘˜è¦\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"âœ… DATA PREPARATION COMPLETE!\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"ğŸ“ Output file: {output_file}\")\n",
    "        print(f\"\\nğŸ“Š Final dataset summary:\")\n",
    "        print(f\"   â€¢ Total samples: {len(training_data['X']):,}\")\n",
    "        print(f\"   â€¢ Total compounds: {len(training_data['compound_names']):,}\")\n",
    "        print(f\"   â€¢ Gene features: {training_data['X'].shape[1]}\")\n",
    "        print(f\"   â€¢ Data shape: {training_data['X'].shape}\")\n",
    "        print(f\"   â€¢ Average samples per compound: {len(training_data['X']) / len(training_data['compound_names']):.1f}\")\n",
    "        \n",
    "        # è§£å‹æ–‡ä»¶ä¿¡æ¯\n",
    "        if loader.decompressed_files:\n",
    "            print(f\"\\nğŸ“¦ Decompressed files saved at:\")\n",
    "            for f in loader.decompressed_files:\n",
    "                print(f\"   â€¢ {f}\")\n",
    "        \n",
    "        return training_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n\" + \"=\" * 70)\n",
    "        print(\"âŒ ERROR DURING DATA PREPARATION\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"   Type: {type(e).__name__}\")\n",
    "        print(f\"   Message: {e}\")\n",
    "        print(f\"\\nğŸ“‹ Full traceback:\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_from_cache():\n",
    "    \"\"\"ç›´æ¥ä»è§£å‹æ–‡ä»¶åŠ è½½æ•°æ®ï¼ˆè·³è¿‡è§£å‹æ­¥éª¤ï¼‰\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"ğŸš€ LOADING FROM CACHED FILES\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    loader = LINCSDataLoader(\"E:/ç§‘ç ”/Models/drugreflector/datasets\")\n",
    "    \n",
    "    # æ£€æŸ¥ç¼“å­˜æ–‡ä»¶\n",
    "    decompressed_dir = Path(\"E:/ç§‘ç ”/Models/drugreflector/datasets/_decompressed\")\n",
    "    cached_file = decompressed_dir / \"GSE92742_Broad_LINCS_Level4_ZSPCINF_mlr12k_n1319138x12328.gctx\"\n",
    "    \n",
    "    if not cached_file.exists():\n",
    "        print(f\"âŒ Cached file not found: {cached_file}\")\n",
    "        print(f\"   Please run main() first to decompress the data.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"âœ“ Found cached file: {cached_file.name}\")\n",
    "    \n",
    "    try:\n",
    "        # åŠ è½½åŸºå› ä¿¡æ¯\n",
    "        print(\"\\nLoading gene and cell information...\")\n",
    "        loader.load_gene_info('GSE92742')\n",
    "        loader.load_cell_info('GSE92742')\n",
    "        \n",
    "        # ç›´æ¥è¯»å–è§£å‹æ–‡ä»¶\n",
    "        print(f\"\\nReading from cached GCTX file...\")\n",
    "        matrix, sample_meta, gene_meta = loader.read_gctx(str(cached_file))\n",
    "        \n",
    "        # è¿‡æ»¤landmark genes\n",
    "        print(f\"\\nFiltering to landmark genes...\")\n",
    "        landmark_ids = set(loader.gene_info['pr_gene_id'].astype(str).values)\n",
    "        gene_mask = gene_meta['id'].isin(landmark_ids)\n",
    "        \n",
    "        matrix = matrix[:, gene_mask]\n",
    "        gene_meta = gene_meta[gene_mask].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"âœ“ Filtered matrix shape: {matrix.shape}\")\n",
    "        \n",
    "        loader.signatures = {\n",
    "            'matrix': matrix,\n",
    "            'row_meta': sample_meta,\n",
    "            'col_meta': gene_meta\n",
    "        }\n",
    "        \n",
    "        # ========= æ–°å¢éƒ¨åˆ†ï¼šåœ¨æ‰€æœ‰ QC filter ä¹‹å‰ï¼ŒæŠŠå®Œæ•´ signatures ç¼“å­˜åˆ°ç£ç›˜ =========\n",
    "        cache_dir = Path(\"E:/ç§‘ç ”/Models/drugreflector/datasets/_cache\")\n",
    "        cache_dir.mkdir(exist_ok=True)\n",
    "        cache_file = cache_dir / \"GSE92742_level4_landmark_signatures.pkl\"\n",
    "        with open(cache_file, \"wb\") as f:\n",
    "            pickle.dump(loader.signatures, f)\n",
    "        print(f\"ğŸ’¾ Saved raw (unfiltered) signatures to: {cache_file}\")\n",
    "        # ======================================================================\n",
    "\n",
    "        # åº”ç”¨ç­›é€‰\n",
    "        print(\"\\nApplying quality filters...\")\n",
    "        training_data = loader.prepare_training_data(\n",
    "            min_replicates=5,\n",
    "            min_cell_lines=2,\n",
    "            min_obs_per_compound=10,\n",
    "            min_compounds_per_cell=200,\n",
    "            min_replicate_similarity=0.12\n",
    "        )\n",
    "        \n",
    "        # åˆ›å»ºfoldåˆ’åˆ†\n",
    "        training_data = loader.create_3fold_splits(training_data)\n",
    "        \n",
    "        print(\"\\nâœ… Data loaded from cache successfully!\")\n",
    "        return training_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Error loading from cache: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7cf518e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸ” DEBUG: LOADING SIGNATURES FROM CACHE AND APPLYING FILTERS\n",
      "======================================================================\n",
      "âœ… Loaded signatures from cache: E:\\ç§‘ç ”\\Models\\drugreflector\\datasets\\_cache\\GSE92742_level4_landmark_signatures.pkl\n",
      "   Matrix shape: (1319138, 978)\n",
      "   row_meta shape: (1319138, 1)\n",
      "   col_meta shape: (978, 1)\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š DEBUG FILTER DISTRIBUTIONS (on RAW signatures, no QC applied yet)\n",
      "======================================================================\n",
      "  Matrix shape: (1319138, 978) (samples Ã— genes)\n",
      "  row_meta shape: (1319138, 1)\n",
      "  col_meta shape: (978, 1)\n",
      "\n",
      "[INFO] 'pert_id' or 'cell_id' not found in row_meta, parsing from 'id' column...\n",
      "       Added columns:\n",
      "         â€¢ pert_id <- id.split('_')[0]\n",
      "         â€¢ cell_id <- id.split('_')[1]\n",
      "       ç¤ºä¾‹ï¼š\n",
      "                                     id pert_id cell_id\n",
      "0  CPC005_A375_6H_X1_B3_DUO52HI53LO:K06  CPC005    A375\n",
      "1  CPC005_A375_6H_X2_B3_DUO52HI53LO:K06  CPC005    A375\n",
      "2  CPC005_A375_6H_X3_B3_DUO52HI53LO:K06  CPC005    A375\n",
      "3  CPC005_A375_6H_X1_B3_DUO52HI53LO:C19  CPC005    A375\n",
      "4  CPC005_A375_6H_X2_B3_DUO52HI53LO:C19  CPC005    A375\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "ğŸ” [Filter 1] Compounds per cell line (æ¯ä¸ª cell æ‹¥æœ‰çš„ compound æ•°é‡)\n",
      "----------------------------------------------------------------------\n",
      "  åŸºæœ¬ç»Ÿè®¡ï¼š\n",
      "count     76.000000\n",
      "mean      12.947368\n",
      "std       29.194472\n",
      "min        1.000000\n",
      "50%        1.000000\n",
      "75%        4.000000\n",
      "90%       61.500000\n",
      "99%      111.500000\n",
      "max      122.000000\n",
      "Name: pert_id, dtype: float64\n",
      "\n",
      "  Top 20 cell lines by #compounds:\n",
      "cell_id\n",
      "VCAP        122\n",
      "MCF7        108\n",
      "PC3         102\n",
      "A549         94\n",
      "A375         94\n",
      "HT29         86\n",
      "HA1E         70\n",
      "HCC515       63\n",
      "HEPG2        60\n",
      "HEK293T      33\n",
      "NPC          21\n",
      "ASC          17\n",
      "NEU           8\n",
      "SKB           7\n",
      "PHH           6\n",
      "SKL           5\n",
      "SKBR3         4\n",
      "MDAMB231      4\n",
      "MCF10A        4\n",
      "BT20          4\n",
      "Name: pert_id, dtype: int64\n",
      "  ğŸ‘‰ Saved full distribution to E:\\ç§‘ç ”\\Models\\drugreflector\\datasets\\_debug_filters\\filter1_compounds_per_cell.csv\n",
      "  ğŸ“ˆ Histogram saved to E:\\ç§‘ç ”\\Models\\drugreflector\\datasets\\_debug_filters\\filter1_compounds_per_cell_hist.png\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "ğŸ” [Filter 2] Replicates per compound (æ¯ä¸ª compound çš„ replicate æ•°)\n",
      "----------------------------------------------------------------------\n",
      "  åŸºæœ¬ç»Ÿè®¡ï¼š\n",
      "count      233.000000\n",
      "mean      5661.536481\n",
      "std       6788.889988\n",
      "min         43.000000\n",
      "50%       2449.000000\n",
      "75%       9343.000000\n",
      "90%      13691.800000\n",
      "99%      23764.800000\n",
      "max      62611.000000\n",
      "dtype: float64\n",
      "\n",
      "  Value counts (å‰ 20 ä¸ª replicate æ•°å¯¹åº”çš„ compound ä¸ªæ•°)ï¼š\n",
      "43     1\n",
      "316    1\n",
      "340    1\n",
      "347    1\n",
      "351    1\n",
      "353    2\n",
      "358    1\n",
      "362    1\n",
      "363    1\n",
      "366    1\n",
      "370    1\n",
      "371    1\n",
      "372    3\n",
      "373    1\n",
      "374    3\n",
      "386    3\n",
      "388    2\n",
      "389    1\n",
      "390    2\n",
      "393    1\n",
      "Name: count, dtype: int64\n",
      "  ğŸ‘‰ Saved full distribution to E:\\ç§‘ç ”\\Models\\drugreflector\\datasets\\_debug_filters\\filter2_replicates_per_compound.csv\n",
      "  ğŸ“ˆ Histogram saved to E:\\ç§‘ç ”\\Models\\drugreflector\\datasets\\_debug_filters\\filter2_replicates_per_compound_hist.png\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "ğŸ” [Filter 3] Cell line diversity per compound (æ¯ä¸ª compound æ¶µç›–çš„ cell line ä¸ªæ•°)\n",
      "----------------------------------------------------------------------\n",
      "  åŸºæœ¬ç»Ÿè®¡ï¼š\n",
      "count    233.000000\n",
      "mean       4.223176\n",
      "std        4.743596\n",
      "min        1.000000\n",
      "50%        2.000000\n",
      "75%        8.000000\n",
      "90%        9.000000\n",
      "99%       13.000000\n",
      "max       51.000000\n",
      "Name: cell_id, dtype: float64\n",
      "\n",
      "  Value counts (å‰ 20 ä¸ª cell line ä¸ªæ•°å¯¹åº”çš„ compound ä¸ªæ•°)ï¼š\n",
      "cell_id\n",
      "1     99\n",
      "2     29\n",
      "3     11\n",
      "4     12\n",
      "5      8\n",
      "6      4\n",
      "7      4\n",
      "8     20\n",
      "9     31\n",
      "10     2\n",
      "11     4\n",
      "12     5\n",
      "13     2\n",
      "14     1\n",
      "51     1\n",
      "Name: count, dtype: int64\n",
      "  ğŸ‘‰ Saved full distribution to E:\\ç§‘ç ”\\Models\\drugreflector\\datasets\\_debug_filters\\filter3_cell_lines_per_compound.csv\n",
      "  ğŸ“ˆ Histogram saved to E:\\ç§‘ç ”\\Models\\drugreflector\\datasets\\_debug_filters\\filter3_cell_lines_per_compound_hist.png\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "ğŸ” [Filter 4] Observations per compound (æ¯ä¸ª compound çš„æ€»è§‚æµ‹æ•°)\n",
      "----------------------------------------------------------------------\n",
      "  åŸºæœ¬ç»Ÿè®¡ï¼š\n",
      "count      233.000000\n",
      "mean      5661.536481\n",
      "std       6788.889988\n",
      "min         43.000000\n",
      "50%       2449.000000\n",
      "75%       9343.000000\n",
      "90%      13691.800000\n",
      "99%      23764.800000\n",
      "max      62611.000000\n",
      "dtype: float64\n",
      "\n",
      "  Value counts (å‰ 20 ä¸ªè§‚æµ‹æ•°å¯¹åº”çš„ compound ä¸ªæ•°)ï¼š\n",
      "43     1\n",
      "316    1\n",
      "340    1\n",
      "347    1\n",
      "351    1\n",
      "353    2\n",
      "358    1\n",
      "362    1\n",
      "363    1\n",
      "366    1\n",
      "370    1\n",
      "371    1\n",
      "372    3\n",
      "373    1\n",
      "374    3\n",
      "386    3\n",
      "388    2\n",
      "389    1\n",
      "390    2\n",
      "393    1\n",
      "Name: count, dtype: int64\n",
      "  ğŸ‘‰ Saved full distribution to E:\\ç§‘ç ”\\Models\\drugreflector\\datasets\\_debug_filters\\filter4_observations_per_compound.csv\n",
      "  ğŸ“ˆ Histogram saved to E:\\ç§‘ç ”\\Models\\drugreflector\\datasets\\_debug_filters\\filter4_observations_per_compound_hist.png\n",
      "\n",
      "âœ… DONE: Filter 1â€“4 distributions printed and saved to E:\\ç§‘ç ”\\Models\\drugreflector\\datasets\\_debug_filters\n",
      "\n",
      "======================================================================\n",
      "ğŸ” APPLYING QUALITY CONTROL FILTERS\n",
      "======================================================================\n",
      "Initial samples: 1319138\n",
      "\n",
      "Filter criteria:\n",
      "  â€¢ Replicates per compound â‰¥ 5\n",
      "  â€¢ Cell lines per compound â‰¥ 2\n",
      "  â€¢ Observations per compound â‰¥ 10\n",
      "  â€¢ Compounds per cell line â‰¥ 200\n",
      "  â€¢ Replicate similarity â‰¥ 0.12\n",
      "\n",
      "[INFO] 'pert_id' or 'cell_id' not found in row_meta, parsing from 'id' column...\n",
      "       Added columns:\n",
      "         â€¢ pert_id <- id.split('_')[0]\n",
      "         â€¢ cell_id <- id.split('_')[1]\n",
      "       ç¤ºä¾‹ï¼š                                      id pert_id cell_id\n",
      "0  CPC005_A375_6H_X1_B3_DUO52HI53LO:K06  CPC005    A375\n",
      "1  CPC005_A375_6H_X2_B3_DUO52HI53LO:K06  CPC005    A375\n",
      "2  CPC005_A375_6H_X3_B3_DUO52HI53LO:K06  CPC005    A375\n",
      "\n",
      "======================================================================\n",
      "FILTER 1: Compounds per cell line\n",
      "======================================================================\n",
      "  Cell lines with â‰¥200 compounds: 0/76\n",
      "  Remaining samples: 0\n",
      "  Remaining compounds: 0\n",
      "\n",
      "======================================================================\n",
      "FILTER 2: Replicates per compound\n",
      "======================================================================\n",
      "  Compounds with â‰¥5 replicates: 0/0\n",
      "  Remaining samples: 0\n",
      "  Remaining compounds: 0\n",
      "\n",
      "======================================================================\n",
      "FILTER 3: Cell line diversity per compound\n",
      "======================================================================\n",
      "  Compounds in â‰¥2 cell lines: 0/0\n",
      "  Remaining samples: 0\n",
      "  Remaining compounds: 0\n",
      "\n",
      "======================================================================\n",
      "FILTER 4: Total observations per compound\n",
      "======================================================================\n",
      "  Compounds with â‰¥10 observations: 0/0\n",
      "  Remaining samples: 0\n",
      "  Remaining compounds: 0\n",
      "\n",
      "======================================================================\n",
      "FILTER 5: Replicate similarity\n",
      "======================================================================\n",
      "\n",
      "ğŸ“Š Calculating replicate similarities...\n",
      "   Processing 0 compounds...\n",
      "   âœ“ Calculated similarities for 0 compounds\n",
      "  Compounds with similarity â‰¥0.12: 0/0\n",
      "  Mean similarity: nan\n",
      "  Median similarity: nan\n",
      "  Remaining samples: 0\n",
      "  Remaining compounds: 0\n",
      "\n",
      "======================================================================\n",
      "âœ… FINAL DATASET\n",
      "======================================================================\n",
      "  Total samples: 0\n",
      "  Total compounds: 0\n",
      "  Gene features: 978\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 49\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mâœ… DEBUG run finished.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m training_data\n\u001b[1;32m---> 49\u001b[0m training_data \u001b[38;5;241m=\u001b[39m \u001b[43mdebug_filters_from_cached_signatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 34\u001b[0m, in \u001b[0;36mdebug_filters_from_cached_signatures\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m loader\u001b[38;5;241m.\u001b[39mdebug_filter_distributions()\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# 3) å†çœŸæ­£åº”ç”¨ä½ å½“å‰è®¾ç½®çš„é˜ˆå€¼ï¼Œçœ‹çœ‹æœ€åè¿˜èƒ½å‰©å¤šå°‘æ ·æœ¬\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m training_data \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_training_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_replicates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_cell_lines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_obs_per_compound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_compounds_per_cell\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_replicate_similarity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.12\u001b[39;49m\n\u001b[0;32m     40\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# è‹¥éœ€è¦ï¼Œä¹Ÿå¯ä»¥é¡ºä¾¿åˆ›å»º folds\u001b[39;00m\n\u001b[0;32m     43\u001b[0m training_data \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mcreate_3fold_splits(training_data)\n",
      "Cell \u001b[1;32mIn[12], line 452\u001b[0m, in \u001b[0;36mLINCSDataLoader.prepare_training_data\u001b[1;34m(self, min_replicates, min_cell_lines, min_obs_per_compound, min_compounds_per_cell, min_replicate_similarity)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Total compounds: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(unique_perts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Gene features: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mworking_matrix\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 452\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Samples per compound (mean): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mworking_matrix\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43munique_perts\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    454\u001b[0m \u001b[38;5;66;03m# ç»Ÿè®¡ç»†èƒç³»åˆ†å¸ƒ\u001b[39;00m\n\u001b[0;32m    455\u001b[0m cell_dist \u001b[38;5;241m=\u001b[39m working_meta\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcell_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msize()\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "def debug_filters_from_cached_signatures():\n",
    "    \"\"\"\n",
    "    åªä»å·²ç»ç¼“å­˜å¥½çš„ signatures å¼€å§‹åš filterï¼Œ\n",
    "    ä¸å†é‡å¤è¯»å–å·¨å¤§çš„ GCTX æ–‡ä»¶ã€‚\n",
    "    ä½¿ç”¨å‰å…ˆè¿è¡Œä¸€æ¬¡ load_from_cache() ç”Ÿæˆç¼“å­˜ã€‚\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"ğŸ” DEBUG: LOADING SIGNATURES FROM CACHE AND APPLYING FILTERS\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    loader = LINCSDataLoader(\"E:/ç§‘ç ”/Models/drugreflector/datasets\")\n",
    "    cache_dir = Path(\"E:/ç§‘ç ”/Models/drugreflector/datasets/_cache\")\n",
    "    cache_file = cache_dir / \"GSE92742_level4_landmark_signatures.pkl\"\n",
    "\n",
    "    if not cache_file.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Cached signatures not found: {cache_file}\\n\"\n",
    "            f\"è¯·å…ˆè¿è¡Œä¸€æ¬¡ load_from_cache() ç”Ÿæˆè¯¥æ–‡ä»¶ã€‚\"\n",
    "        )\n",
    "\n",
    "    # 1) è¯»å–ç¼“å­˜ï¼ŒæŠŠ signatures å¡å› loader\n",
    "    with open(cache_file, \"rb\") as f:\n",
    "        loader.signatures = pickle.load(f)\n",
    "\n",
    "    print(f\"âœ… Loaded signatures from cache: {cache_file}\")\n",
    "    print(\"   Matrix shape:\", loader.signatures['matrix'].shape)\n",
    "    print(\"   row_meta shape:\", loader.signatures['row_meta'].shape)\n",
    "    print(\"   col_meta shape:\", loader.signatures['col_meta'].shape)\n",
    "\n",
    "    # 2) å…ˆä»…åœ¨â€œåŸå§‹ signatures ä¸Šâ€çœ‹ä¸€é 5 ä¸ª filter å¯¹åº”çš„åˆ†å¸ƒ\n",
    "    loader.debug_filter_distributions()\n",
    "\n",
    "    # 3) å†çœŸæ­£åº”ç”¨ä½ å½“å‰è®¾ç½®çš„é˜ˆå€¼ï¼Œçœ‹çœ‹æœ€åè¿˜èƒ½å‰©å¤šå°‘æ ·æœ¬\n",
    "    training_data = loader.prepare_training_data(\n",
    "        min_replicates=5,\n",
    "        min_cell_lines=2,\n",
    "        min_obs_per_compound=10,\n",
    "        min_compounds_per_cell=200,\n",
    "        min_replicate_similarity=0.12\n",
    "    )\n",
    "\n",
    "    # è‹¥éœ€è¦ï¼Œä¹Ÿå¯ä»¥é¡ºä¾¿åˆ›å»º folds\n",
    "    training_data = loader.create_3fold_splits(training_data)\n",
    "\n",
    "    print(\"\\nâœ… DEBUG run finished.\")\n",
    "    return training_data\n",
    "\n",
    "\n",
    "training_data = debug_filters_from_cached_signatures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dc2861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     id\n",
      "0  CPC005_A375_6H_X1_B3_DUO52HI53LO:K06\n",
      "1  CPC005_A375_6H_X2_B3_DUO52HI53LO:K06\n",
      "2  CPC005_A375_6H_X3_B3_DUO52HI53LO:K06\n",
      "3  CPC005_A375_6H_X1_B3_DUO52HI53LO:C19\n",
      "4  CPC005_A375_6H_X2_B3_DUO52HI53LO:C19\n"
     ]
    }
   ],
   "source": [
    "working_meta = pd.read_pickle(\"working_meta_before_filters.pkl\")\n",
    "\n",
    "print(working_meta.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemprop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
