{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1694bdd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "å¼€å§‹æ‰§è¡Œ Round 2 è¶…å‚æ•°ä¼˜åŒ– (Windows æœ¬åœ°ç‰ˆ)\n",
      "æ€»ä»»åŠ¡æ•°: 8\n",
      "è¾“å‡ºç›®å½•: .\\models\\hyperparam_search_1fold_round2_local\n",
      "==================================================\n",
      "\n",
      "Processing Task 0/7: lr0.015_schexponential_wd5e-05_drop0.5\n",
      "  > Output: .\\models\\hyperparam_search_1fold_round2_local\\config_lr0.015_schexponential_wd5e-05_drop0.5_task0\n",
      "  > å¼€å§‹è®­ç»ƒ... (æ—¥å¿—: .\\models\\hyperparam_search_1fold_round2_local\\config_lr0.015_schexponential_wd5e-05_drop0.5_task0\\train_log.txt)\n",
      "\n",
      "================================================================================\n",
      "ğŸ§¬ DRUGREFLECTOR TRAINING\n",
      "================================================================================\n",
      "  Based on Science 2025 paper\n",
      "  Training on LINCS 2020 dataset\n",
      "\n",
      "ğŸ“‹ Training Configuration:\n",
      "  Mode: Single fold (0)\n",
      "  Data file: .\\processed_data\\training_data_lincs2020_chemfiltered_1201.pkl\n",
      "  Output dir: .\\models\\hyperparam_search_1fold_round2_local\\config_lr0.015_schexponential_wd5e-05_drop0.5_task0\n",
      "  Epochs: 50\n",
      "  Batch size: 256\n",
      "  Initial LR: 0.015\n",
      "  Focal Î³: 2.0\n",
      "  Device: cuda\n",
      "  Workers: 0\n",
      "\n",
      "================================================================================\n",
      "ğŸ“‚ Loading Training Data\n",
      "================================================================================\n",
      "  Loading from: processed_data\\training_data_lincs2020_chemfiltered_1201.pkl\n",
      "  âœ“ Data loaded successfully\n",
      "  Keys: ['X', 'y', 'folds', 'sample_meta', 'metadata', 'gene_names', 'compound_names', 'pert_to_idx']\n",
      "\n",
      "ğŸ“Š Data Summary:\n",
      "  Samples: 306,955\n",
      "  Features: 978\n",
      "  Compounds: 9,434\n",
      "  Unique folds: [0 1 2]\n",
      "\n",
      "================================================================================\n",
      "ğŸš€ DrugReflector Trainer Initialized\n",
      "================================================================================\n",
      "  Device: cuda\n",
      "  Initial LR: 0.015\n",
      "  LR Scheduler: exponential\n",
      "  Min LR: 1e-06\n",
      "  Weight Decay: 5e-05\n",
      "  Dropout: 0.5\n",
      "  Focal Î³: 2.0\n",
      "  Batch size: 256\n",
      "  Epochs: 50\n",
      "  Plot Directory: training_plots\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š Training Fold 0\n",
      "================================================================================\n",
      "  Total samples: 306,955\n",
      "  Total compounds: 9,434\n",
      "  Gene features: 978\n",
      "\n",
      "ğŸ”§ Preprocessing signatures...\n",
      "\n",
      "ğŸ“Š Clipping and normalizing signatures...\n",
      "   Clip range: (-2, 2)\n",
      "   âœ“ Mean std after normalization: 1.0000\n",
      "   âœ“ Data range: [-4.44, 4.43]\n",
      "\n",
      "ğŸ“‹ Data Split:\n",
      "  Training samples: 201,549 (65.7%)\n",
      "  Validation samples: 105,406 (34.3%)\n",
      "  Training compounds: 9,434\n",
      "  Validation compounds: 9,434\n",
      "\n",
      "  Train batches: 788\n",
      "  Val batches: 412\n",
      "\n",
      "ğŸ—ï¸  Building model...\n",
      "  Architecture: 978 â†’ 1024 â†’ 2048 â†’ 9434\n",
      "  Parameters: 22,438,106\n",
      "  ğŸ“Š Using exponential LR scheduler\n",
      "\n",
      "================================================================================\n",
      "ğŸ¯ Starting Training\n",
      "================================================================================\n",
      "\n",
      "Training:   0%|          | 0/788 [00:00<?, ?it/s]\n",
      "Training:   0%|          | 0/788 [00:00<?, ?it/s, loss=9.3113]\n",
      "Training:   0%|          | 1/788 [00:00<04:08,  3.17it/s, loss=9.3113]\n",
      "Training:   0%|          | 1/788 [00:00<04:08,  3.17it/s, loss=9.4229]\n",
      "Training:   0%|          | 1/788 [00:00<04:08,  3.17it/s, loss=10.5375]\n",
      "Training:   0%|          | 1/788 [00:00<04:08,  3.17it/s, loss=12.3878]\n",
      "Training:   0%|          | 1/788 [00:00<04:08,  3.17it/s, loss=10.6183]\n",
      "Training:   0%|          | 1/788 [00:00<04:08,  3.17it/s, loss=11.7190]\n",
      "Training:   0%|          | 1/788 [00:00<04:08,  3.17it/s, loss=11.1864]\n",
      "Training:   0%|          | 1/788 [00:00<04:08,  3.17it/s, loss=11.5733]\n",
      "Training:   1%|          | 8/788 [00:00<00:33, 23.47it/s, loss=11.5733]\n",
      "Training:   1%|          | 8/788 [00:00<00:33, 23.47it/s, loss=11.5122]\n",
      "Training:   1%|          | 8/788 [00:00<00:33, 23.47it/s, loss=11.3562]\n",
      "Training:   1%|          | 8/788 [00:00<00:33, 23.47it/s, loss=11.6238]\n",
      "Training:   1%|          | 8/788 [00:00<00:33, 23.47it/s, loss=11.7160]\n",
      "Training:   1%|          | 8/788 [00:00<00:33, 23.47it/s, loss=11.7581]\n",
      "Training:   1%|          | 8/788 [00:00<00:33, 23.47it/s, loss=11.8318]\n",
      "Training:   1%|          | 8/788 [00:00<00:33, 23.47it/s, loss=11.7324]\n",
      "Training:   2%|â–         | 15/788 [00:00<00:20, 37.16it/s, loss=11.7324]\n",
      "Training:   2%|â–         | 15/788 [00:00<00:20, 37.16it/s, loss=12.0407]\n",
      "Training:   2%|â–         | 15/788 [00:00<00:20, 37.16it/s, loss=11.7879]\n",
      "Training:   2%|â–         | 15/788 [00:00<00:20, 37.16it/s, loss=11.7659]\n",
      "Training:   2%|â–         | 15/788 [00:00<00:20, 37.16it/s, loss=12.2493]\n",
      "Training:   2%|â–         | 15/788 [00:00<00:20, 37.16it/s, loss=11.6563]\n",
      "Training:   2%|â–         | 15/788 [00:00<00:20, 37.16it/s, loss=12.2331]\n",
      "Training:   2%|â–         | 15/788 [00:00<00:20, 37.16it/s, loss=11.8078]\n",
      "Training:   2%|â–         | 15/788 [00:00<00:20, 37.16it/s, loss=12.2169]\n",
      "Training:   3%|â–         | 23/788 [00:00<00:15, 48.32it/s, loss=12.2169]\n",
      "Training:   3%|â–         | 23/788 [00:00<00:15, 48.32it/s, loss=11.5895]\n",
      "Training:   3%|â–         | 23/788 [00:00<00:15, 48.32it/s, loss=11.6829]\n",
      "Training:   3%|â–         | 23/788 [00:00<00:15, 48.32it/s, loss=11.9015]\n",
      "Training:   3%|â–         | 23/788 [00:00<00:15, 48.32it/s, loss=11.6504]\n",
      "Training:   3%|â–         | 23/788 [00:00<00:15, 48.32it/s, loss=11.6550]\n",
      "Training:   3%|â–         | 23/788 [00:00<00:15, 48.32it/s, loss=11.6890]\n",
      "Training:   3%|â–         | 23/788 [00:00<00:15, 48.32it/s, loss=11.8359]\n",
      "Training:   3%|â–         | 23/788 [00:00<00:15, 48.32it/s, loss=11.4262]\n",
      "Training:   4%|â–         | 31/788 [00:00<00:13, 55.68it/s, loss=11.4262]\n",
      "Training:   4%|â–         | 31/788 [00:00<00:13, 55.68it/s, loss=12.1571]\n",
      "Training:   4%|â–         | 31/788 [00:00<00:13, 55.68it/s, loss=11.8824]\n",
      "Training:   4%|â–         | 31/788 [00:00<00:13, 55.68it/s, loss=11.8719]\n",
      "Training:   4%|â–         | 31/788 [00:00<00:13, 55.68it/s, loss=11.8825]\n",
      "Training:   4%|â–         | 31/788 [00:00<00:13, 55.68it/s, loss=11.8559]\n",
      "Training:   4%|â–         | 31/788 [00:00<00:13, 55.68it/s, loss=11.8289]\n",
      "Training:   4%|â–         | 31/788 [00:00<00:13, 55.68it/s, loss=11.6958]\n",
      "Training:   4%|â–         | 31/788 [00:00<00:13, 55.68it/s, loss=11.5704]\n",
      "Training:   5%|â–         | 39/788 [00:00<00:12, 60.50it/s, loss=11.5704]\n",
      "Training:   5%|â–         | 39/788 [00:00<00:12, 60.50it/s, loss=11.4853]\n",
      "Training:   5%|â–         | 39/788 [00:00<00:12, 60.50it/s, loss=11.6852]\n",
      "Training:   5%|â–         | 39/788 [00:00<00:12, 60.50it/s, loss=11.6541]\n",
      "Training:   5%|â–         | 39/788 [00:00<00:12, 60.50it/s, loss=11.3004]\n",
      "Training:   5%|â–         | 39/788 [00:00<00:12, 60.50it/s, loss=11.3213]\n",
      "Training:   5%|â–         | 39/788 [00:00<00:12, 60.50it/s, loss=11.0530]\n",
      "Training:   5%|â–         | 39/788 [00:00<00:12, 60.50it/s, loss=11.0698]\n",
      "Training:   5%|â–         | 39/788 [00:00<00:12, 60.50it/s, loss=11.3824]\n",
      "Training:   6%|â–Œ         | 47/788 [00:00<00:11, 63.87it/s, loss=11.3824]\n",
      "Training:   6%|â–Œ         | 47/788 [00:00<00:11, 63.87it/s, loss=11.4395]\n",
      "Training:   6%|â–Œ         | 47/788 [00:01<00:11, 63.87it/s, loss=11.1215]\n",
      "Training:   6%|â–Œ         | 47/788 [00:01<00:11, 63.87it/s, loss=11.2144]\n",
      "Training:   6%|â–Œ         | 47/788 [00:01<00:11, 63.87it/s, loss=11.1769]\n",
      "Training:   6%|â–Œ         | 47/788 [00:01<00:11, 63.87it/s, loss=10.6096]\n",
      "Training:   6%|â–Œ         | 47/788 [00:01<00:11, 63.87it/s, loss=11.0259]\n",
      "Training:   6%|â–Œ         | 47/788 [00:01<00:11, 63.87it/s, loss=11.0825]\n",
      "Training:   6%|â–Œ         | 47/788 [00:01<00:11, 63.87it/s, loss=10.5954]\n",
      "Training:   7%|â–‹         | 55/788 [00:01<00:11, 66.25it/s, loss=10.5954]\n",
      "Training:   7%|â–‹         | 55/788 [00:01<00:11, 66.25it/s, loss=10.8100]\n",
      "Training:   7%|â–‹         | 55/788 [00:01<00:11, 66.25it/s, loss=10.4914]\n",
      "Training:   7%|â–‹         | 55/788 [00:01<00:11, 66.25it/s, loss=10.6645]\n",
      "Training:   7%|â–‹         | 55/788 [00:01<00:11, 66.25it/s, loss=10.1564]\n",
      "Training:   7%|â–‹         | 55/788 [00:01<00:11, 66.25it/s, loss=10.4510]\n",
      "Training:   7%|â–‹         | 55/788 [00:01<00:11, 66.25it/s, loss=10.1807]\n",
      "Training:   7%|â–‹         | 55/788 [00:01<00:11, 66.25it/s, loss=10.1691]\n",
      "Training:   7%|â–‹         | 55/788 [00:01<00:11, 66.25it/s, loss=10.0150]\n",
      "Training:   8%|â–Š         | 63/788 [00:01<00:10, 67.77it/s, loss=10.0150]\n",
      "Training:   8%|â–Š         | 63/788 [00:01<00:10, 67.77it/s, loss=10.1201]\n",
      "Training:   8%|â–Š         | 63/788 [00:01<00:10, 67.77it/s, loss=9.9413] \n",
      "Training:   8%|â–Š         | 63/788 [00:01<00:10, 67.77it/s, loss=9.8147]\n",
      "Training:   8%|â–Š         | 63/788 [00:01<00:10, 67.77it/s, loss=9.6482]\n",
      "Training:   8%|â–Š         | 63/788 [00:01<00:10, 67.77it/s, loss=9.7154]\n",
      "Training:   8%|â–Š         | 63/788 [00:01<00:10, 67.77it/s, loss=9.4688]\n",
      "Training:   8%|â–Š         | 63/788 [00:01<00:10, 67.77it/s, loss=9.2898]\n",
      "Training:   8%|â–Š         | 63/788 [00:01<00:10, 67.77it/s, loss=9.4929]\n",
      "Training:   9%|â–‰         | 71/788 [00:01<00:10, 68.87it/s, loss=9.4929]\n",
      "Training:   9%|â–‰         | 71/788 [00:01<00:10, 68.87it/s, loss=9.4249]\n",
      "Training:   9%|â–‰         | 71/788 [00:01<00:10, 68.87it/s, loss=9.4787]\n",
      "Training:   9%|â–‰         | 71/788 [00:01<00:10, 68.87it/s, loss=9.4939]\n",
      "Training:   9%|â–‰         | 71/788 [00:01<00:10, 68.87it/s, loss=9.1460]\n",
      "Training:   9%|â–‰         | 71/788 [00:01<00:10, 68.87it/s, loss=9.1459]\n",
      "Training:   9%|â–‰         | 71/788 [00:01<00:10, 68.87it/s, loss=9.2256]\n",
      "Training:   9%|â–‰         | 71/788 [00:01<00:10, 68.87it/s, loss=9.2415]\n",
      "Training:   9%|â–‰         | 71/788 [00:01<00:10, 68.87it/s, loss=9.2909]\n",
      "Training:  10%|â–ˆ         | 79/788 [00:01<00:10, 69.48it/s, loss=9.2909]\n",
      "Training:  10%|â–ˆ         | 79/788 [00:01<00:10, 69.48it/s, loss=8.8865]\n",
      "Training:  10%|â–ˆ         | 79/788 [00:01<00:10, 69.48it/s, loss=9.1654]\n",
      "Training:  10%|â–ˆ         | 79/788 [00:01<00:10, 69.48it/s, loss=9.0477]\n",
      "Training:  10%|â–ˆ         | 79/788 [00:01<00:10, 69.48it/s, loss=9.1819]\n",
      "Training:  10%|â–ˆ         | 79/788 [00:01<00:10, 69.48it/s, loss=9.0857]\n",
      "Training:  10%|â–ˆ         | 79/788 [00:01<00:10, 69.48it/s, loss=8.9030]\n",
      "Training:  10%|â–ˆ         | 79/788 [00:01<00:10, 69.48it/s, loss=9.0038]\n",
      "Training:  10%|â–ˆ         | 79/788 [00:01<00:10, 69.48it/s, loss=9.1209]\n",
      "Training:  11%|â–ˆ         | 87/788 [00:01<00:10, 70.07it/s, loss=9.1209]\n",
      "Training:  11%|â–ˆ         | 87/788 [00:01<00:10, 70.07it/s, loss=8.9097]\n",
      "Training:  11%|â–ˆ         | 87/788 [00:01<00:10, 70.07it/s, loss=8.9924]\n",
      "Training:  11%|â–ˆ         | 87/788 [00:01<00:10, 70.07it/s, loss=8.9691]\n",
      "Training:  11%|â–ˆ         | 87/788 [00:01<00:10, 70.07it/s, loss=9.0323]\n",
      "Training:  11%|â–ˆ         | 87/788 [00:01<00:10, 70.07it/s, loss=8.9715]\n",
      "Training:  11%|â–ˆ         | 87/788 [00:01<00:10, 70.07it/s, loss=9.0177]\n",
      "Training:  11%|â–ˆ         | 87/788 [00:01<00:10, 70.07it/s, loss=8.8797]\n",
      "Training:  11%|â–ˆ         | 87/788 [00:01<00:10, 70.07it/s, loss=8.9786]\n",
      "Training:  12%|â–ˆâ–        | 95/788 [00:01<00:09, 70.45it/s, loss=8.9786]\n",
      "Training:  12%|â–ˆâ–        | 95/788 [00:01<00:09, 70.45it/s, loss=8.8192]\n",
      "Training:  12%|â–ˆâ–        | 95/788 [00:01<00:09, 70.45it/s, loss=8.7939]\n",
      "Training:  12%|â–ˆâ–        | 95/788 [00:01<00:09, 70.45it/s, loss=8.8994]\n",
      "Training:  12%|â–ˆâ–        | 95/788 [00:01<00:09, 70.45it/s, loss=8.9270]\n",
      "Training:  12%|â–ˆâ–        | 95/788 [00:01<00:09, 70.45it/s, loss=8.9892]\n",
      "Training:  12%|â–ˆâ–        | 95/788 [00:01<00:09, 70.45it/s, loss=8.9148]\n",
      "Training:  12%|â–ˆâ–        | 95/788 [00:01<00:09, 70.45it/s, loss=8.7578]\n",
      "Training:  12%|â–ˆâ–        | 95/788 [00:01<00:09, 70.45it/s, loss=8.8314]\n",
      "Training:  13%|â–ˆâ–        | 103/788 [00:01<00:09, 70.64it/s, loss=8.8314]\n",
      "Training:  13%|â–ˆâ–        | 103/788 [00:01<00:09, 70.64it/s, loss=8.9279]\n",
      "Training:  13%|â–ˆâ–        | 103/788 [00:01<00:09, 70.64it/s, loss=8.8762]\n",
      "Training:  13%|â–ˆâ–        | 103/788 [00:01<00:09, 70.64it/s, loss=8.7631]\n",
      "Training:  13%|â–ˆâ–        | 103/788 [00:01<00:09, 70.64it/s, loss=8.9171]\n",
      "Training:  13%|â–ˆâ–        | 103/788 [00:01<00:09, 70.64it/s, loss=8.9594]\n",
      "Training:  13%|â–ˆâ–        | 103/788 [00:01<00:09, 70.64it/s, loss=8.9585]\n",
      "Training:  13%|â–ˆâ–        | 103/788 [00:01<00:09, 70.64it/s, loss=8.8783]\n",
      "Training:  13%|â–ˆâ–        | 103/788 [00:01<00:09, 70.64it/s, loss=8.8774]\n",
      "Training:  14%|â–ˆâ–        | 111/788 [00:01<00:09, 70.83it/s, loss=8.8774]\n",
      "Training:  14%|â–ˆâ–        | 111/788 [00:01<00:09, 70.83it/s, loss=8.7176]\n",
      "Training:  14%|â–ˆâ–        | 111/788 [00:01<00:09, 70.83it/s, loss=8.8759]\n",
      "Training:  14%|â–ˆâ–        | 111/788 [00:01<00:09, 70.83it/s, loss=8.8077]\n",
      "Training:  14%|â–ˆâ–        | 111/788 [00:01<00:09, 70.83it/s, loss=8.8948]\n",
      "Training:  14%|â–ˆâ–        | 111/788 [00:01<00:09, 70.83it/s, loss=8.9552]\n",
      "Training:  14%|â–ˆâ–        | 111/788 [00:01<00:09, 70.83it/s, loss=8.7656]\n",
      "Training:  14%|â–ˆâ–        | 111/788 [00:01<00:09, 70.83it/s, loss=8.8334]\n",
      "Training:  14%|â–ˆâ–        | 111/788 [00:01<00:09, 70.83it/s, loss=8.6979]\n",
      "Training:  15%|â–ˆâ–Œ        | 119/788 [00:01<00:09, 71.13it/s, loss=8.6979]\n",
      "Training:  15%|â–ˆâ–Œ        | 119/788 [00:01<00:09, 71.13it/s, loss=8.9454]\n",
      "Training:  15%|â–ˆâ–Œ        | 119/788 [00:02<00:09, 71.13it/s, loss=8.8975]\n",
      "Training:  15%|â–ˆâ–Œ        | 119/788 [00:02<00:09, 71.13it/s, loss=8.8365]\n",
      "Training:  15%|â–ˆâ–Œ        | 119/788 [00:02<00:09, 71.13it/s, loss=8.8227]\n",
      "Training:  15%|â–ˆâ–Œ        | 119/788 [00:02<00:09, 71.13it/s, loss=8.9346]\n",
      "Training:  15%|â–ˆâ–Œ        | 119/788 [00:02<00:09, 71.13it/s, loss=8.9091]\n",
      "Training:  15%|â–ˆâ–Œ        | 119/788 [00:02<00:09, 71.13it/s, loss=8.8767]\n",
      "Training:  15%|â–ˆâ–Œ        | 119/788 [00:02<00:09, 71.13it/s, loss=8.8355]\n",
      "Training:  16%|â–ˆâ–Œ        | 127/788 [00:02<00:09, 71.10it/s, loss=8.8355]\n",
      "Training:  16%|â–ˆâ–Œ        | 127/788 [00:02<00:09, 71.10it/s, loss=8.8752]\n",
      "Training:  16%|â–ˆâ–Œ        | 127/788 [00:02<00:09, 71.10it/s, loss=8.8381]\n",
      "Training:  16%|â–ˆâ–Œ        | 127/788 [00:02<00:09, 71.10it/s, loss=8.8459]\n",
      "Training:  16%|â–ˆâ–Œ        | 127/788 [00:02<00:09, 71.10it/s, loss=8.6923]\n",
      "Training:  16%|â–ˆâ–Œ        | 127/788 [00:02<00:09, 71.10it/s, loss=8.7757]\n",
      "Training:  16%|â–ˆâ–Œ        | 127/788 [00:02<00:09, 71.10it/s, loss=8.8294]\n",
      "Training:  16%|â–ˆâ–Œ        | 127/788 [00:02<00:09, 71.10it/s, loss=8.8606]\n",
      "Training:  16%|â–ˆâ–Œ        | 127/788 [00:02<00:09, 71.10it/s, loss=8.8351]\n",
      "Training:  17%|â–ˆâ–‹        | 135/788 [00:02<00:09, 71.18it/s, loss=8.8351]\n",
      "Training:  17%|â–ˆâ–‹        | 135/788 [00:02<00:09, 71.18it/s, loss=8.8560]\n",
      "Training:  17%|â–ˆâ–‹        | 135/788 [00:02<00:09, 71.18it/s, loss=8.8280]\n",
      "Training:  17%|â–ˆâ–‹        | 135/788 [00:02<00:09, 71.18it/s, loss=8.7728]\n",
      "Training:  17%|â–ˆâ–‹        | 135/788 [00:02<00:09, 71.18it/s, loss=8.8381]\n",
      "Training:  17%|â–ˆâ–‹        | 135/788 [00:02<00:09, 71.18it/s, loss=8.8262]\n",
      "Training:  17%|â–ˆâ–‹        | 135/788 [00:02<00:09, 71.18it/s, loss=8.7597]\n",
      "Training:  17%|â–ˆâ–‹        | 135/788 [00:02<00:09, 71.18it/s, loss=8.7529]\n",
      "Training:  17%|â–ˆâ–‹        | 135/788 [00:02<00:09, 71.18it/s, loss=8.6927]\n",
      "Training:  18%|â–ˆâ–Š        | 143/788 [00:02<00:09, 71.26it/s, loss=8.6927]\n",
      "Training:  18%|â–ˆâ–Š        | 143/788 [00:02<00:09, 71.26it/s, loss=8.8387]\n",
      "Training:  18%|â–ˆâ–Š        | 143/788 [00:02<00:09, 71.26it/s, loss=8.7781]\n",
      "Training:  18%|â–ˆâ–Š        | 143/788 [00:02<00:09, 71.26it/s, loss=8.6604]\n",
      "Training:  18%|â–ˆâ–Š        | 143/788 [00:02<00:09, 71.26it/s, loss=8.9259]\n",
      "Training:  18%|â–ˆâ–Š        | 143/788 [00:02<00:09, 71.26it/s, loss=8.7687]\n",
      "Training:  18%|â–ˆâ–Š        | 143/788 [00:02<00:09, 71.26it/s, loss=8.7298]\n",
      "Training:  18%|â–ˆâ–Š        | 143/788 [00:02<00:09, 71.26it/s, loss=8.6743]\n",
      "Training:  18%|â–ˆâ–Š        | 143/788 [00:02<00:09, 71.26it/s, loss=8.7801]\n",
      "Training:  19%|â–ˆâ–‰        | 151/788 [00:02<00:08, 71.24it/s, loss=8.7801]\n",
      "Training:  19%|â–ˆâ–‰        | 151/788 [00:02<00:08, 71.24it/s, loss=8.8010]\n",
      "Training:  19%|â–ˆâ–‰        | 151/788 [00:02<00:08, 71.24it/s, loss=8.7345]\n",
      "Training:  19%|â–ˆâ–‰        | 151/788 [00:02<00:08, 71.24it/s, loss=8.8384]\n",
      "Training:  19%|â–ˆâ–‰        | 151/788 [00:02<00:08, 71.24it/s, loss=8.9031]\n",
      "Training:  19%|â–ˆâ–‰        | 151/788 [00:02<00:08, 71.24it/s, loss=8.5884]\n",
      "Training:  19%|â–ˆâ–‰        | 151/788 [00:02<00:08, 71.24it/s, loss=8.8634]\n",
      "Training:  19%|â–ˆâ–‰        | 151/788 [00:02<00:08, 71.24it/s, loss=8.8928]\n",
      "Training:  19%|â–ˆâ–‰        | 151/788 [00:02<00:08, 71.24it/s, loss=8.8030]\n",
      "Training:  20%|â–ˆâ–ˆ        | 159/788 [00:02<00:08, 71.28it/s, loss=8.8030]\n",
      "Training:  20%|â–ˆâ–ˆ        | 159/788 [00:02<00:08, 71.28it/s, loss=8.7928]\n",
      "Training:  20%|â–ˆâ–ˆ        | 159/788 [00:02<00:08, 71.28it/s, loss=8.8469]\n",
      "Training:  20%|â–ˆâ–ˆ        | 159/788 [00:02<00:08, 71.28it/s, loss=8.8139]\n",
      "Training:  20%|â–ˆâ–ˆ        | 159/788 [00:02<00:08, 71.28it/s, loss=8.8531]\n",
      "Training:  20%|â–ˆâ–ˆ        | 159/788 [00:02<00:08, 71.28it/s, loss=8.7760]\n",
      "Training:  20%|â–ˆâ–ˆ        | 159/788 [00:02<00:08, 71.28it/s, loss=8.7801]\n",
      "Training:  20%|â–ˆâ–ˆ        | 159/788 [00:02<00:08, 71.28it/s, loss=8.8372]\n",
      "Training:  20%|â–ˆâ–ˆ        | 159/788 [00:02<00:08, 71.28it/s, loss=8.8057]\n",
      "Training:  21%|â–ˆâ–ˆ        | 167/788 [00:02<00:08, 71.26it/s, loss=8.8057]\n",
      "Training:  21%|â–ˆâ–ˆ        | 167/788 [00:02<00:08, 71.26it/s, loss=8.7528]\n",
      "Training:  21%|â–ˆâ–ˆ        | 167/788 [00:02<00:08, 71.26it/s, loss=8.7387]\n",
      "Training:  21%|â–ˆâ–ˆ        | 167/788 [00:02<00:08, 71.26it/s, loss=8.8157]\n",
      "Training:  21%|â–ˆâ–ˆ        | 167/788 [00:02<00:08, 71.26it/s, loss=8.7681]\n",
      "Training:  21%|â–ˆâ–ˆ        | 167/788 [00:02<00:08, 71.26it/s, loss=8.7117]\n",
      "Training:  21%|â–ˆâ–ˆ        | 167/788 [00:02<00:08, 71.26it/s, loss=8.7169]\n",
      "Training:  21%|â–ˆâ–ˆ        | 167/788 [00:02<00:08, 71.26it/s, loss=8.7748]\n",
      "Training:  21%|â–ˆâ–ˆ        | 167/788 [00:02<00:08, 71.26it/s, loss=8.6534]\n",
      "Training:  22%|â–ˆâ–ˆâ–       | 175/788 [00:02<00:08, 71.25it/s, loss=8.6534]\n",
      "Training:  22%|â–ˆâ–ˆâ–       | 175/788 [00:02<00:08, 71.25it/s, loss=8.6601]\n",
      "Training:  22%|â–ˆâ–ˆâ–       | 175/788 [00:02<00:08, 71.25it/s, loss=8.8988]\n",
      "Training:  22%|â–ˆâ–ˆâ–       | 175/788 [00:02<00:08, 71.25it/s, loss=8.7378]\n",
      "Training:  22%|â–ˆâ–ˆâ–       | 175/788 [00:02<00:08, 71.25it/s, loss=8.6982]\n",
      "Training:  22%|â–ˆâ–ˆâ–       | 175/788 [00:02<00:08, 71.25it/s, loss=8.5676]\n",
      "Training:  22%|â–ˆâ–ˆâ–       | 175/788 [00:02<00:08, 71.25it/s, loss=8.6825]\n",
      "Training:  22%|â–ˆâ–ˆâ–       | 175/788 [00:02<00:08, 71.25it/s, loss=8.8972]\n",
      "Training:  22%|â–ˆâ–ˆâ–       | 175/788 [00:02<00:08, 71.25it/s, loss=8.7483]\n",
      "Training:  23%|â–ˆâ–ˆâ–       | 183/788 [00:02<00:09, 61.01it/s, loss=8.7483]\n",
      "Training:  23%|â–ˆâ–ˆâ–       | 183/788 [00:02<00:09, 61.01it/s, loss=8.6195]\n",
      "Training:  23%|â–ˆâ–ˆâ–       | 183/788 [00:02<00:09, 61.01it/s, loss=8.6795]\n",
      "Training:  23%|â–ˆâ–ˆâ–       | 183/788 [00:02<00:09, 61.01it/s, loss=8.7201]\n",
      "Training:  23%|â–ˆâ–ˆâ–       | 183/788 [00:03<00:09, 61.01it/s, loss=8.6662]\n",
      "Training:  23%|â–ˆâ–ˆâ–       | 183/788 [00:03<00:09, 61.01it/s, loss=8.5647]\n",
      "Training:  23%|â–ˆâ–ˆâ–       | 183/788 [00:03<00:09, 61.01it/s, loss=8.6904]\n",
      "Training:  23%|â–ˆâ–ˆâ–       | 183/788 [00:03<00:09, 61.01it/s, loss=8.7154]\n",
      "Training:  23%|â–ˆâ–ˆâ–       | 183/788 [00:03<00:09, 61.01it/s, loss=8.8499]\n",
      "Training:  24%|â–ˆâ–ˆâ–       | 191/788 [00:03<00:09, 63.43it/s, loss=8.8499]\n",
      "Training:  24%|â–ˆâ–ˆâ–       | 191/788 [00:03<00:09, 63.43it/s, loss=8.6197]\n",
      "Training:  24%|â–ˆâ–ˆâ–       | 191/788 [00:03<00:09, 63.43it/s, loss=8.6706]\n",
      "Training:  24%|â–ˆâ–ˆâ–       | 191/788 [00:03<00:09, 63.43it/s, loss=8.7344]\n",
      "Training:  24%|â–ˆâ–ˆâ–       | 191/788 [00:03<00:09, 63.43it/s, loss=8.6590]\n",
      "Training:  24%|â–ˆâ–ˆâ–       | 191/788 [00:03<00:09, 63.43it/s, loss=8.7149]\n",
      "Training:  24%|â–ˆâ–ˆâ–       | 191/788 [00:03<00:09, 63.43it/s, loss=8.8463]\n",
      "Training:  24%|â–ˆâ–ˆâ–       | 191/788 [00:03<00:09, 63.43it/s, loss=8.6449]\n",
      "Training:  25%|â–ˆâ–ˆâ–Œ       | 198/788 [00:03<00:09, 61.79it/s, loss=8.6449]\n",
      "Training:  25%|â–ˆâ–ˆâ–Œ       | 198/788 [00:03<00:09, 61.79it/s, loss=8.6814]\n",
      "Training:  25%|â–ˆâ–ˆâ–Œ       | 198/788 [00:03<00:09, 61.79it/s, loss=8.7359]\n",
      "Training:  25%|â–ˆâ–ˆâ–Œ       | 198/788 [00:03<00:09, 61.79it/s, loss=8.6354]\n",
      "Training:  25%|â–ˆâ–ˆâ–Œ       | 198/788 [00:03<00:09, 61.79it/s, loss=8.7029]\n",
      "Training:  25%|â–ˆâ–ˆâ–Œ       | 198/788 [00:03<00:09, 61.79it/s, loss=8.7029]\n",
      "Training:  25%|â–ˆâ–ˆâ–Œ       | 198/788 [00:03<00:09, 61.79it/s, loss=8.6760]\n",
      "Training:  25%|â–ˆâ–ˆâ–Œ       | 198/788 [00:03<00:09, 61.79it/s, loss=8.6627]\n",
      "Training:  26%|â–ˆâ–ˆâ–Œ       | 205/788 [00:03<00:10, 57.68it/s, loss=8.6627]\n",
      "Training:  26%|â–ˆâ–ˆâ–Œ       | 205/788 [00:03<00:10, 57.68it/s, loss=8.6286]\n",
      "Training:  26%|â–ˆâ–ˆâ–Œ       | 205/788 [00:03<00:10, 57.68it/s, loss=8.6056]\n",
      "Training:  26%|â–ˆâ–ˆâ–Œ       | 205/788 [00:03<00:10, 57.68it/s, loss=8.5810]\n",
      "Training:  26%|â–ˆâ–ˆâ–Œ       | 205/788 [00:03<00:10, 57.68it/s, loss=8.6524]\n",
      "Training:  26%|â–ˆâ–ˆâ–Œ       | 205/788 [00:03<00:10, 57.68it/s, loss=8.6062]\n",
      "Training:  26%|â–ˆâ–ˆâ–Œ       | 205/788 [00:03<00:10, 57.68it/s, loss=8.6259]\n",
      "Training:  26%|â–ˆâ–ˆâ–Œ       | 205/788 [00:03<00:10, 57.68it/s, loss=8.5890]\n",
      "Training:  27%|â–ˆâ–ˆâ–‹       | 212/788 [00:03<00:09, 59.78it/s, loss=8.5890]\n",
      "Training:  27%|â–ˆâ–ˆâ–‹       | 212/788 [00:03<00:09, 59.78it/s, loss=8.9236]\n",
      "Training:  27%|â–ˆâ–ˆâ–‹       | 212/788 [00:03<00:09, 59.78it/s, loss=8.7687]\n",
      "Training:  27%|â–ˆâ–ˆâ–‹       | 212/788 [00:03<00:09, 59.78it/s, loss=8.7518]\n",
      "Training:  27%|â–ˆâ–ˆâ–‹       | 212/788 [00:03<00:09, 59.78it/s, loss=8.6936]\n",
      "Training:  27%|â–ˆâ–ˆâ–‹       | 212/788 [00:03<00:09, 59.78it/s, loss=8.6612]\n",
      "Training:  27%|â–ˆâ–ˆâ–‹       | 212/788 [00:03<00:09, 59.78it/s, loss=8.5462]\n",
      "Training:  27%|â–ˆâ–ˆâ–‹       | 212/788 [00:03<00:09, 59.78it/s, loss=8.7519]\n",
      "Training:  28%|â–ˆâ–ˆâ–Š       | 219/788 [00:03<00:09, 61.33it/s, loss=8.7519]\n",
      "Training:  28%|â–ˆâ–ˆâ–Š       | 219/788 [00:03<00:09, 61.33it/s, loss=8.4744]\n",
      "Training:  28%|â–ˆâ–ˆâ–Š       | 219/788 [00:03<00:09, 61.33it/s, loss=8.6403]\n",
      "Training:  28%|â–ˆâ–ˆâ–Š       | 219/788 [00:03<00:09, 61.33it/s, loss=8.6569]\n",
      "Training:  28%|â–ˆâ–ˆâ–Š       | 219/788 [00:03<00:09, 61.33it/s, loss=8.4478]\n",
      "Training:  28%|â–ˆâ–ˆâ–Š       | 219/788 [00:03<00:09, 61.33it/s, loss=8.6911]\n",
      "Training:  28%|â–ˆâ–ˆâ–Š       | 219/788 [00:03<00:09, 61.33it/s, loss=8.6854]\n",
      "Training:  28%|â–ˆâ–ˆâ–Š       | 219/788 [00:03<00:09, 61.33it/s, loss=8.5347]\n",
      "Training:  29%|â–ˆâ–ˆâ–Š       | 226/788 [00:03<00:08, 63.63it/s, loss=8.5347]\n",
      "Training:  29%|â–ˆâ–ˆâ–Š       | 226/788 [00:03<00:08, 63.63it/s, loss=8.6428]\n",
      "Training:  29%|â–ˆâ–ˆâ–Š       | 226/788 [00:03<00:08, 63.63it/s, loss=8.8297]\n",
      "Training:  29%|â–ˆâ–ˆâ–Š       | 226/788 [00:03<00:08, 63.63it/s, loss=8.5931]\n",
      "Training:  29%|â–ˆâ–ˆâ–Š       | 226/788 [00:03<00:08, 63.63it/s, loss=8.7276]\n",
      "Training:  29%|â–ˆâ–ˆâ–Š       | 226/788 [00:03<00:08, 63.63it/s, loss=8.5901]\n",
      "Training:  29%|â–ˆâ–ˆâ–Š       | 226/788 [00:03<00:08, 63.63it/s, loss=8.7339]\n",
      "Training:  29%|â–ˆâ–ˆâ–Š       | 226/788 [00:03<00:08, 63.63it/s, loss=8.6130]\n",
      "Training:  30%|â–ˆâ–ˆâ–‰       | 233/788 [00:03<00:08, 65.03it/s, loss=8.6130]\n",
      "Training:  30%|â–ˆâ–ˆâ–‰       | 233/788 [00:03<00:08, 65.03it/s, loss=8.7843]\n",
      "Training:  30%|â–ˆâ–ˆâ–‰       | 233/788 [00:03<00:08, 65.03it/s, loss=8.5015]\n",
      "Training:  30%|â–ˆâ–ˆâ–‰       | 233/788 [00:03<00:08, 65.03it/s, loss=8.5560]\n",
      "Training:  30%|â–ˆâ–ˆâ–‰       | 233/788 [00:03<00:08, 65.03it/s, loss=8.4394]\n",
      "Training:  30%|â–ˆâ–ˆâ–‰       | 233/788 [00:03<00:08, 65.03it/s, loss=8.5890]\n",
      "Training:  30%|â–ˆâ–ˆâ–‰       | 233/788 [00:03<00:08, 65.03it/s, loss=8.7652]\n",
      "Training:  30%|â–ˆâ–ˆâ–‰       | 233/788 [00:03<00:08, 65.03it/s, loss=8.6131]\n",
      "Training:  30%|â–ˆâ–ˆâ–‰       | 233/788 [00:03<00:08, 65.03it/s, loss=8.6166]\n",
      "Training:  31%|â–ˆâ–ˆâ–ˆ       | 241/788 [00:03<00:08, 66.64it/s, loss=8.6166]\n",
      "Training:  31%|â–ˆâ–ˆâ–ˆ       | 241/788 [00:03<00:08, 66.64it/s, loss=8.7494]\n",
      "Training:  31%|â–ˆâ–ˆâ–ˆ       | 241/788 [00:03<00:08, 66.64it/s, loss=8.5474]\n",
      "Training:  31%|â–ˆâ–ˆâ–ˆ       | 241/788 [00:03<00:08, 66.64it/s, loss=8.6132]\n",
      "Training:  31%|â–ˆâ–ˆâ–ˆ       | 241/788 [00:03<00:08, 66.64it/s, loss=8.6500]\n",
      "Training:  31%|â–ˆâ–ˆâ–ˆ       | 241/788 [00:03<00:08, 66.64it/s, loss=8.6946]\n",
      "Training:  31%|â–ˆâ–ˆâ–ˆ       | 241/788 [00:03<00:08, 66.64it/s, loss=8.4870]\n",
      "Training:  31%|â–ˆâ–ˆâ–ˆ       | 241/788 [00:03<00:08, 66.64it/s, loss=8.5981]\n",
      "Training:  31%|â–ˆâ–ˆâ–ˆ       | 241/788 [00:03<00:08, 66.64it/s, loss=8.5921]\n",
      "Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 249/788 [00:03<00:07, 68.08it/s, loss=8.5921]\n",
      "Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 249/788 [00:03<00:07, 68.08it/s, loss=8.5806]\n",
      "Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 249/788 [00:03<00:07, 68.08it/s, loss=8.5289]\n",
      "Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 249/788 [00:04<00:07, 68.08it/s, loss=8.6593]\n",
      "Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 249/788 [00:04<00:07, 68.08it/s, loss=8.5015]\n",
      "Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 249/788 [00:04<00:07, 68.08it/s, loss=8.6144]\n",
      "Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 249/788 [00:04<00:07, 68.08it/s, loss=8.6007]\n",
      "Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 249/788 [00:04<00:07, 68.08it/s, loss=8.6346]\n",
      "Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 256/788 [00:04<00:07, 67.33it/s, loss=8.6346]\n",
      "Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 256/788 [00:04<00:07, 67.33it/s, loss=8.7302]\n",
      "Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 256/788 [00:04<00:07, 67.33it/s, loss=8.6934]\n",
      "Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 256/788 [00:04<00:07, 67.33it/s, loss=8.7396]\n",
      "Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 256/788 [00:04<00:07, 67.33it/s, loss=8.6760]\n",
      "Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 256/788 [00:04<00:07, 67.33it/s, loss=8.5496]\n",
      "Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 256/788 [00:04<00:07, 67.33it/s, loss=8.6352]\n",
      "Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 256/788 [00:04<00:07, 67.33it/s, loss=8.7575]\n",
      "Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 256/788 [00:04<00:07, 67.33it/s, loss=8.6098]\n",
      "Training:  34%|â–ˆâ–ˆâ–ˆâ–      | 264/788 [00:04<00:07, 68.27it/s, loss=8.6098]\n",
      "Training:  34%|â–ˆâ–ˆâ–ˆâ–      | 264/788 [00:04<00:07, 68.27it/s, loss=8.5781]\n",
      "Training:  34%|â–ˆâ–ˆâ–ˆâ–      | 264/788 [00:04<00:07, 68.27it/s, loss=8.6600]\n",
      "Training:  34%|â–ˆâ–ˆâ–ˆâ–      | 264/788 [00:04<00:07, 68.27it/s, loss=8.5568]\n",
      "Training:  34%|â–ˆâ–ˆâ–ˆâ–      | 264/788 [00:04<00:07, 68.27it/s, loss=8.5689]\n",
      "Training:  34%|â–ˆâ–ˆâ–ˆâ–      | 264/788 [00:04<00:07, 68.27it/s, loss=8.5540]\n",
      "Training:  34%|â–ˆâ–ˆâ–ˆâ–      | 264/788 [00:04<00:07, 68.27it/s, loss=8.4956]\n",
      "Training:  34%|â–ˆâ–ˆâ–ˆâ–      | 264/788 [00:04<00:07, 68.27it/s, loss=8.6029]\n",
      "Training:  34%|â–ˆâ–ˆâ–ˆâ–      | 264/788 [00:04<00:07, 68.27it/s, loss=8.6225]\n",
      "Training:  35%|â–ˆâ–ˆâ–ˆâ–      | 272/788 [00:04<00:07, 69.26it/s, loss=8.6225]\n",
      "Training:  35%|â–ˆâ–ˆâ–ˆâ–      | 272/788 [00:04<00:07, 69.26it/s, loss=8.5654]\n",
      "Training:  35%|â–ˆâ–ˆâ–ˆâ–      | 272/788 [00:04<00:07, 69.26it/s, loss=8.3963]\n",
      "Training:  35%|â–ˆâ–ˆâ–ˆâ–      | 272/788 [00:04<00:07, 69.26it/s, loss=8.5173]\n",
      "Training:  35%|â–ˆâ–ˆâ–ˆâ–      | 272/788 [00:04<00:07, 69.26it/s, loss=8.6132]\n",
      "Training:  35%|â–ˆâ–ˆâ–ˆâ–      | 272/788 [00:04<00:07, 69.26it/s, loss=8.7032]\n",
      "Training:  35%|â–ˆâ–ˆâ–ˆâ–      | 272/788 [00:04<00:07, 69.26it/s, loss=8.6962]\n",
      "Training:  35%|â–ˆâ–ˆâ–ˆâ–      | 272/788 [00:04<00:07, 69.26it/s, loss=8.5358]\n",
      "Training:  35%|â–ˆâ–ˆâ–ˆâ–      | 272/788 [00:04<00:07, 69.26it/s, loss=8.6429]\n",
      "Training:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 280/788 [00:04<00:07, 69.96it/s, loss=8.6429]\n",
      "Training:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 280/788 [00:04<00:07, 69.96it/s, loss=8.6837]\n",
      "Training:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 280/788 [00:04<00:07, 69.96it/s, loss=8.4768]\n",
      "Training:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 280/788 [00:04<00:07, 69.96it/s, loss=8.6012]\n",
      "Training:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 280/788 [00:04<00:07, 69.96it/s, loss=8.6793]\n",
      "Training:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 280/788 [00:04<00:07, 69.96it/s, loss=8.4702]\n",
      "Training:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 280/788 [00:04<00:07, 69.96it/s, loss=8.6025]\n",
      "Training:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 280/788 [00:04<00:07, 69.96it/s, loss=8.6876]\n",
      "Training:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 280/788 [00:04<00:07, 69.96it/s, loss=8.5282]\n",
      "Training:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 288/788 [00:04<00:07, 70.45it/s, loss=8.5282]\n",
      "Training:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 288/788 [00:04<00:07, 70.45it/s, loss=8.6571]\n",
      "Training:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 288/788 [00:04<00:07, 70.45it/s, loss=8.5410]\n",
      "Training:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 288/788 [00:04<00:07, 70.45it/s, loss=8.6063]\n",
      "Training:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 288/788 [00:04<00:07, 70.45it/s, loss=8.5899]\n",
      "Training:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 288/788 [00:04<00:07, 70.45it/s, loss=8.5363]\n",
      "Training:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 288/788 [00:04<00:07, 70.45it/s, loss=8.5400]\n",
      "Training:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 288/788 [00:04<00:07, 70.45it/s, loss=8.5286]\n",
      "Training:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 288/788 [00:04<00:07, 70.45it/s, loss=8.6048]\n",
      "Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 296/788 [00:04<00:07, 69.69it/s, loss=8.6048]\n",
      "Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 296/788 [00:04<00:07, 69.69it/s, loss=8.6143]\n",
      "Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 296/788 [00:04<00:07, 69.69it/s, loss=8.6627]\n",
      "Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 296/788 [00:04<00:07, 69.69it/s, loss=8.6639]\n",
      "Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 296/788 [00:04<00:07, 69.69it/s, loss=8.5168]\n",
      "Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 296/788 [00:04<00:07, 69.69it/s, loss=8.5166]\n",
      "Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 296/788 [00:04<00:07, 69.69it/s, loss=8.4876]\n",
      "Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 296/788 [00:04<00:07, 69.69it/s, loss=8.5475]\n",
      "Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 303/788 [00:04<00:06, 69.34it/s, loss=8.5475]\n",
      "Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 303/788 [00:04<00:06, 69.34it/s, loss=8.5153]\n",
      "Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 303/788 [00:04<00:06, 69.34it/s, loss=8.5307]\n",
      "Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 303/788 [00:04<00:06, 69.34it/s, loss=8.4739]\n",
      "Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 303/788 [00:04<00:06, 69.34it/s, loss=8.6691]\n",
      "Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 303/788 [00:04<00:06, 69.34it/s, loss=8.5017]\n",
      "Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 303/788 [00:04<00:06, 69.34it/s, loss=8.5881]\n",
      "Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 303/788 [00:04<00:06, 69.34it/s, loss=8.7660]\n",
      "Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 303/788 [00:04<00:06, 69.34it/s, loss=8.5022]\n",
      "Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 311/788 [00:04<00:06, 70.13it/s, loss=8.5022]\n",
      "Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 311/788 [00:04<00:06, 70.13it/s, loss=8.4572]\n",
      "Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 311/788 [00:04<00:06, 70.13it/s, loss=8.5389]\n",
      "Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 311/788 [00:04<00:06, 70.13it/s, loss=8.6261]\n",
      "Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 311/788 [00:04<00:06, 70.13it/s, loss=8.4592]\n",
      "Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 311/788 [00:04<00:06, 70.13it/s, loss=8.4845]\n",
      "Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 311/788 [00:04<00:06, 70.13it/s, loss=8.4838]\n",
      "Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 311/788 [00:04<00:06, 70.13it/s, loss=8.5330]\n",
      "Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 311/788 [00:04<00:06, 70.13it/s, loss=8.6737]\n",
      "Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 319/788 [00:04<00:06, 70.37it/s, loss=8.6737]\n",
      "Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 319/788 [00:04<00:06, 70.37it/s, loss=8.2639]\n",
      "Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 319/788 [00:04<00:06, 70.37it/s, loss=8.4878]\n",
      "Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 319/788 [00:05<00:06, 70.37it/s, loss=8.4196]\n",
      "Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 319/788 [00:05<00:06, 70.37it/s, loss=8.6054]\n",
      "Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 319/788 [00:05<00:06, 70.37it/s, loss=8.6718]\n",
      "Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 319/788 [00:05<00:06, 70.37it/s, loss=8.4984]\n",
      "Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 319/788 [00:05<00:06, 70.37it/s, loss=8.6176]\n",
      "Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 319/788 [00:05<00:06, 70.37it/s, loss=8.3386]\n",
      "Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 327/788 [00:05<00:06, 70.70it/s, loss=8.3386]\n",
      "Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 327/788 [00:05<00:06, 70.70it/s, loss=8.5600]\n",
      "Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 327/788 [00:05<00:06, 70.70it/s, loss=8.5410]\n",
      "Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 327/788 [00:05<00:06, 70.70it/s, loss=8.5217]\n",
      "Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 327/788 [00:05<00:06, 70.70it/s, loss=8.5929]\n",
      "Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 327/788 [00:05<00:06, 70.70it/s, loss=8.7223]\n",
      "Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 327/788 [00:05<00:06, 70.70it/s, loss=8.5879]\n",
      "Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 327/788 [00:05<00:06, 70.70it/s, loss=8.5426]\n",
      "Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 327/788 [00:05<00:06, 70.70it/s, loss=8.5301]\n",
      "Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 335/788 [00:05<00:06, 70.94it/s, loss=8.5301]\n",
      "Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 335/788 [00:05<00:06, 70.94it/s, loss=8.5735]\n",
      "Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 335/788 [00:05<00:06, 70.94it/s, loss=8.5531]\n",
      "Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 335/788 [00:05<00:06, 70.94it/s, loss=8.6185]\n",
      "Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 335/788 [00:05<00:06, 70.94it/s, loss=8.6491]\n",
      "Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 335/788 [00:05<00:06, 70.94it/s, loss=8.6596]\n",
      "Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 335/788 [00:05<00:06, 70.94it/s, loss=8.6992]\n",
      "Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 335/788 [00:05<00:06, 70.94it/s, loss=8.4542]\n",
      "Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 335/788 [00:05<00:06, 70.94it/s, loss=8.6570]\n",
      "Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 343/788 [00:05<00:06, 70.44it/s, loss=8.6570]\n",
      "Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 343/788 [00:05<00:06, 70.44it/s, loss=8.6190]\n",
      "Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 343/788 [00:05<00:06, 70.44it/s, loss=8.5322]\n",
      "Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 343/788 [00:05<00:06, 70.44it/s, loss=8.6226]\n",
      "Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 343/788 [00:05<00:06, 70.44it/s, loss=8.6372]\n",
      "Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 343/788 [00:05<00:06, 70.44it/s, loss=8.4720]\n",
      "Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 343/788 [00:05<00:06, 70.44it/s, loss=8.6242]\n",
      "Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 343/788 [00:05<00:06, 70.44it/s, loss=8.5279]\n",
      "Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 343/788 [00:05<00:06, 70.44it/s, loss=8.7142]\n",
      "Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 351/788 [00:05<00:06, 70.46it/s, loss=8.7142]\n",
      "Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 351/788 [00:05<00:06, 70.46it/s, loss=8.5607]\n",
      "Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 351/788 [00:05<00:06, 70.46it/s, loss=8.8027]\n",
      "Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 351/788 [00:05<00:06, 70.46it/s, loss=8.4770]\n",
      "Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 351/788 [00:05<00:06, 70.46it/s, loss=8.5005]\n",
      "Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 351/788 [00:05<00:06, 70.46it/s, loss=8.4519]\n",
      "Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 351/788 [00:05<00:06, 70.46it/s, loss=8.5062]\n",
      "Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 351/788 [00:05<00:06, 70.46it/s, loss=8.4830]\n",
      "Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 351/788 [00:05<00:06, 70.46it/s, loss=8.5152]\n",
      "Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 359/788 [00:05<00:06, 70.58it/s, loss=8.5152]\n",
      "Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 359/788 [00:05<00:06, 70.58it/s, loss=8.8001]\n",
      "Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 359/788 [00:05<00:06, 70.58it/s, loss=8.4179]\n",
      "Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 359/788 [00:05<00:06, 70.58it/s, loss=8.4797]\n",
      "Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 359/788 [00:05<00:06, 70.58it/s, loss=8.6704]\n",
      "Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 359/788 [00:05<00:06, 70.58it/s, loss=8.5891]\n",
      "Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 359/788 [00:05<00:06, 70.58it/s, loss=8.4195]\n",
      "Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 359/788 [00:05<00:06, 70.58it/s, loss=8.3802]\n",
      "Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 359/788 [00:05<00:06, 70.58it/s, loss=8.5010]\n",
      "Training:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 367/788 [00:05<00:05, 70.74it/s, loss=8.5010]\n",
      "Training:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 367/788 [00:05<00:05, 70.74it/s, loss=8.5463]\n",
      "Training:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 367/788 [00:05<00:05, 70.74it/s, loss=8.5355]\n",
      "Training:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 367/788 [00:05<00:05, 70.74it/s, loss=8.6500]\n",
      "Training:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 367/788 [00:05<00:05, 70.74it/s, loss=8.4567]\n",
      "Training:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 367/788 [00:05<00:05, 70.74it/s, loss=8.5318]\n",
      "Training:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 367/788 [00:05<00:05, 70.74it/s, loss=8.4696]\n",
      "Training:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 367/788 [00:05<00:05, 70.74it/s, loss=8.5469]\n",
      "Training:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 367/788 [00:05<00:05, 70.74it/s, loss=8.5255]\n",
      "Training:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 375/788 [00:05<00:05, 70.83it/s, loss=8.5255]\n",
      "Training:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 375/788 [00:05<00:05, 70.83it/s, loss=8.6198]\n",
      "Training:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 375/788 [00:05<00:05, 70.83it/s, loss=8.4982]\n",
      "Training:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 375/788 [00:05<00:05, 70.83it/s, loss=8.3578]\n",
      "Training:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 375/788 [00:05<00:05, 70.83it/s, loss=8.4957]\n",
      "Training:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 375/788 [00:05<00:05, 70.83it/s, loss=8.6331]\n",
      "Training:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 375/788 [00:05<00:05, 70.83it/s, loss=8.5972]\n",
      "Training:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 375/788 [00:05<00:05, 70.83it/s, loss=8.6482]\n",
      "Training:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 375/788 [00:05<00:05, 70.83it/s, loss=8.5193]\n",
      "Training:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 383/788 [00:05<00:05, 71.14it/s, loss=8.5193]\n",
      "Training:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 383/788 [00:05<00:05, 71.14it/s, loss=8.7614]\n",
      "Training:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 383/788 [00:05<00:05, 71.14it/s, loss=8.6250]\n",
      "Training:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 383/788 [00:05<00:05, 71.14it/s, loss=8.5727]\n",
      "Training:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 383/788 [00:05<00:05, 71.14it/s, loss=8.6549]\n",
      "Training:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 383/788 [00:05<00:05, 71.14it/s, loss=8.7000]\n",
      "Training:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 383/788 [00:05<00:05, 71.14it/s, loss=8.4456]\n",
      "Training:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 383/788 [00:05<00:05, 71.14it/s, loss=8.6180]\n",
      "Training:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 383/788 [00:05<00:05, 71.14it/s, loss=8.3751]\n",
      "Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 391/788 [00:05<00:05, 70.95it/s, loss=8.3751]\n",
      "Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 391/788 [00:05<00:05, 70.95it/s, loss=8.4186]\n",
      "Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 391/788 [00:06<00:05, 70.95it/s, loss=8.5346]\n",
      "Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 391/788 [00:06<00:05, 70.95it/s, loss=8.2569]\n",
      "Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 391/788 [00:06<00:05, 70.95it/s, loss=8.4498]\n",
      "Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 391/788 [00:06<00:05, 70.95it/s, loss=8.7263]\n",
      "Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 391/788 [00:06<00:05, 70.95it/s, loss=8.4603]\n",
      "Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 391/788 [00:06<00:05, 70.95it/s, loss=8.4587]\n",
      "Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 391/788 [00:06<00:05, 70.95it/s, loss=8.5042]\n",
      "Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 399/788 [00:06<00:05, 71.05it/s, loss=8.5042]\n",
      "Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 399/788 [00:06<00:05, 71.05it/s, loss=8.4310]\n",
      "Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 399/788 [00:06<00:05, 71.05it/s, loss=8.4902]\n",
      "Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 399/788 [00:06<00:05, 71.05it/s, loss=8.5560]\n",
      "Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 399/788 [00:06<00:05, 71.05it/s, loss=8.5563]\n",
      "Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 399/788 [00:06<00:05, 71.05it/s, loss=8.5589]\n",
      "Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 399/788 [00:06<00:05, 71.05it/s, loss=8.4769]\n",
      "Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 399/788 [00:06<00:05, 71.05it/s, loss=8.4792]\n",
      "Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 399/788 [00:06<00:05, 71.05it/s, loss=8.5945]\n",
      "Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 407/788 [00:06<00:05, 70.83it/s, loss=8.5945]\n",
      "Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 407/788 [00:06<00:05, 70.83it/s, loss=8.5078]\n",
      "Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 407/788 [00:06<00:05, 70.83it/s, loss=8.3923]\n",
      "Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 407/788 [00:06<00:05, 70.83it/s, loss=8.4387]\n",
      "Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 407/788 [00:06<00:05, 70.83it/s, loss=8.5563]\n",
      "Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 407/788 [00:06<00:05, 70.83it/s, loss=8.2923]\n",
      "Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 407/788 [00:06<00:05, 70.83it/s, loss=8.3765]\n",
      "Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 407/788 [00:06<00:05, 70.83it/s, loss=8.4785]\n",
      "Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 407/788 [00:06<00:05, 70.83it/s, loss=8.3961]\n",
      "Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 415/788 [00:06<00:05, 71.03it/s, loss=8.3961]\n",
      "Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 415/788 [00:06<00:05, 71.03it/s, loss=8.5001]\n",
      "Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 415/788 [00:06<00:05, 71.03it/s, loss=8.4384]\n",
      "Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 415/788 [00:06<00:05, 71.03it/s, loss=8.4735]\n",
      "Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 415/788 [00:06<00:05, 71.03it/s, loss=8.6843]\n",
      "Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 415/788 [00:06<00:05, 71.03it/s, loss=8.5286]\n",
      "Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 415/788 [00:06<00:05, 71.03it/s, loss=8.5780]\n",
      "Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 415/788 [00:06<00:05, 71.03it/s, loss=8.4202]\n",
      "Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 415/788 [00:06<00:05, 71.03it/s, loss=8.5713]\n",
      "Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 423/788 [00:06<00:05, 71.02it/s, loss=8.5713]\n",
      "Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 423/788 [00:06<00:05, 71.02it/s, loss=8.2801]\n",
      "Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 423/788 [00:06<00:05, 71.02it/s, loss=8.5533]\n",
      "Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 423/788 [00:06<00:05, 71.02it/s, loss=8.5777]\n",
      "Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 423/788 [00:06<00:05, 71.02it/s, loss=8.4716]\n",
      "Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 423/788 [00:06<00:05, 71.02it/s, loss=8.4734]\n",
      "Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 423/788 [00:06<00:05, 71.02it/s, loss=8.3097]\n",
      "Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 423/788 [00:06<00:05, 71.02it/s, loss=8.5834]\n",
      "Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 423/788 [00:06<00:05, 71.02it/s, loss=8.5877]\n",
      "Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 431/788 [00:06<00:05, 71.16it/s, loss=8.5877]\n",
      "Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 431/788 [00:06<00:05, 71.16it/s, loss=8.6677]\n",
      "Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 431/788 [00:06<00:05, 71.16it/s, loss=8.3437]\n",
      "Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 431/788 [00:06<00:05, 71.16it/s, loss=8.4096]\n",
      "Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 431/788 [00:06<00:05, 71.16it/s, loss=8.7295]\n",
      "Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 431/788 [00:06<00:05, 71.16it/s, loss=8.3936]\n",
      "Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 431/788 [00:06<00:05, 71.16it/s, loss=8.2972]\n",
      "Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 431/788 [00:06<00:05, 71.16it/s, loss=8.4758]\n",
      "Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 431/788 [00:06<00:05, 71.16it/s, loss=8.4073]\n",
      "Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 439/788 [00:06<00:04, 70.97it/s, loss=8.4073]\n",
      "Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 439/788 [00:06<00:04, 70.97it/s, loss=8.5894]\n",
      "Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 439/788 [00:06<00:04, 70.97it/s, loss=8.5493]\n",
      "Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 439/788 [00:06<00:04, 70.97it/s, loss=8.5448]\n",
      "Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 439/788 [00:06<00:04, 70.97it/s, loss=8.2936]\n",
      "Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 439/788 [00:06<00:04, 70.97it/s, loss=8.4648]\n",
      "Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 439/788 [00:06<00:04, 70.97it/s, loss=8.4420]\n",
      "Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 439/788 [00:06<00:04, 70.97it/s, loss=8.5726]\n",
      "Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 439/788 [00:06<00:04, 70.97it/s, loss=8.5039]\n",
      "Training:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 447/788 [00:06<00:04, 71.05it/s, loss=8.5039]\n",
      "Training:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 447/788 [00:06<00:04, 71.05it/s, loss=8.6611]\n",
      "Training:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 447/788 [00:06<00:04, 71.05it/s, loss=8.6787]\n",
      "Training:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 447/788 [00:06<00:04, 71.05it/s, loss=8.5905]\n",
      "Training:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 447/788 [00:06<00:04, 71.05it/s, loss=8.3925]\n",
      "Training:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 447/788 [00:06<00:04, 71.05it/s, loss=8.3361]\n",
      "Training:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 447/788 [00:06<00:04, 71.05it/s, loss=8.4098]\n",
      "Training:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 447/788 [00:06<00:04, 71.05it/s, loss=8.4421]\n",
      "Training:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 447/788 [00:06<00:04, 71.05it/s, loss=8.4310]\n",
      "Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 455/788 [00:06<00:04, 70.78it/s, loss=8.4310]\n",
      "Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 455/788 [00:06<00:04, 70.78it/s, loss=8.5338]\n",
      "Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 455/788 [00:06<00:04, 70.78it/s, loss=8.1985]\n",
      "Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 455/788 [00:06<00:04, 70.78it/s, loss=8.3585]\n",
      "Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 455/788 [00:06<00:04, 70.78it/s, loss=8.3987]\n",
      "Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 455/788 [00:06<00:04, 70.78it/s, loss=8.4572]\n",
      "Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 455/788 [00:06<00:04, 70.78it/s, loss=8.4543]\n",
      "Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 455/788 [00:06<00:04, 70.78it/s, loss=8.5829]\n",
      "Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 455/788 [00:06<00:04, 70.78it/s, loss=8.5646]\n",
      "Training:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 463/788 [00:06<00:04, 70.71it/s, loss=8.5646]\n",
      "Training:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 463/788 [00:07<00:04, 70.71it/s, loss=8.4764]\n",
      "Training:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 463/788 [00:07<00:04, 70.71it/s, loss=8.5393]\n",
      "Training:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 463/788 [00:07<00:04, 70.71it/s, loss=8.5077]\n",
      "Training:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 463/788 [00:07<00:04, 70.71it/s, loss=8.5722]\n",
      "Training:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 463/788 [00:07<00:04, 70.71it/s, loss=8.6152]\n",
      "Training:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 463/788 [00:07<00:04, 70.71it/s, loss=8.5016]\n",
      "Training:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 463/788 [00:07<00:04, 70.71it/s, loss=8.4063]\n",
      "Training:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 463/788 [00:07<00:04, 70.71it/s, loss=8.4477]\n",
      "Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 471/788 [00:07<00:04, 70.65it/s, loss=8.4477]\n",
      "Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 471/788 [00:07<00:04, 70.65it/s, loss=8.5529]\n",
      "Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 471/788 [00:07<00:04, 70.65it/s, loss=8.5969]\n",
      "Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 471/788 [00:07<00:04, 70.65it/s, loss=8.3160]\n",
      "Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 471/788 [00:07<00:04, 70.65it/s, loss=8.6253]\n",
      "Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 471/788 [00:07<00:04, 70.65it/s, loss=8.5702]\n",
      "Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 471/788 [00:07<00:04, 70.65it/s, loss=8.5606]\n",
      "Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 471/788 [00:07<00:04, 70.65it/s, loss=8.5899]\n",
      "Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 471/788 [00:07<00:04, 70.65it/s, loss=8.4722]\n",
      "Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 479/788 [00:07<00:04, 70.52it/s, loss=8.4722]\n",
      "Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 479/788 [00:07<00:04, 70.52it/s, loss=8.6659]\n",
      "Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 479/788 [00:07<00:04, 70.52it/s, loss=8.3789]\n",
      "Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 479/788 [00:07<00:04, 70.52it/s, loss=8.5312]\n",
      "Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 479/788 [00:07<00:04, 70.52it/s, loss=8.4317]\n",
      "Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 479/788 [00:07<00:04, 70.52it/s, loss=8.4703]\n",
      "Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 479/788 [00:07<00:04, 70.52it/s, loss=8.5524]\n",
      "Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 479/788 [00:07<00:04, 70.52it/s, loss=8.4022]\n",
      "Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 479/788 [00:07<00:04, 70.52it/s, loss=8.3224]\n",
      "Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 487/788 [00:07<00:04, 70.97it/s, loss=8.3224]\n",
      "Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 487/788 [00:07<00:04, 70.97it/s, loss=8.5585]\n",
      "Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 487/788 [00:07<00:04, 70.97it/s, loss=8.4740]\n",
      "Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 487/788 [00:07<00:04, 70.97it/s, loss=8.1935]\n",
      "Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 487/788 [00:07<00:04, 70.97it/s, loss=8.5691]\n",
      "Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 487/788 [00:07<00:04, 70.97it/s, loss=8.3076]\n",
      "Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 487/788 [00:07<00:04, 70.97it/s, loss=8.4233]\n",
      "Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 487/788 [00:07<00:04, 70.97it/s, loss=8.4409]\n",
      "Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 487/788 [00:07<00:04, 70.97it/s, loss=8.7410]\n",
      "Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 495/788 [00:07<00:04, 70.73it/s, loss=8.7410]\n",
      "Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 495/788 [00:07<00:04, 70.73it/s, loss=8.5678]\n",
      "Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 495/788 [00:07<00:04, 70.73it/s, loss=8.6059]\n",
      "Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 495/788 [00:07<00:04, 70.73it/s, loss=8.5117]\n",
      "Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 495/788 [00:07<00:04, 70.73it/s, loss=8.4807]\n",
      "Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 495/788 [00:07<00:04, 70.73it/s, loss=8.5241]\n",
      "Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 495/788 [00:07<00:04, 70.73it/s, loss=8.4357]\n",
      "Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 495/788 [00:07<00:04, 70.73it/s, loss=8.4124]\n",
      "Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 495/788 [00:07<00:04, 70.73it/s, loss=8.5077]\n",
      "Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 503/788 [00:07<00:04, 71.16it/s, loss=8.5077]\n",
      "Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 503/788 [00:07<00:04, 71.16it/s, loss=8.5051]\n",
      "Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 503/788 [00:07<00:04, 71.16it/s, loss=8.5749]\n",
      "Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 503/788 [00:07<00:04, 71.16it/s, loss=8.5075]\n",
      "Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 503/788 [00:07<00:04, 71.16it/s, loss=8.4427]\n",
      "Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 503/788 [00:07<00:04, 71.16it/s, loss=8.4908]\n",
      "Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 503/788 [00:07<00:04, 71.16it/s, loss=8.6444]\n",
      "Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 503/788 [00:07<00:04, 71.16it/s, loss=8.5230]\n",
      "Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 503/788 [00:07<00:04, 71.16it/s, loss=8.4835]\n",
      "Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 511/788 [00:07<00:03, 71.12it/s, loss=8.4835]\n",
      "Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 511/788 [00:07<00:03, 71.12it/s, loss=8.5356]\n",
      "Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 511/788 [00:07<00:03, 71.12it/s, loss=8.3490]\n",
      "Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 511/788 [00:07<00:03, 71.12it/s, loss=8.2352]\n",
      "Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 511/788 [00:07<00:03, 71.12it/s, loss=8.4258]\n",
      "Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 511/788 [00:07<00:03, 71.12it/s, loss=8.4453]\n",
      "Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 511/788 [00:07<00:03, 71.12it/s, loss=8.3473]\n",
      "Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 511/788 [00:07<00:03, 71.12it/s, loss=8.1870]\n",
      "Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 511/788 [00:07<00:03, 71.12it/s, loss=8.5862]\n",
      "Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 519/788 [00:07<00:03, 71.15it/s, loss=8.5862]\n",
      "Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 519/788 [00:07<00:03, 71.15it/s, loss=8.4318]\n",
      "Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 519/788 [00:07<00:03, 71.15it/s, loss=8.4558]\n",
      "Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 519/788 [00:07<00:03, 71.15it/s, loss=8.5496]\n",
      "Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 519/788 [00:07<00:03, 71.15it/s, loss=8.3570]\n",
      "Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 519/788 [00:07<00:03, 71.15it/s, loss=8.7159]\n",
      "Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 519/788 [00:07<00:03, 71.15it/s, loss=8.7081]\n",
      "Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 519/788 [00:07<00:03, 71.15it/s, loss=8.5118]\n",
      "Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 519/788 [00:07<00:03, 71.15it/s, loss=8.6575]\n",
      "Training:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 527/788 [00:07<00:03, 71.61it/s, loss=8.6575]\n",
      "Training:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 527/788 [00:07<00:03, 71.61it/s, loss=8.4268]\n",
      "Training:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 527/788 [00:07<00:03, 71.61it/s, loss=8.4632]\n",
      "Training:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 527/788 [00:07<00:03, 71.61it/s, loss=8.2400]\n",
      "Training:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 527/788 [00:07<00:03, 71.61it/s, loss=8.4311]\n",
      "Training:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 527/788 [00:07<00:03, 71.61it/s, loss=8.4732]\n",
      "Training:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 527/788 [00:07<00:03, 71.61it/s, loss=8.5210]\n",
      "Training:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 527/788 [00:07<00:03, 71.61it/s, loss=8.5151]\n",
      "Training:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 527/788 [00:08<00:03, 71.61it/s, loss=8.5118]\n",
      "Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 535/788 [00:08<00:03, 71.35it/s, loss=8.5118]\n",
      "Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 535/788 [00:08<00:03, 71.35it/s, loss=8.4349]\n",
      "Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 535/788 [00:08<00:03, 71.35it/s, loss=8.5771]\n",
      "Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 535/788 [00:08<00:03, 71.35it/s, loss=8.4568]\n",
      "Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 535/788 [00:08<00:03, 71.35it/s, loss=8.5091]\n",
      "Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 535/788 [00:08<00:03, 71.35it/s, loss=8.6081]\n",
      "Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 535/788 [00:08<00:03, 71.35it/s, loss=8.4200]\n",
      "Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 535/788 [00:08<00:03, 71.35it/s, loss=8.5692]\n",
      "Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 535/788 [00:08<00:03, 71.35it/s, loss=8.5531]\n",
      "Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 543/788 [00:08<00:03, 71.62it/s, loss=8.5531]\n",
      "Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 543/788 [00:08<00:03, 71.62it/s, loss=8.3705]\n",
      "Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 543/788 [00:08<00:03, 71.62it/s, loss=8.3168]\n",
      "Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 543/788 [00:08<00:03, 71.62it/s, loss=8.5255]\n",
      "Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 543/788 [00:08<00:03, 71.62it/s, loss=8.4603]\n",
      "Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 543/788 [00:08<00:03, 71.62it/s, loss=8.1823]\n",
      "Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 543/788 [00:08<00:03, 71.62it/s, loss=8.3722]\n",
      "Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 543/788 [00:08<00:03, 71.62it/s, loss=8.4521]\n",
      "Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 543/788 [00:08<00:03, 71.62it/s, loss=8.4713]\n",
      "Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 551/788 [00:08<00:03, 71.51it/s, loss=8.4713]\n",
      "Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 551/788 [00:08<00:03, 71.51it/s, loss=8.2604]\n",
      "Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 551/788 [00:08<00:03, 71.51it/s, loss=8.5081]\n",
      "Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 551/788 [00:08<00:03, 71.51it/s, loss=8.3540]\n",
      "Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 551/788 [00:08<00:03, 71.51it/s, loss=8.6783]\n",
      "Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 551/788 [00:08<00:03, 71.51it/s, loss=8.4208]\n",
      "Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 551/788 [00:08<00:03, 71.51it/s, loss=8.4730]\n",
      "Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 551/788 [00:08<00:03, 71.51it/s, loss=8.4781]\n",
      "Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 551/788 [00:08<00:03, 71.51it/s, loss=8.4550]\n",
      "Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 559/788 [00:08<00:03, 71.12it/s, loss=8.4550]\n",
      "Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 559/788 [00:08<00:03, 71.12it/s, loss=8.5050]\n",
      "Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 559/788 [00:08<00:03, 71.12it/s, loss=8.4247]\n",
      "Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 559/788 [00:08<00:03, 71.12it/s, loss=8.3446]\n",
      "Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 559/788 [00:08<00:03, 71.12it/s, loss=8.4037]\n",
      "Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 559/788 [00:08<00:03, 71.12it/s, loss=8.4849]\n",
      "Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 559/788 [00:08<00:03, 71.12it/s, loss=8.5011]\n",
      "Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 559/788 [00:08<00:03, 71.12it/s, loss=8.4380]\n",
      "Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 559/788 [00:08<00:03, 71.12it/s, loss=8.4116]\n",
      "Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 567/788 [00:08<00:03, 71.34it/s, loss=8.4116]\n",
      "Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 567/788 [00:08<00:03, 71.34it/s, loss=8.4320]\n",
      "Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 567/788 [00:08<00:03, 71.34it/s, loss=8.4392]\n",
      "Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 567/788 [00:08<00:03, 71.34it/s, loss=8.4222]\n",
      "Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 567/788 [00:08<00:03, 71.34it/s, loss=8.4795]\n",
      "Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 567/788 [00:08<00:03, 71.34it/s, loss=8.4138]\n",
      "Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 567/788 [00:08<00:03, 71.34it/s, loss=8.3907]\n",
      "Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 567/788 [00:08<00:03, 71.34it/s, loss=8.6426]\n",
      "Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 567/788 [00:08<00:03, 71.34it/s, loss=8.5128]\n",
      "Training:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 575/788 [00:08<00:02, 71.23it/s, loss=8.5128]\n",
      "Training:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 575/788 [00:08<00:02, 71.23it/s, loss=8.3292]\n",
      "Training:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 575/788 [00:08<00:02, 71.23it/s, loss=8.4929]\n",
      "Training:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 575/788 [00:08<00:02, 71.23it/s, loss=8.3157]\n",
      "Training:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 575/788 [00:08<00:02, 71.23it/s, loss=8.4587]\n",
      "Training:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 575/788 [00:08<00:02, 71.23it/s, loss=8.3604]\n",
      "Training:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 575/788 [00:08<00:02, 71.23it/s, loss=8.4189]\n",
      "Training:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 575/788 [00:08<00:02, 71.23it/s, loss=8.3529]\n",
      "Training:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 575/788 [00:08<00:02, 71.23it/s, loss=8.3774]\n",
      "Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 583/788 [00:08<00:02, 71.54it/s, loss=8.3774]\n",
      "Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 583/788 [00:08<00:02, 71.54it/s, loss=8.4855]\n",
      "Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 583/788 [00:08<00:02, 71.54it/s, loss=8.5981]\n",
      "Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 583/788 [00:08<00:02, 71.54it/s, loss=8.4545]\n",
      "Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 583/788 [00:08<00:02, 71.54it/s, loss=8.2531]\n",
      "Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 583/788 [00:08<00:02, 71.54it/s, loss=8.5163]\n",
      "Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 583/788 [00:08<00:02, 71.54it/s, loss=8.4839]\n",
      "Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 583/788 [00:08<00:02, 71.54it/s, loss=8.1305]\n",
      "Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 583/788 [00:08<00:02, 71.54it/s, loss=8.4161]\n",
      "Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 591/788 [00:08<00:02, 71.27it/s, loss=8.4161]\n",
      "Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 591/788 [00:08<00:02, 71.27it/s, loss=8.4731]\n",
      "Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 591/788 [00:08<00:02, 71.27it/s, loss=8.3379]\n",
      "Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 591/788 [00:08<00:02, 71.27it/s, loss=8.3217]\n",
      "Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 591/788 [00:08<00:02, 71.27it/s, loss=8.3895]\n",
      "Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 591/788 [00:08<00:02, 71.27it/s, loss=8.3232]\n",
      "Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 591/788 [00:08<00:02, 71.27it/s, loss=8.6557]\n",
      "Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 591/788 [00:08<00:02, 71.27it/s, loss=8.4069]\n",
      "Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 591/788 [00:08<00:02, 71.27it/s, loss=8.4537]\n",
      "Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 599/788 [00:08<00:02, 71.51it/s, loss=8.4537]\n",
      "Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 599/788 [00:08<00:02, 71.51it/s, loss=8.5056]\n",
      "Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 599/788 [00:08<00:02, 71.51it/s, loss=8.4816]\n",
      "Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 599/788 [00:08<00:02, 71.51it/s, loss=8.3822]\n",
      "Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 599/788 [00:08<00:02, 71.51it/s, loss=8.3625]\n",
      "Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 599/788 [00:08<00:02, 71.51it/s, loss=8.3429]\n",
      "Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 599/788 [00:08<00:02, 71.51it/s, loss=8.3851]\n",
      "Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 599/788 [00:08<00:02, 71.51it/s, loss=8.3634]\n",
      "Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 599/788 [00:09<00:02, 71.51it/s, loss=8.5546]\n",
      "Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 607/788 [00:09<00:02, 71.44it/s, loss=8.5546]\n",
      "Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 607/788 [00:09<00:02, 71.44it/s, loss=8.3886]\n",
      "Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 607/788 [00:09<00:02, 71.44it/s, loss=8.4264]\n",
      "Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 607/788 [00:09<00:02, 71.44it/s, loss=8.3717]\n",
      "Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 607/788 [00:09<00:02, 71.44it/s, loss=8.1963]\n",
      "Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 607/788 [00:09<00:02, 71.44it/s, loss=8.5517]\n",
      "Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 607/788 [00:09<00:02, 71.44it/s, loss=8.4082]\n",
      "Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 607/788 [00:09<00:02, 71.44it/s, loss=8.5033]\n",
      "Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 607/788 [00:09<00:02, 71.44it/s, loss=8.4179]\n",
      "Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 615/788 [00:09<00:02, 71.49it/s, loss=8.4179]\n",
      "Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 615/788 [00:09<00:02, 71.49it/s, loss=8.4729]\n",
      "Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 615/788 [00:09<00:02, 71.49it/s, loss=8.5348]\n",
      "Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 615/788 [00:09<00:02, 71.49it/s, loss=8.6172]\n",
      "Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 615/788 [00:09<00:02, 71.49it/s, loss=8.4870]\n",
      "Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 615/788 [00:09<00:02, 71.49it/s, loss=8.5173]\n",
      "Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 615/788 [00:09<00:02, 71.49it/s, loss=8.4799]\n",
      "Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 615/788 [00:09<00:02, 71.49it/s, loss=8.3977]\n",
      "Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 615/788 [00:09<00:02, 71.49it/s, loss=8.3147]\n",
      "Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 623/788 [00:09<00:02, 71.46it/s, loss=8.3147]\n",
      "Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 623/788 [00:09<00:02, 71.46it/s, loss=8.3006]\n",
      "Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 623/788 [00:09<00:02, 71.46it/s, loss=8.4128]\n",
      "Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 623/788 [00:09<00:02, 71.46it/s, loss=8.3344]\n",
      "Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 623/788 [00:09<00:02, 71.46it/s, loss=8.4396]\n",
      "Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 623/788 [00:09<00:02, 71.46it/s, loss=8.4413]\n",
      "Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 623/788 [00:09<00:02, 71.46it/s, loss=8.4117]\n",
      "Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 623/788 [00:09<00:02, 71.46it/s, loss=8.4434]\n",
      "Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 623/788 [00:09<00:02, 71.46it/s, loss=8.4873]\n",
      "Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 631/788 [00:09<00:02, 71.45it/s, loss=8.4873]\n",
      "Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 631/788 [00:09<00:02, 71.45it/s, loss=8.6019]\n",
      "Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 631/788 [00:09<00:02, 71.45it/s, loss=8.5038]\n",
      "Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 631/788 [00:09<00:02, 71.45it/s, loss=8.5534]\n",
      "Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 631/788 [00:09<00:02, 71.45it/s, loss=8.2947]\n",
      "Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 631/788 [00:09<00:02, 71.45it/s, loss=8.1994]\n",
      "Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 631/788 [00:09<00:02, 71.45it/s, loss=8.2327]\n",
      "Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 631/788 [00:09<00:02, 71.45it/s, loss=8.4217]\n",
      "Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 631/788 [00:09<00:02, 71.45it/s, loss=8.4345]\n",
      "Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 639/788 [00:09<00:02, 70.88it/s, loss=8.4345]\n",
      "Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 639/788 [00:09<00:02, 70.88it/s, loss=8.2886]\n",
      "Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 639/788 [00:09<00:02, 70.88it/s, loss=8.5525]\n",
      "Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 639/788 [00:09<00:02, 70.88it/s, loss=8.4040]\n",
      "Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 639/788 [00:09<00:02, 70.88it/s, loss=8.4306]\n",
      "Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 639/788 [00:09<00:02, 70.88it/s, loss=8.3129]\n",
      "Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 639/788 [00:09<00:02, 70.88it/s, loss=8.4358]\n",
      "Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 639/788 [00:09<00:02, 70.88it/s, loss=8.7121]\n",
      "Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 639/788 [00:09<00:02, 70.88it/s, loss=8.4910]\n",
      "Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 647/788 [00:09<00:01, 71.18it/s, loss=8.4910]\n",
      "Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 647/788 [00:09<00:01, 71.18it/s, loss=8.5873]\n",
      "Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 647/788 [00:09<00:01, 71.18it/s, loss=8.5248]\n",
      "Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 647/788 [00:09<00:01, 71.18it/s, loss=8.3658]\n",
      "Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 647/788 [00:09<00:01, 71.18it/s, loss=8.4182]\n",
      "Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 647/788 [00:09<00:01, 71.18it/s, loss=8.4354]\n",
      "Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 647/788 [00:09<00:01, 71.18it/s, loss=8.4970]\n",
      "Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 647/788 [00:09<00:01, 71.18it/s, loss=8.0691]\n",
      "Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 647/788 [00:09<00:01, 71.18it/s, loss=8.2932]\n",
      "Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 655/788 [00:09<00:01, 71.48it/s, loss=8.2932]\n",
      "Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 655/788 [00:09<00:01, 71.48it/s, loss=8.6521]\n",
      "Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 655/788 [00:09<00:01, 71.48it/s, loss=8.4267]\n",
      "Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 655/788 [00:09<00:01, 71.48it/s, loss=8.4512]\n",
      "Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 655/788 [00:09<00:01, 71.48it/s, loss=8.4419]\n",
      "Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 655/788 [00:09<00:01, 71.48it/s, loss=8.5290]\n",
      "Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 655/788 [00:09<00:01, 71.48it/s, loss=8.2618]\n",
      "Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 655/788 [00:09<00:01, 71.48it/s, loss=8.5371]\n",
      "Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 655/788 [00:09<00:01, 71.48it/s, loss=8.3534]\n",
      "Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 663/788 [00:09<00:01, 71.08it/s, loss=8.3534]\n",
      "Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 663/788 [00:09<00:01, 71.08it/s, loss=8.5389]\n",
      "Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 663/788 [00:09<00:01, 71.08it/s, loss=8.4181]\n",
      "Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 663/788 [00:09<00:01, 71.08it/s, loss=8.3740]\n",
      "Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 663/788 [00:09<00:01, 71.08it/s, loss=8.3836]\n",
      "Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 663/788 [00:09<00:01, 71.08it/s, loss=8.3334]\n",
      "Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 663/788 [00:09<00:01, 71.08it/s, loss=8.2926]\n",
      "Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 663/788 [00:09<00:01, 71.08it/s, loss=8.4457]\n",
      "Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 663/788 [00:09<00:01, 71.08it/s, loss=8.5181]\n",
      "Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 671/788 [00:09<00:01, 70.99it/s, loss=8.5181]\n",
      "Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 671/788 [00:09<00:01, 70.99it/s, loss=8.3666]\n",
      "Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 671/788 [00:09<00:01, 70.99it/s, loss=8.5374]\n",
      "Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 671/788 [00:09<00:01, 70.99it/s, loss=8.4533]\n",
      "Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 671/788 [00:09<00:01, 70.99it/s, loss=8.4639]\n",
      "Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 671/788 [00:09<00:01, 70.99it/s, loss=8.4419]\n",
      "Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 671/788 [00:09<00:01, 70.99it/s, loss=8.5567]\n",
      "Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 671/788 [00:10<00:01, 70.99it/s, loss=8.3466]\n",
      "Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 671/788 [00:10<00:01, 70.99it/s, loss=8.3205]\n",
      "Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 679/788 [00:10<00:01, 70.86it/s, loss=8.3205]\n",
      "Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 679/788 [00:10<00:01, 70.86it/s, loss=8.4421]\n",
      "Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 679/788 [00:10<00:01, 70.86it/s, loss=8.2497]\n",
      "Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 679/788 [00:10<00:01, 70.86it/s, loss=8.4208]\n",
      "Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 679/788 [00:10<00:01, 70.86it/s, loss=8.2675]\n",
      "Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 679/788 [00:10<00:01, 70.86it/s, loss=8.2640]\n",
      "Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 679/788 [00:10<00:01, 70.86it/s, loss=8.5038]\n",
      "Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 679/788 [00:10<00:01, 70.86it/s, loss=8.3756]\n",
      "Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 679/788 [00:10<00:01, 70.86it/s, loss=8.2972]\n",
      "Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 687/788 [00:10<00:01, 71.13it/s, loss=8.2972]\n",
      "Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 687/788 [00:10<00:01, 71.13it/s, loss=8.5555]\n",
      "Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 687/788 [00:10<00:01, 71.13it/s, loss=8.6535]\n",
      "Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 687/788 [00:10<00:01, 71.13it/s, loss=8.2721]\n",
      "Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 687/788 [00:10<00:01, 71.13it/s, loss=8.5937]\n",
      "Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 687/788 [00:10<00:01, 71.13it/s, loss=8.4806]\n",
      "Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 687/788 [00:10<00:01, 71.13it/s, loss=8.4911]\n",
      "Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 687/788 [00:10<00:01, 71.13it/s, loss=8.2892]\n",
      "Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 687/788 [00:10<00:01, 71.13it/s, loss=8.5210]\n",
      "Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 695/788 [00:10<00:01, 71.23it/s, loss=8.5210]\n",
      "Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 695/788 [00:10<00:01, 71.23it/s, loss=8.5623]\n",
      "Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 695/788 [00:10<00:01, 71.23it/s, loss=8.1940]\n",
      "Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 695/788 [00:10<00:01, 71.23it/s, loss=8.3329]\n",
      "Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 695/788 [00:10<00:01, 71.23it/s, loss=8.4403]\n",
      "Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 695/788 [00:10<00:01, 71.23it/s, loss=8.4034]\n",
      "Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 695/788 [00:10<00:01, 71.23it/s, loss=8.5441]\n",
      "Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 695/788 [00:10<00:01, 71.23it/s, loss=8.4038]\n",
      "Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 695/788 [00:10<00:01, 71.23it/s, loss=8.4659]\n",
      "Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 703/788 [00:10<00:01, 70.99it/s, loss=8.4659]\n",
      "Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 703/788 [00:10<00:01, 70.99it/s, loss=8.4437]\n",
      "Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 703/788 [00:10<00:01, 70.99it/s, loss=8.2357]\n",
      "Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 703/788 [00:10<00:01, 70.99it/s, loss=8.6067]\n",
      "Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 703/788 [00:10<00:01, 70.99it/s, loss=8.3865]\n",
      "Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 703/788 [00:10<00:01, 70.99it/s, loss=8.2445]\n",
      "Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 703/788 [00:10<00:01, 70.99it/s, loss=8.2311]\n",
      "Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 703/788 [00:10<00:01, 70.99it/s, loss=8.5274]\n",
      "Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 703/788 [00:10<00:01, 70.99it/s, loss=8.4491]\n",
      "Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 711/788 [00:10<00:01, 71.03it/s, loss=8.4491]\n",
      "Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 711/788 [00:10<00:01, 71.03it/s, loss=8.3176]\n",
      "Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 711/788 [00:10<00:01, 71.03it/s, loss=8.2247]\n",
      "Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 711/788 [00:10<00:01, 71.03it/s, loss=8.2381]\n",
      "Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 711/788 [00:10<00:01, 71.03it/s, loss=8.4504]\n",
      "Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 711/788 [00:10<00:01, 71.03it/s, loss=8.2827]\n",
      "Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 711/788 [00:10<00:01, 71.03it/s, loss=8.4213]\n",
      "Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 711/788 [00:10<00:01, 71.03it/s, loss=8.3322]\n",
      "Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 711/788 [00:10<00:01, 71.03it/s, loss=8.3447]\n",
      "Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 719/788 [00:10<00:00, 71.16it/s, loss=8.3447]\n",
      "Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 719/788 [00:10<00:00, 71.16it/s, loss=8.4877]\n",
      "Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 719/788 [00:10<00:00, 71.16it/s, loss=8.3931]\n",
      "Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 719/788 [00:10<00:00, 71.16it/s, loss=8.6011]\n",
      "Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 719/788 [00:10<00:00, 71.16it/s, loss=8.5599]\n",
      "Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 719/788 [00:10<00:00, 71.16it/s, loss=8.4373]\n",
      "Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 719/788 [00:10<00:00, 71.16it/s, loss=8.3756]\n",
      "Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 719/788 [00:10<00:00, 71.16it/s, loss=8.3582]\n",
      "Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 719/788 [00:10<00:00, 71.16it/s, loss=8.4738]\n",
      "Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 727/788 [00:10<00:00, 71.16it/s, loss=8.4738]\n",
      "Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 727/788 [00:10<00:00, 71.16it/s, loss=8.3408]\n",
      "Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 727/788 [00:10<00:00, 71.16it/s, loss=8.2247]\n",
      "Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 727/788 [00:10<00:00, 71.16it/s, loss=8.5077]\n",
      "Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 727/788 [00:10<00:00, 71.16it/s, loss=8.2736]\n",
      "Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 727/788 [00:10<00:00, 71.16it/s, loss=8.5462]\n",
      "Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 727/788 [00:10<00:00, 71.16it/s, loss=8.3752]\n",
      "Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 727/788 [00:10<00:00, 71.16it/s, loss=8.3445]\n",
      "Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 727/788 [00:10<00:00, 71.16it/s, loss=8.4805]\n",
      "Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 735/788 [00:10<00:00, 70.93it/s, loss=8.4805]\n",
      "Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 735/788 [00:10<00:00, 70.93it/s, loss=8.7395]\n",
      "Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 735/788 [00:10<00:00, 70.93it/s, loss=8.4145]\n",
      "Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 735/788 [00:10<00:00, 70.93it/s, loss=8.3128]\n",
      "Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 735/788 [00:10<00:00, 70.93it/s, loss=8.3813]\n",
      "Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 735/788 [00:10<00:00, 70.93it/s, loss=8.4272]\n",
      "Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 735/788 [00:10<00:00, 70.93it/s, loss=8.3888]\n",
      "Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 735/788 [00:10<00:00, 70.93it/s, loss=8.3799]\n",
      "Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 735/788 [00:10<00:00, 70.93it/s, loss=8.1411]\n",
      "Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 743/788 [00:10<00:00, 71.27it/s, loss=8.1411]\n",
      "Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 743/788 [00:10<00:00, 71.27it/s, loss=8.3833]\n",
      "Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 743/788 [00:10<00:00, 71.27it/s, loss=8.4859]\n",
      "Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 743/788 [00:10<00:00, 71.27it/s, loss=8.2437]\n",
      "Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 743/788 [00:10<00:00, 71.27it/s, loss=8.2201]\n",
      "Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 743/788 [00:10<00:00, 71.27it/s, loss=8.5158]\n",
      "Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 743/788 [00:11<00:00, 71.27it/s, loss=8.3002]\n",
      "Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 743/788 [00:11<00:00, 71.27it/s, loss=8.4164]\n",
      "Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 743/788 [00:11<00:00, 71.27it/s, loss=8.2493]\n",
      "Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 751/788 [00:11<00:00, 71.39it/s, loss=8.2493]\n",
      "Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 751/788 [00:11<00:00, 71.39it/s, loss=8.3710]\n",
      "Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 751/788 [00:11<00:00, 71.39it/s, loss=8.5233]\n",
      "Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 751/788 [00:11<00:00, 71.39it/s, loss=8.4476]\n",
      "Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 751/788 [00:11<00:00, 71.39it/s, loss=8.3546]\n",
      "Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 751/788 [00:11<00:00, 71.39it/s, loss=8.3517]\n",
      "Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 751/788 [00:11<00:00, 71.39it/s, loss=8.3406]\n",
      "Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 751/788 [00:11<00:00, 71.39it/s, loss=8.2004]\n",
      "Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 751/788 [00:11<00:00, 71.39it/s, loss=8.5202]\n",
      "Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 759/788 [00:11<00:00, 71.33it/s, loss=8.5202]\n",
      "Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 759/788 [00:11<00:00, 71.33it/s, loss=8.4308]\n",
      "Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 759/788 [00:11<00:00, 71.33it/s, loss=8.3232]\n",
      "Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 759/788 [00:11<00:00, 71.33it/s, loss=8.5063]\n",
      "Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 759/788 [00:11<00:00, 71.33it/s, loss=8.4537]\n",
      "Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 759/788 [00:11<00:00, 71.33it/s, loss=8.3525]\n",
      "Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 759/788 [00:11<00:00, 71.33it/s, loss=8.2773]\n",
      "Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 759/788 [00:11<00:00, 71.33it/s, loss=8.5472]\n",
      "Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 759/788 [00:11<00:00, 71.33it/s, loss=8.3489]\n",
      "Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 767/788 [00:11<00:00, 71.34it/s, loss=8.3489]\n",
      "Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 767/788 [00:11<00:00, 71.34it/s, loss=8.4633]\n",
      "Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 767/788 [00:11<00:00, 71.34it/s, loss=8.4743]\n",
      "Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 767/788 [00:11<00:00, 71.34it/s, loss=8.1266]\n",
      "Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 767/788 [00:11<00:00, 71.34it/s, loss=8.6351]\n",
      "Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 767/788 [00:11<00:00, 71.34it/s, loss=8.3271]\n",
      "Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 767/788 [00:11<00:00, 71.34it/s, loss=8.4909]\n",
      "Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 767/788 [00:11<00:00, 71.34it/s, loss=8.4113]\n",
      "Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 767/788 [00:11<00:00, 71.34it/s, loss=8.3850]\n",
      "Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 775/788 [00:11<00:00, 71.23it/s, loss=8.3850]\n",
      "Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 775/788 [00:11<00:00, 71.23it/s, loss=8.1570]\n",
      "Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 775/788 [00:11<00:00, 71.23it/s, loss=8.3072]\n",
      "Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 775/788 [00:11<00:00, 71.23it/s, loss=8.4747]\n",
      "Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 775/788 [00:11<00:00, 71.23it/s, loss=8.4905]\n",
      "Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 775/788 [00:11<00:00, 71.23it/s, loss=8.4018]\n",
      "Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 775/788 [00:11<00:00, 71.23it/s, loss=8.3993]\n",
      "Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 775/788 [00:11<00:00, 71.23it/s, loss=8.5528]\n",
      "Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 775/788 [00:11<00:00, 71.23it/s, loss=8.3899]\n",
      "Training:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 783/788 [00:11<00:00, 70.90it/s, loss=8.3899]\n",
      "Training:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 783/788 [00:11<00:00, 70.90it/s, loss=8.5168]\n",
      "Training:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 783/788 [00:11<00:00, 70.90it/s, loss=8.4595]\n",
      "Training:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 783/788 [00:11<00:00, 70.90it/s, loss=8.3885]\n",
      "Training:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 783/788 [00:11<00:00, 70.90it/s, loss=8.4594]\n",
      "Training:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 783/788 [00:11<00:00, 70.90it/s, loss=8.4306]\n",
      "                                                                        \n",
      "\n",
      "Validating:   0%|          | 0/412 [00:00<?, ?it/s]\n",
      "Validating:   5%|â–Œ         | 21/412 [00:00<00:01, 200.66it/s]\n",
      "Validating:  10%|â–ˆ         | 43/412 [00:00<00:01, 209.08it/s]\n",
      "Validating:  16%|â–ˆâ–Œ        | 66/412 [00:00<00:01, 214.77it/s]\n",
      "Validating:  21%|â–ˆâ–ˆâ–       | 88/412 [00:00<00:01, 215.74it/s]\n",
      "Validating:  27%|â–ˆâ–ˆâ–‹       | 110/412 [00:00<00:01, 211.99it/s]\n",
      "Validating:  32%|â–ˆâ–ˆâ–ˆâ–      | 133/412 [00:00<00:01, 214.96it/s]\n",
      "Validating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 155/412 [00:00<00:01, 211.03it/s]\n",
      "Validating:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 177/412 [00:00<00:01, 213.74it/s]\n",
      "Validating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 199/412 [00:00<00:01, 211.24it/s]\n",
      "Validating:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 221/412 [00:01<00:00, 213.57it/s]\n",
      "Validating:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 244/412 [00:01<00:00, 215.84it/s]\n",
      "Validating:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 266/412 [00:01<00:00, 213.62it/s]\n",
      "Validating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 289/412 [00:01<00:00, 215.15it/s]\n",
      "Validating:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 311/412 [00:01<00:00, 215.64it/s]\n",
      "Validating:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 333/412 [00:01<00:00, 208.47it/s]\n",
      "Validating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 355/412 [00:01<00:00, 211.29it/s]\n",
      "Validating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 377/412 [00:01<00:00, 177.72it/s]\n",
      "Validating:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 399/412 [00:01<00:00, 188.43it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# ============================================================================\n",
    "# 1. é…ç½®åŒºåŸŸ (Configuration)\n",
    "# ============================================================================\n",
    "\n",
    "# è·¯å¾„é…ç½® (è¯·æ ¹æ®ä½ çš„å®é™…ç›®å½•ç»“æ„è°ƒæ•´)\n",
    "# å‡è®¾ Notebook åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹ï¼Œå¦‚æœä¸åœ¨ï¼Œè¯·ä¿®æ”¹ PROJECT_ROOT\n",
    "PROJECT_ROOT = \".\" \n",
    "TRAIN_SCRIPT = os.path.join(PROJECT_ROOT, \"drugreflector_train\", \"train.py\")\n",
    "EXTRACT_SCRIPT = os.path.join(PROJECT_ROOT, \"drugreflector_train\", \"extract_single_fold_metrics.py\")\n",
    "DATA_FILE = os.path.join(PROJECT_ROOT, \"processed_data\", \"training_data_lincs2020_chemfiltered_1201.pkl\")\n",
    "BASE_OUTPUT_DIR = os.path.join(PROJECT_ROOT, \"models\", \"hyperparam_search_1fold_round2_local\")\n",
    "\n",
    "# æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨\n",
    "if not os.path.exists(TRAIN_SCRIPT):\n",
    "    print(f\"è­¦å‘Š: æ‰¾ä¸åˆ°è®­ç»ƒè„šæœ¬: {TRAIN_SCRIPT}\")\n",
    "if not os.path.exists(DATA_FILE):\n",
    "    print(f\"è­¦å‘Š: æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶: {DATA_FILE}\")\n",
    "\n",
    "# è®­ç»ƒé€šç”¨å‚æ•°\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 256\n",
    "DEVICE = \"cuda\"  # Windowsä¸Šé€šå¸¸æ˜¾å¼æŒ‡å®šcudaï¼Œå¦‚æœæ²¡æœ‰GPUåˆ™æ”¹ä¸º \"cpu\"\n",
    "NUM_WORKERS = 0  # Windowsä¸‹å¤šè¿›ç¨‹Dataloaderå¸¸æœ‰é—®é¢˜ï¼Œå»ºè®®è®¾ä¸º0æˆ–è¾ƒå°å€¼(å¦‚2)\n",
    "FOLD_ID = 0\n",
    "FOCAL_GAMMA = 2.0\n",
    "CURRENT_MIN_LR = 1e-6\n",
    "PLOT_DIR = \"training_plots\"\n",
    "\n",
    "# Round 2 ä»»åŠ¡åˆ—è¡¨ (Task_ID, LR, Scheduler, WD, Dropout)\n",
    "# å¯¹åº”ä½ çš„ submit_round2_hyperparams.sh\n",
    "TASKS = [\n",
    "    {\"id\": 0, \"lr\": 0.015, \"scheduler\": \"exponential\", \"wd\": 5e-5, \"dropout\": 0.5},\n",
    "    {\"id\": 1, \"lr\": 0.015, \"scheduler\": \"exponential\", \"wd\": 5e-5, \"dropout\": 0.55},\n",
    "    {\"id\": 2, \"lr\": 0.015, \"scheduler\": \"exponential\", \"wd\": 5e-5, \"dropout\": 0.6},\n",
    "    {\"id\": 3, \"lr\": 0.015, \"scheduler\": \"exponential\", \"wd\": 5e-5, \"dropout\": 0.64},\n",
    "    {\"id\": 4, \"lr\": 0.02,  \"scheduler\": \"exponential\", \"wd\": 5e-5, \"dropout\": 0.6},\n",
    "    {\"id\": 5, \"lr\": 0.02,  \"scheduler\": \"exponential\", \"wd\": 5e-5, \"dropout\": 0.64},\n",
    "    {\"id\": 6, \"lr\": 0.03,  \"scheduler\": \"exponential\", \"wd\": 5e-5, \"dropout\": 0.64},\n",
    "    {\"id\": 7, \"lr\": 0.05,  \"scheduler\": \"exponential\", \"wd\": 5e-5, \"dropout\": 0.64},\n",
    "]\n",
    "\n",
    "# ============================================================================\n",
    "# 2. è¾…åŠ©å‡½æ•°\n",
    "# ============================================================================\n",
    "\n",
    "def get_scheduler_params(scheduler_type):\n",
    "    \"\"\"æ ¹æ®è°ƒåº¦å™¨ç±»å‹è¿”å›è¡°å‡å‚æ•°\"\"\"\n",
    "    if scheduler_type == \"step\":\n",
    "        return 0.1, \"30 40\"\n",
    "    elif scheduler_type == \"exponential\":\n",
    "        return 0.95, None\n",
    "    else:  # cosine\n",
    "        return 0.1, None\n",
    "\n",
    "def run_command(cmd_args, log_file_path):\n",
    "    \"\"\"è¿è¡Œç³»ç»Ÿå‘½ä»¤å¹¶å°†è¾“å‡ºé‡å®šå‘åˆ°æ–‡ä»¶å’Œæ§åˆ¶å°\"\"\"\n",
    "    with open(log_file_path, \"w\", encoding=\"utf-8\") as log_file:\n",
    "        # å¯åŠ¨è¿›ç¨‹\n",
    "        process = subprocess.Popen(\n",
    "            cmd_args,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.STDOUT,\n",
    "            universal_newlines=True,\n",
    "            bufsize=1\n",
    "        )\n",
    "        \n",
    "        # å®æ—¶è¯»å–è¾“å‡º\n",
    "        for line in process.stdout:\n",
    "            print(line, end=\"\")  # è¾“å‡ºåˆ° Notebook\n",
    "            log_file.write(line) # è¾“å‡ºåˆ°æ—¥å¿—æ–‡ä»¶\n",
    "            \n",
    "        process.wait()\n",
    "        return process.returncode\n",
    "\n",
    "# ============================================================================\n",
    "# 3. ä¸»å¾ªç¯ (Training Loop)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"==================================================\")\n",
    "print(f\"å¼€å§‹æ‰§è¡Œ Round 2 è¶…å‚æ•°ä¼˜åŒ– (Windows æœ¬åœ°ç‰ˆ)\")\n",
    "print(f\"æ€»ä»»åŠ¡æ•°: {len(TASKS)}\")\n",
    "print(f\"è¾“å‡ºç›®å½•: {BASE_OUTPUT_DIR}\")\n",
    "print(\"==================================================\\n\")\n",
    "\n",
    "if not os.path.exists(BASE_OUTPUT_DIR):\n",
    "    os.makedirs(BASE_OUTPUT_DIR)\n",
    "\n",
    "for task in TASKS:\n",
    "    task_id = task[\"id\"]\n",
    "    lr = task[\"lr\"]\n",
    "    scheduler = task[\"scheduler\"]\n",
    "    wd = task[\"wd\"]\n",
    "    dropout = task[\"dropout\"]\n",
    "    \n",
    "    # å‡†å¤‡è·¯å¾„å’Œå‚æ•°\n",
    "    config_id = f\"lr{lr}_sch{scheduler}_wd{wd}_drop{dropout}\"\n",
    "    output_dir = os.path.join(BASE_OUTPUT_DIR, f\"config_{config_id}_task{task_id}\")\n",
    "    \n",
    "    # åˆ›å»ºä»»åŠ¡è¾“å‡ºç›®å½•\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # è·å– LR Decay å‚æ•°\n",
    "    lr_decay_rate, lr_decay_epochs = get_scheduler_params(scheduler)\n",
    "    \n",
    "    print(f\"Processing Task {task_id}/{len(TASKS)-1}: {config_id}\")\n",
    "    print(f\"  > Output: {output_dir}\")\n",
    "    \n",
    "    # æ„å»ºè®­ç»ƒå‘½ä»¤\n",
    "    # æ³¨æ„: Windowsä¸‹ subprocess æ¥æ”¶åˆ—è¡¨å½¢å¼çš„å‚æ•°æ›´å®‰å…¨\n",
    "    cmd = [\n",
    "        sys.executable, \"-u\", TRAIN_SCRIPT,\n",
    "        \"--data-file\", DATA_FILE,\n",
    "        \"--output-dir\", output_dir,\n",
    "        \"--fold\", str(FOLD_ID),\n",
    "        \"--epochs\", str(EPOCHS),\n",
    "        \"--batch-size\", str(BATCH_SIZE),\n",
    "        \"--learning-rate\", str(lr),\n",
    "        \"--lr-scheduler\", scheduler,\n",
    "        \"--lr-decay-rate\", str(lr_decay_rate),\n",
    "        \"--min-lr\", str(CURRENT_MIN_LR),\n",
    "        \"--weight-decay\", str(wd),\n",
    "        \"--dropout\", str(dropout),\n",
    "        \"--focal-gamma\", str(FOCAL_GAMMA),\n",
    "        \"--plot-dir\", PLOT_DIR,\n",
    "        \"--device\", DEVICE,\n",
    "        \"--num-workers\", str(NUM_WORKERS),\n",
    "        \"--save-every\", \"10\",\n",
    "        \"--quiet\"\n",
    "    ]\n",
    "    \n",
    "    if lr_decay_epochs:\n",
    "        cmd.extend([\"--lr-decay-epochs\", lr_decay_epochs])\n",
    "    \n",
    "    # è®°å½•å¼€å§‹æ—¶é—´\n",
    "    start_time = time.time()\n",
    "    log_file = os.path.join(output_dir, \"train_log.txt\")\n",
    "    \n",
    "    print(f\"  > å¼€å§‹è®­ç»ƒ... (æ—¥å¿—: {log_file})\")\n",
    "    \n",
    "    # --- æ‰§è¡Œè®­ç»ƒ ---\n",
    "    return_code = run_command(cmd, log_file)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    \n",
    "    # è®°å½•é…ç½®ä¿¡æ¯åˆ° config.txt (æ¨¡ä»¿åŸè„šæœ¬è¡Œä¸º)\n",
    "    with open(os.path.join(output_dir, \"config.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Task ID: {task_id}\\n\")\n",
    "        f.write(f\"Config: {config_id}\\n\")\n",
    "        f.write(f\"Status: {'Success' if return_code == 0 else 'Failed'}\\n\")\n",
    "        f.write(f\"Elapsed: {elapsed:.2f}s\\n\")\n",
    "        f.write(f\"Params: LR={lr}, Drop={dropout}, WD={wd}, Sch={scheduler}\\n\")\n",
    "\n",
    "    if return_code == 0:\n",
    "        print(f\"  âœ“ è®­ç»ƒå®Œæˆ (è€—æ—¶: {elapsed/60:.1f} min)\")\n",
    "        \n",
    "        # --- æå–æŒ‡æ ‡ ---\n",
    "        print(\"  > æå–æŒ‡æ ‡...\")\n",
    "        extract_cmd = [\n",
    "            sys.executable, \"-u\", EXTRACT_SCRIPT,\n",
    "            \"--output-dir\", output_dir,\n",
    "            \"--fold-id\", str(FOLD_ID)\n",
    "        ]\n",
    "        extract_code = run_command(extract_cmd, os.path.join(output_dir, \"extract_log.txt\"))\n",
    "        \n",
    "        if extract_code == 0:\n",
    "            print(\"  âœ“ æŒ‡æ ‡æå–æˆåŠŸ\")\n",
    "        else:\n",
    "            print(\"  âœ— æŒ‡æ ‡æå–å¤±è´¥\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"  âœ— è®­ç»ƒå¤±è´¥ (è¿”å›ç : {return_code})\")\n",
    "        print(\"  è¯·æ£€æŸ¥ä¸Šæ–¹æ—¥å¿—æˆ– train_log.txt\")\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    # ç¨å¾®ä¼‘æ¯ä¸€ä¸‹ï¼Œè®©æ˜¾å¡å–˜å£æ°”\n",
    "    time.sleep(2)\n",
    "\n",
    "print(\"æ‰€æœ‰ä»»åŠ¡æ‰§è¡Œå®Œæ¯•ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd595082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\11234\\AppData\\Local\\Temp\\ipykernel_26428\\3445058657.py\", line 16, in <module>\n",
      "    runpy.run_path(script_path, run_name=\"__main__\")\n",
      "  File \"<frozen runpy>\", line 291, in run_path\n",
      "  File \"<frozen runpy>\", line 98, in _run_module_code\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"E:\\ç§‘ç ”\\Models\\drugreflector\\drugreflector_twotower\\train.py\", line 16, in <module>\n",
      "    from trainer import TwoTowerTrainer\n",
      "  File \"E:\\ç§‘ç ”\\Models\\drugreflector\\drugreflector_twotower\\trainer.py\", line 18, in <module>\n",
      "    from models import TowTowerModel\n",
      "  File \"E:\\ç§‘ç ”\\Models\\drugreflector\\drugreflector_twotower\\models.py\", line 13, in <module>\n",
      "    from chemprop import nn as cnn\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\chemprop\\__init__.py\", line 1, in <module>\n",
      "    from . import data, exceptions, featurizers, models, nn, schedulers, utils\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\chemprop\\models\\__init__.py\", line 1, in <module>\n",
      "    from .model import MPNN\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\chemprop\\models\\model.py\", line 7, in <module>\n",
      "    from lightning import pytorch as pl\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\lightning\\__init__.py\", line 20, in <module>\n",
      "    from lightning.pytorch.callbacks import Callback  # noqa: E402\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\lightning\\pytorch\\__init__.py\", line 27, in <module>\n",
      "    from lightning.pytorch.callbacks import Callback  # noqa: E402\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\__init__.py\", line 14, in <module>\n",
      "    from lightning.pytorch.callbacks.batch_size_finder import BatchSizeFinder\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\batch_size_finder.py\", line 26, in <module>\n",
      "    from lightning.pytorch.callbacks.callback import Callback\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\callback.py\", line 22, in <module>\n",
      "    from lightning.pytorch.utilities.types import STEP_OUTPUT\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\lightning\\pytorch\\utilities\\types.py\", line 36, in <module>\n",
      "    from torchmetrics import Metric\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\torchmetrics\\__init__.py\", line 22, in <module>\n",
      "    from torchmetrics import functional  # noqa: E402\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\torchmetrics\\functional\\__init__.py\", line 14, in <module>\n",
      "    from torchmetrics.functional.audio._deprecated import _permutation_invariant_training as permutation_invariant_training\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\torchmetrics\\functional\\audio\\__init__.py\", line 14, in <module>\n",
      "    from torchmetrics.functional.audio.pit import permutation_invariant_training, pit_permutate\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\torchmetrics\\functional\\audio\\pit.py\", line 22, in <module>\n",
      "    from torchmetrics.utilities import rank_zero_warn\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\torchmetrics\\utilities\\__init__.py\", line 14, in <module>\n",
      "    from torchmetrics.utilities.checks import check_forward_full_state_property\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\torchmetrics\\utilities\\checks.py\", line 25, in <module>\n",
      "    from torchmetrics.metric import Metric\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\torchmetrics\\metric.py\", line 30, in <module>\n",
      "    from torchmetrics.utilities.data import (\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\torchmetrics\\utilities\\data.py\", line 22, in <module>\n",
      "    from torchmetrics.utilities.imports import _TORCH_GREATER_EQUAL_1_12, _XLA_AVAILABLE\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\torchmetrics\\utilities\\imports.py\", line 45, in <module>\n",
      "    _TORCHVISION_GREATER_EQUAL_0_8: Optional[bool] = compare_version(\"torchvision\", operator.ge, \"0.8.0\")\n",
      "                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\lightning_utilities\\core\\imports.py\", line 78, in compare_version\n",
      "    pkg = importlib.import_module(package)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\torchvision\\__init__.py\", line 6, in <module>\n",
      "    from torchvision import _meta_registrations, datasets, io, models, ops, transforms, utils\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\torchvision\\_meta_registrations.py\", line 163, in <module>\n",
      "    @torch._custom_ops.impl_abstract(\"torchvision::nms\")\n",
      "     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\torch\\_custom_ops.py\", line 253, in impl_abstract\n",
      "    return torch.library.impl_abstract(qualname, func, _stacklevel=2)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\torch\\library.py\", line 421, in impl_abstract\n",
      "    source = torch._library.utils.get_source(_stacklevel + 1)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\torch\\_library\\utils.py\", line 39, in get_source\n",
      "    frame = inspect.getframeinfo(sys._getframe(stacklevel))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\inspect.py\", line 1688, in getframeinfo\n",
      "    lines, lnum = findsource(frame)\n",
      "                  ^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\inspect.py\", line 1071, in findsource\n",
      "    module = getmodule(object, file)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\inspect.py\", line 994, in getmodule\n",
      "    f = getabsfile(module)\n",
      "        ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\inspect.py\", line 963, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\inspect.py\", line 939, in getsourcefile\n",
      "    if any(filename.endswith(s) for s in all_bytecode_suffixes):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\inspect.py\", line 939, in <genexpr>\n",
      "    if any(filename.endswith(s) for s in all_bytecode_suffixes):\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'WindowsPath' object has no attribute 'endswith'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2168, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1457, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1348, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1195, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1085, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1153, in get_records\n",
      "    mod = inspect.getmodule(cf.tb_frame)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\inspect.py\", line 980, in getmodule\n",
      "    file = getabsfile(object, _filename)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\inspect.py\", line 963, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\inspect.py\", line 948, in getsourcefile\n",
      "    module = getmodule(object, filename)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\inspect.py\", line 994, in getmodule\n",
      "    f = getabsfile(module)\n",
      "        ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\inspect.py\", line 963, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\inspect.py\", line 939, in getsourcefile\n",
      "    if any(filename.endswith(s) for s in all_bytecode_suffixes):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\inspect.py\", line 939, in <genexpr>\n",
      "    if any(filename.endswith(s) for s in all_bytecode_suffixes):\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'WindowsPath' object has no attribute 'endswith'\n"
     ]
    }
   ],
   "source": [
    "import sys, runpy\n",
    "from pathlib import Path\n",
    "\n",
    "argv_backup = sys.argv\n",
    "sys.argv = [\n",
    "    \"train.py\",\n",
    "    \"--data-file\", \"processed_data/training_data_lincs2020_final.pkl\",\n",
    "    \"--compound-info\", \"datasets/LINCS2020/compoundinfo.txt\",\n",
    "    \"--fold\", \"0\",\n",
    "]\n",
    "\n",
    "# è‡ªåŠ¨åŠ å…¥è„šæœ¬æ‰€åœ¨ç›®å½•åˆ° sys.path\n",
    "script_path = Path(\"drugreflector_twotower/train.py\").resolve()\n",
    "sys.path.append(str(script_path.parent))\n",
    "\n",
    "runpy.run_path(script_path, run_name=\"__main__\")\n",
    "\n",
    "sys.argv = argv_backup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9416de63",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3033893754.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    $python train.py \\\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "$python train.py \\\n",
    "    --data-file processed_data/training_data_lincs2020_final.pkl \\\n",
    "    --compound-info datasets/LINCS2020/compoundinfo.txt \\\n",
    "    --fold 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091039e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mchemprop\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn \u001b[38;5;28;01mas\u001b[39;00m cnn\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\chemprop\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m data, exceptions, featurizers, models, nn, schedulers, utils\n\u001b[0;32m      3\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeaturizers\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutils\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexceptions\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mschedulers\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      5\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.1.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\chemprop\\models\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MPNN\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulti\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MulticomponentMPNN\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_model, save_model\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\chemprop\\models\\model.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Iterable\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pytorch \u001b[38;5;28;01mas\u001b[39;00m pl\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tensor, nn, optim\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\lightning\\__init__.py:20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Fabric  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mseed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m seed_everything  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Callback  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LightningDataModule, LightningModule  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Trainer  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\lightning\\pytorch\\__init__.py:27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mseed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m seed_everything  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwarnings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m disable_possible_user_warnings  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Callback  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LightningDataModule, LightningModule  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Trainer  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\__init__.py:14\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright The Lightning AI team.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatch_size_finder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BatchSizeFinder\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallback\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Callback\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpoint\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Checkpoint\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\batch_size_finder.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping_extensions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m override\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpl\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallback\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Callback\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtuner\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatch_size_scaling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _scale_batch_size\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MisconfigurationException, _TunerExitException\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\callback.py:22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Optimizer\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpl\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m STEP_OUTPUT\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mCallback\u001b[39;00m:\n\u001b[0;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Abstract base class used to build new callbacks.\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03m    Subclass this class and override any of the relevant hooks\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\lightning\\pytorch\\utilities\\types.py:36\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Optimizer\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlr_scheduler\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LRScheduler, ReduceLROnPlateau\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Metric\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping_extensions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NotRequired, Required\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ProcessGroup\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\torchmetrics\\__init__.py:22\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(PIL, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPILLOW_VERSION\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     20\u001b[0m         PIL\u001b[38;5;241m.\u001b[39mPILLOW_VERSION \u001b[38;5;241m=\u001b[39m PIL\u001b[38;5;241m.\u001b[39m__version__\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m functional  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maggregation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     CatMetric,\n\u001b[0;32m     25\u001b[0m     MaxMetric,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m     SumMetric,\n\u001b[0;32m     31\u001b[0m )\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_deprecated\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _PermutationInvariantTraining \u001b[38;5;28;01mas\u001b[39;00m PermutationInvariantTraining  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\torchmetrics\\functional\\__init__.py:14\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright The Lightning team.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_deprecated\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _permutation_invariant_training \u001b[38;5;28;01mas\u001b[39;00m permutation_invariant_training\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_deprecated\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _pit_permutate \u001b[38;5;28;01mas\u001b[39;00m pit_permutate\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_deprecated\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m     _scale_invariant_signal_distortion_ratio \u001b[38;5;28;01mas\u001b[39;00m scale_invariant_signal_distortion_ratio,\n\u001b[0;32m     18\u001b[0m )\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\torchmetrics\\functional\\audio\\__init__.py:14\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright The Lightning team.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpit\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m permutation_invariant_training, pit_permutate\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m     scale_invariant_signal_distortion_ratio,\n\u001b[0;32m     17\u001b[0m     signal_distortion_ratio,\n\u001b[0;32m     18\u001b[0m     source_aggregated_signal_distortion_ratio,\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msnr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     21\u001b[0m     complex_scale_invariant_signal_noise_ratio,\n\u001b[0;32m     22\u001b[0m     scale_invariant_signal_noise_ratio,\n\u001b[0;32m     23\u001b[0m     signal_noise_ratio,\n\u001b[0;32m     24\u001b[0m )\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\torchmetrics\\functional\\audio\\pit.py:22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping_extensions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Literal\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rank_zero_warn\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimports\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _SCIPY_AVAILABLE\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# _ps_dict: cache of permutations\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# it's necessary to cache it, otherwise it will consume a large amount of time\u001b[39;00m\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\torchmetrics\\utilities\\__init__.py:14\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright The Lightning team.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchecks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m check_forward_full_state_property\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m     dim_zero_cat,\n\u001b[0;32m     17\u001b[0m     dim_zero_max,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m     dim_zero_sum,\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m class_reduce, reduce\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\torchmetrics\\utilities\\checks.py:25\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetric\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Metric\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m select_topk, to_onehot\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menums\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataType\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\torchmetrics\\metric.py:30\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Module\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     31\u001b[0m     _flatten,\n\u001b[0;32m     32\u001b[0m     _squeeze_if_scalar,\n\u001b[0;32m     33\u001b[0m     dim_zero_cat,\n\u001b[0;32m     34\u001b[0m     dim_zero_max,\n\u001b[0;32m     35\u001b[0m     dim_zero_mean,\n\u001b[0;32m     36\u001b[0m     dim_zero_min,\n\u001b[0;32m     37\u001b[0m     dim_zero_sum,\n\u001b[0;32m     38\u001b[0m )\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gather_all_tensors\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TorchMetricsUserError\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\torchmetrics\\utilities\\data.py:22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TorchMetricsUserWarning\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimports\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _TORCH_GREATER_EQUAL_1_12, _XLA_AVAILABLE\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprints\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rank_zero_warn\n\u001b[0;32m     25\u001b[0m METRIC_EPS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-6\u001b[39m\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\torchmetrics\\utilities\\imports.py:45\u001b[0m\n\u001b[0;32m     43\u001b[0m _PYCOCOTOOLS_AVAILABLE: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m package_available(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpycocotools\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     44\u001b[0m _TORCHVISION_AVAILABLE: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m package_available(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchvision\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 45\u001b[0m _TORCHVISION_GREATER_EQUAL_0_8: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mcompare_version\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorchvision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mge\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0.8.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m _TORCHVISION_GREATER_EQUAL_0_13: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m compare_version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchvision\u001b[39m\u001b[38;5;124m\"\u001b[39m, operator\u001b[38;5;241m.\u001b[39mge, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.13.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     47\u001b[0m _TQDM_AVAILABLE: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m package_available(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtqdm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\lightning_utilities\\core\\imports.py:78\u001b[0m, in \u001b[0;36mcompare_version\u001b[1;34m(package, op, version, use_base_version)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compare package version with some requirements.\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m>>> compare_version(\"torch\", operator.ge, \"0.1\")\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     75\u001b[0m \n\u001b[0;32m     76\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 78\u001b[0m     pkg \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\torchvision\\__init__.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodulefinder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Module\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\torchvision\\_meta_registrations.py:25\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fn\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;129;43m@register_meta\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mroi_align\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;21;43mmeta_roi_align\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrois\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspatial_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooled_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooled_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maligned\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrois\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrois must have shape as Tensor[K, 5]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrois\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\envs\\chemprop\\Lib\\site-packages\\torchvision\\_meta_registrations.py:18\u001b[0m, in \u001b[0;36mregister_meta.<locals>.wrapper\u001b[1;34m(fn)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(fn):\n\u001b[1;32m---> 18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextension\u001b[49m\u001b[38;5;241m.\u001b[39m_has_ops():\n\u001b[0;32m     19\u001b[0m         get_meta_lib()\u001b[38;5;241m.\u001b[39mimpl(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mtorchvision, op_name), overload_name), fn)\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn\n",
      "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "from chemprop import nn as cnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48f8775f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e15b45e",
   "metadata": {},
   "source": [
    "#!/bin/bash\n",
    "#SBATCH -J twotower-train\n",
    "#SBATCH -p gpu_4l\n",
    "#SBATCH -N 1 \n",
    "#SBATCH -o ../err_out/twotower_%j.out\n",
    "#SBATCH -e ../err_out/twotower_%j.err\n",
    "#SBATCH --no-requeue\n",
    "#SBATCH -A tangc_g1\n",
    "#SBATCH --qos=tangcg4c\n",
    "#SBATCH --gres=gpu:1\n",
    "#SBATCH --overcommit\n",
    "#SBATCH --mincpus=30\n",
    "\n",
    "source /appsnew/source/Tensorflow2.11_Keras2.11.0_keras-nightly2.13.0_torch1.13.1_py3.9.4_cuda11.4.4.sh\n",
    "unset LD_LIBRARY_PATH\n",
    "\n",
    "python -u ../drugreflector_twotower/train.py \\\n",
    "    --data-file ../datasets/training_data_lincs2020_final.pkl \\\n",
    "    --compound-info ../datasets/LINCS2020/compoundinfo_beta.txt \\\n",
    "    --output-dir ../models/twotower_ensemble \\\n",
    "    --all-folds \\\n",
    "    --chem-hidden-dim 512 \\\n",
    "    --fusion-method concat \\\n",
    "    --mpnn-depth 3 \\\n",
    "    --epochs 50 \\\n",
    "    --batch-size 256 \\\n",
    "    --learning-rate 0.0139 \\\n",
    "    --use-3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd0ceb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemprop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
